{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#CSS = \"\"\"\n",
    "#.output {\n",
    "#    flex-direction: row;\n",
    "#}\n",
    "#\"\"\"\n",
    "\n",
    "#HTML('<style>{}</style>'.format(CSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chocolate_mousse': 5, 'tacos': 24, 'garlic_bread': 11, 'frozen_yogurt': 10, 'sushi': 23, 'chicken_wings': 3, 'tiramisu': 25, 'deviled_eggs': 6, 'hamburger': 12, 'waffles': 26, 'french_toast': 8, 'cheesecake': 1, 'pizza': 20, 'fried_rice': 9, 'macarons': 16, 'nachos': 17, 'macaroni_and_cheese': 15, 'chocolate_cake': 4, 'steak': 22, 'hummus': 13, 'chicken_curry': 2, 'pancakes': 19, 'apple_pie': 0, 'lasagna': 14, 'onion_rings': 18, 'dumplings': 7, 'sashimi': 21}\n"
     ]
    }
   ],
   "source": [
    "number_of_iteration=10\n",
    "cwd=os.getcwd()\n",
    "visual_m_path = os.path.join(cwd,'data/imgs_mean_feature_vectors.pkl')\n",
    "textual_m_path = os.path.join(cwd,'data/docs_extracted_features.pkl')\n",
    "models_folder_name = os.path.join(cwd,'models',str(number_of_iteration))\n",
    "path_to_save_test_results=os.path.join(models_folder_name, 'test_results.pkl')\n",
    "model_checkpoint_path = os.path.join(cwd,models_folder_name,'gmu.ckpt')\n",
    "models_folder_name_test = os.path.join(cwd,os.pardir,'text_feature_extraction','models','test',str(number_of_iteration))\n",
    "path_to_preprocessed_texts_test=os.path.join(models_folder_name_test,'recipes_test_dataset.pkl')\n",
    "\n",
    "\n",
    "df_visual_m = pd.read_pickle(visual_m_path)\n",
    "df_textual_m = pd.read_pickle(textual_m_path)\n",
    "\n",
    "number_of_recipes=len(df_visual_m)\n",
    "unique_labels=sorted(set(df_visual_m.mean_vector_labels.values))\n",
    "number_of_classes=len(unique_labels)\n",
    "possible_class_indices=list(range(0,number_of_classes))\n",
    "labels2class_indices=dict(zip(unique_labels,possible_class_indices))\n",
    "print(labels2class_indices)\n",
    "\n",
    "#np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value: visual features  0.5558735478366387\n",
      "Max value in visual feature vectors after subraction of mean value:  22.56199106092801\n",
      "Mean value: textual features  0.013499930763029529\n",
      "Max value in textual feature vectors after subraction of mean value:  3.519134565925813\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_embeddings</th>\n",
       "      <th>labels</th>\n",
       "      <th>text_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.11329869829923188, -0.6441225998849662, -0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.07474968697600654, -0.28306451085312634, 0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.10599670561622081, -0.5485997736729018, 0....</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>11.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.5233624146389693, -0.32503846481726917, 0....</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.07889076414967888, -0.2116037304174958, 0....</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>13.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.07946860561194324, -0.2566648633469014, -0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>14.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.1800028915393702, -0.6000170525947526, 0.1...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>15.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.28211158437034184, -0.503643364081505, 0.2...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-0.41500734646954496, -0.40415873356834714, 0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[-0.38648066177147605, -0.48593944596184546, 0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[-0.11808992227722907, -0.13458911551453934, 0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[-0.22233212234939284, -0.1979024863085554, -0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[-0.08471857641348418, -0.559017298364654, -0....</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[-0.027834884908365817, -0.4463838338459294, 0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[-0.011526498846268104, -0.4014876533002968, 0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.07568393993445587, 0.18173006428900282, -0....</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.11993878107278198, 0.13665470402595217, -0....</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.36336042299016497, -0.26849277411003303, -0...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>11.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.41915961905088583, 0.30774946810341, 0.0777...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[-0.03108438250694706, 0.24778843820546667, -0...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0018196716644160666, -0.09403709886171152, ...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.226157589689446, -0.21843183432901234, -0.3...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[-0.37221488045095963, 0.15941117175552535, -0...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[-0.3998149931080913, 0.36037593488667613, -0....</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[-0.1703639576813445, -0.09615897017645816, -0...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[-0.4037997305160365, -0.012025206058038617, -...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[-0.24562844964158864, -0.22083867574920935, 0...</td>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.05659051096808063, -0.28589183773174304, -0...</td>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[-0.47327678027169323, -0.011298772057821004, ...</td>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[-0.25067830622049087, -0.24191728353487582, 0...</td>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>[0.4708458284532324, 0.7217121344166731, -0.70...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>[0.12419525208469703, 0.18482393348853915, 0.3...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>[-0.05141209618191453, 0.55971178687658, 0.236...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>[0.10458004412553981, 0.5648145217121964, -0.1...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>[0.3630706935028643, 0.565899796272484, -0.326...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>[0.34942447868798676, 0.14594550857952454, -0....</td>\n",
       "      <td>tacos</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>[-0.35736008751753723, 0.5371146822211336, -0....</td>\n",
       "      <td>tacos</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>[0.7463816534651476, 0.2869543568779235, -0.10...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>[-0.21550115603384534, 0.45582593070031935, -0...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>[-0.0025939224634417357, 0.37468226410808103, ...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>[0.3207366863736894, 0.5524853514976054, -0.48...</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>[0.4244060160817136, 0.45012659634051966, -0.5...</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>[0.2937218252484289, 0.52740398501471, -0.5844...</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>[-0.0139463671688339, 0.6422751719935768, -0.6...</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>[0.09960700667966998, 0.3342375113885559, -0.2...</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>[-0.04262138664029187, 0.5035471854634082, -0....</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>[0.3768222583359232, 0.4499174545359371, -0.67...</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>[0.43418761283413504, 0.45749500018450967, -0....</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>[0.2918933418782059, 0.6923211730742701, -0.20...</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>[0.33254293432761617, 0.08518671482203785, -0....</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>[-0.4934857931002106, 0.11483502153039708, -0....</td>\n",
       "      <td>waffles</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>[-0.09611500094057411, -0.20871532762801706, -...</td>\n",
       "      <td>waffles</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>[0.003307216144157723, 0.005713223931523985, -...</td>\n",
       "      <td>waffles</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>[0.27163321227237797, -0.28940085713958213, -0...</td>\n",
       "      <td>waffles</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>[0.29004043491855896, -0.0040635356498566234, ...</td>\n",
       "      <td>waffles</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>[0.013618091409093954, -0.12388211539501581, -...</td>\n",
       "      <td>waffles</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>[-0.47174940825568706, -0.13215411582983644, 0...</td>\n",
       "      <td>waffles</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>[-0.5636584578472996, -0.3188487955657733, -0....</td>\n",
       "      <td>waffles</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>[-0.09456783768722951, 0.02148465835402908, 0....</td>\n",
       "      <td>waffles</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>[0.053729910759766616, 0.13244119448045463, 0....</td>\n",
       "      <td>waffles</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        doc_embeddings         labels  \\\n",
       "0    [-0.11329869829923188, -0.6441225998849662, -0...      apple_pie   \n",
       "1    [-0.07474968697600654, -0.28306451085312634, 0...      apple_pie   \n",
       "2    [-0.10599670561622081, -0.5485997736729018, 0....      apple_pie   \n",
       "3    [-0.5233624146389693, -0.32503846481726917, 0....      apple_pie   \n",
       "4    [-0.07889076414967888, -0.2116037304174958, 0....      apple_pie   \n",
       "5    [-0.07946860561194324, -0.2566648633469014, -0...      apple_pie   \n",
       "6    [-0.1800028915393702, -0.6000170525947526, 0.1...      apple_pie   \n",
       "7    [-0.28211158437034184, -0.503643364081505, 0.2...      apple_pie   \n",
       "8    [-0.41500734646954496, -0.40415873356834714, 0...      apple_pie   \n",
       "9    [-0.38648066177147605, -0.48593944596184546, 0...      apple_pie   \n",
       "10   [-0.11808992227722907, -0.13458911551453934, 0...      apple_pie   \n",
       "11   [-0.22233212234939284, -0.1979024863085554, -0...      apple_pie   \n",
       "12   [-0.08471857641348418, -0.559017298364654, -0....      apple_pie   \n",
       "13   [-0.027834884908365817, -0.4463838338459294, 0...      apple_pie   \n",
       "14   [-0.011526498846268104, -0.4014876533002968, 0...      apple_pie   \n",
       "15   [0.07568393993445587, 0.18173006428900282, -0....     cheesecake   \n",
       "16   [0.11993878107278198, 0.13665470402595217, -0....     cheesecake   \n",
       "17   [0.36336042299016497, -0.26849277411003303, -0...     cheesecake   \n",
       "18   [0.41915961905088583, 0.30774946810341, 0.0777...     cheesecake   \n",
       "19   [-0.03108438250694706, 0.24778843820546667, -0...     cheesecake   \n",
       "20   [0.0018196716644160666, -0.09403709886171152, ...     cheesecake   \n",
       "21   [0.226157589689446, -0.21843183432901234, -0.3...     cheesecake   \n",
       "22   [-0.37221488045095963, 0.15941117175552535, -0...     cheesecake   \n",
       "23   [-0.3998149931080913, 0.36037593488667613, -0....     cheesecake   \n",
       "24   [-0.1703639576813445, -0.09615897017645816, -0...     cheesecake   \n",
       "25   [-0.4037997305160365, -0.012025206058038617, -...     cheesecake   \n",
       "26   [-0.24562844964158864, -0.22083867574920935, 0...  chicken_curry   \n",
       "27   [0.05659051096808063, -0.28589183773174304, -0...  chicken_curry   \n",
       "28   [-0.47327678027169323, -0.011298772057821004, ...  chicken_curry   \n",
       "29   [-0.25067830622049087, -0.24191728353487582, 0...  chicken_curry   \n",
       "..                                                 ...            ...   \n",
       "286  [0.4708458284532324, 0.7217121344166731, -0.70...          tacos   \n",
       "287  [0.12419525208469703, 0.18482393348853915, 0.3...          tacos   \n",
       "288  [-0.05141209618191453, 0.55971178687658, 0.236...          tacos   \n",
       "289  [0.10458004412553981, 0.5648145217121964, -0.1...          tacos   \n",
       "290  [0.3630706935028643, 0.565899796272484, -0.326...          tacos   \n",
       "291  [0.34942447868798676, 0.14594550857952454, -0....          tacos   \n",
       "292  [-0.35736008751753723, 0.5371146822211336, -0....          tacos   \n",
       "293  [0.7463816534651476, 0.2869543568779235, -0.10...          tacos   \n",
       "294  [-0.21550115603384534, 0.45582593070031935, -0...          tacos   \n",
       "295  [-0.0025939224634417357, 0.37468226410808103, ...          tacos   \n",
       "296  [0.3207366863736894, 0.5524853514976054, -0.48...       tiramisu   \n",
       "297  [0.4244060160817136, 0.45012659634051966, -0.5...       tiramisu   \n",
       "298  [0.2937218252484289, 0.52740398501471, -0.5844...       tiramisu   \n",
       "299  [-0.0139463671688339, 0.6422751719935768, -0.6...       tiramisu   \n",
       "300  [0.09960700667966998, 0.3342375113885559, -0.2...       tiramisu   \n",
       "301  [-0.04262138664029187, 0.5035471854634082, -0....       tiramisu   \n",
       "302  [0.3768222583359232, 0.4499174545359371, -0.67...       tiramisu   \n",
       "303  [0.43418761283413504, 0.45749500018450967, -0....       tiramisu   \n",
       "304  [0.2918933418782059, 0.6923211730742701, -0.20...       tiramisu   \n",
       "305  [0.33254293432761617, 0.08518671482203785, -0....       tiramisu   \n",
       "306  [-0.4934857931002106, 0.11483502153039708, -0....        waffles   \n",
       "307  [-0.09611500094057411, -0.20871532762801706, -...        waffles   \n",
       "308  [0.003307216144157723, 0.005713223931523985, -...        waffles   \n",
       "309  [0.27163321227237797, -0.28940085713958213, -0...        waffles   \n",
       "310  [0.29004043491855896, -0.0040635356498566234, ...        waffles   \n",
       "311  [0.013618091409093954, -0.12388211539501581, -...        waffles   \n",
       "312  [-0.47174940825568706, -0.13215411582983644, 0...        waffles   \n",
       "313  [-0.5636584578472996, -0.3188487955657733, -0....        waffles   \n",
       "314  [-0.09456783768722951, 0.02148465835402908, 0....        waffles   \n",
       "315  [0.053729910759766616, 0.13244119448045463, 0....        waffles   \n",
       "\n",
       "    text_names  \n",
       "0        1.txt  \n",
       "1       10.txt  \n",
       "2       11.txt  \n",
       "3       12.txt  \n",
       "4       13.txt  \n",
       "5       14.txt  \n",
       "6       15.txt  \n",
       "7        2.txt  \n",
       "8        3.txt  \n",
       "9        4.txt  \n",
       "10       5.txt  \n",
       "11       6.txt  \n",
       "12       7.txt  \n",
       "13       8.txt  \n",
       "14       9.txt  \n",
       "15       1.txt  \n",
       "16      10.txt  \n",
       "17      11.txt  \n",
       "18       2.txt  \n",
       "19       3.txt  \n",
       "20       4.txt  \n",
       "21       5.txt  \n",
       "22       6.txt  \n",
       "23       7.txt  \n",
       "24       8.txt  \n",
       "25       9.txt  \n",
       "26       1.txt  \n",
       "27       2.txt  \n",
       "28       3.txt  \n",
       "29       4.txt  \n",
       "..         ...  \n",
       "286      1.txt  \n",
       "287     10.txt  \n",
       "288      2.txt  \n",
       "289      3.txt  \n",
       "290      4.txt  \n",
       "291      5.txt  \n",
       "292      6.txt  \n",
       "293      7.txt  \n",
       "294      8.txt  \n",
       "295      9.txt  \n",
       "296      1.txt  \n",
       "297     10.txt  \n",
       "298      2.txt  \n",
       "299      3.txt  \n",
       "300      4.txt  \n",
       "301      5.txt  \n",
       "302      6.txt  \n",
       "303      7.txt  \n",
       "304      8.txt  \n",
       "305      9.txt  \n",
       "306      1.txt  \n",
       "307     10.txt  \n",
       "308      2.txt  \n",
       "309      3.txt  \n",
       "310      4.txt  \n",
       "311      5.txt  \n",
       "312      6.txt  \n",
       "313      7.txt  \n",
       "314      8.txt  \n",
       "315      9.txt  \n",
       "\n",
       "[316 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_normalized_features_visual=df_visual_m.mean_feature_vectors.tolist()\n",
    "mean_visual=np.mean(not_normalized_features_visual)\n",
    "print(\"Mean value: visual features \", mean_visual)\n",
    "subtracted_mean_visual= (not_normalized_features_visual-mean_visual)\n",
    "max_visual=np.max(subtracted_mean_visual)\n",
    "print(\"Max value in visual feature vectors after subraction of mean value: \", max_visual)\n",
    "normalized_features_visual = (subtracted_mean_visual/max_visual).tolist()\n",
    "df_visual_m.mean_feature_vectors=normalized_features_visual\n",
    "\n",
    "not_normalized_features_textual=df_textual_m.doc_embeddings.tolist()\n",
    "mean_textual=np.mean(not_normalized_features_textual)\n",
    "print(\"Mean value: textual features \", mean_textual)\n",
    "subtracted_mean_textual= (not_normalized_features_textual-mean_textual)\n",
    "max_textual=np.max(subtracted_mean_textual)\n",
    "print(\"Max value in textual feature vectors after subraction of mean value: \", max_textual)\n",
    "normalized_features_textual = (subtracted_mean_textual/max_textual).tolist()\n",
    "df_textual_m.doc_embeddings=normalized_features_textual\n",
    "\n",
    "\n",
    "df_textual_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples:  [1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 153, 154, 155, 156, 158, 159, 160, 161, 162, 164, 165, 167, 168, 169, 170, 171, 173, 174, 175, 176, 178, 179, 180, 181, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 232, 234, 235, 236, 237, 238, 240, 241, 243, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 268, 269, 270, 271, 272, 273, 275, 276, 278, 279, 280, 281, 282, 283, 285, 286, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314]\n",
      "test samples:  [0, 4, 8, 20, 22, 28, 38, 39, 47, 54, 60, 67, 73, 85, 93, 99, 105, 111, 123, 125, 133, 138, 147, 152, 157, 163, 166, 172, 177, 182, 190, 197, 202, 210, 217, 229, 233, 239, 242, 244, 246, 256, 267, 274, 277, 284, 287, 293, 302, 303, 313, 315]\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed_texts_test = pd.read_pickle(path_to_preprocessed_texts_test)\n",
    "test_samples=df_preprocessed_texts_test.index.values.tolist()\n",
    "\n",
    "#test_samples=[3,5,6,\n",
    "#              11,17,18,\n",
    "#              23,26,33,34,\n",
    "#              38,39,44\n",
    "#             ]\n",
    "all_samples=set(range(0,number_of_recipes))\n",
    "train_samples=list(all_samples.difference(test_samples))\n",
    "print(\"train samples: \", train_samples)\n",
    "print(\"test samples: \", test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 2, 3, 3, 3, 4, 4, 5, 6, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 13, 13, 14, 14, 15, 15, 15, 16, 17, 17, 18, 19, 20, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 25, 25, 26, 26]\n",
      "(264, 202)\n",
      "(264, 27)\n"
     ]
    }
   ],
   "source": [
    "df_visual_m_train=df_visual_m.iloc[train_samples]\n",
    "df_visual_m_test=df_visual_m.iloc[test_samples]\n",
    "df_textual_m_train=df_textual_m.iloc[train_samples]\n",
    "df_textual_m_test=df_textual_m.iloc[test_samples]\n",
    "\n",
    "visual_m_train_inputs=list(df_visual_m_train.mean_feature_vectors.values)\n",
    "visual_m_test_inputs=list(df_visual_m_test.mean_feature_vectors.values)\n",
    "\n",
    "textual_m_train_inputs=list(df_textual_m_train.doc_embeddings.values)\n",
    "textual_m_test_inputs=list(df_textual_m_test.doc_embeddings.values)\n",
    "\n",
    "train_correct_class_ids=[labels2class_indices[l] for l in df_visual_m_train.mean_vector_labels]\n",
    "test_correct_class_ids=[labels2class_indices[l] for l in df_visual_m_test.mean_vector_labels]\n",
    "\n",
    "number_of_training_samples=len(visual_m_train_inputs)\n",
    "number_of_test_samples=len(visual_m_test_inputs)\n",
    "len_of_visual_features_vec=len(visual_m_train_inputs[0])\n",
    "len_of_textual_features_vec=len(textual_m_train_inputs[0])\n",
    "\n",
    "print(test_correct_class_ids)\n",
    "print(np.shape(visual_m_train_inputs))\n",
    "print(np.shape(textual_m_train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "learning_rate=0.001\n",
    "hidden_state_dim = 27\n",
    "#z_dim=1\n",
    "number_of_training_iterations=20000\n",
    "print_valid_every=20\n",
    "num_repeat_training=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_embeddings</th>\n",
       "      <th>labels</th>\n",
       "      <th>text_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.11329869829923188, -0.6441225998849662, -0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.07889076414967888, -0.2116037304174958, 0....</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>13.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-0.41500734646954496, -0.40415873356834714, 0...</td>\n",
       "      <td>apple_pie</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0018196716644160666, -0.09403709886171152, ...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[-0.37221488045095963, 0.15941117175552535, -0...</td>\n",
       "      <td>cheesecake</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[-0.47327678027169323, -0.011298772057821004, ...</td>\n",
       "      <td>chicken_curry</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.06220996901184445, 0.3393704623708661, 0.49...</td>\n",
       "      <td>chicken_wings</td>\n",
       "      <td>13.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.23359608008893995, 0.1726547022163415, 0.32...</td>\n",
       "      <td>chicken_wings</td>\n",
       "      <td>14.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.2844618296471188, 0.3743878599254398, 0.252...</td>\n",
       "      <td>chicken_wings</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>[0.049437621588535006, 0.3873440151839275, 0.3...</td>\n",
       "      <td>chocolate_cake</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[0.005246176771439245, 0.4336387087522666, -0....</td>\n",
       "      <td>chocolate_cake</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>[0.20052023933246357, -0.007778699472581977, -...</td>\n",
       "      <td>chocolate_mousse</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>[0.12761792512715212, -0.08306458959668783, -0...</td>\n",
       "      <td>deviled_eggs</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>[-0.3890306742051276, -0.28388236235279246, 0....</td>\n",
       "      <td>dumplings</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[0.4887286044815729, -0.44843270538923524, 0.1...</td>\n",
       "      <td>french_toast</td>\n",
       "      <td>14.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[0.2865353280274722, -0.2075294793687004, -0.1...</td>\n",
       "      <td>french_toast</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[-0.32691051092812917, 0.025105538055489807, -...</td>\n",
       "      <td>fried_rice</td>\n",
       "      <td>12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>[-0.08713123579690392, 0.2930824082248947, 0.2...</td>\n",
       "      <td>fried_rice</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[0.01814603032105441, 0.30488299911996736, -0....</td>\n",
       "      <td>frozen_yogurt</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[0.17409272996451705, 0.3666403653890165, -0.1...</td>\n",
       "      <td>frozen_yogurt</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>[-0.32694425003363037, -0.05229618946954002, 0...</td>\n",
       "      <td>garlic_bread</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>[0.034681060772145944, -0.24339738354111606, 0...</td>\n",
       "      <td>garlic_bread</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>[-0.15734423672860687, -0.13266970423875263, -...</td>\n",
       "      <td>hamburger</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>[0.5152736925997148, 0.07830772278678728, 0.59...</td>\n",
       "      <td>hummus</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>[-0.039032893177338386, -0.04677780900953855, ...</td>\n",
       "      <td>hummus</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>[-0.2961405144602718, 0.22512314397233563, 0.2...</td>\n",
       "      <td>lasagna</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>[-0.005625945501844912, -0.238267633708876, -0...</td>\n",
       "      <td>lasagna</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>[0.25772516370652526, 0.009489941631141365, 0....</td>\n",
       "      <td>macaroni_and_cheese</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>[0.21931051599973606, 0.1391449955328983, 0.06...</td>\n",
       "      <td>macaroni_and_cheese</td>\n",
       "      <td>15.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>[0.5184439490310835, -0.2885368514920793, 0.18...</td>\n",
       "      <td>macaroni_and_cheese</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>[0.003029945101028819, -0.15117827969985273, -...</td>\n",
       "      <td>macarons</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[0.13549211068222441, 0.18648505937811097, 0.0...</td>\n",
       "      <td>nachos</td>\n",
       "      <td>12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>[0.09446753483610576, 0.2261619087014454, 0.06...</td>\n",
       "      <td>nachos</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>[0.058802332059202954, 0.03911879902394547, -0...</td>\n",
       "      <td>onion_rings</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>[0.3494005293430176, 0.15849765837439808, 0.23...</td>\n",
       "      <td>pancakes</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>[-0.35584416511765493, -0.6137784749375513, -0...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>[-0.7825078770473083, -0.244264183843442, -0.5...</td>\n",
       "      <td>sashimi</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[-0.00707018513783637, -0.566972240975464, -0....</td>\n",
       "      <td>sashimi</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>[-0.22439591565572406, -0.6087644238149892, -0...</td>\n",
       "      <td>steak</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>[-0.41566566552206474, -0.41304668439483855, -...</td>\n",
       "      <td>steak</td>\n",
       "      <td>12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>[-0.17589010812861938, -0.07039237126823106, -...</td>\n",
       "      <td>steak</td>\n",
       "      <td>14.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>[-0.5959493224072112, -0.4330175926324245, -0....</td>\n",
       "      <td>steak</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>[0.16066192375594665, -0.20259633773759364, -0...</td>\n",
       "      <td>sushi</td>\n",
       "      <td>13.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>[0.27343369828246367, -0.17846950678116877, -0...</td>\n",
       "      <td>sushi</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>[0.338076046005996, -0.3870151352720759, -0.41...</td>\n",
       "      <td>sushi</td>\n",
       "      <td>22.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>[0.08316067476177193, -0.19134558247599706, -0...</td>\n",
       "      <td>sushi</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>[0.12419525208469703, 0.18482393348853915, 0.3...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>[0.7463816534651476, 0.2869543568779235, -0.10...</td>\n",
       "      <td>tacos</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>[0.3768222583359232, 0.4499174545359371, -0.67...</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>[0.43418761283413504, 0.45749500018450967, -0....</td>\n",
       "      <td>tiramisu</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>[-0.5636584578472996, -0.3188487955657733, -0....</td>\n",
       "      <td>waffles</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>[0.053729910759766616, 0.13244119448045463, 0....</td>\n",
       "      <td>waffles</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        doc_embeddings               labels  \\\n",
       "0    [-0.11329869829923188, -0.6441225998849662, -0...            apple_pie   \n",
       "4    [-0.07889076414967888, -0.2116037304174958, 0....            apple_pie   \n",
       "8    [-0.41500734646954496, -0.40415873356834714, 0...            apple_pie   \n",
       "20   [0.0018196716644160666, -0.09403709886171152, ...           cheesecake   \n",
       "22   [-0.37221488045095963, 0.15941117175552535, -0...           cheesecake   \n",
       "28   [-0.47327678027169323, -0.011298772057821004, ...        chicken_curry   \n",
       "38   [0.06220996901184445, 0.3393704623708661, 0.49...        chicken_wings   \n",
       "39   [0.23359608008893995, 0.1726547022163415, 0.32...        chicken_wings   \n",
       "47   [0.2844618296471188, 0.3743878599254398, 0.252...        chicken_wings   \n",
       "54   [0.049437621588535006, 0.3873440151839275, 0.3...       chocolate_cake   \n",
       "60   [0.005246176771439245, 0.4336387087522666, -0....       chocolate_cake   \n",
       "67   [0.20052023933246357, -0.007778699472581977, -...     chocolate_mousse   \n",
       "73   [0.12761792512715212, -0.08306458959668783, -0...         deviled_eggs   \n",
       "85   [-0.3890306742051276, -0.28388236235279246, 0....            dumplings   \n",
       "93   [0.4887286044815729, -0.44843270538923524, 0.1...         french_toast   \n",
       "99   [0.2865353280274722, -0.2075294793687004, -0.1...         french_toast   \n",
       "105  [-0.32691051092812917, 0.025105538055489807, -...           fried_rice   \n",
       "111  [-0.08713123579690392, 0.2930824082248947, 0.2...           fried_rice   \n",
       "123  [0.01814603032105441, 0.30488299911996736, -0....        frozen_yogurt   \n",
       "125  [0.17409272996451705, 0.3666403653890165, -0.1...        frozen_yogurt   \n",
       "133  [-0.32694425003363037, -0.05229618946954002, 0...         garlic_bread   \n",
       "138  [0.034681060772145944, -0.24339738354111606, 0...         garlic_bread   \n",
       "147  [-0.15734423672860687, -0.13266970423875263, -...            hamburger   \n",
       "152  [0.5152736925997148, 0.07830772278678728, 0.59...               hummus   \n",
       "157  [-0.039032893177338386, -0.04677780900953855, ...               hummus   \n",
       "163  [-0.2961405144602718, 0.22512314397233563, 0.2...              lasagna   \n",
       "166  [-0.005625945501844912, -0.238267633708876, -0...              lasagna   \n",
       "172  [0.25772516370652526, 0.009489941631141365, 0....  macaroni_and_cheese   \n",
       "177  [0.21931051599973606, 0.1391449955328983, 0.06...  macaroni_and_cheese   \n",
       "182  [0.5184439490310835, -0.2885368514920793, 0.18...  macaroni_and_cheese   \n",
       "190  [0.003029945101028819, -0.15117827969985273, -...             macarons   \n",
       "197  [0.13549211068222441, 0.18648505937811097, 0.0...               nachos   \n",
       "202  [0.09446753483610576, 0.2261619087014454, 0.06...               nachos   \n",
       "210  [0.058802332059202954, 0.03911879902394547, -0...          onion_rings   \n",
       "217  [0.3494005293430176, 0.15849765837439808, 0.23...             pancakes   \n",
       "229  [-0.35584416511765493, -0.6137784749375513, -0...                pizza   \n",
       "233  [-0.7825078770473083, -0.244264183843442, -0.5...              sashimi   \n",
       "239  [-0.00707018513783637, -0.566972240975464, -0....              sashimi   \n",
       "242  [-0.22439591565572406, -0.6087644238149892, -0...                steak   \n",
       "244  [-0.41566566552206474, -0.41304668439483855, -...                steak   \n",
       "246  [-0.17589010812861938, -0.07039237126823106, -...                steak   \n",
       "256  [-0.5959493224072112, -0.4330175926324245, -0....                steak   \n",
       "267  [0.16066192375594665, -0.20259633773759364, -0...                sushi   \n",
       "274  [0.27343369828246367, -0.17846950678116877, -0...                sushi   \n",
       "277  [0.338076046005996, -0.3870151352720759, -0.41...                sushi   \n",
       "284  [0.08316067476177193, -0.19134558247599706, -0...                sushi   \n",
       "287  [0.12419525208469703, 0.18482393348853915, 0.3...                tacos   \n",
       "293  [0.7463816534651476, 0.2869543568779235, -0.10...                tacos   \n",
       "302  [0.3768222583359232, 0.4499174545359371, -0.67...             tiramisu   \n",
       "303  [0.43418761283413504, 0.45749500018450967, -0....             tiramisu   \n",
       "313  [-0.5636584578472996, -0.3188487955657733, -0....              waffles   \n",
       "315  [0.053729910759766616, 0.13244119448045463, 0....              waffles   \n",
       "\n",
       "    text_names  \n",
       "0        1.txt  \n",
       "4       13.txt  \n",
       "8        3.txt  \n",
       "20       4.txt  \n",
       "22       6.txt  \n",
       "28       3.txt  \n",
       "38      13.txt  \n",
       "39      14.txt  \n",
       "47       6.txt  \n",
       "54       3.txt  \n",
       "60       9.txt  \n",
       "67       7.txt  \n",
       "73       4.txt  \n",
       "85       7.txt  \n",
       "93      14.txt  \n",
       "99       7.txt  \n",
       "105     12.txt  \n",
       "111      5.txt  \n",
       "123      4.txt  \n",
       "125      6.txt  \n",
       "133      4.txt  \n",
       "138      9.txt  \n",
       "147      9.txt  \n",
       "152      2.txt  \n",
       "157      7.txt  \n",
       "163      2.txt  \n",
       "166      5.txt  \n",
       "172     10.txt  \n",
       "177     15.txt  \n",
       "182      6.txt  \n",
       "190      5.txt  \n",
       "197     12.txt  \n",
       "202      6.txt  \n",
       "210      5.txt  \n",
       "217      4.txt  \n",
       "229      7.txt  \n",
       "233      2.txt  \n",
       "239      8.txt  \n",
       "242     10.txt  \n",
       "244     12.txt  \n",
       "246     14.txt  \n",
       "256      3.txt  \n",
       "267     13.txt  \n",
       "274      2.txt  \n",
       "277     22.txt  \n",
       "284      8.txt  \n",
       "287     10.txt  \n",
       "293      7.txt  \n",
       "302      6.txt  \n",
       "303      7.txt  \n",
       "313      7.txt  \n",
       "315      9.txt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_textual_m_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 11  4 18]\n",
      "(4, 202)\n",
      "(4, 27)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "def create_training_batch():\n",
    "    inputs_visual=[]\n",
    "    inputs_textual=[]\n",
    "    correct_classes=[]\n",
    "    for i in range(batch_size):\n",
    "        train_sample_index=np.random.choice(range(0,number_of_training_samples),1)[0]\n",
    "        inputs_visual.append(visual_m_train_inputs[train_sample_index])\n",
    "        inputs_textual.append(textual_m_train_inputs[train_sample_index])\n",
    "        correct_classes.append(train_correct_class_ids[train_sample_index])\n",
    "    return np.array(inputs_visual),np.array(inputs_textual),np.array(correct_classes)\n",
    "\n",
    "inputs_visual,inputs_textual,correct_classes=create_training_batch()\n",
    "print(np.array(correct_classes))\n",
    "print(np.shape(inputs_visual))\n",
    "print(np.shape(inputs_textual))\n",
    "print(np.shape(correct_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual = tf.placeholder(tf.float32, shape=[None,len_of_visual_features_vec])\n",
    "textual = tf.placeholder(tf.float32, shape=[None,len_of_textual_features_vec])\n",
    "target = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "#v_reduced = tf.layers.dense(visual,\n",
    "#                      hidden_state_dim,\n",
    "##                      activation=tf.nn.tanh)\n",
    "#                       activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "h_v = tf.layers.dense(visual,\n",
    "                      hidden_state_dim,\n",
    "                      activation=tf.nn.tanh)\n",
    "\n",
    "\n",
    "h_t = tf.layers.dense(textual,\n",
    "                      hidden_state_dim,\n",
    "                      activation=tf.nn.tanh)\n",
    "\n",
    "z = tf.layers.dense(tf.concat([visual,textual], axis=1),\n",
    "                    hidden_state_dim,\n",
    "                    kernel_initializer=tf.initializers.random_uniform,\n",
    "                    activation=tf.nn.sigmoid)\n",
    "\n",
    "#h = tf.concat([z * h_v,(1 - z) * h_t], axis=1)\n",
    "h = tf.concat([(1-z) * h_v, z * h_t], axis=1)\n",
    "#h = z * h_v + (1 - z) * h_t\n",
    "#h = (1-z) * h_v +  z * h_t\n",
    "\n",
    "\n",
    "logits = tf.layers.dense(h, number_of_classes)\n",
    "scores = tf.nn.sigmoid(logits)\n",
    "\n",
    "multi_class_labels=tf.one_hot(target, depth=number_of_classes)\n",
    "loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(multi_class_labels=multi_class_labels,\n",
    "                                       logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction=tf.equal(tf.argmax(logits, axis=1), tf.argmax(multi_class_labels,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy=0\n",
    "\n",
    "def show_validation_result(session,accuracy_res, loss_res, scores_res,z_res):\n",
    "    print(\"Validation: Loss: \", loss_res,\" Accuracy: \", accuracy_res)\n",
    "    global best_accuracy\n",
    "    if accuracy_res > best_accuracy:\n",
    "        best_accuracy=accuracy_res\n",
    "        df_performance=pd.DataFrame(data={'Accuracy':[accuracy_res], 'Loss':[loss_res]})\n",
    "        class_scores=list(np.around(scores_res,4))\n",
    "        predicted_labels = [np.argmax(one_recipe_class_scores) for one_recipe_class_scores in class_scores]\n",
    "        df_scores=pd.DataFrame(data={'Class scores':class_scores,\n",
    "                               'Predicted labels':predicted_labels,\n",
    "                               'Correct labels':test_correct_class_ids,\n",
    "                               'Trust to visual modality': list(np.around(1-z_res,4)),\n",
    "                               'Trust to textual modality': list(np.around(z_res,4))})\n",
    "        df_res=pd.concat([df_scores, df_performance], axis=1)\n",
    "        df_res.to_pickle(path_to_save_test_results)\n",
    "        saver.save(session, model_checkpoint_path)\n",
    "        display(df_res)\n",
    "        #print(df_scores)\n",
    "        \n",
    "    \n",
    "\n",
    "def train(num_of_run):\n",
    "    #with tf.Session() as session:\n",
    "        session=tf.InteractiveSession()\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    \n",
    "        print(\"Start model training\")\n",
    "    \n",
    "        for train_iter in range(number_of_training_iterations):\n",
    "            if(train_iter==0):\n",
    "                accuracy_res, loss_res, scores_res,z_res = session.run([accuracy, loss, scores, z],\n",
    "                                                                          {visual: visual_m_test_inputs,\n",
    "                                                                           textual: textual_m_test_inputs,\n",
    "                                                                           target: test_correct_class_ids})        \n",
    "                \n",
    "                show_validation_result(session,accuracy_res, loss_res, scores_res,z_res)\n",
    "            \n",
    "            inputs_visual,inputs_textual,correct_classes=create_training_batch()\n",
    "            _, l = session.run([train_op, loss], {visual: inputs_visual,\n",
    "                                           textual: inputs_textual,\n",
    "                                           target: correct_classes})\n",
    "            print(train_iter, \": Training: loss: \", l)\n",
    "        \n",
    "            if (train_iter)%print_valid_every==0:\n",
    "                accuracy_res, loss_res, scores_res,z_res = session.run([accuracy, loss, scores, z],\n",
    "                                                                          {visual: visual_m_test_inputs,\n",
    "                                                                           textual: textual_m_test_inputs,\n",
    "                                                                           target: test_correct_class_ids})        \n",
    "                \n",
    "                show_validation_result(session,accuracy_res, loss_res, scores_res,z_res)\n",
    "                \n",
    "                if accuracy_res==1.0:\n",
    "                    #saver.save(session, model_checkpoint_path)\n",
    "                    return 0\n",
    "                \n",
    "        #saver.save(session, model_checkpoint_path)\n",
    "        session.close()\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "Validation: Loss:  0.6955  Accuracy:  0.01923077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.4993, 0.5091, 0.5001, 0.4981, 0.5227, 0.494...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.2245, 0.324, 0.1933, 0.2098, 0.1259, 0.1964...</td>\n",
       "      <td>[0.7755, 0.676, 0.8067, 0.7902, 0.8741, 0.8036...</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.6955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5201, 0.5361, 0.5074, 0.4937, 0.4886, 0.451...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6778, 0.6248, 0.4293, 0.5012, 0.3995, 0.396...</td>\n",
       "      <td>[0.3222, 0.3752, 0.5707, 0.4988, 0.6005, 0.603...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.4869, 0.5057, 0.5097, 0.4806, 0.5153, 0.505...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.4112, 0.3977, 0.2679, 0.2338, 0.146, 0.2432...</td>\n",
       "      <td>[0.5888, 0.6023, 0.7321, 0.7662, 0.854, 0.7568...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.4891, 0.4835, 0.5483, 0.5001, 0.4607, 0.477...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.5915, 0.4715, 0.4486, 0.5585, 0.6776, 0.546...</td>\n",
       "      <td>[0.4085, 0.5285, 0.5514, 0.4415, 0.3224, 0.453...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.5094, 0.5011, 0.4949, 0.4948, 0.5098, 0.499...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.157, 0.1306, 0.144, 0.1306, 0.1126, 0.0795,...</td>\n",
       "      <td>[0.843, 0.8694, 0.856, 0.8694, 0.8874, 0.9205,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.5026, 0.5062, 0.514, 0.4961, 0.5092, 0.5213...</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.1125, 0.095, 0.1106, 0.1575, 0.1214, 0.2062...</td>\n",
       "      <td>[0.8875, 0.905, 0.8894, 0.8425, 0.8786, 0.7938...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.5407, 0.5242, 0.4296, 0.4623, 0.5959, 0.475...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8742, 0.6417, 0.8277, 0.7006, 0.6697, 0.688...</td>\n",
       "      <td>[0.1258, 0.3583, 0.1723, 0.2994, 0.3303, 0.312...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.4794, 0.4752, 0.4821, 0.4682, 0.5023, 0.528...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.7048, 0.4036, 0.6063, 0.4072, 0.4417, 0.581...</td>\n",
       "      <td>[0.2952, 0.5964, 0.3937, 0.5928, 0.5583, 0.418...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.4818, 0.5047, 0.4855, 0.4723, 0.53, 0.5514,...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4765, 0.2927, 0.4979, 0.3415, 0.3083, 0.472...</td>\n",
       "      <td>[0.5235, 0.7073, 0.5021, 0.6585, 0.6917, 0.527...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.4994, 0.4958, 0.5786, 0.4333, 0.4923, 0.427...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.9025, 0.9208, 0.9457, 0.8855, 0.9428, 0.881...</td>\n",
       "      <td>[0.0975, 0.0792, 0.0543, 0.1145, 0.0572, 0.118...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.4911, 0.5682, 0.4817, 0.455, 0.5088, 0.5011...</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.8318, 0.8607, 0.8122, 0.757, 0.7473, 0.7627...</td>\n",
       "      <td>[0.1682, 0.1393, 0.1878, 0.243, 0.2527, 0.2373...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.4759, 0.5114, 0.521, 0.4954, 0.5033, 0.5233...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.6079, 0.7182, 0.6494, 0.5978, 0.7362, 0.674...</td>\n",
       "      <td>[0.3921, 0.2818, 0.3506, 0.4022, 0.2638, 0.325...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.4828, 0.4967, 0.4518, 0.5516, 0.5439, 0.489...</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.8857, 0.9154, 0.9506, 0.9557, 0.8947, 0.930...</td>\n",
       "      <td>[0.1143, 0.0846, 0.0494, 0.0443, 0.1053, 0.069...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.5057, 0.4932, 0.5249, 0.5042, 0.4175, 0.573...</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.8939, 0.873, 0.8252, 0.8234, 0.8593, 0.9196...</td>\n",
       "      <td>[0.1061, 0.127, 0.1748, 0.1766, 0.1407, 0.0804...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.4507, 0.4677, 0.5155, 0.5046, 0.4908, 0.466...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7085, 0.4922, 0.7675, 0.7905, 0.7279, 0.716...</td>\n",
       "      <td>[0.2915, 0.5078, 0.2325, 0.2095, 0.2721, 0.284...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.4567, 0.4851, 0.5265, 0.4596, 0.536, 0.4734...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3895, 0.4688, 0.6605, 0.4886, 0.4491, 0.636...</td>\n",
       "      <td>[0.6105, 0.5312, 0.3395, 0.5114, 0.5509, 0.363...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.4988, 0.4953, 0.5015, 0.5043, 0.493, 0.5114...</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.1458, 0.101, 0.1003, 0.2241, 0.2221, 0.2145...</td>\n",
       "      <td>[0.8542, 0.899, 0.8997, 0.7759, 0.7779, 0.7855...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.4888, 0.4994, 0.5077, 0.4709, 0.4928, 0.503...</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.4258, 0.3899, 0.3842, 0.6285, 0.6137, 0.532...</td>\n",
       "      <td>[0.5742, 0.6101, 0.6158, 0.3715, 0.3863, 0.467...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.5011, 0.4962, 0.5191, 0.501, 0.5039, 0.5012...</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.1474, 0.423, 0.1224, 0.1607, 0.3534, 0.1648...</td>\n",
       "      <td>[0.8526, 0.577, 0.8776, 0.8393, 0.6466, 0.8352...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.4833, 0.5032, 0.5376, 0.4966, 0.5179, 0.488...</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.2073, 0.5794, 0.14, 0.3256, 0.3841, 0.2611,...</td>\n",
       "      <td>[0.7927, 0.4206, 0.86, 0.6744, 0.6159, 0.7389,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.488, 0.4932, 0.4622, 0.468, 0.5287, 0.5497,...</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.2731, 0.424, 0.3824, 0.5347, 0.1989, 0.3367...</td>\n",
       "      <td>[0.7269, 0.576, 0.6176, 0.4653, 0.8011, 0.6633...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.5107, 0.5208, 0.4756, 0.5238, 0.5086, 0.526...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.1862, 0.3054, 0.1978, 0.4875, 0.1515, 0.143...</td>\n",
       "      <td>[0.8138, 0.6946, 0.8022, 0.5125, 0.8485, 0.856...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.5017, 0.5109, 0.4895, 0.5108, 0.4883, 0.501...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.1399, 0.2617, 0.2023, 0.3623, 0.2057, 0.317...</td>\n",
       "      <td>[0.8601, 0.7383, 0.7977, 0.6377, 0.7943, 0.682...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.5088, 0.5142, 0.5123, 0.485, 0.4854, 0.5352...</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.632, 0.5957, 0.4599, 0.5145, 0.4864, 0.7108...</td>\n",
       "      <td>[0.368, 0.4043, 0.5401, 0.4855, 0.5136, 0.2892...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.5054, 0.4972, 0.5129, 0.5016, 0.5034, 0.507...</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0776, 0.0956, 0.08, 0.1199, 0.1143, 0.1177,...</td>\n",
       "      <td>[0.9224, 0.9044, 0.92, 0.8801, 0.8857, 0.8823,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.493, 0.4982, 0.5103, 0.4922, 0.5259, 0.5251...</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.2191, 0.1713, 0.6359, 0.4285, 0.3141, 0.407...</td>\n",
       "      <td>[0.7809, 0.8287, 0.3641, 0.5715, 0.6859, 0.592...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.5065, 0.494, 0.4949, 0.5094, 0.4867, 0.4969...</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.0716, 0.0699, 0.2521, 0.1997, 0.1145, 0.170...</td>\n",
       "      <td>[0.9284, 0.9301, 0.7479, 0.8003, 0.8855, 0.829...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.4967, 0.4568, 0.5007, 0.5078, 0.4574, 0.482...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.5868, 0.4861, 0.6248, 0.5594, 0.8282, 0.573...</td>\n",
       "      <td>[0.4132, 0.5139, 0.3752, 0.4406, 0.1718, 0.426...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.5364, 0.4527, 0.5145, 0.499, 0.3848, 0.5011...</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.8891, 0.7841, 0.8256, 0.8421, 0.9571, 0.844...</td>\n",
       "      <td>[0.1109, 0.2159, 0.1744, 0.1579, 0.0429, 0.155...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.4723, 0.4418, 0.5283, 0.5204, 0.4328, 0.509...</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.9082, 0.7084, 0.9269, 0.9029, 0.9667, 0.878...</td>\n",
       "      <td>[0.0918, 0.2916, 0.0731, 0.0971, 0.0333, 0.121...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.4333, 0.5026, 0.6169, 0.5149, 0.5295, 0.533...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.5556, 0.5327, 0.4973, 0.5074, 0.4729, 0.470...</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.7117, 0.6244, 0.5708, 0.501, 0.5557, 0.3007...</td>\n",
       "      <td>[0.2883, 0.3756, 0.4292, 0.499, 0.4443, 0.6993...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.5032, 0.5026, 0.4899, 0.4825, 0.5521, 0.484...</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.5129, 0.6392, 0.5793, 0.6565, 0.5077, 0.550...</td>\n",
       "      <td>[0.4871, 0.3608, 0.4207, 0.3435, 0.4923, 0.449...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.4893, 0.5007, 0.4958, 0.4956, 0.5049, 0.506...</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.0531, 0.0677, 0.0921, 0.0572, 0.0796, 0.096...</td>\n",
       "      <td>[0.9469, 0.9323, 0.9079, 0.9428, 0.9204, 0.903...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.4706, 0.5163, 0.5446, 0.4967, 0.5282, 0.502...</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.3964, 0.5243, 0.1876, 0.2469, 0.3795, 0.289...</td>\n",
       "      <td>[0.6036, 0.4757, 0.8124, 0.7531, 0.6205, 0.710...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.4944, 0.496, 0.4906, 0.5023, 0.4937, 0.5037...</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.1155, 0.1425, 0.2877, 0.1889, 0.1027, 0.247...</td>\n",
       "      <td>[0.8845, 0.8575, 0.7123, 0.8111, 0.8973, 0.752...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.5063, 0.4973, 0.4947, 0.5069, 0.4699, 0.495...</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.1432, 0.1534, 0.2276, 0.2346, 0.2061, 0.189...</td>\n",
       "      <td>[0.8568, 0.8466, 0.7724, 0.7654, 0.7939, 0.811...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.5055, 0.5047, 0.4918, 0.5578, 0.4271, 0.523...</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.6732, 0.8142, 0.7907, 0.8702, 0.756, 0.8739...</td>\n",
       "      <td>[0.3268, 0.1858, 0.2093, 0.1298, 0.244, 0.1261...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.5129, 0.4942, 0.4809, 0.5439, 0.4808, 0.516...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3923, 0.2852, 0.319, 0.3164, 0.3827, 0.4102...</td>\n",
       "      <td>[0.6077, 0.7148, 0.681, 0.6836, 0.6173, 0.5898...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.4983, 0.495, 0.5003, 0.5252, 0.4907, 0.5133...</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.2041, 0.0813, 0.0806, 0.1428, 0.1326, 0.095...</td>\n",
       "      <td>[0.7959, 0.9187, 0.9194, 0.8572, 0.8674, 0.904...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.5769, 0.5403, 0.4356, 0.5516, 0.5027, 0.462...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.705, 0.732, 0.588, 0.7753, 0.7048, 0.48, 0....</td>\n",
       "      <td>[0.295, 0.268, 0.412, 0.2247, 0.2952, 0.52, 0....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.4921, 0.4971, 0.5059, 0.511, 0.4915, 0.521,...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0807, 0.0718, 0.0821, 0.089, 0.1213, 0.1537...</td>\n",
       "      <td>[0.9193, 0.9282, 0.9179, 0.911, 0.8787, 0.8463...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.4994, 0.5005, 0.4699, 0.4964, 0.4895, 0.491...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2051, 0.1674, 0.3218, 0.3012, 0.1842, 0.276...</td>\n",
       "      <td>[0.7949, 0.8326, 0.6782, 0.6988, 0.8158, 0.723...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.5171, 0.5074, 0.4539, 0.5461, 0.4954, 0.473...</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.883, 0.8613, 0.8339, 0.9204, 0.8228, 0.8921...</td>\n",
       "      <td>[0.117, 0.1387, 0.1661, 0.0796, 0.1772, 0.1079...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.5409, 0.4995, 0.4441, 0.5393, 0.4954, 0.457...</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.9052, 0.8969, 0.9218, 0.9236, 0.8709, 0.960...</td>\n",
       "      <td>[0.0948, 0.1031, 0.0782, 0.0764, 0.1291, 0.039...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.5254, 0.4981, 0.4324, 0.5266, 0.4657, 0.459...</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.4953, 0.4307, 0.5412, 0.4996, 0.4421, 0.376...</td>\n",
       "      <td>[0.5047, 0.5693, 0.4588, 0.5004, 0.5579, 0.623...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.5026, 0.4998, 0.5151, 0.4999, 0.4566, 0.536...</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.6019, 0.5219, 0.4591, 0.5104, 0.5497, 0.573...</td>\n",
       "      <td>[0.3981, 0.4781, 0.5409, 0.4896, 0.4503, 0.426...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.5142, 0.4969, 0.4973, 0.4486, 0.5089, 0.474...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.4496, 0.5321, 0.3323, 0.5121, 0.3448, 0.487...</td>\n",
       "      <td>[0.5504, 0.4679, 0.6677, 0.4879, 0.6552, 0.512...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.4973, 0.5125, 0.5161, 0.5034, 0.4923, 0.476...</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.2122, 0.3068, 0.1482, 0.187, 0.3053, 0.2192...</td>\n",
       "      <td>[0.7878, 0.6932, 0.8518, 0.813, 0.6947, 0.7808...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.4998, 0.5528, 0.4994, 0.5001, 0.5125, 0.469...</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.4611, 0.8621, 0.6401, 0.6303, 0.6463, 0.635...</td>\n",
       "      <td>[0.5389, 0.1379, 0.3599, 0.3697, 0.3537, 0.364...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.4879, 0.4584, 0.4467, 0.4674, 0.5492, 0.518...</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.4482, 0.545, 0.7286, 0.3694, 0.2268, 0.5348...</td>\n",
       "      <td>[0.5518, 0.455, 0.2714, 0.6306, 0.7732, 0.4652...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.496, 0.502, 0.5532, 0.4861, 0.5332, 0.4269,...</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.5411, 0.4726, 0.669, 0.585, 0.6332, 0.5617,...</td>\n",
       "      <td>[0.4589, 0.5274, 0.331, 0.415, 0.3668, 0.4383,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.4993, 0.5091, 0.5001, 0.4981, 0.5227, 0.494...               0   \n",
       "1   [0.5201, 0.5361, 0.5074, 0.4937, 0.4886, 0.451...               0   \n",
       "2   [0.4869, 0.5057, 0.5097, 0.4806, 0.5153, 0.505...               0   \n",
       "3   [0.4891, 0.4835, 0.5483, 0.5001, 0.4607, 0.477...               1   \n",
       "4   [0.5094, 0.5011, 0.4949, 0.4948, 0.5098, 0.499...               1   \n",
       "5   [0.5026, 0.5062, 0.514, 0.4961, 0.5092, 0.5213...               2   \n",
       "6   [0.5407, 0.5242, 0.4296, 0.4623, 0.5959, 0.475...               3   \n",
       "7   [0.4794, 0.4752, 0.4821, 0.4682, 0.5023, 0.528...               3   \n",
       "8   [0.4818, 0.5047, 0.4855, 0.4723, 0.53, 0.5514,...               3   \n",
       "9   [0.4994, 0.4958, 0.5786, 0.4333, 0.4923, 0.427...               4   \n",
       "10  [0.4911, 0.5682, 0.4817, 0.455, 0.5088, 0.5011...               4   \n",
       "11  [0.4759, 0.5114, 0.521, 0.4954, 0.5033, 0.5233...               5   \n",
       "12  [0.4828, 0.4967, 0.4518, 0.5516, 0.5439, 0.489...               6   \n",
       "13  [0.5057, 0.4932, 0.5249, 0.5042, 0.4175, 0.573...               7   \n",
       "14  [0.4507, 0.4677, 0.5155, 0.5046, 0.4908, 0.466...               8   \n",
       "15  [0.4567, 0.4851, 0.5265, 0.4596, 0.536, 0.4734...               8   \n",
       "16  [0.4988, 0.4953, 0.5015, 0.5043, 0.493, 0.5114...               9   \n",
       "17  [0.4888, 0.4994, 0.5077, 0.4709, 0.4928, 0.503...               9   \n",
       "18  [0.5011, 0.4962, 0.5191, 0.501, 0.5039, 0.5012...              10   \n",
       "19  [0.4833, 0.5032, 0.5376, 0.4966, 0.5179, 0.488...              10   \n",
       "20  [0.488, 0.4932, 0.4622, 0.468, 0.5287, 0.5497,...              11   \n",
       "21  [0.5107, 0.5208, 0.4756, 0.5238, 0.5086, 0.526...              11   \n",
       "22  [0.5017, 0.5109, 0.4895, 0.5108, 0.4883, 0.501...              12   \n",
       "23  [0.5088, 0.5142, 0.5123, 0.485, 0.4854, 0.5352...              13   \n",
       "24  [0.5054, 0.4972, 0.5129, 0.5016, 0.5034, 0.507...              13   \n",
       "25  [0.493, 0.4982, 0.5103, 0.4922, 0.5259, 0.5251...              14   \n",
       "26  [0.5065, 0.494, 0.4949, 0.5094, 0.4867, 0.4969...              14   \n",
       "27  [0.4967, 0.4568, 0.5007, 0.5078, 0.4574, 0.482...              15   \n",
       "28  [0.5364, 0.4527, 0.5145, 0.499, 0.3848, 0.5011...              15   \n",
       "29  [0.4723, 0.4418, 0.5283, 0.5204, 0.4328, 0.509...              15   \n",
       "30  [0.4333, 0.5026, 0.6169, 0.5149, 0.5295, 0.533...              16   \n",
       "31  [0.5556, 0.5327, 0.4973, 0.5074, 0.4729, 0.470...              17   \n",
       "32  [0.5032, 0.5026, 0.4899, 0.4825, 0.5521, 0.484...              17   \n",
       "33  [0.4893, 0.5007, 0.4958, 0.4956, 0.5049, 0.506...              18   \n",
       "34  [0.4706, 0.5163, 0.5446, 0.4967, 0.5282, 0.502...              19   \n",
       "35  [0.4944, 0.496, 0.4906, 0.5023, 0.4937, 0.5037...              20   \n",
       "36  [0.5063, 0.4973, 0.4947, 0.5069, 0.4699, 0.495...              21   \n",
       "37  [0.5055, 0.5047, 0.4918, 0.5578, 0.4271, 0.523...              21   \n",
       "38  [0.5129, 0.4942, 0.4809, 0.5439, 0.4808, 0.516...              22   \n",
       "39  [0.4983, 0.495, 0.5003, 0.5252, 0.4907, 0.5133...              22   \n",
       "40  [0.5769, 0.5403, 0.4356, 0.5516, 0.5027, 0.462...              22   \n",
       "41  [0.4921, 0.4971, 0.5059, 0.511, 0.4915, 0.521,...              22   \n",
       "42  [0.4994, 0.5005, 0.4699, 0.4964, 0.4895, 0.491...              23   \n",
       "43  [0.5171, 0.5074, 0.4539, 0.5461, 0.4954, 0.473...              23   \n",
       "44  [0.5409, 0.4995, 0.4441, 0.5393, 0.4954, 0.457...              23   \n",
       "45  [0.5254, 0.4981, 0.4324, 0.5266, 0.4657, 0.459...              23   \n",
       "46  [0.5026, 0.4998, 0.5151, 0.4999, 0.4566, 0.536...              24   \n",
       "47  [0.5142, 0.4969, 0.4973, 0.4486, 0.5089, 0.474...              24   \n",
       "48  [0.4973, 0.5125, 0.5161, 0.5034, 0.4923, 0.476...              25   \n",
       "49  [0.4998, 0.5528, 0.4994, 0.5001, 0.5125, 0.469...              25   \n",
       "50  [0.4879, 0.4584, 0.4467, 0.4674, 0.5492, 0.518...              26   \n",
       "51  [0.496, 0.502, 0.5532, 0.4861, 0.5332, 0.4269,...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  4  [0.2245, 0.324, 0.1933, 0.2098, 0.1259, 0.1964...   \n",
       "1                 23  [0.6778, 0.6248, 0.4293, 0.5012, 0.3995, 0.396...   \n",
       "2                 19  [0.4112, 0.3977, 0.2679, 0.2338, 0.146, 0.2432...   \n",
       "3                 19  [0.5915, 0.4715, 0.4486, 0.5585, 0.6776, 0.546...   \n",
       "4                 24  [0.157, 0.1306, 0.144, 0.1306, 0.1126, 0.0795,...   \n",
       "5                 24  [0.1125, 0.095, 0.1106, 0.1575, 0.1214, 0.2062...   \n",
       "6                  4  [0.8742, 0.6417, 0.8277, 0.7006, 0.6697, 0.688...   \n",
       "7                  5  [0.7048, 0.4036, 0.6063, 0.4072, 0.4417, 0.581...   \n",
       "8                  5  [0.4765, 0.2927, 0.4979, 0.3415, 0.3083, 0.472...   \n",
       "9                  2  [0.9025, 0.9208, 0.9457, 0.8855, 0.9428, 0.881...   \n",
       "10                24  [0.8318, 0.8607, 0.8122, 0.757, 0.7473, 0.7627...   \n",
       "11                 7  [0.6079, 0.7182, 0.6494, 0.5978, 0.7362, 0.674...   \n",
       "12                25  [0.8857, 0.9154, 0.9506, 0.9557, 0.8947, 0.930...   \n",
       "13                10  [0.8939, 0.873, 0.8252, 0.8234, 0.8593, 0.9196...   \n",
       "14                22  [0.7085, 0.4922, 0.7675, 0.7905, 0.7279, 0.716...   \n",
       "15                22  [0.3895, 0.4688, 0.6605, 0.4886, 0.4491, 0.636...   \n",
       "16                 7  [0.1458, 0.101, 0.1003, 0.2241, 0.2221, 0.2145...   \n",
       "17                14  [0.4258, 0.3899, 0.3842, 0.6285, 0.6137, 0.532...   \n",
       "18                 6  [0.1474, 0.423, 0.1224, 0.1607, 0.3534, 0.1648...   \n",
       "19                 6  [0.2073, 0.5794, 0.14, 0.3256, 0.3841, 0.2611,...   \n",
       "20                 5  [0.2731, 0.424, 0.3824, 0.5347, 0.1989, 0.3367...   \n",
       "21                12  [0.1862, 0.3054, 0.1978, 0.4875, 0.1515, 0.143...   \n",
       "22                11  [0.1399, 0.2617, 0.2023, 0.3623, 0.2057, 0.317...   \n",
       "23                24  [0.632, 0.5957, 0.4599, 0.5145, 0.4864, 0.7108...   \n",
       "24                 2  [0.0776, 0.0956, 0.08, 0.1199, 0.1143, 0.1177,...   \n",
       "25                 4  [0.2191, 0.1713, 0.6359, 0.4285, 0.3141, 0.407...   \n",
       "26                 9  [0.0716, 0.0699, 0.2521, 0.1997, 0.1145, 0.170...   \n",
       "27                18  [0.5868, 0.4861, 0.6248, 0.5594, 0.8282, 0.573...   \n",
       "28                10  [0.8891, 0.7841, 0.8256, 0.8421, 0.9571, 0.844...   \n",
       "29                19  [0.9082, 0.7084, 0.9269, 0.9029, 0.9667, 0.878...   \n",
       "30                 2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "31                11  [0.7117, 0.6244, 0.5708, 0.501, 0.5557, 0.3007...   \n",
       "32                25  [0.5129, 0.6392, 0.5793, 0.6565, 0.5077, 0.550...   \n",
       "33                26  [0.0531, 0.0677, 0.0921, 0.0572, 0.0796, 0.096...   \n",
       "34                 2  [0.3964, 0.5243, 0.1876, 0.2469, 0.3795, 0.289...   \n",
       "35                24  [0.1155, 0.1425, 0.2877, 0.1889, 0.1027, 0.247...   \n",
       "36                13  [0.1432, 0.1534, 0.2276, 0.2346, 0.2061, 0.189...   \n",
       "37                11  [0.6732, 0.8142, 0.7907, 0.8702, 0.756, 0.8739...   \n",
       "38                23  [0.3923, 0.2852, 0.319, 0.3164, 0.3827, 0.4102...   \n",
       "39                 3  [0.2041, 0.0813, 0.0806, 0.1428, 0.1326, 0.095...   \n",
       "40                23  [0.705, 0.732, 0.588, 0.7753, 0.7048, 0.48, 0....   \n",
       "41                23  [0.0807, 0.0718, 0.0821, 0.089, 0.1213, 0.1537...   \n",
       "42                 8  [0.2051, 0.1674, 0.3218, 0.3012, 0.1842, 0.276...   \n",
       "43                13  [0.883, 0.8613, 0.8339, 0.9204, 0.8228, 0.8921...   \n",
       "44                 9  [0.9052, 0.8969, 0.9218, 0.9236, 0.8709, 0.960...   \n",
       "45                13  [0.4953, 0.4307, 0.5412, 0.4996, 0.4421, 0.376...   \n",
       "46                 5  [0.6019, 0.5219, 0.4591, 0.5104, 0.5497, 0.573...   \n",
       "47                24  [0.4496, 0.5321, 0.3323, 0.5121, 0.3448, 0.487...   \n",
       "48                17  [0.2122, 0.3068, 0.1482, 0.187, 0.3053, 0.2192...   \n",
       "49                14  [0.4611, 0.8621, 0.6401, 0.6303, 0.6463, 0.635...   \n",
       "50                 4  [0.4482, 0.545, 0.7286, 0.3694, 0.2268, 0.5348...   \n",
       "51                14  [0.5411, 0.4726, 0.669, 0.585, 0.6332, 0.5617,...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy    Loss  \n",
       "0   [0.7755, 0.676, 0.8067, 0.7902, 0.8741, 0.8036...  0.019231  0.6955  \n",
       "1   [0.3222, 0.3752, 0.5707, 0.4988, 0.6005, 0.603...       NaN     NaN  \n",
       "2   [0.5888, 0.6023, 0.7321, 0.7662, 0.854, 0.7568...       NaN     NaN  \n",
       "3   [0.4085, 0.5285, 0.5514, 0.4415, 0.3224, 0.453...       NaN     NaN  \n",
       "4   [0.843, 0.8694, 0.856, 0.8694, 0.8874, 0.9205,...       NaN     NaN  \n",
       "5   [0.8875, 0.905, 0.8894, 0.8425, 0.8786, 0.7938...       NaN     NaN  \n",
       "6   [0.1258, 0.3583, 0.1723, 0.2994, 0.3303, 0.312...       NaN     NaN  \n",
       "7   [0.2952, 0.5964, 0.3937, 0.5928, 0.5583, 0.418...       NaN     NaN  \n",
       "8   [0.5235, 0.7073, 0.5021, 0.6585, 0.6917, 0.527...       NaN     NaN  \n",
       "9   [0.0975, 0.0792, 0.0543, 0.1145, 0.0572, 0.118...       NaN     NaN  \n",
       "10  [0.1682, 0.1393, 0.1878, 0.243, 0.2527, 0.2373...       NaN     NaN  \n",
       "11  [0.3921, 0.2818, 0.3506, 0.4022, 0.2638, 0.325...       NaN     NaN  \n",
       "12  [0.1143, 0.0846, 0.0494, 0.0443, 0.1053, 0.069...       NaN     NaN  \n",
       "13  [0.1061, 0.127, 0.1748, 0.1766, 0.1407, 0.0804...       NaN     NaN  \n",
       "14  [0.2915, 0.5078, 0.2325, 0.2095, 0.2721, 0.284...       NaN     NaN  \n",
       "15  [0.6105, 0.5312, 0.3395, 0.5114, 0.5509, 0.363...       NaN     NaN  \n",
       "16  [0.8542, 0.899, 0.8997, 0.7759, 0.7779, 0.7855...       NaN     NaN  \n",
       "17  [0.5742, 0.6101, 0.6158, 0.3715, 0.3863, 0.467...       NaN     NaN  \n",
       "18  [0.8526, 0.577, 0.8776, 0.8393, 0.6466, 0.8352...       NaN     NaN  \n",
       "19  [0.7927, 0.4206, 0.86, 0.6744, 0.6159, 0.7389,...       NaN     NaN  \n",
       "20  [0.7269, 0.576, 0.6176, 0.4653, 0.8011, 0.6633...       NaN     NaN  \n",
       "21  [0.8138, 0.6946, 0.8022, 0.5125, 0.8485, 0.856...       NaN     NaN  \n",
       "22  [0.8601, 0.7383, 0.7977, 0.6377, 0.7943, 0.682...       NaN     NaN  \n",
       "23  [0.368, 0.4043, 0.5401, 0.4855, 0.5136, 0.2892...       NaN     NaN  \n",
       "24  [0.9224, 0.9044, 0.92, 0.8801, 0.8857, 0.8823,...       NaN     NaN  \n",
       "25  [0.7809, 0.8287, 0.3641, 0.5715, 0.6859, 0.592...       NaN     NaN  \n",
       "26  [0.9284, 0.9301, 0.7479, 0.8003, 0.8855, 0.829...       NaN     NaN  \n",
       "27  [0.4132, 0.5139, 0.3752, 0.4406, 0.1718, 0.426...       NaN     NaN  \n",
       "28  [0.1109, 0.2159, 0.1744, 0.1579, 0.0429, 0.155...       NaN     NaN  \n",
       "29  [0.0918, 0.2916, 0.0731, 0.0971, 0.0333, 0.121...       NaN     NaN  \n",
       "30  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       NaN     NaN  \n",
       "31  [0.2883, 0.3756, 0.4292, 0.499, 0.4443, 0.6993...       NaN     NaN  \n",
       "32  [0.4871, 0.3608, 0.4207, 0.3435, 0.4923, 0.449...       NaN     NaN  \n",
       "33  [0.9469, 0.9323, 0.9079, 0.9428, 0.9204, 0.903...       NaN     NaN  \n",
       "34  [0.6036, 0.4757, 0.8124, 0.7531, 0.6205, 0.710...       NaN     NaN  \n",
       "35  [0.8845, 0.8575, 0.7123, 0.8111, 0.8973, 0.752...       NaN     NaN  \n",
       "36  [0.8568, 0.8466, 0.7724, 0.7654, 0.7939, 0.811...       NaN     NaN  \n",
       "37  [0.3268, 0.1858, 0.2093, 0.1298, 0.244, 0.1261...       NaN     NaN  \n",
       "38  [0.6077, 0.7148, 0.681, 0.6836, 0.6173, 0.5898...       NaN     NaN  \n",
       "39  [0.7959, 0.9187, 0.9194, 0.8572, 0.8674, 0.904...       NaN     NaN  \n",
       "40  [0.295, 0.268, 0.412, 0.2247, 0.2952, 0.52, 0....       NaN     NaN  \n",
       "41  [0.9193, 0.9282, 0.9179, 0.911, 0.8787, 0.8463...       NaN     NaN  \n",
       "42  [0.7949, 0.8326, 0.6782, 0.6988, 0.8158, 0.723...       NaN     NaN  \n",
       "43  [0.117, 0.1387, 0.1661, 0.0796, 0.1772, 0.1079...       NaN     NaN  \n",
       "44  [0.0948, 0.1031, 0.0782, 0.0764, 0.1291, 0.039...       NaN     NaN  \n",
       "45  [0.5047, 0.5693, 0.4588, 0.5004, 0.5579, 0.623...       NaN     NaN  \n",
       "46  [0.3981, 0.4781, 0.5409, 0.4896, 0.4503, 0.426...       NaN     NaN  \n",
       "47  [0.5504, 0.4679, 0.6677, 0.4879, 0.6552, 0.512...       NaN     NaN  \n",
       "48  [0.7878, 0.6932, 0.8518, 0.813, 0.6947, 0.7808...       NaN     NaN  \n",
       "49  [0.5389, 0.1379, 0.3599, 0.3697, 0.3537, 0.364...       NaN     NaN  \n",
       "50  [0.5518, 0.455, 0.2714, 0.6306, 0.7732, 0.4652...       NaN     NaN  \n",
       "51  [0.4589, 0.5274, 0.331, 0.415, 0.3668, 0.4383,...       NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Training: loss:  0.6979154\n",
      "Validation: Loss:  0.6943165  Accuracy:  0.01923077\n",
      "1 : Training: loss:  0.69063485\n",
      "2 : Training: loss:  0.6926439\n",
      "3 : Training: loss:  0.7002935\n",
      "4 : Training: loss:  0.6948166\n",
      "5 : Training: loss:  0.68777055\n",
      "6 : Training: loss:  0.6936111\n",
      "7 : Training: loss:  0.6803202\n",
      "8 : Training: loss:  0.6789374\n",
      "9 : Training: loss:  0.68382037\n",
      "10 : Training: loss:  0.6766239\n",
      "11 : Training: loss:  0.6792958\n",
      "12 : Training: loss:  0.6808055\n",
      "13 : Training: loss:  0.6763594\n",
      "14 : Training: loss:  0.6795404\n",
      "15 : Training: loss:  0.6769553\n",
      "16 : Training: loss:  0.6707012\n",
      "17 : Training: loss:  0.67910796\n",
      "18 : Training: loss:  0.68152934\n",
      "19 : Training: loss:  0.65986633\n",
      "20 : Training: loss:  0.6739189\n",
      "Validation: Loss:  0.6698643  Accuracy:  0.01923077\n",
      "21 : Training: loss:  0.6649807\n",
      "22 : Training: loss:  0.6723937\n",
      "23 : Training: loss:  0.6514553\n",
      "24 : Training: loss:  0.67350245\n",
      "25 : Training: loss:  0.65896404\n",
      "26 : Training: loss:  0.6518119\n",
      "27 : Training: loss:  0.65791136\n",
      "28 : Training: loss:  0.6551643\n",
      "29 : Training: loss:  0.6422809\n",
      "30 : Training: loss:  0.6520493\n",
      "31 : Training: loss:  0.6452614\n",
      "32 : Training: loss:  0.6504478\n",
      "33 : Training: loss:  0.6424062\n",
      "34 : Training: loss:  0.650264\n",
      "35 : Training: loss:  0.63684005\n",
      "36 : Training: loss:  0.6478146\n",
      "37 : Training: loss:  0.62967926\n",
      "38 : Training: loss:  0.63415724\n",
      "39 : Training: loss:  0.62978363\n",
      "40 : Training: loss:  0.63696617\n",
      "Validation: Loss:  0.63115716  Accuracy:  0.03846154\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.4583, 0.4908, 0.434, 0.4648, 0.4541, 0.4481...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2089, 0.3206, 0.1865, 0.2111, 0.1282, 0.192...</td>\n",
       "      <td>[0.7911, 0.6794, 0.8135, 0.7889, 0.8718, 0.807...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.631157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.4879, 0.5043, 0.4379, 0.4536, 0.4306, 0.406...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6566, 0.6141, 0.423, 0.5091, 0.3898, 0.3907...</td>\n",
       "      <td>[0.3434, 0.3859, 0.577, 0.4909, 0.6102, 0.6093...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.4489, 0.4869, 0.4447, 0.4476, 0.4503, 0.457...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.3883, 0.3963, 0.2634, 0.2348, 0.1473, 0.236...</td>\n",
       "      <td>[0.6117, 0.6037, 0.7366, 0.7652, 0.8527, 0.763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.4514, 0.4547, 0.4806, 0.4637, 0.4063, 0.427...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.563, 0.4692, 0.4344, 0.5549, 0.6724, 0.5445...</td>\n",
       "      <td>[0.437, 0.5308, 0.5656, 0.4451, 0.3276, 0.4555...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.4573, 0.48, 0.4191, 0.4581, 0.4339, 0.4475,...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1421, 0.1277, 0.1393, 0.1275, 0.1148, 0.078...</td>\n",
       "      <td>[0.8579, 0.8723, 0.8607, 0.8725, 0.8852, 0.921...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.4568, 0.4908, 0.4548, 0.4654, 0.445, 0.4779...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1092, 0.0973, 0.1072, 0.1525, 0.1209, 0.202...</td>\n",
       "      <td>[0.8908, 0.9027, 0.8928, 0.8475, 0.8791, 0.797...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.5157, 0.4899, 0.3742, 0.445, 0.5275, 0.4159...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8627, 0.6587, 0.8264, 0.7138, 0.6568, 0.692...</td>\n",
       "      <td>[0.1373, 0.3413, 0.1736, 0.2862, 0.3432, 0.307...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.4481, 0.452, 0.432, 0.4475, 0.4472, 0.4824,...</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.6906, 0.4158, 0.6059, 0.4131, 0.4309, 0.582...</td>\n",
       "      <td>[0.3094, 0.5842, 0.3941, 0.5869, 0.5691, 0.417...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.4537, 0.4894, 0.4422, 0.4563, 0.4781, 0.511...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4668, 0.3037, 0.5, 0.3478, 0.3003, 0.4651, ...</td>\n",
       "      <td>[0.5332, 0.6963, 0.5, 0.6522, 0.6997, 0.5349, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.48, 0.4691, 0.5416, 0.4138, 0.4631, 0.4003,...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.8936, 0.9199, 0.942, 0.8838, 0.9415, 0.8746...</td>\n",
       "      <td>[0.1064, 0.0801, 0.0579, 0.1162, 0.0585, 0.125...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.4693, 0.5478, 0.4382, 0.4374, 0.468, 0.4704...</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.8138, 0.8619, 0.8088, 0.748, 0.7521, 0.751,...</td>\n",
       "      <td>[0.1862, 0.1381, 0.1912, 0.252, 0.2479, 0.249,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.4474, 0.4937, 0.477, 0.4747, 0.4602, 0.4888...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.5823, 0.7197, 0.638, 0.5912, 0.7402, 0.6651...</td>\n",
       "      <td>[0.4177, 0.2803, 0.362, 0.4088, 0.2598, 0.3349...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.4647, 0.4766, 0.4227, 0.5282, 0.5135, 0.461...</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.881, 0.9185, 0.9515, 0.954, 0.8934, 0.9322,...</td>\n",
       "      <td>[0.119, 0.0815, 0.0485, 0.046, 0.1066, 0.0678,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.5015, 0.482, 0.507, 0.4936, 0.4021, 0.5568,...</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.893, 0.8737, 0.818, 0.8083, 0.8573, 0.9228,...</td>\n",
       "      <td>[0.107, 0.1263, 0.182, 0.1917, 0.1427, 0.0772,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.4248, 0.4462, 0.4657, 0.4884, 0.4396, 0.422...</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.689, 0.5024, 0.7632, 0.7877, 0.7281, 0.7112...</td>\n",
       "      <td>[0.311, 0.4976, 0.2368, 0.2123, 0.2719, 0.2888...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.432, 0.4679, 0.4758, 0.4346, 0.4852, 0.4364...</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.3612, 0.469, 0.6477, 0.4819, 0.4572, 0.6236...</td>\n",
       "      <td>[0.6388, 0.531, 0.3523, 0.5181, 0.5428, 0.3764...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.4535, 0.4815, 0.4428, 0.4734, 0.4289, 0.467...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1418, 0.1028, 0.0971, 0.2211, 0.2173, 0.214...</td>\n",
       "      <td>[0.8582, 0.8972, 0.9029, 0.7789, 0.7827, 0.785...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.457, 0.4847, 0.4608, 0.4506, 0.4438, 0.4652...</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4168, 0.3968, 0.3749, 0.6254, 0.602, 0.5294...</td>\n",
       "      <td>[0.5832, 0.6032, 0.6251, 0.3746, 0.398, 0.4706...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.4568, 0.4807, 0.4572, 0.4682, 0.4387, 0.457...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1412, 0.4211, 0.1198, 0.1578, 0.3551, 0.161...</td>\n",
       "      <td>[0.8588, 0.5789, 0.8802, 0.8422, 0.6449, 0.838...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.4451, 0.4909, 0.4841, 0.4678, 0.4594, 0.45,...</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.1981, 0.5786, 0.1371, 0.3175, 0.3908, 0.251...</td>\n",
       "      <td>[0.8019, 0.4214, 0.8629, 0.6825, 0.6092, 0.748...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.4533, 0.4808, 0.4153, 0.4472, 0.4725, 0.509...</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.2614, 0.428, 0.3732, 0.5286, 0.1973, 0.3333...</td>\n",
       "      <td>[0.7386, 0.572, 0.6268, 0.4714, 0.8027, 0.6667...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.4537, 0.4981, 0.4078, 0.4871, 0.4332, 0.472...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1756, 0.3146, 0.1977, 0.4792, 0.1488, 0.146...</td>\n",
       "      <td>[0.8244, 0.6854, 0.8023, 0.5208, 0.8512, 0.853...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.4498, 0.4901, 0.4241, 0.4734, 0.4198, 0.449...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1354, 0.2653, 0.2019, 0.3593, 0.1996, 0.318...</td>\n",
       "      <td>[0.8646, 0.7347, 0.7981, 0.6407, 0.8004, 0.681...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.475, 0.4924, 0.4595, 0.4581, 0.4293, 0.4941...</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.6254, 0.6025, 0.4666, 0.5119, 0.4826, 0.694...</td>\n",
       "      <td>[0.3746, 0.3975, 0.5334, 0.4881, 0.5174, 0.305...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.455, 0.481, 0.443, 0.4676, 0.4289, 0.4577, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.0782, 0.0966, 0.0801, 0.12, 0.1111, 0.1132,...</td>\n",
       "      <td>[0.9218, 0.9034, 0.9199, 0.88, 0.8889, 0.8868,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.4585, 0.482, 0.456, 0.4698, 0.4709, 0.4851,...</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2064, 0.1767, 0.6281, 0.4283, 0.307, 0.4047...</td>\n",
       "      <td>[0.7936, 0.8233, 0.3719, 0.5717, 0.693, 0.5953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.4549, 0.4752, 0.4271, 0.4745, 0.4173, 0.447...</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.0697, 0.071, 0.2534, 0.1936, 0.1107, 0.173,...</td>\n",
       "      <td>[0.9303, 0.929, 0.7466, 0.8064, 0.8893, 0.827,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.4568, 0.4309, 0.4441, 0.4767, 0.4029, 0.431...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.5663, 0.4995, 0.6171, 0.5524, 0.8223, 0.582...</td>\n",
       "      <td>[0.4337, 0.5005, 0.3829, 0.4476, 0.1777, 0.417...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.5147, 0.4362, 0.4879, 0.4841, 0.3597, 0.473...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.8852, 0.7914, 0.8178, 0.8306, 0.9553, 0.853...</td>\n",
       "      <td>[0.1148, 0.2086, 0.1822, 0.1694, 0.0447, 0.146...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.4529, 0.4165, 0.4872, 0.4988, 0.4014, 0.468...</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.9032, 0.7191, 0.9246, 0.8983, 0.9654, 0.882...</td>\n",
       "      <td>[0.0968, 0.2809, 0.0754, 0.1017, 0.0346, 0.117...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.436, 0.4916, 0.6017, 0.5032, 0.5226, 0.5184...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.5158, 0.5027, 0.4397, 0.472, 0.4114, 0.4249...</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.7063, 0.6247, 0.5737, 0.5031, 0.5352, 0.308...</td>\n",
       "      <td>[0.2937, 0.3753, 0.4263, 0.4969, 0.4648, 0.691...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.4674, 0.4839, 0.4377, 0.4583, 0.4864, 0.434...</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.5034, 0.6535, 0.5797, 0.6585, 0.4912, 0.553...</td>\n",
       "      <td>[0.4966, 0.3465, 0.4203, 0.3415, 0.5088, 0.446...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.4287, 0.4789, 0.4131, 0.4527, 0.417, 0.4462...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0477, 0.0706, 0.0895, 0.0552, 0.0802, 0.095...</td>\n",
       "      <td>[0.9523, 0.9294, 0.9105, 0.9448, 0.9198, 0.905...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.4264, 0.4897, 0.4786, 0.4609, 0.4617, 0.450...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.3659, 0.5356, 0.1837, 0.2444, 0.3815, 0.278...</td>\n",
       "      <td>[0.6341, 0.4644, 0.8163, 0.7556, 0.6185, 0.721...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.4372, 0.4765, 0.4137, 0.4616, 0.414, 0.447,...</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.1089, 0.1436, 0.2846, 0.1857, 0.1013, 0.247...</td>\n",
       "      <td>[0.8911, 0.8564, 0.7154, 0.8143, 0.8987, 0.752...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.452, 0.4801, 0.424, 0.4704, 0.3995, 0.4447,...</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.1431, 0.1486, 0.2245, 0.2322, 0.1986, 0.189...</td>\n",
       "      <td>[0.8569, 0.8514, 0.7755, 0.7678, 0.8014, 0.810...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.4802, 0.4909, 0.4593, 0.5372, 0.3964, 0.490...</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.671, 0.8216, 0.7931, 0.866, 0.7464, 0.8773,...</td>\n",
       "      <td>[0.329, 0.1784, 0.2069, 0.134, 0.2536, 0.1227,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.4743, 0.4683, 0.4296, 0.5096, 0.4308, 0.470...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.3813, 0.2917, 0.3159, 0.317, 0.3685, 0.4289...</td>\n",
       "      <td>[0.6187, 0.7083, 0.6841, 0.683, 0.6315, 0.5711...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.4464, 0.4734, 0.4333, 0.4889, 0.4224, 0.462...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.1975, 0.0815, 0.0802, 0.1425, 0.1285, 0.098...</td>\n",
       "      <td>[0.8025, 0.9185, 0.9198, 0.8575, 0.8715, 0.902...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.5467, 0.5093, 0.3858, 0.5212, 0.458, 0.4157...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6891, 0.7314, 0.5767, 0.7744, 0.6925, 0.495...</td>\n",
       "      <td>[0.3109, 0.2686, 0.4233, 0.2256, 0.3075, 0.504...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.4385, 0.4764, 0.434, 0.4708, 0.4164, 0.4668...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.0763, 0.0722, 0.079, 0.0898, 0.1169, 0.1579...</td>\n",
       "      <td>[0.9237, 0.9278, 0.921, 0.9102, 0.8831, 0.8421...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.4451, 0.4801, 0.3999, 0.46, 0.4137, 0.4389,...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1987, 0.168, 0.3182, 0.2966, 0.1819, 0.2774...</td>\n",
       "      <td>[0.8013, 0.832, 0.6818, 0.7034, 0.8181, 0.7226...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.5021, 0.4945, 0.4276, 0.5287, 0.4673, 0.446...</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.8766, 0.8664, 0.8346, 0.9139, 0.8224, 0.894...</td>\n",
       "      <td>[0.1234, 0.1336, 0.1654, 0.0861, 0.1776, 0.105...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.5238, 0.4827, 0.418, 0.5196, 0.4633, 0.4316...</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.9031, 0.8984, 0.9223, 0.9174, 0.8681, 0.963...</td>\n",
       "      <td>[0.0969, 0.1016, 0.0777, 0.0826, 0.1319, 0.036...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.4782, 0.4791, 0.3746, 0.495, 0.4018, 0.4121...</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.4864, 0.4306, 0.5392, 0.49, 0.4367, 0.3857,...</td>\n",
       "      <td>[0.5136, 0.5694, 0.4608, 0.51, 0.5633, 0.6143,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.4735, 0.4865, 0.4804, 0.4774, 0.4177, 0.509...</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.6085, 0.5209, 0.4636, 0.5055, 0.5363, 0.566...</td>\n",
       "      <td>[0.3915, 0.4791, 0.5364, 0.4945, 0.4637, 0.433...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.4745, 0.4759, 0.4416, 0.4195, 0.4503, 0.429...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.4327, 0.5271, 0.3284, 0.5078, 0.3317, 0.484...</td>\n",
       "      <td>[0.5673, 0.4729, 0.6716, 0.4922, 0.6683, 0.515...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.4456, 0.4866, 0.4421, 0.4619, 0.4222, 0.424...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1911, 0.3117, 0.1464, 0.1788, 0.3068, 0.217...</td>\n",
       "      <td>[0.8089, 0.6883, 0.8536, 0.8212, 0.6932, 0.783...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.4715, 0.5334, 0.455, 0.4748, 0.4633, 0.4334...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4289, 0.8673, 0.6367, 0.6199, 0.6508, 0.630...</td>\n",
       "      <td>[0.5711, 0.1327, 0.3633, 0.3801, 0.3492, 0.369...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.4664, 0.4486, 0.4094, 0.4482, 0.512, 0.4898...</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.4313, 0.5386, 0.7201, 0.3662, 0.2277, 0.532...</td>\n",
       "      <td>[0.5687, 0.4614, 0.2799, 0.6338, 0.7723, 0.467...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.4644, 0.479, 0.5002, 0.4622, 0.4821, 0.3844...</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.5225, 0.4785, 0.6607, 0.5802, 0.6298, 0.548...</td>\n",
       "      <td>[0.4775, 0.5215, 0.3393, 0.4198, 0.3702, 0.451...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.4583, 0.4908, 0.434, 0.4648, 0.4541, 0.4481...               0   \n",
       "1   [0.4879, 0.5043, 0.4379, 0.4536, 0.4306, 0.406...               0   \n",
       "2   [0.4489, 0.4869, 0.4447, 0.4476, 0.4503, 0.457...               0   \n",
       "3   [0.4514, 0.4547, 0.4806, 0.4637, 0.4063, 0.427...               1   \n",
       "4   [0.4573, 0.48, 0.4191, 0.4581, 0.4339, 0.4475,...               1   \n",
       "5   [0.4568, 0.4908, 0.4548, 0.4654, 0.445, 0.4779...               2   \n",
       "6   [0.5157, 0.4899, 0.3742, 0.445, 0.5275, 0.4159...               3   \n",
       "7   [0.4481, 0.452, 0.432, 0.4475, 0.4472, 0.4824,...               3   \n",
       "8   [0.4537, 0.4894, 0.4422, 0.4563, 0.4781, 0.511...               3   \n",
       "9   [0.48, 0.4691, 0.5416, 0.4138, 0.4631, 0.4003,...               4   \n",
       "10  [0.4693, 0.5478, 0.4382, 0.4374, 0.468, 0.4704...               4   \n",
       "11  [0.4474, 0.4937, 0.477, 0.4747, 0.4602, 0.4888...               5   \n",
       "12  [0.4647, 0.4766, 0.4227, 0.5282, 0.5135, 0.461...               6   \n",
       "13  [0.5015, 0.482, 0.507, 0.4936, 0.4021, 0.5568,...               7   \n",
       "14  [0.4248, 0.4462, 0.4657, 0.4884, 0.4396, 0.422...               8   \n",
       "15  [0.432, 0.4679, 0.4758, 0.4346, 0.4852, 0.4364...               8   \n",
       "16  [0.4535, 0.4815, 0.4428, 0.4734, 0.4289, 0.467...               9   \n",
       "17  [0.457, 0.4847, 0.4608, 0.4506, 0.4438, 0.4652...               9   \n",
       "18  [0.4568, 0.4807, 0.4572, 0.4682, 0.4387, 0.457...              10   \n",
       "19  [0.4451, 0.4909, 0.4841, 0.4678, 0.4594, 0.45,...              10   \n",
       "20  [0.4533, 0.4808, 0.4153, 0.4472, 0.4725, 0.509...              11   \n",
       "21  [0.4537, 0.4981, 0.4078, 0.4871, 0.4332, 0.472...              11   \n",
       "22  [0.4498, 0.4901, 0.4241, 0.4734, 0.4198, 0.449...              12   \n",
       "23  [0.475, 0.4924, 0.4595, 0.4581, 0.4293, 0.4941...              13   \n",
       "24  [0.455, 0.481, 0.443, 0.4676, 0.4289, 0.4577, ...              13   \n",
       "25  [0.4585, 0.482, 0.456, 0.4698, 0.4709, 0.4851,...              14   \n",
       "26  [0.4549, 0.4752, 0.4271, 0.4745, 0.4173, 0.447...              14   \n",
       "27  [0.4568, 0.4309, 0.4441, 0.4767, 0.4029, 0.431...              15   \n",
       "28  [0.5147, 0.4362, 0.4879, 0.4841, 0.3597, 0.473...              15   \n",
       "29  [0.4529, 0.4165, 0.4872, 0.4988, 0.4014, 0.468...              15   \n",
       "30  [0.436, 0.4916, 0.6017, 0.5032, 0.5226, 0.5184...              16   \n",
       "31  [0.5158, 0.5027, 0.4397, 0.472, 0.4114, 0.4249...              17   \n",
       "32  [0.4674, 0.4839, 0.4377, 0.4583, 0.4864, 0.434...              17   \n",
       "33  [0.4287, 0.4789, 0.4131, 0.4527, 0.417, 0.4462...              18   \n",
       "34  [0.4264, 0.4897, 0.4786, 0.4609, 0.4617, 0.450...              19   \n",
       "35  [0.4372, 0.4765, 0.4137, 0.4616, 0.414, 0.447,...              20   \n",
       "36  [0.452, 0.4801, 0.424, 0.4704, 0.3995, 0.4447,...              21   \n",
       "37  [0.4802, 0.4909, 0.4593, 0.5372, 0.3964, 0.490...              21   \n",
       "38  [0.4743, 0.4683, 0.4296, 0.5096, 0.4308, 0.470...              22   \n",
       "39  [0.4464, 0.4734, 0.4333, 0.4889, 0.4224, 0.462...              22   \n",
       "40  [0.5467, 0.5093, 0.3858, 0.5212, 0.458, 0.4157...              22   \n",
       "41  [0.4385, 0.4764, 0.434, 0.4708, 0.4164, 0.4668...              22   \n",
       "42  [0.4451, 0.4801, 0.3999, 0.46, 0.4137, 0.4389,...              23   \n",
       "43  [0.5021, 0.4945, 0.4276, 0.5287, 0.4673, 0.446...              23   \n",
       "44  [0.5238, 0.4827, 0.418, 0.5196, 0.4633, 0.4316...              23   \n",
       "45  [0.4782, 0.4791, 0.3746, 0.495, 0.4018, 0.4121...              23   \n",
       "46  [0.4735, 0.4865, 0.4804, 0.4774, 0.4177, 0.509...              24   \n",
       "47  [0.4745, 0.4759, 0.4416, 0.4195, 0.4503, 0.429...              24   \n",
       "48  [0.4456, 0.4866, 0.4421, 0.4619, 0.4222, 0.424...              25   \n",
       "49  [0.4715, 0.5334, 0.455, 0.4748, 0.4633, 0.4334...              25   \n",
       "50  [0.4664, 0.4486, 0.4094, 0.4482, 0.512, 0.4898...              26   \n",
       "51  [0.4644, 0.479, 0.5002, 0.4622, 0.4821, 0.3844...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  1  [0.2089, 0.3206, 0.1865, 0.2111, 0.1282, 0.192...   \n",
       "1                 23  [0.6566, 0.6141, 0.423, 0.5091, 0.3898, 0.3907...   \n",
       "2                 19  [0.3883, 0.3963, 0.2634, 0.2348, 0.1473, 0.236...   \n",
       "3                 19  [0.563, 0.4692, 0.4344, 0.5549, 0.6724, 0.5445...   \n",
       "4                  8  [0.1421, 0.1277, 0.1393, 0.1275, 0.1148, 0.078...   \n",
       "5                  1  [0.1092, 0.0973, 0.1072, 0.1525, 0.1209, 0.202...   \n",
       "6                  4  [0.8627, 0.6587, 0.8264, 0.7138, 0.6568, 0.692...   \n",
       "7                 20  [0.6906, 0.4158, 0.6059, 0.4131, 0.4309, 0.582...   \n",
       "8                  5  [0.4668, 0.3037, 0.5, 0.3478, 0.3003, 0.4651, ...   \n",
       "9                 26  [0.8936, 0.9199, 0.942, 0.8838, 0.9415, 0.8746...   \n",
       "10                24  [0.8138, 0.8619, 0.8088, 0.748, 0.7521, 0.751,...   \n",
       "11                 6  [0.5823, 0.7197, 0.638, 0.5912, 0.7402, 0.6651...   \n",
       "12                25  [0.881, 0.9185, 0.9515, 0.954, 0.8934, 0.9322,...   \n",
       "13                10  [0.893, 0.8737, 0.818, 0.8083, 0.8573, 0.9228,...   \n",
       "14                22  [0.689, 0.5024, 0.7632, 0.7877, 0.7281, 0.7112...   \n",
       "15                26  [0.3612, 0.469, 0.6477, 0.4819, 0.4572, 0.6236...   \n",
       "16                 8  [0.1418, 0.1028, 0.0971, 0.2211, 0.2173, 0.214...   \n",
       "17                26  [0.4168, 0.3968, 0.3749, 0.6254, 0.602, 0.5294...   \n",
       "18                 8  [0.1412, 0.4211, 0.1198, 0.1578, 0.3551, 0.161...   \n",
       "19                 6  [0.1981, 0.5786, 0.1371, 0.3175, 0.3908, 0.251...   \n",
       "20                 5  [0.2614, 0.428, 0.3732, 0.5286, 0.1973, 0.3333...   \n",
       "21                 1  [0.1756, 0.3146, 0.1977, 0.4792, 0.1488, 0.146...   \n",
       "22                 1  [0.1354, 0.2653, 0.2019, 0.3593, 0.1996, 0.318...   \n",
       "23                24  [0.6254, 0.6025, 0.4666, 0.5119, 0.4826, 0.694...   \n",
       "24                 9  [0.0782, 0.0966, 0.0801, 0.12, 0.1111, 0.1132,...   \n",
       "25                25  [0.2064, 0.1767, 0.6281, 0.4283, 0.307, 0.4047...   \n",
       "26                20  [0.0697, 0.071, 0.2534, 0.1936, 0.1107, 0.173,...   \n",
       "27                18  [0.5663, 0.4995, 0.6171, 0.5524, 0.8223, 0.582...   \n",
       "28                18  [0.8852, 0.7914, 0.8178, 0.8306, 0.9553, 0.853...   \n",
       "29                19  [0.9032, 0.7191, 0.9246, 0.8983, 0.9654, 0.882...   \n",
       "30                19  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "31                11  [0.7063, 0.6247, 0.5737, 0.5031, 0.5352, 0.308...   \n",
       "32                25  [0.5034, 0.6535, 0.5797, 0.6585, 0.4912, 0.553...   \n",
       "33                 8  [0.0477, 0.0706, 0.0895, 0.0552, 0.0802, 0.095...   \n",
       "34                19  [0.3659, 0.5356, 0.1837, 0.2444, 0.3815, 0.278...   \n",
       "35                 9  [0.1089, 0.1436, 0.2846, 0.1857, 0.1013, 0.247...   \n",
       "36                20  [0.1431, 0.1486, 0.2245, 0.2322, 0.1986, 0.189...   \n",
       "37                20  [0.671, 0.8216, 0.7931, 0.866, 0.7464, 0.8773,...   \n",
       "38                20  [0.3813, 0.2917, 0.3159, 0.317, 0.3685, 0.4289...   \n",
       "39                20  [0.1975, 0.0815, 0.0802, 0.1425, 0.1285, 0.098...   \n",
       "40                23  [0.6891, 0.7314, 0.5767, 0.7744, 0.6925, 0.495...   \n",
       "41                20  [0.0763, 0.0722, 0.079, 0.0898, 0.1169, 0.1579...   \n",
       "42                 8  [0.1987, 0.168, 0.3182, 0.2966, 0.1819, 0.2774...   \n",
       "43                13  [0.8766, 0.8664, 0.8346, 0.9139, 0.8224, 0.894...   \n",
       "44                 9  [0.9031, 0.8984, 0.9223, 0.9174, 0.8681, 0.963...   \n",
       "45                20  [0.4864, 0.4306, 0.5392, 0.49, 0.4367, 0.3857,...   \n",
       "46                 5  [0.6085, 0.5209, 0.4636, 0.5055, 0.5363, 0.566...   \n",
       "47                24  [0.4327, 0.5271, 0.3284, 0.5078, 0.3317, 0.484...   \n",
       "48                 1  [0.1911, 0.3117, 0.1464, 0.1788, 0.3068, 0.217...   \n",
       "49                 1  [0.4289, 0.8673, 0.6367, 0.6199, 0.6508, 0.630...   \n",
       "50                12  [0.4313, 0.5386, 0.7201, 0.3662, 0.2277, 0.532...   \n",
       "51                14  [0.5225, 0.4785, 0.6607, 0.5802, 0.6298, 0.548...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.7911, 0.6794, 0.8135, 0.7889, 0.8718, 0.807...  0.038462  0.631157  \n",
       "1   [0.3434, 0.3859, 0.577, 0.4909, 0.6102, 0.6093...       NaN       NaN  \n",
       "2   [0.6117, 0.6037, 0.7366, 0.7652, 0.8527, 0.763...       NaN       NaN  \n",
       "3   [0.437, 0.5308, 0.5656, 0.4451, 0.3276, 0.4555...       NaN       NaN  \n",
       "4   [0.8579, 0.8723, 0.8607, 0.8725, 0.8852, 0.921...       NaN       NaN  \n",
       "5   [0.8908, 0.9027, 0.8928, 0.8475, 0.8791, 0.797...       NaN       NaN  \n",
       "6   [0.1373, 0.3413, 0.1736, 0.2862, 0.3432, 0.307...       NaN       NaN  \n",
       "7   [0.3094, 0.5842, 0.3941, 0.5869, 0.5691, 0.417...       NaN       NaN  \n",
       "8   [0.5332, 0.6963, 0.5, 0.6522, 0.6997, 0.5349, ...       NaN       NaN  \n",
       "9   [0.1064, 0.0801, 0.0579, 0.1162, 0.0585, 0.125...       NaN       NaN  \n",
       "10  [0.1862, 0.1381, 0.1912, 0.252, 0.2479, 0.249,...       NaN       NaN  \n",
       "11  [0.4177, 0.2803, 0.362, 0.4088, 0.2598, 0.3349...       NaN       NaN  \n",
       "12  [0.119, 0.0815, 0.0485, 0.046, 0.1066, 0.0678,...       NaN       NaN  \n",
       "13  [0.107, 0.1263, 0.182, 0.1917, 0.1427, 0.0772,...       NaN       NaN  \n",
       "14  [0.311, 0.4976, 0.2368, 0.2123, 0.2719, 0.2888...       NaN       NaN  \n",
       "15  [0.6388, 0.531, 0.3523, 0.5181, 0.5428, 0.3764...       NaN       NaN  \n",
       "16  [0.8582, 0.8972, 0.9029, 0.7789, 0.7827, 0.785...       NaN       NaN  \n",
       "17  [0.5832, 0.6032, 0.6251, 0.3746, 0.398, 0.4706...       NaN       NaN  \n",
       "18  [0.8588, 0.5789, 0.8802, 0.8422, 0.6449, 0.838...       NaN       NaN  \n",
       "19  [0.8019, 0.4214, 0.8629, 0.6825, 0.6092, 0.748...       NaN       NaN  \n",
       "20  [0.7386, 0.572, 0.6268, 0.4714, 0.8027, 0.6667...       NaN       NaN  \n",
       "21  [0.8244, 0.6854, 0.8023, 0.5208, 0.8512, 0.853...       NaN       NaN  \n",
       "22  [0.8646, 0.7347, 0.7981, 0.6407, 0.8004, 0.681...       NaN       NaN  \n",
       "23  [0.3746, 0.3975, 0.5334, 0.4881, 0.5174, 0.305...       NaN       NaN  \n",
       "24  [0.9218, 0.9034, 0.9199, 0.88, 0.8889, 0.8868,...       NaN       NaN  \n",
       "25  [0.7936, 0.8233, 0.3719, 0.5717, 0.693, 0.5953...       NaN       NaN  \n",
       "26  [0.9303, 0.929, 0.7466, 0.8064, 0.8893, 0.827,...       NaN       NaN  \n",
       "27  [0.4337, 0.5005, 0.3829, 0.4476, 0.1777, 0.417...       NaN       NaN  \n",
       "28  [0.1148, 0.2086, 0.1822, 0.1694, 0.0447, 0.146...       NaN       NaN  \n",
       "29  [0.0968, 0.2809, 0.0754, 0.1017, 0.0346, 0.117...       NaN       NaN  \n",
       "30  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       NaN       NaN  \n",
       "31  [0.2937, 0.3753, 0.4263, 0.4969, 0.4648, 0.691...       NaN       NaN  \n",
       "32  [0.4966, 0.3465, 0.4203, 0.3415, 0.5088, 0.446...       NaN       NaN  \n",
       "33  [0.9523, 0.9294, 0.9105, 0.9448, 0.9198, 0.905...       NaN       NaN  \n",
       "34  [0.6341, 0.4644, 0.8163, 0.7556, 0.6185, 0.721...       NaN       NaN  \n",
       "35  [0.8911, 0.8564, 0.7154, 0.8143, 0.8987, 0.752...       NaN       NaN  \n",
       "36  [0.8569, 0.8514, 0.7755, 0.7678, 0.8014, 0.810...       NaN       NaN  \n",
       "37  [0.329, 0.1784, 0.2069, 0.134, 0.2536, 0.1227,...       NaN       NaN  \n",
       "38  [0.6187, 0.7083, 0.6841, 0.683, 0.6315, 0.5711...       NaN       NaN  \n",
       "39  [0.8025, 0.9185, 0.9198, 0.8575, 0.8715, 0.902...       NaN       NaN  \n",
       "40  [0.3109, 0.2686, 0.4233, 0.2256, 0.3075, 0.504...       NaN       NaN  \n",
       "41  [0.9237, 0.9278, 0.921, 0.9102, 0.8831, 0.8421...       NaN       NaN  \n",
       "42  [0.8013, 0.832, 0.6818, 0.7034, 0.8181, 0.7226...       NaN       NaN  \n",
       "43  [0.1234, 0.1336, 0.1654, 0.0861, 0.1776, 0.105...       NaN       NaN  \n",
       "44  [0.0969, 0.1016, 0.0777, 0.0826, 0.1319, 0.036...       NaN       NaN  \n",
       "45  [0.5136, 0.5694, 0.4608, 0.51, 0.5633, 0.6143,...       NaN       NaN  \n",
       "46  [0.3915, 0.4791, 0.5364, 0.4945, 0.4637, 0.433...       NaN       NaN  \n",
       "47  [0.5673, 0.4729, 0.6716, 0.4922, 0.6683, 0.515...       NaN       NaN  \n",
       "48  [0.8089, 0.6883, 0.8536, 0.8212, 0.6932, 0.783...       NaN       NaN  \n",
       "49  [0.5711, 0.1327, 0.3633, 0.3801, 0.3492, 0.369...       NaN       NaN  \n",
       "50  [0.5687, 0.4614, 0.2799, 0.6338, 0.7723, 0.467...       NaN       NaN  \n",
       "51  [0.4775, 0.5215, 0.3393, 0.4198, 0.3702, 0.451...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 : Training: loss:  0.61920094\n",
      "42 : Training: loss:  0.62100536\n",
      "43 : Training: loss:  0.61620194\n",
      "44 : Training: loss:  0.60918313\n",
      "45 : Training: loss:  0.6052737\n",
      "46 : Training: loss:  0.6182846\n",
      "47 : Training: loss:  0.59938204\n",
      "48 : Training: loss:  0.6194034\n",
      "49 : Training: loss:  0.58511853\n",
      "50 : Training: loss:  0.60338944\n",
      "51 : Training: loss:  0.59903115\n",
      "52 : Training: loss:  0.5914879\n",
      "53 : Training: loss:  0.59200746\n",
      "54 : Training: loss:  0.60962605\n",
      "55 : Training: loss:  0.5960706\n",
      "56 : Training: loss:  0.5965242\n",
      "57 : Training: loss:  0.5867036\n",
      "58 : Training: loss:  0.5907285\n",
      "59 : Training: loss:  0.56980044\n",
      "60 : Training: loss:  0.57791597\n",
      "Validation: Loss:  0.5729713  Accuracy:  0.057692308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.4131, 0.4613, 0.3809, 0.4261, 0.3957, 0.401...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1993, 0.3196, 0.1796, 0.2034, 0.1283, 0.189...</td>\n",
       "      <td>[0.8007, 0.6804, 0.8204, 0.7966, 0.8717, 0.810...</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.572971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.4524, 0.4771, 0.3925, 0.4203, 0.3891, 0.370...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6419, 0.6101, 0.4131, 0.4986, 0.3838, 0.385...</td>\n",
       "      <td>[0.3581, 0.3899, 0.5869, 0.5014, 0.6162, 0.614...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.4071, 0.4601, 0.3938, 0.412, 0.3961, 0.4128...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3718, 0.3972, 0.2573, 0.2263, 0.1468, 0.232...</td>\n",
       "      <td>[0.6282, 0.6028, 0.7427, 0.7737, 0.8532, 0.767...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.412, 0.4274, 0.4284, 0.4281, 0.3611, 0.3848...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.5411, 0.4665, 0.4194, 0.5375, 0.6679, 0.539...</td>\n",
       "      <td>[0.4589, 0.5335, 0.5806, 0.4625, 0.3321, 0.460...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.4021, 0.4453, 0.3561, 0.4134, 0.3667, 0.393...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1318, 0.1256, 0.1329, 0.1197, 0.1139, 0.078...</td>\n",
       "      <td>[0.8682, 0.8744, 0.8671, 0.8803, 0.8861, 0.922...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.4099, 0.4614, 0.4012, 0.4266, 0.386, 0.4297...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1051, 0.0991, 0.104, 0.1462, 0.1186, 0.2023...</td>\n",
       "      <td>[0.8949, 0.9009, 0.896, 0.8538, 0.8814, 0.7977...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.4921, 0.4685, 0.3424, 0.4283, 0.4864, 0.379...</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.8564, 0.6661, 0.8236, 0.708, 0.649, 0.6973,...</td>\n",
       "      <td>[0.1436, 0.3339, 0.1764, 0.292, 0.351, 0.3027,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.4178, 0.4308, 0.3946, 0.4234, 0.4059, 0.446...</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.6815, 0.4218, 0.6025, 0.406, 0.4259, 0.5826...</td>\n",
       "      <td>[0.3185, 0.5782, 0.3975, 0.594, 0.5741, 0.4174...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.4258, 0.4711, 0.4078, 0.4343, 0.4378, 0.478...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.4617, 0.3089, 0.499, 0.3449, 0.2961, 0.4597...</td>\n",
       "      <td>[0.5383, 0.6911, 0.501, 0.6551, 0.7039, 0.5403...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.4653, 0.4547, 0.5198, 0.4004, 0.4465, 0.384...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.8872, 0.9192, 0.9393, 0.8789, 0.9405, 0.869...</td>\n",
       "      <td>[0.1128, 0.0808, 0.0607, 0.1211, 0.0595, 0.130...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.4456, 0.5261, 0.404, 0.4181, 0.4341, 0.442,...</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.7961, 0.8623, 0.8032, 0.7322, 0.752, 0.741,...</td>\n",
       "      <td>[0.2039, 0.1377, 0.1968, 0.2678, 0.248, 0.259,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.4192, 0.4721, 0.44, 0.4508, 0.4224, 0.4567,...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.5595, 0.7185, 0.6277, 0.5755, 0.7424, 0.655...</td>\n",
       "      <td>[0.4405, 0.2815, 0.3723, 0.4245, 0.2576, 0.344...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.4507, 0.4609, 0.4018, 0.5122, 0.4888, 0.433...</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.8749, 0.9194, 0.9505, 0.9516, 0.8901, 0.931...</td>\n",
       "      <td>[0.1251, 0.0806, 0.0495, 0.0484, 0.1099, 0.068...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.4967, 0.4751, 0.4968, 0.4871, 0.3932, 0.547...</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.8864, 0.8752, 0.8147, 0.797, 0.8561, 0.9243...</td>\n",
       "      <td>[0.1136, 0.1248, 0.1853, 0.203, 0.1439, 0.0757...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.3972, 0.4236, 0.4246, 0.4656, 0.3979, 0.384...</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.6717, 0.5061, 0.7562, 0.7769, 0.7284, 0.704...</td>\n",
       "      <td>[0.3283, 0.4939, 0.2438, 0.2231, 0.2716, 0.295...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.4038, 0.4467, 0.4355, 0.4081, 0.4431, 0.402...</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.3445, 0.4692, 0.6362, 0.4662, 0.46, 0.6111,...</td>\n",
       "      <td>[0.6555, 0.5308, 0.3638, 0.5338, 0.54, 0.3889,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.4059, 0.4527, 0.3879, 0.4333, 0.3691, 0.418...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1364, 0.1033, 0.0937, 0.2131, 0.2124, 0.214...</td>\n",
       "      <td>[0.8636, 0.8967, 0.9063, 0.7869, 0.7876, 0.785...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.4232, 0.4624, 0.4188, 0.4231, 0.3988, 0.426...</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4071, 0.4013, 0.3654, 0.6138, 0.5923, 0.525...</td>\n",
       "      <td>[0.5929, 0.5987, 0.6346, 0.3862, 0.4077, 0.474...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.412, 0.4531, 0.4032, 0.43, 0.381, 0.4122, 0...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.136, 0.4172, 0.1169, 0.1526, 0.3554, 0.1572...</td>\n",
       "      <td>[0.864, 0.5828, 0.8831, 0.8474, 0.6446, 0.8428...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.4049, 0.4657, 0.434, 0.433, 0.4046, 0.4082,...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1909, 0.5756, 0.1338, 0.3072, 0.3934, 0.242...</td>\n",
       "      <td>[0.8091, 0.4244, 0.8662, 0.6928, 0.6066, 0.757...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.4164, 0.4571, 0.3744, 0.4182, 0.422, 0.4687...</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.2534, 0.4303, 0.3641, 0.5181, 0.1947, 0.330...</td>\n",
       "      <td>[0.7466, 0.5697, 0.6359, 0.4819, 0.8053, 0.669...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.399, 0.4629, 0.3498, 0.4421, 0.367, 0.4162,...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1659, 0.3149, 0.1919, 0.464, 0.1463, 0.1464...</td>\n",
       "      <td>[0.8341, 0.6851, 0.8081, 0.536, 0.8537, 0.8536...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.4005, 0.458, 0.3696, 0.4314, 0.3612, 0.3993...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1292, 0.2671, 0.198, 0.3488, 0.1964, 0.3158...</td>\n",
       "      <td>[0.8708, 0.7329, 0.802, 0.6512, 0.8036, 0.6842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.4425, 0.4681, 0.4173, 0.4293, 0.385, 0.4565...</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.6126, 0.6045, 0.4647, 0.5005, 0.4814, 0.682...</td>\n",
       "      <td>[0.3874, 0.3955, 0.5353, 0.4995, 0.5186, 0.317...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.4016, 0.4489, 0.3806, 0.4235, 0.3622, 0.403...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0774, 0.0979, 0.0792, 0.1171, 0.1087, 0.110...</td>\n",
       "      <td>[0.9226, 0.9021, 0.9208, 0.8829, 0.8913, 0.889...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.4205, 0.4576, 0.4093, 0.4391, 0.4206, 0.442...</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1994, 0.1788, 0.6191, 0.4175, 0.2999, 0.403...</td>\n",
       "      <td>[0.8006, 0.8212, 0.3809, 0.5825, 0.7001, 0.596...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.4034, 0.4433, 0.3683, 0.4317, 0.3553, 0.395...</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.0668, 0.0714, 0.2479, 0.185, 0.1075, 0.1736...</td>\n",
       "      <td>[0.9332, 0.9286, 0.7521, 0.815, 0.8925, 0.8264...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.4206, 0.4063, 0.3969, 0.4446, 0.358, 0.3873...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.5466, 0.5068, 0.6069, 0.5342, 0.818, 0.5846...</td>\n",
       "      <td>[0.4534, 0.4932, 0.3931, 0.4658, 0.182, 0.4154...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.4986, 0.4206, 0.4628, 0.4689, 0.3389, 0.447...</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.8771, 0.7959, 0.8116, 0.8186, 0.9541, 0.856...</td>\n",
       "      <td>[0.1229, 0.2041, 0.1884, 0.1814, 0.0459, 0.143...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.4355, 0.3981, 0.456, 0.4797, 0.376, 0.4352,...</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8962, 0.7243, 0.9219, 0.8907, 0.9645, 0.883...</td>\n",
       "      <td>[0.1038, 0.2757, 0.0781, 0.1093, 0.0355, 0.116...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.4358, 0.4866, 0.593, 0.497, 0.5195, 0.5081,...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.4809, 0.4767, 0.3967, 0.4411, 0.3681, 0.385...</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.6978, 0.6234, 0.5687, 0.4955, 0.5222, 0.313...</td>\n",
       "      <td>[0.3022, 0.3766, 0.4313, 0.5045, 0.4778, 0.686...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.434, 0.4604, 0.3953, 0.43, 0.434, 0.3895, 0...</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.4938, 0.6602, 0.5751, 0.6505, 0.4786, 0.554...</td>\n",
       "      <td>[0.5062, 0.3398, 0.4249, 0.3495, 0.5214, 0.445...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.3679, 0.4395, 0.3431, 0.4002, 0.3405, 0.382...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.044, 0.0724, 0.0858, 0.0513, 0.0791, 0.0937...</td>\n",
       "      <td>[0.956, 0.9276, 0.9142, 0.9487, 0.9209, 0.9063...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.3845, 0.4588, 0.4236, 0.4221, 0.4062, 0.403...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3464, 0.5424, 0.1786, 0.2335, 0.3831, 0.269...</td>\n",
       "      <td>[0.6536, 0.4576, 0.8214, 0.7665, 0.6169, 0.730...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.3799, 0.4413, 0.3493, 0.4132, 0.3452, 0.389...</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1032, 0.1437, 0.2769, 0.1774, 0.0993, 0.245...</td>\n",
       "      <td>[0.8968, 0.8563, 0.7231, 0.8226, 0.9007, 0.754...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.3959, 0.4474, 0.3613, 0.4247, 0.3346, 0.390...</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.1386, 0.1447, 0.2183, 0.225, 0.1932, 0.1896...</td>\n",
       "      <td>[0.8614, 0.8553, 0.7817, 0.775, 0.8068, 0.8104...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.4545, 0.4764, 0.431, 0.5148, 0.3674, 0.4596...</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.6553, 0.8215, 0.7889, 0.8571, 0.7372, 0.878...</td>\n",
       "      <td>[0.3447, 0.1785, 0.2111, 0.1429, 0.2628, 0.121...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.4346, 0.4421, 0.385, 0.4735, 0.3849, 0.4262...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.3694, 0.295, 0.3092, 0.307, 0.3602, 0.4414,...</td>\n",
       "      <td>[0.6306, 0.705, 0.6908, 0.693, 0.6398, 0.5586,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.3952, 0.4414, 0.3751, 0.4455, 0.3612, 0.409...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.1909, 0.0807, 0.0782, 0.1378, 0.1257, 0.099...</td>\n",
       "      <td>[0.8091, 0.9193, 0.9218, 0.8622, 0.8743, 0.900...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.518, 0.4865, 0.3538, 0.496, 0.4251, 0.384, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.6762, 0.7295, 0.5653, 0.7657, 0.6855, 0.504...</td>\n",
       "      <td>[0.3238, 0.2705, 0.4347, 0.2343, 0.3145, 0.495...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.3822, 0.4418, 0.37, 0.4222, 0.3483, 0.4091,...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.0725, 0.0721, 0.0758, 0.0863, 0.1138, 0.160...</td>\n",
       "      <td>[0.9275, 0.9279, 0.9242, 0.9137, 0.8862, 0.839...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.3894, 0.4445, 0.3382, 0.4141, 0.3446, 0.382...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1915, 0.1669, 0.3101, 0.285, 0.1772, 0.2763...</td>\n",
       "      <td>[0.8085, 0.8331, 0.6899, 0.715, 0.8228, 0.7237...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.4848, 0.4806, 0.4063, 0.5118, 0.4411, 0.422...</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.8662, 0.865, 0.8282, 0.9055, 0.8181, 0.8936...</td>\n",
       "      <td>[0.1338, 0.135, 0.1718, 0.0945, 0.1819, 0.1064...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.51, 0.4712, 0.4004, 0.5054, 0.4412, 0.4113,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.8958, 0.8966, 0.9191, 0.9096, 0.8627, 0.963...</td>\n",
       "      <td>[0.1042, 0.1034, 0.0809, 0.0904, 0.1373, 0.036...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.4289, 0.4485, 0.3237, 0.4555, 0.3425, 0.363...</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.4712, 0.424, 0.5282, 0.4724, 0.429, 0.3883,...</td>\n",
       "      <td>[0.5288, 0.576, 0.4718, 0.5276, 0.571, 0.6117,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.4508, 0.4714, 0.4522, 0.4564, 0.3879, 0.484...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.6044, 0.5217, 0.4622, 0.5007, 0.5304, 0.562...</td>\n",
       "      <td>[0.3956, 0.4783, 0.5378, 0.4993, 0.4696, 0.437...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.4366, 0.4503, 0.3958, 0.3885, 0.4024, 0.388...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.4186, 0.5266, 0.3204, 0.4949, 0.325, 0.4815...</td>\n",
       "      <td>[0.5814, 0.4734, 0.6796, 0.5051, 0.675, 0.5185...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.3902, 0.4486, 0.3746, 0.4132, 0.3556, 0.367...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1726, 0.3153, 0.1411, 0.1646, 0.3052, 0.212...</td>\n",
       "      <td>[0.8274, 0.6847, 0.8589, 0.8354, 0.6948, 0.787...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.4446, 0.5086, 0.4146, 0.4474, 0.4218, 0.397...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4006, 0.8693, 0.6285, 0.6011, 0.6527, 0.621...</td>\n",
       "      <td>[0.5994, 0.1307, 0.3715, 0.3989, 0.3473, 0.378...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.4426, 0.4346, 0.382, 0.4289, 0.4799, 0.4622...</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.4221, 0.5378, 0.7128, 0.3574, 0.2259, 0.531...</td>\n",
       "      <td>[0.5779, 0.4622, 0.2872, 0.6426, 0.7741, 0.468...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.4356, 0.4574, 0.46, 0.4382, 0.442, 0.3503, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.511, 0.4818, 0.6523, 0.5684, 0.6247, 0.5407...</td>\n",
       "      <td>[0.489, 0.5182, 0.3477, 0.4316, 0.3753, 0.4593...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.4131, 0.4613, 0.3809, 0.4261, 0.3957, 0.401...               0   \n",
       "1   [0.4524, 0.4771, 0.3925, 0.4203, 0.3891, 0.370...               0   \n",
       "2   [0.4071, 0.4601, 0.3938, 0.412, 0.3961, 0.4128...               0   \n",
       "3   [0.412, 0.4274, 0.4284, 0.4281, 0.3611, 0.3848...               1   \n",
       "4   [0.4021, 0.4453, 0.3561, 0.4134, 0.3667, 0.393...               1   \n",
       "5   [0.4099, 0.4614, 0.4012, 0.4266, 0.386, 0.4297...               2   \n",
       "6   [0.4921, 0.4685, 0.3424, 0.4283, 0.4864, 0.379...               3   \n",
       "7   [0.4178, 0.4308, 0.3946, 0.4234, 0.4059, 0.446...               3   \n",
       "8   [0.4258, 0.4711, 0.4078, 0.4343, 0.4378, 0.478...               3   \n",
       "9   [0.4653, 0.4547, 0.5198, 0.4004, 0.4465, 0.384...               4   \n",
       "10  [0.4456, 0.5261, 0.404, 0.4181, 0.4341, 0.442,...               4   \n",
       "11  [0.4192, 0.4721, 0.44, 0.4508, 0.4224, 0.4567,...               5   \n",
       "12  [0.4507, 0.4609, 0.4018, 0.5122, 0.4888, 0.433...               6   \n",
       "13  [0.4967, 0.4751, 0.4968, 0.4871, 0.3932, 0.547...               7   \n",
       "14  [0.3972, 0.4236, 0.4246, 0.4656, 0.3979, 0.384...               8   \n",
       "15  [0.4038, 0.4467, 0.4355, 0.4081, 0.4431, 0.402...               8   \n",
       "16  [0.4059, 0.4527, 0.3879, 0.4333, 0.3691, 0.418...               9   \n",
       "17  [0.4232, 0.4624, 0.4188, 0.4231, 0.3988, 0.426...               9   \n",
       "18  [0.412, 0.4531, 0.4032, 0.43, 0.381, 0.4122, 0...              10   \n",
       "19  [0.4049, 0.4657, 0.434, 0.433, 0.4046, 0.4082,...              10   \n",
       "20  [0.4164, 0.4571, 0.3744, 0.4182, 0.422, 0.4687...              11   \n",
       "21  [0.399, 0.4629, 0.3498, 0.4421, 0.367, 0.4162,...              11   \n",
       "22  [0.4005, 0.458, 0.3696, 0.4314, 0.3612, 0.3993...              12   \n",
       "23  [0.4425, 0.4681, 0.4173, 0.4293, 0.385, 0.4565...              13   \n",
       "24  [0.4016, 0.4489, 0.3806, 0.4235, 0.3622, 0.403...              13   \n",
       "25  [0.4205, 0.4576, 0.4093, 0.4391, 0.4206, 0.442...              14   \n",
       "26  [0.4034, 0.4433, 0.3683, 0.4317, 0.3553, 0.395...              14   \n",
       "27  [0.4206, 0.4063, 0.3969, 0.4446, 0.358, 0.3873...              15   \n",
       "28  [0.4986, 0.4206, 0.4628, 0.4689, 0.3389, 0.447...              15   \n",
       "29  [0.4355, 0.3981, 0.456, 0.4797, 0.376, 0.4352,...              15   \n",
       "30  [0.4358, 0.4866, 0.593, 0.497, 0.5195, 0.5081,...              16   \n",
       "31  [0.4809, 0.4767, 0.3967, 0.4411, 0.3681, 0.385...              17   \n",
       "32  [0.434, 0.4604, 0.3953, 0.43, 0.434, 0.3895, 0...              17   \n",
       "33  [0.3679, 0.4395, 0.3431, 0.4002, 0.3405, 0.382...              18   \n",
       "34  [0.3845, 0.4588, 0.4236, 0.4221, 0.4062, 0.403...              19   \n",
       "35  [0.3799, 0.4413, 0.3493, 0.4132, 0.3452, 0.389...              20   \n",
       "36  [0.3959, 0.4474, 0.3613, 0.4247, 0.3346, 0.390...              21   \n",
       "37  [0.4545, 0.4764, 0.431, 0.5148, 0.3674, 0.4596...              21   \n",
       "38  [0.4346, 0.4421, 0.385, 0.4735, 0.3849, 0.4262...              22   \n",
       "39  [0.3952, 0.4414, 0.3751, 0.4455, 0.3612, 0.409...              22   \n",
       "40  [0.518, 0.4865, 0.3538, 0.496, 0.4251, 0.384, ...              22   \n",
       "41  [0.3822, 0.4418, 0.37, 0.4222, 0.3483, 0.4091,...              22   \n",
       "42  [0.3894, 0.4445, 0.3382, 0.4141, 0.3446, 0.382...              23   \n",
       "43  [0.4848, 0.4806, 0.4063, 0.5118, 0.4411, 0.422...              23   \n",
       "44  [0.51, 0.4712, 0.4004, 0.5054, 0.4412, 0.4113,...              23   \n",
       "45  [0.4289, 0.4485, 0.3237, 0.4555, 0.3425, 0.363...              23   \n",
       "46  [0.4508, 0.4714, 0.4522, 0.4564, 0.3879, 0.484...              24   \n",
       "47  [0.4366, 0.4503, 0.3958, 0.3885, 0.4024, 0.388...              24   \n",
       "48  [0.3902, 0.4486, 0.3746, 0.4132, 0.3556, 0.367...              25   \n",
       "49  [0.4446, 0.5086, 0.4146, 0.4474, 0.4218, 0.397...              25   \n",
       "50  [0.4426, 0.4346, 0.382, 0.4289, 0.4799, 0.4622...              26   \n",
       "51  [0.4356, 0.4574, 0.46, 0.4382, 0.442, 0.3503, ...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  1  [0.1993, 0.3196, 0.1796, 0.2034, 0.1283, 0.189...   \n",
       "1                 23  [0.6419, 0.6101, 0.4131, 0.4986, 0.3838, 0.385...   \n",
       "2                  1  [0.3718, 0.3972, 0.2573, 0.2263, 0.1468, 0.232...   \n",
       "3                 19  [0.5411, 0.4665, 0.4194, 0.5375, 0.6679, 0.539...   \n",
       "4                  8  [0.1318, 0.1256, 0.1329, 0.1197, 0.1139, 0.078...   \n",
       "5                  1  [0.1051, 0.0991, 0.104, 0.1462, 0.1186, 0.2023...   \n",
       "6                 21  [0.8564, 0.6661, 0.8236, 0.708, 0.649, 0.6973,...   \n",
       "7                 20  [0.6815, 0.4218, 0.6025, 0.406, 0.4259, 0.5826...   \n",
       "8                  8  [0.4617, 0.3089, 0.499, 0.3449, 0.2961, 0.4597...   \n",
       "9                 26  [0.8872, 0.9192, 0.9393, 0.8789, 0.9405, 0.869...   \n",
       "10                24  [0.7961, 0.8623, 0.8032, 0.7322, 0.752, 0.741,...   \n",
       "11                 6  [0.5595, 0.7185, 0.6277, 0.5755, 0.7424, 0.655...   \n",
       "12                25  [0.8749, 0.9194, 0.9505, 0.9516, 0.8901, 0.931...   \n",
       "13                10  [0.8864, 0.8752, 0.8147, 0.797, 0.8561, 0.9243...   \n",
       "14                20  [0.6717, 0.5061, 0.7562, 0.7769, 0.7284, 0.704...   \n",
       "15                26  [0.3445, 0.4692, 0.6362, 0.4662, 0.46, 0.6111,...   \n",
       "16                 8  [0.1364, 0.1033, 0.0937, 0.2131, 0.2124, 0.214...   \n",
       "17                26  [0.4071, 0.4013, 0.3654, 0.6138, 0.5923, 0.525...   \n",
       "18                 8  [0.136, 0.4172, 0.1169, 0.1526, 0.3554, 0.1572...   \n",
       "19                 8  [0.1909, 0.5756, 0.1338, 0.3072, 0.3934, 0.242...   \n",
       "20                 5  [0.2534, 0.4303, 0.3641, 0.5181, 0.1947, 0.330...   \n",
       "21                 1  [0.1659, 0.3149, 0.1919, 0.464, 0.1463, 0.1464...   \n",
       "22                 1  [0.1292, 0.2671, 0.198, 0.3488, 0.1964, 0.3158...   \n",
       "23                24  [0.6126, 0.6045, 0.4647, 0.5005, 0.4814, 0.682...   \n",
       "24                 1  [0.0774, 0.0979, 0.0792, 0.1171, 0.1087, 0.110...   \n",
       "25                25  [0.1994, 0.1788, 0.6191, 0.4175, 0.2999, 0.403...   \n",
       "26                20  [0.0668, 0.0714, 0.2479, 0.185, 0.1075, 0.1736...   \n",
       "27                18  [0.5466, 0.5068, 0.6069, 0.5342, 0.818, 0.5846...   \n",
       "28                18  [0.8771, 0.7959, 0.8116, 0.8186, 0.9541, 0.856...   \n",
       "29                19  [0.8962, 0.7243, 0.9219, 0.8907, 0.9645, 0.883...   \n",
       "30                19  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "31                11  [0.6978, 0.6234, 0.5687, 0.4955, 0.5222, 0.313...   \n",
       "32                25  [0.4938, 0.6602, 0.5751, 0.6505, 0.4786, 0.554...   \n",
       "33                 8  [0.044, 0.0724, 0.0858, 0.0513, 0.0791, 0.0937...   \n",
       "34                 1  [0.3464, 0.5424, 0.1786, 0.2335, 0.3831, 0.269...   \n",
       "35                 8  [0.1032, 0.1437, 0.2769, 0.1774, 0.0993, 0.245...   \n",
       "36                20  [0.1386, 0.1447, 0.2183, 0.225, 0.1932, 0.1896...   \n",
       "37                20  [0.6553, 0.8215, 0.7889, 0.8571, 0.7372, 0.878...   \n",
       "38                20  [0.3694, 0.295, 0.3092, 0.307, 0.3602, 0.4414,...   \n",
       "39                20  [0.1909, 0.0807, 0.0782, 0.1378, 0.1257, 0.099...   \n",
       "40                12  [0.6762, 0.7295, 0.5653, 0.7657, 0.6855, 0.504...   \n",
       "41                20  [0.0725, 0.0721, 0.0758, 0.0863, 0.1138, 0.160...   \n",
       "42                 8  [0.1915, 0.1669, 0.3101, 0.285, 0.1772, 0.2763...   \n",
       "43                13  [0.8662, 0.865, 0.8282, 0.9055, 0.8181, 0.8936...   \n",
       "44                23  [0.8958, 0.8966, 0.9191, 0.9096, 0.8627, 0.963...   \n",
       "45                20  [0.4712, 0.424, 0.5282, 0.4724, 0.429, 0.3883,...   \n",
       "46                 9  [0.6044, 0.5217, 0.4622, 0.5007, 0.5304, 0.562...   \n",
       "47                24  [0.4186, 0.5266, 0.3204, 0.4949, 0.325, 0.4815...   \n",
       "48                 1  [0.1726, 0.3153, 0.1411, 0.1646, 0.3052, 0.212...   \n",
       "49                 1  [0.4006, 0.8693, 0.6285, 0.6011, 0.6527, 0.621...   \n",
       "50                12  [0.4221, 0.5378, 0.7128, 0.3574, 0.2259, 0.531...   \n",
       "51                26  [0.511, 0.4818, 0.6523, 0.5684, 0.6247, 0.5407...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8007, 0.6804, 0.8204, 0.7966, 0.8717, 0.810...  0.057692  0.572971  \n",
       "1   [0.3581, 0.3899, 0.5869, 0.5014, 0.6162, 0.614...       NaN       NaN  \n",
       "2   [0.6282, 0.6028, 0.7427, 0.7737, 0.8532, 0.767...       NaN       NaN  \n",
       "3   [0.4589, 0.5335, 0.5806, 0.4625, 0.3321, 0.460...       NaN       NaN  \n",
       "4   [0.8682, 0.8744, 0.8671, 0.8803, 0.8861, 0.922...       NaN       NaN  \n",
       "5   [0.8949, 0.9009, 0.896, 0.8538, 0.8814, 0.7977...       NaN       NaN  \n",
       "6   [0.1436, 0.3339, 0.1764, 0.292, 0.351, 0.3027,...       NaN       NaN  \n",
       "7   [0.3185, 0.5782, 0.3975, 0.594, 0.5741, 0.4174...       NaN       NaN  \n",
       "8   [0.5383, 0.6911, 0.501, 0.6551, 0.7039, 0.5403...       NaN       NaN  \n",
       "9   [0.1128, 0.0808, 0.0607, 0.1211, 0.0595, 0.130...       NaN       NaN  \n",
       "10  [0.2039, 0.1377, 0.1968, 0.2678, 0.248, 0.259,...       NaN       NaN  \n",
       "11  [0.4405, 0.2815, 0.3723, 0.4245, 0.2576, 0.344...       NaN       NaN  \n",
       "12  [0.1251, 0.0806, 0.0495, 0.0484, 0.1099, 0.068...       NaN       NaN  \n",
       "13  [0.1136, 0.1248, 0.1853, 0.203, 0.1439, 0.0757...       NaN       NaN  \n",
       "14  [0.3283, 0.4939, 0.2438, 0.2231, 0.2716, 0.295...       NaN       NaN  \n",
       "15  [0.6555, 0.5308, 0.3638, 0.5338, 0.54, 0.3889,...       NaN       NaN  \n",
       "16  [0.8636, 0.8967, 0.9063, 0.7869, 0.7876, 0.785...       NaN       NaN  \n",
       "17  [0.5929, 0.5987, 0.6346, 0.3862, 0.4077, 0.474...       NaN       NaN  \n",
       "18  [0.864, 0.5828, 0.8831, 0.8474, 0.6446, 0.8428...       NaN       NaN  \n",
       "19  [0.8091, 0.4244, 0.8662, 0.6928, 0.6066, 0.757...       NaN       NaN  \n",
       "20  [0.7466, 0.5697, 0.6359, 0.4819, 0.8053, 0.669...       NaN       NaN  \n",
       "21  [0.8341, 0.6851, 0.8081, 0.536, 0.8537, 0.8536...       NaN       NaN  \n",
       "22  [0.8708, 0.7329, 0.802, 0.6512, 0.8036, 0.6842...       NaN       NaN  \n",
       "23  [0.3874, 0.3955, 0.5353, 0.4995, 0.5186, 0.317...       NaN       NaN  \n",
       "24  [0.9226, 0.9021, 0.9208, 0.8829, 0.8913, 0.889...       NaN       NaN  \n",
       "25  [0.8006, 0.8212, 0.3809, 0.5825, 0.7001, 0.596...       NaN       NaN  \n",
       "26  [0.9332, 0.9286, 0.7521, 0.815, 0.8925, 0.8264...       NaN       NaN  \n",
       "27  [0.4534, 0.4932, 0.3931, 0.4658, 0.182, 0.4154...       NaN       NaN  \n",
       "28  [0.1229, 0.2041, 0.1884, 0.1814, 0.0459, 0.143...       NaN       NaN  \n",
       "29  [0.1038, 0.2757, 0.0781, 0.1093, 0.0355, 0.116...       NaN       NaN  \n",
       "30  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       NaN       NaN  \n",
       "31  [0.3022, 0.3766, 0.4313, 0.5045, 0.4778, 0.686...       NaN       NaN  \n",
       "32  [0.5062, 0.3398, 0.4249, 0.3495, 0.5214, 0.445...       NaN       NaN  \n",
       "33  [0.956, 0.9276, 0.9142, 0.9487, 0.9209, 0.9063...       NaN       NaN  \n",
       "34  [0.6536, 0.4576, 0.8214, 0.7665, 0.6169, 0.730...       NaN       NaN  \n",
       "35  [0.8968, 0.8563, 0.7231, 0.8226, 0.9007, 0.754...       NaN       NaN  \n",
       "36  [0.8614, 0.8553, 0.7817, 0.775, 0.8068, 0.8104...       NaN       NaN  \n",
       "37  [0.3447, 0.1785, 0.2111, 0.1429, 0.2628, 0.121...       NaN       NaN  \n",
       "38  [0.6306, 0.705, 0.6908, 0.693, 0.6398, 0.5586,...       NaN       NaN  \n",
       "39  [0.8091, 0.9193, 0.9218, 0.8622, 0.8743, 0.900...       NaN       NaN  \n",
       "40  [0.3238, 0.2705, 0.4347, 0.2343, 0.3145, 0.495...       NaN       NaN  \n",
       "41  [0.9275, 0.9279, 0.9242, 0.9137, 0.8862, 0.839...       NaN       NaN  \n",
       "42  [0.8085, 0.8331, 0.6899, 0.715, 0.8228, 0.7237...       NaN       NaN  \n",
       "43  [0.1338, 0.135, 0.1718, 0.0945, 0.1819, 0.1064...       NaN       NaN  \n",
       "44  [0.1042, 0.1034, 0.0809, 0.0904, 0.1373, 0.036...       NaN       NaN  \n",
       "45  [0.5288, 0.576, 0.4718, 0.5276, 0.571, 0.6117,...       NaN       NaN  \n",
       "46  [0.3956, 0.4783, 0.5378, 0.4993, 0.4696, 0.437...       NaN       NaN  \n",
       "47  [0.5814, 0.4734, 0.6796, 0.5051, 0.675, 0.5185...       NaN       NaN  \n",
       "48  [0.8274, 0.6847, 0.8589, 0.8354, 0.6948, 0.787...       NaN       NaN  \n",
       "49  [0.5994, 0.1307, 0.3715, 0.3989, 0.3473, 0.378...       NaN       NaN  \n",
       "50  [0.5779, 0.4622, 0.2872, 0.6426, 0.7741, 0.468...       NaN       NaN  \n",
       "51  [0.489, 0.5182, 0.3477, 0.4316, 0.3753, 0.4593...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 : Training: loss:  0.5764607\n",
      "62 : Training: loss:  0.54253954\n",
      "63 : Training: loss:  0.5771533\n",
      "64 : Training: loss:  0.5661147\n",
      "65 : Training: loss:  0.5493403\n",
      "66 : Training: loss:  0.56602514\n",
      "67 : Training: loss:  0.53924847\n",
      "68 : Training: loss:  0.5467713\n",
      "69 : Training: loss:  0.5420561\n",
      "70 : Training: loss:  0.53070015\n",
      "71 : Training: loss:  0.5312155\n",
      "72 : Training: loss:  0.5763477\n",
      "73 : Training: loss:  0.5273502\n",
      "74 : Training: loss:  0.48795706\n",
      "75 : Training: loss:  0.512969\n",
      "76 : Training: loss:  0.52680564\n",
      "77 : Training: loss:  0.5065078\n",
      "78 : Training: loss:  0.5151712\n",
      "79 : Training: loss:  0.48224053\n",
      "80 : Training: loss:  0.47736117\n",
      "Validation: Loss:  0.5020239  Accuracy:  0.03846154\n",
      "81 : Training: loss:  0.4805682\n",
      "82 : Training: loss:  0.499967\n",
      "83 : Training: loss:  0.46528363\n",
      "84 : Training: loss:  0.45405933\n",
      "85 : Training: loss:  0.47379282\n",
      "86 : Training: loss:  0.5115035\n",
      "87 : Training: loss:  0.5139156\n",
      "88 : Training: loss:  0.4595508\n",
      "89 : Training: loss:  0.45584595\n",
      "90 : Training: loss:  0.4376551\n",
      "91 : Training: loss:  0.42666897\n",
      "92 : Training: loss:  0.4400812\n",
      "93 : Training: loss:  0.39909726\n",
      "94 : Training: loss:  0.43606344\n",
      "95 : Training: loss:  0.46455556\n",
      "96 : Training: loss:  0.4595866\n",
      "97 : Training: loss:  0.49816567\n",
      "98 : Training: loss:  0.45307222\n",
      "99 : Training: loss:  0.3843477\n",
      "100 : Training: loss:  0.3979285\n",
      "Validation: Loss:  0.42313832  Accuracy:  0.03846154\n",
      "101 : Training: loss:  0.4367298\n",
      "102 : Training: loss:  0.4118606\n",
      "103 : Training: loss:  0.4895904\n",
      "104 : Training: loss:  0.38435495\n",
      "105 : Training: loss:  0.39015287\n",
      "106 : Training: loss:  0.44548115\n",
      "107 : Training: loss:  0.39645755\n",
      "108 : Training: loss:  0.40843272\n",
      "109 : Training: loss:  0.3901799\n",
      "110 : Training: loss:  0.37081283\n",
      "111 : Training: loss:  0.39773616\n",
      "112 : Training: loss:  0.42228934\n",
      "113 : Training: loss:  0.33694705\n",
      "114 : Training: loss:  0.38595104\n",
      "115 : Training: loss:  0.34283966\n",
      "116 : Training: loss:  0.33781293\n",
      "117 : Training: loss:  0.3127824\n",
      "118 : Training: loss:  0.34871605\n",
      "119 : Training: loss:  0.34454015\n",
      "120 : Training: loss:  0.32933715\n",
      "Validation: Loss:  0.3537188  Accuracy:  0.057692308\n",
      "121 : Training: loss:  0.33497357\n",
      "122 : Training: loss:  0.3273095\n",
      "123 : Training: loss:  0.3935495\n",
      "124 : Training: loss:  0.34811422\n",
      "125 : Training: loss:  0.35877207\n",
      "126 : Training: loss:  0.31703293\n",
      "127 : Training: loss:  0.29088002\n",
      "128 : Training: loss:  0.3494475\n",
      "129 : Training: loss:  0.2723358\n",
      "130 : Training: loss:  0.36000106\n",
      "131 : Training: loss:  0.2960859\n",
      "132 : Training: loss:  0.3333271\n",
      "133 : Training: loss:  0.31409246\n",
      "134 : Training: loss:  0.30632666\n",
      "135 : Training: loss:  0.33428425\n",
      "136 : Training: loss:  0.30279583\n",
      "137 : Training: loss:  0.2677828\n",
      "138 : Training: loss:  0.29746622\n",
      "139 : Training: loss:  0.24284713\n",
      "140 : Training: loss:  0.27244866\n",
      "Validation: Loss:  0.29850605  Accuracy:  0.057692308\n",
      "141 : Training: loss:  0.26866987\n",
      "142 : Training: loss:  0.26933417\n",
      "143 : Training: loss:  0.27986035\n",
      "144 : Training: loss:  0.31609407\n",
      "145 : Training: loss:  0.24410066\n",
      "146 : Training: loss:  0.2867619\n",
      "147 : Training: loss:  0.28908342\n",
      "148 : Training: loss:  0.27931336\n",
      "149 : Training: loss:  0.32199115\n",
      "150 : Training: loss:  0.29345295\n",
      "151 : Training: loss:  0.23491876\n",
      "152 : Training: loss:  0.23279399\n",
      "153 : Training: loss:  0.23222493\n",
      "154 : Training: loss:  0.21821569\n",
      "155 : Training: loss:  0.22209394\n",
      "156 : Training: loss:  0.21664312\n",
      "157 : Training: loss:  0.29210344\n",
      "158 : Training: loss:  0.2652738\n",
      "159 : Training: loss:  0.35099864\n",
      "160 : Training: loss:  0.3149798\n",
      "Validation: Loss:  0.2590614  Accuracy:  0.057692308\n",
      "161 : Training: loss:  0.19463497\n",
      "162 : Training: loss:  0.27098835\n",
      "163 : Training: loss:  0.2547205\n",
      "164 : Training: loss:  0.22316374\n",
      "165 : Training: loss:  0.21981163\n",
      "166 : Training: loss:  0.26190418\n",
      "167 : Training: loss:  0.27685183\n",
      "168 : Training: loss:  0.23505335\n",
      "169 : Training: loss:  0.20421655\n",
      "170 : Training: loss:  0.25189155\n",
      "171 : Training: loss:  0.21587117\n",
      "172 : Training: loss:  0.28573868\n",
      "173 : Training: loss:  0.18529272\n",
      "174 : Training: loss:  0.2630522\n",
      "175 : Training: loss:  0.24252795\n",
      "176 : Training: loss:  0.17074357\n",
      "177 : Training: loss:  0.25949332\n",
      "178 : Training: loss:  0.27715662\n",
      "179 : Training: loss:  0.1965765\n",
      "180 : Training: loss:  0.21792258\n",
      "Validation: Loss:  0.23256166  Accuracy:  0.07692308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0972, 0.1424, 0.0775, 0.109, 0.0745, 0.0895...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1457, 0.2624, 0.1316, 0.157, 0.1085, 0.1404...</td>\n",
       "      <td>[0.8543, 0.7376, 0.8684, 0.843, 0.8915, 0.8596...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.232562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.143, 0.1846, 0.1022, 0.1349, 0.1058, 0.1048...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5334, 0.513, 0.3188, 0.4117, 0.3116, 0.2938...</td>\n",
       "      <td>[0.4666, 0.487, 0.6812, 0.5883, 0.6884, 0.7062...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.097, 0.1475, 0.0815, 0.1068, 0.0768, 0.0927...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2732, 0.328, 0.1948, 0.1681, 0.1182, 0.1689...</td>\n",
       "      <td>[0.7268, 0.672, 0.8052, 0.8319, 0.8818, 0.8311...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0934, 0.1302, 0.0848, 0.1086, 0.0702, 0.078...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.3928, 0.3702, 0.2918, 0.407, 0.573, 0.4199,...</td>\n",
       "      <td>[0.6072, 0.6298, 0.7082, 0.593, 0.427, 0.5801,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0663, 0.1059, 0.0503, 0.0785, 0.0475, 0.062...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0836, 0.0893, 0.0884, 0.079, 0.0896, 0.054,...</td>\n",
       "      <td>[0.9164, 0.9107, 0.9116, 0.921, 0.9104, 0.946,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.1028, 0.1506, 0.0913, 0.1173, 0.0787, 0.105...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0807, 0.0834, 0.0801, 0.1101, 0.0978, 0.160...</td>\n",
       "      <td>[0.9193, 0.9166, 0.9199, 0.8899, 0.9022, 0.839...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.2364, 0.2347, 0.127, 0.2169, 0.1897, 0.138,...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.8002, 0.6225, 0.7775, 0.6405, 0.5597, 0.650...</td>\n",
       "      <td>[0.1998, 0.3775, 0.2225, 0.3595, 0.4403, 0.349...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.1636, 0.194, 0.1367, 0.1793, 0.1355, 0.1617...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.6163, 0.3815, 0.5492, 0.3547, 0.3575, 0.541...</td>\n",
       "      <td>[0.3837, 0.6185, 0.4508, 0.6453, 0.6425, 0.458...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1879, 0.239, 0.162, 0.2035, 0.1669, 0.2023,...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.4226, 0.2999, 0.4793, 0.3175, 0.2612, 0.415...</td>\n",
       "      <td>[0.5774, 0.7001, 0.5207, 0.6825, 0.7388, 0.585...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.2811, 0.2838, 0.3, 0.2468, 0.2628, 0.2054, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.829, 0.8906, 0.9035, 0.8242, 0.9212, 0.7925...</td>\n",
       "      <td>[0.171, 0.1094, 0.0965, 0.1758, 0.0788, 0.2075...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.164, 0.2332, 0.1196, 0.1669, 0.1309, 0.1442...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.651, 0.8018, 0.6909, 0.5825, 0.6675, 0.5719...</td>\n",
       "      <td>[0.349, 0.1982, 0.3091, 0.4175, 0.3325, 0.4281...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.1215, 0.1725, 0.1138, 0.1468, 0.1018, 0.124...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.406, 0.6358, 0.5021, 0.4383, 0.6732, 0.5093...</td>\n",
       "      <td>[0.594, 0.3642, 0.4979, 0.5617, 0.3268, 0.4907...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.2744, 0.2759, 0.209, 0.3169, 0.2605, 0.2128...</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.7917, 0.8877, 0.9211, 0.9137, 0.8278, 0.885...</td>\n",
       "      <td>[0.2083, 0.1123, 0.0789, 0.0863, 0.1722, 0.114...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.4194, 0.4005, 0.4112, 0.4103, 0.3108, 0.447...</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.8511, 0.8386, 0.7613, 0.729, 0.8084, 0.9136...</td>\n",
       "      <td>[0.1489, 0.1614, 0.2387, 0.271, 0.1916, 0.0864...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.1158, 0.1474, 0.1047, 0.1655, 0.0909, 0.093...</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.5248, 0.4365, 0.657, 0.6653, 0.6495, 0.5706...</td>\n",
       "      <td>[0.4752, 0.5635, 0.343, 0.3347, 0.3505, 0.4294...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.1193, 0.1652, 0.1134, 0.1274, 0.1068, 0.106...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2396, 0.4045, 0.5189, 0.3543, 0.3942, 0.470...</td>\n",
       "      <td>[0.7604, 0.5955, 0.4811, 0.6457, 0.6058, 0.529...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0969, 0.1421, 0.0828, 0.1141, 0.0703, 0.096...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1037, 0.0841, 0.0705, 0.1637, 0.171, 0.1742...</td>\n",
       "      <td>[0.8963, 0.9159, 0.9295, 0.8363, 0.829, 0.8258...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.1418, 0.1885, 0.1263, 0.1537, 0.1113, 0.130...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3335, 0.3577, 0.2976, 0.5295, 0.5204, 0.449...</td>\n",
       "      <td>[0.6665, 0.6423, 0.7024, 0.4705, 0.4796, 0.550...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0867, 0.1307, 0.0736, 0.1002, 0.0608, 0.082...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1049, 0.3527, 0.0909, 0.119, 0.3149, 0.1183...</td>\n",
       "      <td>[0.8951, 0.6473, 0.9091, 0.881, 0.6851, 0.8817...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0902, 0.1432, 0.087, 0.1063, 0.0686, 0.0856...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1427, 0.5111, 0.1018, 0.2358, 0.3525, 0.168...</td>\n",
       "      <td>[0.8573, 0.4889, 0.8982, 0.7642, 0.6475, 0.831...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.1174, 0.1655, 0.0942, 0.13, 0.0964, 0.1312,...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1999, 0.3783, 0.3043, 0.4446, 0.161, 0.2677...</td>\n",
       "      <td>[0.8001, 0.6217, 0.6957, 0.5554, 0.839, 0.7323...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0625, 0.1072, 0.0485, 0.0826, 0.0442, 0.063...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1122, 0.2491, 0.1441, 0.3613, 0.1009, 0.111...</td>\n",
       "      <td>[0.8878, 0.7509, 0.8559, 0.6387, 0.8991, 0.888...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0736, 0.1218, 0.0608, 0.0917, 0.0528, 0.070...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0928, 0.2167, 0.1613, 0.2721, 0.1434, 0.252...</td>\n",
       "      <td>[0.9072, 0.7833, 0.8387, 0.7279, 0.8566, 0.747...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.1364, 0.173, 0.1063, 0.137, 0.0896, 0.1281,...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.514, 0.5423, 0.4089, 0.4042, 0.4083, 0.5592...</td>\n",
       "      <td>[0.486, 0.4577, 0.5911, 0.5958, 0.5917, 0.4408...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.076, 0.1171, 0.0626, 0.0907, 0.0527, 0.0732...</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0648, 0.0822, 0.0696, 0.0948, 0.0865, 0.085...</td>\n",
       "      <td>[0.9352, 0.9178, 0.9304, 0.9052, 0.9135, 0.914...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.129, 0.1739, 0.1115, 0.1503, 0.109, 0.1278,...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1563, 0.1619, 0.5501, 0.3473, 0.2506, 0.336...</td>\n",
       "      <td>[0.8437, 0.8381, 0.4499, 0.6527, 0.7494, 0.663...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0766, 0.1174, 0.0607, 0.0943, 0.0528, 0.071...</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0502, 0.0578, 0.2053, 0.1389, 0.0798, 0.142...</td>\n",
       "      <td>[0.9498, 0.9422, 0.7947, 0.8611, 0.9202, 0.857...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.1173, 0.1403, 0.0935, 0.136, 0.0822, 0.089,...</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.4175, 0.4581, 0.5009, 0.4057, 0.7613, 0.497...</td>\n",
       "      <td>[0.5825, 0.5419, 0.4991, 0.5943, 0.2387, 0.502...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.2942, 0.2522, 0.2533, 0.2916, 0.1722, 0.228...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8204, 0.7511, 0.7413, 0.7282, 0.9341, 0.825...</td>\n",
       "      <td>[0.1796, 0.2489, 0.2587, 0.2718, 0.0659, 0.174...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.213, 0.1954, 0.1858, 0.2544, 0.1563, 0.164,...</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.8263, 0.6759, 0.8813, 0.811, 0.9481, 0.8288...</td>\n",
       "      <td>[0.1737, 0.3241, 0.1187, 0.189, 0.0519, 0.1712...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.4169, 0.4526, 0.5433, 0.4604, 0.4901, 0.443...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.9999, 1.0, 0.9999, 1.0, 1.0, 1.0, 1.0, 0.99...</td>\n",
       "      <td>[1e-04, 0.0, 1e-04, 0.0, 0.0, 0.0, 0.0, 1e-04,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.178, 0.201, 0.1243, 0.1673, 0.109, 0.1199, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.6295, 0.5363, 0.5083, 0.4229, 0.423, 0.2791...</td>\n",
       "      <td>[0.3705, 0.4637, 0.4917, 0.5771, 0.577, 0.7209...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.1363, 0.1757, 0.1066, 0.1443, 0.1041, 0.098...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.404, 0.6069, 0.5069, 0.562, 0.3743, 0.482, ...</td>\n",
       "      <td>[0.596, 0.3931, 0.4931, 0.438, 0.6257, 0.518, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.047, 0.084, 0.0372, 0.0586, 0.0324, 0.0466,...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0275, 0.0604, 0.0581, 0.0329, 0.0592, 0.065...</td>\n",
       "      <td>[0.9725, 0.9396, 0.9419, 0.9671, 0.9408, 0.935...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0815, 0.1304, 0.0794, 0.0983, 0.0713, 0.081...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2378, 0.5095, 0.125, 0.1676, 0.3349, 0.1787...</td>\n",
       "      <td>[0.7622, 0.4905, 0.875, 0.8324, 0.6651, 0.8213...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0525, 0.093, 0.0415, 0.0669, 0.0363, 0.0513...</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0713, 0.1063, 0.2132, 0.1262, 0.0686, 0.188...</td>\n",
       "      <td>[0.9287, 0.8937, 0.7868, 0.8738, 0.9314, 0.812...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0699, 0.1132, 0.0556, 0.0868, 0.0459, 0.066...</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1091, 0.0971, 0.1747, 0.1798, 0.1373, 0.157...</td>\n",
       "      <td>[0.8909, 0.9029, 0.8253, 0.8202, 0.8627, 0.842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.1875, 0.2495, 0.1718, 0.2429, 0.131, 0.1755...</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.5319, 0.7367, 0.7103, 0.7761, 0.5781, 0.836...</td>\n",
       "      <td>[0.4681, 0.2633, 0.2897, 0.2239, 0.4219, 0.163...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.1356, 0.1661, 0.1079, 0.1607, 0.1026, 0.121...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.2987, 0.2463, 0.2432, 0.2456, 0.2705, 0.422...</td>\n",
       "      <td>[0.7013, 0.7537, 0.7568, 0.7544, 0.7295, 0.577...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0801, 0.1222, 0.0672, 0.1039, 0.0591, 0.080...</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1433, 0.059, 0.059, 0.1029, 0.0896, 0.0832,...</td>\n",
       "      <td>[0.8567, 0.941, 0.941, 0.8971, 0.9104, 0.9168,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.2197, 0.229, 0.1249, 0.2198, 0.1585, 0.1375...</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.5783, 0.6524, 0.4648, 0.6887, 0.5905, 0.453...</td>\n",
       "      <td>[0.4217, 0.3476, 0.5352, 0.3113, 0.4095, 0.546...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0687, 0.1109, 0.0592, 0.0853, 0.0493, 0.071...</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0561, 0.0557, 0.056, 0.0692, 0.0836, 0.1464...</td>\n",
       "      <td>[0.9439, 0.9443, 0.944, 0.9308, 0.9164, 0.8536...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0595, 0.0974, 0.0436, 0.0732, 0.0386, 0.054...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1368, 0.1169, 0.2362, 0.2086, 0.1182, 0.218...</td>\n",
       "      <td>[0.8632, 0.8831, 0.7638, 0.7914, 0.8818, 0.781...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.2217, 0.2496, 0.1676, 0.2501, 0.1625, 0.164...</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.7461, 0.7674, 0.7121, 0.8112, 0.6584, 0.826...</td>\n",
       "      <td>[0.2539, 0.2326, 0.2879, 0.1888, 0.3416, 0.173...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.2953, 0.2872, 0.2051, 0.299, 0.2097, 0.1979...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.8048, 0.8082, 0.8571, 0.8233, 0.7176, 0.943...</td>\n",
       "      <td>[0.1952, 0.1918, 0.1429, 0.1767, 0.2824, 0.056...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0782, 0.1127, 0.0496, 0.0983, 0.0453, 0.059...</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.354, 0.2972, 0.418, 0.3507, 0.2938, 0.3205,...</td>\n",
       "      <td>[0.646, 0.7028, 0.582, 0.6493, 0.7062, 0.6795,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.2191, 0.257, 0.2072, 0.2267, 0.1598, 0.2292...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5647, 0.4628, 0.4365, 0.4542, 0.4678, 0.506...</td>\n",
       "      <td>[0.4353, 0.5372, 0.5635, 0.5458, 0.5322, 0.493...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.1329, 0.1715, 0.1075, 0.1233, 0.1055, 0.107...</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.342, 0.459, 0.2657, 0.4228, 0.2631, 0.4138,...</td>\n",
       "      <td>[0.658, 0.541, 0.7343, 0.5772, 0.7369, 0.5862,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0495, 0.0868, 0.0398, 0.0605, 0.0346, 0.043...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0911, 0.2522, 0.0846, 0.09, 0.2272, 0.1278,...</td>\n",
       "      <td>[0.9089, 0.7478, 0.9154, 0.91, 0.7728, 0.8722,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.145, 0.2047, 0.1134, 0.1474, 0.1063, 0.1135...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2565, 0.8304, 0.5039, 0.4542, 0.5673, 0.476...</td>\n",
       "      <td>[0.7435, 0.1696, 0.4961, 0.5458, 0.4327, 0.523...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1885, 0.2178, 0.1454, 0.1932, 0.1866, 0.187...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3575, 0.4721, 0.6489, 0.3004, 0.1897, 0.464...</td>\n",
       "      <td>[0.6425, 0.5279, 0.3511, 0.6996, 0.8103, 0.535...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.1486, 0.1844, 0.1385, 0.1675, 0.1266, 0.097...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4029, 0.4203, 0.5524, 0.4589, 0.5453, 0.415...</td>\n",
       "      <td>[0.5971, 0.5797, 0.4476, 0.5411, 0.4547, 0.584...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0972, 0.1424, 0.0775, 0.109, 0.0745, 0.0895...               0   \n",
       "1   [0.143, 0.1846, 0.1022, 0.1349, 0.1058, 0.1048...               0   \n",
       "2   [0.097, 0.1475, 0.0815, 0.1068, 0.0768, 0.0927...               0   \n",
       "3   [0.0934, 0.1302, 0.0848, 0.1086, 0.0702, 0.078...               1   \n",
       "4   [0.0663, 0.1059, 0.0503, 0.0785, 0.0475, 0.062...               1   \n",
       "5   [0.1028, 0.1506, 0.0913, 0.1173, 0.0787, 0.105...               2   \n",
       "6   [0.2364, 0.2347, 0.127, 0.2169, 0.1897, 0.138,...               3   \n",
       "7   [0.1636, 0.194, 0.1367, 0.1793, 0.1355, 0.1617...               3   \n",
       "8   [0.1879, 0.239, 0.162, 0.2035, 0.1669, 0.2023,...               3   \n",
       "9   [0.2811, 0.2838, 0.3, 0.2468, 0.2628, 0.2054, ...               4   \n",
       "10  [0.164, 0.2332, 0.1196, 0.1669, 0.1309, 0.1442...               4   \n",
       "11  [0.1215, 0.1725, 0.1138, 0.1468, 0.1018, 0.124...               5   \n",
       "12  [0.2744, 0.2759, 0.209, 0.3169, 0.2605, 0.2128...               6   \n",
       "13  [0.4194, 0.4005, 0.4112, 0.4103, 0.3108, 0.447...               7   \n",
       "14  [0.1158, 0.1474, 0.1047, 0.1655, 0.0909, 0.093...               8   \n",
       "15  [0.1193, 0.1652, 0.1134, 0.1274, 0.1068, 0.106...               8   \n",
       "16  [0.0969, 0.1421, 0.0828, 0.1141, 0.0703, 0.096...               9   \n",
       "17  [0.1418, 0.1885, 0.1263, 0.1537, 0.1113, 0.130...               9   \n",
       "18  [0.0867, 0.1307, 0.0736, 0.1002, 0.0608, 0.082...              10   \n",
       "19  [0.0902, 0.1432, 0.087, 0.1063, 0.0686, 0.0856...              10   \n",
       "20  [0.1174, 0.1655, 0.0942, 0.13, 0.0964, 0.1312,...              11   \n",
       "21  [0.0625, 0.1072, 0.0485, 0.0826, 0.0442, 0.063...              11   \n",
       "22  [0.0736, 0.1218, 0.0608, 0.0917, 0.0528, 0.070...              12   \n",
       "23  [0.1364, 0.173, 0.1063, 0.137, 0.0896, 0.1281,...              13   \n",
       "24  [0.076, 0.1171, 0.0626, 0.0907, 0.0527, 0.0732...              13   \n",
       "25  [0.129, 0.1739, 0.1115, 0.1503, 0.109, 0.1278,...              14   \n",
       "26  [0.0766, 0.1174, 0.0607, 0.0943, 0.0528, 0.071...              14   \n",
       "27  [0.1173, 0.1403, 0.0935, 0.136, 0.0822, 0.089,...              15   \n",
       "28  [0.2942, 0.2522, 0.2533, 0.2916, 0.1722, 0.228...              15   \n",
       "29  [0.213, 0.1954, 0.1858, 0.2544, 0.1563, 0.164,...              15   \n",
       "30  [0.4169, 0.4526, 0.5433, 0.4604, 0.4901, 0.443...              16   \n",
       "31  [0.178, 0.201, 0.1243, 0.1673, 0.109, 0.1199, ...              17   \n",
       "32  [0.1363, 0.1757, 0.1066, 0.1443, 0.1041, 0.098...              17   \n",
       "33  [0.047, 0.084, 0.0372, 0.0586, 0.0324, 0.0466,...              18   \n",
       "34  [0.0815, 0.1304, 0.0794, 0.0983, 0.0713, 0.081...              19   \n",
       "35  [0.0525, 0.093, 0.0415, 0.0669, 0.0363, 0.0513...              20   \n",
       "36  [0.0699, 0.1132, 0.0556, 0.0868, 0.0459, 0.066...              21   \n",
       "37  [0.1875, 0.2495, 0.1718, 0.2429, 0.131, 0.1755...              21   \n",
       "38  [0.1356, 0.1661, 0.1079, 0.1607, 0.1026, 0.121...              22   \n",
       "39  [0.0801, 0.1222, 0.0672, 0.1039, 0.0591, 0.080...              22   \n",
       "40  [0.2197, 0.229, 0.1249, 0.2198, 0.1585, 0.1375...              22   \n",
       "41  [0.0687, 0.1109, 0.0592, 0.0853, 0.0493, 0.071...              22   \n",
       "42  [0.0595, 0.0974, 0.0436, 0.0732, 0.0386, 0.054...              23   \n",
       "43  [0.2217, 0.2496, 0.1676, 0.2501, 0.1625, 0.164...              23   \n",
       "44  [0.2953, 0.2872, 0.2051, 0.299, 0.2097, 0.1979...              23   \n",
       "45  [0.0782, 0.1127, 0.0496, 0.0983, 0.0453, 0.059...              23   \n",
       "46  [0.2191, 0.257, 0.2072, 0.2267, 0.1598, 0.2292...              24   \n",
       "47  [0.1329, 0.1715, 0.1075, 0.1233, 0.1055, 0.107...              24   \n",
       "48  [0.0495, 0.0868, 0.0398, 0.0605, 0.0346, 0.043...              25   \n",
       "49  [0.145, 0.2047, 0.1134, 0.1474, 0.1063, 0.1135...              25   \n",
       "50  [0.1885, 0.2178, 0.1454, 0.1932, 0.1866, 0.187...              26   \n",
       "51  [0.1486, 0.1844, 0.1385, 0.1675, 0.1266, 0.097...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  1  [0.1457, 0.2624, 0.1316, 0.157, 0.1085, 0.1404...   \n",
       "1                  1  [0.5334, 0.513, 0.3188, 0.4117, 0.3116, 0.2938...   \n",
       "2                  1  [0.2732, 0.328, 0.1948, 0.1681, 0.1182, 0.1689...   \n",
       "3                 20  [0.3928, 0.3702, 0.2918, 0.407, 0.573, 0.4199,...   \n",
       "4                  8  [0.0836, 0.0893, 0.0884, 0.079, 0.0896, 0.054,...   \n",
       "5                  8  [0.0807, 0.0834, 0.0801, 0.1101, 0.0978, 0.160...   \n",
       "6                  8  [0.8002, 0.6225, 0.7775, 0.6405, 0.5597, 0.650...   \n",
       "7                  8  [0.6163, 0.3815, 0.5492, 0.3547, 0.3575, 0.541...   \n",
       "8                  8  [0.4226, 0.2999, 0.4793, 0.3175, 0.2612, 0.415...   \n",
       "9                 26  [0.829, 0.8906, 0.9035, 0.8242, 0.9212, 0.7925...   \n",
       "10                 8  [0.651, 0.8018, 0.6909, 0.5825, 0.6675, 0.5719...   \n",
       "11                 8  [0.406, 0.6358, 0.5021, 0.4383, 0.6732, 0.5093...   \n",
       "12                20  [0.7917, 0.8877, 0.9211, 0.9137, 0.8278, 0.885...   \n",
       "13                20  [0.8511, 0.8386, 0.7613, 0.729, 0.8084, 0.9136...   \n",
       "14                20  [0.5248, 0.4365, 0.657, 0.6653, 0.6495, 0.5706...   \n",
       "15                 8  [0.2396, 0.4045, 0.5189, 0.3543, 0.3942, 0.470...   \n",
       "16                 8  [0.1037, 0.0841, 0.0705, 0.1637, 0.171, 0.1742...   \n",
       "17                 8  [0.3335, 0.3577, 0.2976, 0.5295, 0.5204, 0.449...   \n",
       "18                 8  [0.1049, 0.3527, 0.0909, 0.119, 0.3149, 0.1183...   \n",
       "19                 8  [0.1427, 0.5111, 0.1018, 0.2358, 0.3525, 0.168...   \n",
       "20                 1  [0.1999, 0.3783, 0.3043, 0.4446, 0.161, 0.2677...   \n",
       "21                 1  [0.1122, 0.2491, 0.1441, 0.3613, 0.1009, 0.111...   \n",
       "22                 1  [0.0928, 0.2167, 0.1613, 0.2721, 0.1434, 0.252...   \n",
       "23                 1  [0.514, 0.5423, 0.4089, 0.4042, 0.4083, 0.5592...   \n",
       "24                 8  [0.0648, 0.0822, 0.0696, 0.0948, 0.0865, 0.085...   \n",
       "25                 1  [0.1563, 0.1619, 0.5501, 0.3473, 0.2506, 0.336...   \n",
       "26                 8  [0.0502, 0.0578, 0.2053, 0.1389, 0.0798, 0.142...   \n",
       "27                20  [0.4175, 0.4581, 0.5009, 0.4057, 0.7613, 0.497...   \n",
       "28                15  [0.8204, 0.7511, 0.7413, 0.7282, 0.9341, 0.825...   \n",
       "29                26  [0.8263, 0.6759, 0.8813, 0.811, 0.9481, 0.8288...   \n",
       "30                19  [0.9999, 1.0, 0.9999, 1.0, 1.0, 1.0, 1.0, 0.99...   \n",
       "31                20  [0.6295, 0.5363, 0.5083, 0.4229, 0.423, 0.2791...   \n",
       "32                 1  [0.404, 0.6069, 0.5069, 0.562, 0.3743, 0.482, ...   \n",
       "33                 8  [0.0275, 0.0604, 0.0581, 0.0329, 0.0592, 0.065...   \n",
       "34                 1  [0.2378, 0.5095, 0.125, 0.1676, 0.3349, 0.1787...   \n",
       "35                 8  [0.0713, 0.1063, 0.2132, 0.1262, 0.0686, 0.188...   \n",
       "36                 8  [0.1091, 0.0971, 0.1747, 0.1798, 0.1373, 0.157...   \n",
       "37                20  [0.5319, 0.7367, 0.7103, 0.7761, 0.5781, 0.836...   \n",
       "38                20  [0.2987, 0.2463, 0.2432, 0.2456, 0.2705, 0.422...   \n",
       "39                 8  [0.1433, 0.059, 0.059, 0.1029, 0.0896, 0.0832,...   \n",
       "40                12  [0.5783, 0.6524, 0.4648, 0.6887, 0.5905, 0.453...   \n",
       "41                 8  [0.0561, 0.0557, 0.056, 0.0692, 0.0836, 0.1464...   \n",
       "42                 8  [0.1368, 0.1169, 0.2362, 0.2086, 0.1182, 0.218...   \n",
       "43                20  [0.7461, 0.7674, 0.7121, 0.8112, 0.6584, 0.826...   \n",
       "44                23  [0.8048, 0.8082, 0.8571, 0.8233, 0.7176, 0.943...   \n",
       "45                20  [0.354, 0.2972, 0.418, 0.3507, 0.2938, 0.3205,...   \n",
       "46                 9  [0.5647, 0.4628, 0.4365, 0.4542, 0.4678, 0.506...   \n",
       "47                 8  [0.342, 0.459, 0.2657, 0.4228, 0.2631, 0.4138,...   \n",
       "48                 8  [0.0911, 0.2522, 0.0846, 0.09, 0.2272, 0.1278,...   \n",
       "49                 1  [0.2565, 0.8304, 0.5039, 0.4542, 0.5673, 0.476...   \n",
       "50                 8  [0.3575, 0.4721, 0.6489, 0.3004, 0.1897, 0.464...   \n",
       "51                26  [0.4029, 0.4203, 0.5524, 0.4589, 0.5453, 0.415...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8543, 0.7376, 0.8684, 0.843, 0.8915, 0.8596...  0.076923  0.232562  \n",
       "1   [0.4666, 0.487, 0.6812, 0.5883, 0.6884, 0.7062...       NaN       NaN  \n",
       "2   [0.7268, 0.672, 0.8052, 0.8319, 0.8818, 0.8311...       NaN       NaN  \n",
       "3   [0.6072, 0.6298, 0.7082, 0.593, 0.427, 0.5801,...       NaN       NaN  \n",
       "4   [0.9164, 0.9107, 0.9116, 0.921, 0.9104, 0.946,...       NaN       NaN  \n",
       "5   [0.9193, 0.9166, 0.9199, 0.8899, 0.9022, 0.839...       NaN       NaN  \n",
       "6   [0.1998, 0.3775, 0.2225, 0.3595, 0.4403, 0.349...       NaN       NaN  \n",
       "7   [0.3837, 0.6185, 0.4508, 0.6453, 0.6425, 0.458...       NaN       NaN  \n",
       "8   [0.5774, 0.7001, 0.5207, 0.6825, 0.7388, 0.585...       NaN       NaN  \n",
       "9   [0.171, 0.1094, 0.0965, 0.1758, 0.0788, 0.2075...       NaN       NaN  \n",
       "10  [0.349, 0.1982, 0.3091, 0.4175, 0.3325, 0.4281...       NaN       NaN  \n",
       "11  [0.594, 0.3642, 0.4979, 0.5617, 0.3268, 0.4907...       NaN       NaN  \n",
       "12  [0.2083, 0.1123, 0.0789, 0.0863, 0.1722, 0.114...       NaN       NaN  \n",
       "13  [0.1489, 0.1614, 0.2387, 0.271, 0.1916, 0.0864...       NaN       NaN  \n",
       "14  [0.4752, 0.5635, 0.343, 0.3347, 0.3505, 0.4294...       NaN       NaN  \n",
       "15  [0.7604, 0.5955, 0.4811, 0.6457, 0.6058, 0.529...       NaN       NaN  \n",
       "16  [0.8963, 0.9159, 0.9295, 0.8363, 0.829, 0.8258...       NaN       NaN  \n",
       "17  [0.6665, 0.6423, 0.7024, 0.4705, 0.4796, 0.550...       NaN       NaN  \n",
       "18  [0.8951, 0.6473, 0.9091, 0.881, 0.6851, 0.8817...       NaN       NaN  \n",
       "19  [0.8573, 0.4889, 0.8982, 0.7642, 0.6475, 0.831...       NaN       NaN  \n",
       "20  [0.8001, 0.6217, 0.6957, 0.5554, 0.839, 0.7323...       NaN       NaN  \n",
       "21  [0.8878, 0.7509, 0.8559, 0.6387, 0.8991, 0.888...       NaN       NaN  \n",
       "22  [0.9072, 0.7833, 0.8387, 0.7279, 0.8566, 0.747...       NaN       NaN  \n",
       "23  [0.486, 0.4577, 0.5911, 0.5958, 0.5917, 0.4408...       NaN       NaN  \n",
       "24  [0.9352, 0.9178, 0.9304, 0.9052, 0.9135, 0.914...       NaN       NaN  \n",
       "25  [0.8437, 0.8381, 0.4499, 0.6527, 0.7494, 0.663...       NaN       NaN  \n",
       "26  [0.9498, 0.9422, 0.7947, 0.8611, 0.9202, 0.857...       NaN       NaN  \n",
       "27  [0.5825, 0.5419, 0.4991, 0.5943, 0.2387, 0.502...       NaN       NaN  \n",
       "28  [0.1796, 0.2489, 0.2587, 0.2718, 0.0659, 0.174...       NaN       NaN  \n",
       "29  [0.1737, 0.3241, 0.1187, 0.189, 0.0519, 0.1712...       NaN       NaN  \n",
       "30  [1e-04, 0.0, 1e-04, 0.0, 0.0, 0.0, 0.0, 1e-04,...       NaN       NaN  \n",
       "31  [0.3705, 0.4637, 0.4917, 0.5771, 0.577, 0.7209...       NaN       NaN  \n",
       "32  [0.596, 0.3931, 0.4931, 0.438, 0.6257, 0.518, ...       NaN       NaN  \n",
       "33  [0.9725, 0.9396, 0.9419, 0.9671, 0.9408, 0.935...       NaN       NaN  \n",
       "34  [0.7622, 0.4905, 0.875, 0.8324, 0.6651, 0.8213...       NaN       NaN  \n",
       "35  [0.9287, 0.8937, 0.7868, 0.8738, 0.9314, 0.812...       NaN       NaN  \n",
       "36  [0.8909, 0.9029, 0.8253, 0.8202, 0.8627, 0.842...       NaN       NaN  \n",
       "37  [0.4681, 0.2633, 0.2897, 0.2239, 0.4219, 0.163...       NaN       NaN  \n",
       "38  [0.7013, 0.7537, 0.7568, 0.7544, 0.7295, 0.577...       NaN       NaN  \n",
       "39  [0.8567, 0.941, 0.941, 0.8971, 0.9104, 0.9168,...       NaN       NaN  \n",
       "40  [0.4217, 0.3476, 0.5352, 0.3113, 0.4095, 0.546...       NaN       NaN  \n",
       "41  [0.9439, 0.9443, 0.944, 0.9308, 0.9164, 0.8536...       NaN       NaN  \n",
       "42  [0.8632, 0.8831, 0.7638, 0.7914, 0.8818, 0.781...       NaN       NaN  \n",
       "43  [0.2539, 0.2326, 0.2879, 0.1888, 0.3416, 0.173...       NaN       NaN  \n",
       "44  [0.1952, 0.1918, 0.1429, 0.1767, 0.2824, 0.056...       NaN       NaN  \n",
       "45  [0.646, 0.7028, 0.582, 0.6493, 0.7062, 0.6795,...       NaN       NaN  \n",
       "46  [0.4353, 0.5372, 0.5635, 0.5458, 0.5322, 0.493...       NaN       NaN  \n",
       "47  [0.658, 0.541, 0.7343, 0.5772, 0.7369, 0.5862,...       NaN       NaN  \n",
       "48  [0.9089, 0.7478, 0.9154, 0.91, 0.7728, 0.8722,...       NaN       NaN  \n",
       "49  [0.7435, 0.1696, 0.4961, 0.5458, 0.4327, 0.523...       NaN       NaN  \n",
       "50  [0.6425, 0.5279, 0.3511, 0.6996, 0.8103, 0.535...       NaN       NaN  \n",
       "51  [0.5971, 0.5797, 0.4476, 0.5411, 0.4547, 0.584...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 : Training: loss:  0.26695192\n",
      "182 : Training: loss:  0.18966036\n",
      "183 : Training: loss:  0.2810824\n",
      "184 : Training: loss:  0.23560692\n",
      "185 : Training: loss:  0.21681985\n",
      "186 : Training: loss:  0.20749037\n",
      "187 : Training: loss:  0.23247492\n",
      "188 : Training: loss:  0.20060243\n",
      "189 : Training: loss:  0.19319057\n",
      "190 : Training: loss:  0.23820812\n",
      "191 : Training: loss:  0.19542252\n",
      "192 : Training: loss:  0.19380072\n",
      "193 : Training: loss:  0.19073717\n",
      "194 : Training: loss:  0.17860222\n",
      "195 : Training: loss:  0.20313431\n",
      "196 : Training: loss:  0.18754429\n",
      "197 : Training: loss:  0.17264214\n",
      "198 : Training: loss:  0.22609337\n",
      "199 : Training: loss:  0.16231854\n",
      "200 : Training: loss:  0.18650633\n",
      "Validation: Loss:  0.21332504  Accuracy:  0.09615385\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0821, 0.1191, 0.0618, 0.0888, 0.0602, 0.073...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1421, 0.2584, 0.1275, 0.1534, 0.1066, 0.135...</td>\n",
       "      <td>[0.8579, 0.7416, 0.8725, 0.8466, 0.8934, 0.864...</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.213325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1213, 0.1579, 0.0825, 0.1112, 0.0876, 0.087...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5225, 0.5044, 0.3087, 0.4021, 0.3049, 0.283...</td>\n",
       "      <td>[0.4775, 0.4956, 0.6913, 0.5979, 0.6951, 0.716...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0811, 0.1231, 0.0644, 0.0864, 0.0616, 0.075...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2657, 0.3227, 0.1887, 0.1638, 0.1158, 0.162...</td>\n",
       "      <td>[0.7343, 0.6773, 0.8113, 0.8362, 0.8842, 0.837...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0745, 0.1053, 0.0638, 0.0844, 0.0545, 0.060...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.3757, 0.3585, 0.2758, 0.391, 0.5611, 0.4019...</td>\n",
       "      <td>[0.6243, 0.6415, 0.7242, 0.609, 0.4389, 0.5981...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0534, 0.0848, 0.0379, 0.0609, 0.0363, 0.049...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0799, 0.086, 0.084, 0.0759, 0.0868, 0.0514,...</td>\n",
       "      <td>[0.9201, 0.914, 0.916, 0.9241, 0.9132, 0.9486,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0869, 0.1263, 0.0731, 0.0961, 0.0636, 0.086...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0787, 0.0812, 0.0776, 0.1065, 0.0965, 0.157...</td>\n",
       "      <td>[0.9213, 0.9188, 0.9224, 0.8935, 0.9035, 0.842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.2104, 0.209, 0.1083, 0.1918, 0.1644, 0.1192...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7943, 0.6162, 0.7718, 0.632, 0.55, 0.6441, ...</td>\n",
       "      <td>[0.2057, 0.3838, 0.2282, 0.368, 0.45, 0.3559, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.143, 0.1696, 0.1144, 0.1544, 0.1148, 0.1379...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.6084, 0.3731, 0.5431, 0.347, 0.3496, 0.5351...</td>\n",
       "      <td>[0.3916, 0.6269, 0.4569, 0.653, 0.6504, 0.4649...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1689, 0.214, 0.1401, 0.18, 0.1458, 0.1782, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.4196, 0.297, 0.4773, 0.3138, 0.2578, 0.4116...</td>\n",
       "      <td>[0.5804, 0.703, 0.5227, 0.6862, 0.7422, 0.5884...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.2545, 0.2601, 0.2682, 0.2231, 0.2424, 0.185...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.8184, 0.886, 0.8966, 0.8138, 0.9177, 0.7782...</td>\n",
       "      <td>[0.1816, 0.114, 0.1034, 0.1862, 0.0823, 0.2218...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.1324, 0.1936, 0.0917, 0.1347, 0.1031, 0.113...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.6257, 0.7885, 0.6686, 0.5562, 0.6499, 0.543...</td>\n",
       "      <td>[0.3743, 0.2115, 0.3314, 0.4438, 0.3501, 0.456...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0963, 0.139, 0.0857, 0.1148, 0.078, 0.0966,...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3837, 0.6198, 0.4802, 0.4177, 0.6584, 0.485...</td>\n",
       "      <td>[0.6163, 0.3802, 0.5198, 0.5823, 0.3416, 0.514...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.2425, 0.2442, 0.1797, 0.2801, 0.2244, 0.181...</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.7739, 0.8768, 0.9128, 0.9054, 0.816, 0.8742...</td>\n",
       "      <td>[0.2261, 0.1232, 0.0871, 0.0946, 0.184, 0.1258...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.3976, 0.3814, 0.3875, 0.3881, 0.2902, 0.422...</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.8413, 0.8265, 0.7466, 0.7112, 0.7985, 0.908...</td>\n",
       "      <td>[0.1587, 0.1735, 0.2534, 0.2888, 0.2015, 0.091...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0942, 0.1206, 0.0809, 0.134, 0.0716, 0.0741...</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.5072, 0.4241, 0.6417, 0.65, 0.6372, 0.5509,...</td>\n",
       "      <td>[0.4928, 0.5759, 0.3583, 0.35, 0.3628, 0.4491,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0984, 0.1371, 0.0884, 0.1025, 0.0847, 0.084...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2288, 0.3954, 0.503, 0.3407, 0.3823, 0.4512...</td>\n",
       "      <td>[0.7712, 0.6046, 0.497, 0.6593, 0.6177, 0.5488...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0803, 0.1169, 0.0647, 0.0917, 0.0555, 0.077...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1002, 0.0811, 0.0673, 0.1572, 0.1666, 0.169...</td>\n",
       "      <td>[0.8998, 0.9189, 0.9327, 0.8428, 0.8334, 0.831...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.118, 0.1579, 0.1003, 0.126, 0.0892, 0.1055,...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3238, 0.3488, 0.2861, 0.5152, 0.5109, 0.438...</td>\n",
       "      <td>[0.6762, 0.6512, 0.7139, 0.4848, 0.4891, 0.561...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0695, 0.1046, 0.0551, 0.0776, 0.0459, 0.064...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0997, 0.3418, 0.0867, 0.114, 0.3062, 0.1126...</td>\n",
       "      <td>[0.9003, 0.6582, 0.9133, 0.886, 0.6938, 0.8874...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0718, 0.1145, 0.0648, 0.0821, 0.0514, 0.066...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1356, 0.4996, 0.097, 0.2262, 0.3444, 0.1593...</td>\n",
       "      <td>[0.8644, 0.5004, 0.903, 0.7738, 0.6556, 0.8407...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.1004, 0.1403, 0.0766, 0.1082, 0.0786, 0.109...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1965, 0.3716, 0.2988, 0.4394, 0.1586, 0.262...</td>\n",
       "      <td>[0.8035, 0.6284, 0.7012, 0.5606, 0.8414, 0.737...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0497, 0.0845, 0.0361, 0.0632, 0.0331, 0.048...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1074, 0.2383, 0.1388, 0.3521, 0.0967, 0.107...</td>\n",
       "      <td>[0.8926, 0.7617, 0.8612, 0.6479, 0.9033, 0.892...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0605, 0.0993, 0.047, 0.0727, 0.0413, 0.0564...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0898, 0.2104, 0.1574, 0.2659, 0.1391, 0.246...</td>\n",
       "      <td>[0.9102, 0.7896, 0.8426, 0.7341, 0.8609, 0.754...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.1127, 0.1435, 0.0829, 0.1103, 0.0708, 0.103...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5009, 0.5302, 0.4004, 0.3915, 0.3988, 0.543...</td>\n",
       "      <td>[0.4991, 0.4698, 0.5996, 0.6085, 0.6012, 0.456...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0628, 0.0956, 0.0484, 0.0721, 0.0413, 0.058...</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.064, 0.0808, 0.0687, 0.0929, 0.0849, 0.084,...</td>\n",
       "      <td>[0.936, 0.9192, 0.9313, 0.9071, 0.9151, 0.916,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.1103, 0.1482, 0.0909, 0.1258, 0.0899, 0.106...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1529, 0.1592, 0.5408, 0.341, 0.2465, 0.3295...</td>\n",
       "      <td>[0.8471, 0.8408, 0.4592, 0.659, 0.7535, 0.6705...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0629, 0.0955, 0.0468, 0.0747, 0.0412, 0.057...</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0488, 0.0556, 0.2006, 0.1354, 0.0774, 0.138...</td>\n",
       "      <td>[0.9512, 0.9444, 0.7994, 0.8646, 0.9226, 0.861...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0934, 0.1136, 0.0707, 0.1062, 0.0641, 0.068...</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3988, 0.4448, 0.4792, 0.3859, 0.7525, 0.480...</td>\n",
       "      <td>[0.6012, 0.5552, 0.5208, 0.6141, 0.2475, 0.519...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.2545, 0.2212, 0.2144, 0.2549, 0.1472, 0.193...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8061, 0.7349, 0.7214, 0.7068, 0.9305, 0.814...</td>\n",
       "      <td>[0.1939, 0.2651, 0.2786, 0.2932, 0.0695, 0.185...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.1787, 0.1659, 0.1505, 0.2157, 0.1303, 0.133...</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.811, 0.6567, 0.8691, 0.7934, 0.9448, 0.8155...</td>\n",
       "      <td>[0.189, 0.3433, 0.1309, 0.2066, 0.0552, 0.1845...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.4108, 0.447, 0.5353, 0.4534, 0.4868, 0.434,...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.9999, 1.0, 0.9999, 1.0, 1.0, 1.0, 1.0, 0.99...</td>\n",
       "      <td>[1e-04, 0.0, 1e-04, 0.0, 0.0, 0.0, 0.0, 1e-04,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.1546, 0.1747, 0.104, 0.1427, 0.0925, 0.1019...</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.6225, 0.5267, 0.4997, 0.4151, 0.4158, 0.274...</td>\n",
       "      <td>[0.3775, 0.4733, 0.5003, 0.5849, 0.5842, 0.725...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.1153, 0.1486, 0.0863, 0.1198, 0.0847, 0.080...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3959, 0.5992, 0.497, 0.5521, 0.3673, 0.474,...</td>\n",
       "      <td>[0.6041, 0.4008, 0.503, 0.4479, 0.6327, 0.526,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0373, 0.0655, 0.0273, 0.0443, 0.0242, 0.035...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0263, 0.0584, 0.0551, 0.0314, 0.0572, 0.062...</td>\n",
       "      <td>[0.9737, 0.9416, 0.9449, 0.9686, 0.9428, 0.938...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.067, 0.1066, 0.0615, 0.0782, 0.0565, 0.0651...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2278, 0.505, 0.1186, 0.1611, 0.3295, 0.1692...</td>\n",
       "      <td>[0.7722, 0.495, 0.8814, 0.8389, 0.6705, 0.8308...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0417, 0.0731, 0.0306, 0.0509, 0.0272, 0.039...</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0689, 0.1024, 0.2073, 0.1218, 0.0664, 0.182...</td>\n",
       "      <td>[0.9311, 0.8976, 0.7927, 0.8782, 0.9336, 0.817...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0565, 0.0908, 0.042, 0.0676, 0.0352, 0.0523...</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1058, 0.0924, 0.1701, 0.1747, 0.131, 0.1526...</td>\n",
       "      <td>[0.8942, 0.9076, 0.8299, 0.8253, 0.869, 0.8474...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.156, 0.2141, 0.139, 0.2024, 0.1051, 0.1428,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.511, 0.7154, 0.695, 0.7604, 0.5512, 0.8257,...</td>\n",
       "      <td>[0.489, 0.2846, 0.305, 0.2396, 0.4488, 0.1743,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.1154, 0.1408, 0.0878, 0.1334, 0.084, 0.1002...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2895, 0.2386, 0.2337, 0.2374, 0.2618, 0.415...</td>\n",
       "      <td>[0.7105, 0.7614, 0.7663, 0.7626, 0.7382, 0.584...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0662, 0.1001, 0.0523, 0.0829, 0.0465, 0.064...</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1386, 0.0565, 0.0568, 0.0998, 0.086, 0.081,...</td>\n",
       "      <td>[0.8614, 0.9435, 0.9432, 0.9002, 0.914, 0.919,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.1891, 0.199, 0.1035, 0.1866, 0.1336, 0.1157...</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.5648, 0.641, 0.4486, 0.6775, 0.5784, 0.4425...</td>\n",
       "      <td>[0.4352, 0.359, 0.5514, 0.3225, 0.4216, 0.5575...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0565, 0.0899, 0.0455, 0.0673, 0.0384, 0.057...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0544, 0.0538, 0.0541, 0.0672, 0.0809, 0.143...</td>\n",
       "      <td>[0.9456, 0.9462, 0.9459, 0.9328, 0.9191, 0.856...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0463, 0.0752, 0.0316, 0.0549, 0.0282, 0.040...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1311, 0.1096, 0.2272, 0.1997, 0.1113, 0.209...</td>\n",
       "      <td>[0.8689, 0.8904, 0.7728, 0.8003, 0.8887, 0.790...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.1777, 0.2056, 0.1293, 0.2011, 0.1226, 0.126...</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.7189, 0.7361, 0.6859, 0.7881, 0.624, 0.8061...</td>\n",
       "      <td>[0.2811, 0.2639, 0.3141, 0.2119, 0.376, 0.1939...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.2499, 0.2471, 0.1671, 0.2523, 0.1681, 0.160...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.782, 0.7786, 0.8403, 0.8, 0.685, 0.9359, 0....</td>\n",
       "      <td>[0.218, 0.2214, 0.1597, 0.2, 0.315, 0.0641, 0....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0593, 0.086, 0.0353, 0.0729, 0.0322, 0.0438...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3372, 0.2757, 0.401, 0.3328, 0.2739, 0.3052...</td>\n",
       "      <td>[0.6628, 0.7243, 0.599, 0.6672, 0.7261, 0.6948...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1978, 0.2312, 0.182, 0.2013, 0.1405, 0.2046...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5613, 0.456, 0.4337, 0.4493, 0.4639, 0.5008...</td>\n",
       "      <td>[0.4387, 0.544, 0.5663, 0.5507, 0.5361, 0.4992...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.1152, 0.1486, 0.0894, 0.1041, 0.0895, 0.091...</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3389, 0.4551, 0.2614, 0.4182, 0.2607, 0.407...</td>\n",
       "      <td>[0.6611, 0.5449, 0.7386, 0.5818, 0.7393, 0.592...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0369, 0.0649, 0.0274, 0.0434, 0.0243, 0.031...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0835, 0.241, 0.0772, 0.0829, 0.2167, 0.1167...</td>\n",
       "      <td>[0.9165, 0.759, 0.9228, 0.9171, 0.7833, 0.8833...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.1172, 0.1686, 0.0874, 0.1167, 0.0831, 0.089...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.239, 0.8217, 0.4822, 0.433, 0.5515, 0.4518,...</td>\n",
       "      <td>[0.761, 0.1783, 0.5178, 0.567, 0.4485, 0.5482,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1665, 0.1927, 0.1239, 0.1678, 0.1606, 0.162...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3522, 0.4654, 0.6419, 0.2958, 0.1867, 0.456...</td>\n",
       "      <td>[0.6478, 0.5346, 0.3581, 0.7042, 0.8133, 0.543...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.1252, 0.1567, 0.112, 0.1397, 0.1044, 0.08, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.3907, 0.4139, 0.5376, 0.4449, 0.5362, 0.400...</td>\n",
       "      <td>[0.6093, 0.5861, 0.4624, 0.5551, 0.4638, 0.599...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0821, 0.1191, 0.0618, 0.0888, 0.0602, 0.073...               0   \n",
       "1   [0.1213, 0.1579, 0.0825, 0.1112, 0.0876, 0.087...               0   \n",
       "2   [0.0811, 0.1231, 0.0644, 0.0864, 0.0616, 0.075...               0   \n",
       "3   [0.0745, 0.1053, 0.0638, 0.0844, 0.0545, 0.060...               1   \n",
       "4   [0.0534, 0.0848, 0.0379, 0.0609, 0.0363, 0.049...               1   \n",
       "5   [0.0869, 0.1263, 0.0731, 0.0961, 0.0636, 0.086...               2   \n",
       "6   [0.2104, 0.209, 0.1083, 0.1918, 0.1644, 0.1192...               3   \n",
       "7   [0.143, 0.1696, 0.1144, 0.1544, 0.1148, 0.1379...               3   \n",
       "8   [0.1689, 0.214, 0.1401, 0.18, 0.1458, 0.1782, ...               3   \n",
       "9   [0.2545, 0.2601, 0.2682, 0.2231, 0.2424, 0.185...               4   \n",
       "10  [0.1324, 0.1936, 0.0917, 0.1347, 0.1031, 0.113...               4   \n",
       "11  [0.0963, 0.139, 0.0857, 0.1148, 0.078, 0.0966,...               5   \n",
       "12  [0.2425, 0.2442, 0.1797, 0.2801, 0.2244, 0.181...               6   \n",
       "13  [0.3976, 0.3814, 0.3875, 0.3881, 0.2902, 0.422...               7   \n",
       "14  [0.0942, 0.1206, 0.0809, 0.134, 0.0716, 0.0741...               8   \n",
       "15  [0.0984, 0.1371, 0.0884, 0.1025, 0.0847, 0.084...               8   \n",
       "16  [0.0803, 0.1169, 0.0647, 0.0917, 0.0555, 0.077...               9   \n",
       "17  [0.118, 0.1579, 0.1003, 0.126, 0.0892, 0.1055,...               9   \n",
       "18  [0.0695, 0.1046, 0.0551, 0.0776, 0.0459, 0.064...              10   \n",
       "19  [0.0718, 0.1145, 0.0648, 0.0821, 0.0514, 0.066...              10   \n",
       "20  [0.1004, 0.1403, 0.0766, 0.1082, 0.0786, 0.109...              11   \n",
       "21  [0.0497, 0.0845, 0.0361, 0.0632, 0.0331, 0.048...              11   \n",
       "22  [0.0605, 0.0993, 0.047, 0.0727, 0.0413, 0.0564...              12   \n",
       "23  [0.1127, 0.1435, 0.0829, 0.1103, 0.0708, 0.103...              13   \n",
       "24  [0.0628, 0.0956, 0.0484, 0.0721, 0.0413, 0.058...              13   \n",
       "25  [0.1103, 0.1482, 0.0909, 0.1258, 0.0899, 0.106...              14   \n",
       "26  [0.0629, 0.0955, 0.0468, 0.0747, 0.0412, 0.057...              14   \n",
       "27  [0.0934, 0.1136, 0.0707, 0.1062, 0.0641, 0.068...              15   \n",
       "28  [0.2545, 0.2212, 0.2144, 0.2549, 0.1472, 0.193...              15   \n",
       "29  [0.1787, 0.1659, 0.1505, 0.2157, 0.1303, 0.133...              15   \n",
       "30  [0.4108, 0.447, 0.5353, 0.4534, 0.4868, 0.434,...              16   \n",
       "31  [0.1546, 0.1747, 0.104, 0.1427, 0.0925, 0.1019...              17   \n",
       "32  [0.1153, 0.1486, 0.0863, 0.1198, 0.0847, 0.080...              17   \n",
       "33  [0.0373, 0.0655, 0.0273, 0.0443, 0.0242, 0.035...              18   \n",
       "34  [0.067, 0.1066, 0.0615, 0.0782, 0.0565, 0.0651...              19   \n",
       "35  [0.0417, 0.0731, 0.0306, 0.0509, 0.0272, 0.039...              20   \n",
       "36  [0.0565, 0.0908, 0.042, 0.0676, 0.0352, 0.0523...              21   \n",
       "37  [0.156, 0.2141, 0.139, 0.2024, 0.1051, 0.1428,...              21   \n",
       "38  [0.1154, 0.1408, 0.0878, 0.1334, 0.084, 0.1002...              22   \n",
       "39  [0.0662, 0.1001, 0.0523, 0.0829, 0.0465, 0.064...              22   \n",
       "40  [0.1891, 0.199, 0.1035, 0.1866, 0.1336, 0.1157...              22   \n",
       "41  [0.0565, 0.0899, 0.0455, 0.0673, 0.0384, 0.057...              22   \n",
       "42  [0.0463, 0.0752, 0.0316, 0.0549, 0.0282, 0.040...              23   \n",
       "43  [0.1777, 0.2056, 0.1293, 0.2011, 0.1226, 0.126...              23   \n",
       "44  [0.2499, 0.2471, 0.1671, 0.2523, 0.1681, 0.160...              23   \n",
       "45  [0.0593, 0.086, 0.0353, 0.0729, 0.0322, 0.0438...              23   \n",
       "46  [0.1978, 0.2312, 0.182, 0.2013, 0.1405, 0.2046...              24   \n",
       "47  [0.1152, 0.1486, 0.0894, 0.1041, 0.0895, 0.091...              24   \n",
       "48  [0.0369, 0.0649, 0.0274, 0.0434, 0.0243, 0.031...              25   \n",
       "49  [0.1172, 0.1686, 0.0874, 0.1167, 0.0831, 0.089...              25   \n",
       "50  [0.1665, 0.1927, 0.1239, 0.1678, 0.1606, 0.162...              26   \n",
       "51  [0.1252, 0.1567, 0.112, 0.1397, 0.1044, 0.08, ...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  1  [0.1421, 0.2584, 0.1275, 0.1534, 0.1066, 0.135...   \n",
       "1                  1  [0.5225, 0.5044, 0.3087, 0.4021, 0.3049, 0.283...   \n",
       "2                  1  [0.2657, 0.3227, 0.1887, 0.1638, 0.1158, 0.162...   \n",
       "3                 20  [0.3757, 0.3585, 0.2758, 0.391, 0.5611, 0.4019...   \n",
       "4                  8  [0.0799, 0.086, 0.084, 0.0759, 0.0868, 0.0514,...   \n",
       "5                  8  [0.0787, 0.0812, 0.0776, 0.1065, 0.0965, 0.157...   \n",
       "6                  8  [0.7943, 0.6162, 0.7718, 0.632, 0.55, 0.6441, ...   \n",
       "7                  8  [0.6084, 0.3731, 0.5431, 0.347, 0.3496, 0.5351...   \n",
       "8                  8  [0.4196, 0.297, 0.4773, 0.3138, 0.2578, 0.4116...   \n",
       "9                 26  [0.8184, 0.886, 0.8966, 0.8138, 0.9177, 0.7782...   \n",
       "10                 8  [0.6257, 0.7885, 0.6686, 0.5562, 0.6499, 0.543...   \n",
       "11                 8  [0.3837, 0.6198, 0.4802, 0.4177, 0.6584, 0.485...   \n",
       "12                20  [0.7739, 0.8768, 0.9128, 0.9054, 0.816, 0.8742...   \n",
       "13                20  [0.8413, 0.8265, 0.7466, 0.7112, 0.7985, 0.908...   \n",
       "14                20  [0.5072, 0.4241, 0.6417, 0.65, 0.6372, 0.5509,...   \n",
       "15                 8  [0.2288, 0.3954, 0.503, 0.3407, 0.3823, 0.4512...   \n",
       "16                 8  [0.1002, 0.0811, 0.0673, 0.1572, 0.1666, 0.169...   \n",
       "17                 8  [0.3238, 0.3488, 0.2861, 0.5152, 0.5109, 0.438...   \n",
       "18                 8  [0.0997, 0.3418, 0.0867, 0.114, 0.3062, 0.1126...   \n",
       "19                 8  [0.1356, 0.4996, 0.097, 0.2262, 0.3444, 0.1593...   \n",
       "20                 1  [0.1965, 0.3716, 0.2988, 0.4394, 0.1586, 0.262...   \n",
       "21                 1  [0.1074, 0.2383, 0.1388, 0.3521, 0.0967, 0.107...   \n",
       "22                 1  [0.0898, 0.2104, 0.1574, 0.2659, 0.1391, 0.246...   \n",
       "23                 1  [0.5009, 0.5302, 0.4004, 0.3915, 0.3988, 0.543...   \n",
       "24                 8  [0.064, 0.0808, 0.0687, 0.0929, 0.0849, 0.084,...   \n",
       "25                 1  [0.1529, 0.1592, 0.5408, 0.341, 0.2465, 0.3295...   \n",
       "26                 8  [0.0488, 0.0556, 0.2006, 0.1354, 0.0774, 0.138...   \n",
       "27                 8  [0.3988, 0.4448, 0.4792, 0.3859, 0.7525, 0.480...   \n",
       "28                15  [0.8061, 0.7349, 0.7214, 0.7068, 0.9305, 0.814...   \n",
       "29                20  [0.811, 0.6567, 0.8691, 0.7934, 0.9448, 0.8155...   \n",
       "30                19  [0.9999, 1.0, 0.9999, 1.0, 1.0, 1.0, 1.0, 0.99...   \n",
       "31                20  [0.6225, 0.5267, 0.4997, 0.4151, 0.4158, 0.274...   \n",
       "32                 1  [0.3959, 0.5992, 0.497, 0.5521, 0.3673, 0.474,...   \n",
       "33                 8  [0.0263, 0.0584, 0.0551, 0.0314, 0.0572, 0.062...   \n",
       "34                 1  [0.2278, 0.505, 0.1186, 0.1611, 0.3295, 0.1692...   \n",
       "35                 8  [0.0689, 0.1024, 0.2073, 0.1218, 0.0664, 0.182...   \n",
       "36                 8  [0.1058, 0.0924, 0.1701, 0.1747, 0.131, 0.1526...   \n",
       "37                23  [0.511, 0.7154, 0.695, 0.7604, 0.5512, 0.8257,...   \n",
       "38                23  [0.2895, 0.2386, 0.2337, 0.2374, 0.2618, 0.415...   \n",
       "39                 8  [0.1386, 0.0565, 0.0568, 0.0998, 0.086, 0.081,...   \n",
       "40                12  [0.5648, 0.641, 0.4486, 0.6775, 0.5784, 0.4425...   \n",
       "41                23  [0.0544, 0.0538, 0.0541, 0.0672, 0.0809, 0.143...   \n",
       "42                 8  [0.1311, 0.1096, 0.2272, 0.1997, 0.1113, 0.209...   \n",
       "43                20  [0.7189, 0.7361, 0.6859, 0.7881, 0.624, 0.8061...   \n",
       "44                23  [0.782, 0.7786, 0.8403, 0.8, 0.685, 0.9359, 0....   \n",
       "45                23  [0.3372, 0.2757, 0.401, 0.3328, 0.2739, 0.3052...   \n",
       "46                 9  [0.5613, 0.456, 0.4337, 0.4493, 0.4639, 0.5008...   \n",
       "47                 8  [0.3389, 0.4551, 0.2614, 0.4182, 0.2607, 0.407...   \n",
       "48                 8  [0.0835, 0.241, 0.0772, 0.0829, 0.2167, 0.1167...   \n",
       "49                 1  [0.239, 0.8217, 0.4822, 0.433, 0.5515, 0.4518,...   \n",
       "50                 8  [0.3522, 0.4654, 0.6419, 0.2958, 0.1867, 0.456...   \n",
       "51                26  [0.3907, 0.4139, 0.5376, 0.4449, 0.5362, 0.400...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8579, 0.7416, 0.8725, 0.8466, 0.8934, 0.864...  0.096154  0.213325  \n",
       "1   [0.4775, 0.4956, 0.6913, 0.5979, 0.6951, 0.716...       NaN       NaN  \n",
       "2   [0.7343, 0.6773, 0.8113, 0.8362, 0.8842, 0.837...       NaN       NaN  \n",
       "3   [0.6243, 0.6415, 0.7242, 0.609, 0.4389, 0.5981...       NaN       NaN  \n",
       "4   [0.9201, 0.914, 0.916, 0.9241, 0.9132, 0.9486,...       NaN       NaN  \n",
       "5   [0.9213, 0.9188, 0.9224, 0.8935, 0.9035, 0.842...       NaN       NaN  \n",
       "6   [0.2057, 0.3838, 0.2282, 0.368, 0.45, 0.3559, ...       NaN       NaN  \n",
       "7   [0.3916, 0.6269, 0.4569, 0.653, 0.6504, 0.4649...       NaN       NaN  \n",
       "8   [0.5804, 0.703, 0.5227, 0.6862, 0.7422, 0.5884...       NaN       NaN  \n",
       "9   [0.1816, 0.114, 0.1034, 0.1862, 0.0823, 0.2218...       NaN       NaN  \n",
       "10  [0.3743, 0.2115, 0.3314, 0.4438, 0.3501, 0.456...       NaN       NaN  \n",
       "11  [0.6163, 0.3802, 0.5198, 0.5823, 0.3416, 0.514...       NaN       NaN  \n",
       "12  [0.2261, 0.1232, 0.0871, 0.0946, 0.184, 0.1258...       NaN       NaN  \n",
       "13  [0.1587, 0.1735, 0.2534, 0.2888, 0.2015, 0.091...       NaN       NaN  \n",
       "14  [0.4928, 0.5759, 0.3583, 0.35, 0.3628, 0.4491,...       NaN       NaN  \n",
       "15  [0.7712, 0.6046, 0.497, 0.6593, 0.6177, 0.5488...       NaN       NaN  \n",
       "16  [0.8998, 0.9189, 0.9327, 0.8428, 0.8334, 0.831...       NaN       NaN  \n",
       "17  [0.6762, 0.6512, 0.7139, 0.4848, 0.4891, 0.561...       NaN       NaN  \n",
       "18  [0.9003, 0.6582, 0.9133, 0.886, 0.6938, 0.8874...       NaN       NaN  \n",
       "19  [0.8644, 0.5004, 0.903, 0.7738, 0.6556, 0.8407...       NaN       NaN  \n",
       "20  [0.8035, 0.6284, 0.7012, 0.5606, 0.8414, 0.737...       NaN       NaN  \n",
       "21  [0.8926, 0.7617, 0.8612, 0.6479, 0.9033, 0.892...       NaN       NaN  \n",
       "22  [0.9102, 0.7896, 0.8426, 0.7341, 0.8609, 0.754...       NaN       NaN  \n",
       "23  [0.4991, 0.4698, 0.5996, 0.6085, 0.6012, 0.456...       NaN       NaN  \n",
       "24  [0.936, 0.9192, 0.9313, 0.9071, 0.9151, 0.916,...       NaN       NaN  \n",
       "25  [0.8471, 0.8408, 0.4592, 0.659, 0.7535, 0.6705...       NaN       NaN  \n",
       "26  [0.9512, 0.9444, 0.7994, 0.8646, 0.9226, 0.861...       NaN       NaN  \n",
       "27  [0.6012, 0.5552, 0.5208, 0.6141, 0.2475, 0.519...       NaN       NaN  \n",
       "28  [0.1939, 0.2651, 0.2786, 0.2932, 0.0695, 0.185...       NaN       NaN  \n",
       "29  [0.189, 0.3433, 0.1309, 0.2066, 0.0552, 0.1845...       NaN       NaN  \n",
       "30  [1e-04, 0.0, 1e-04, 0.0, 0.0, 0.0, 0.0, 1e-04,...       NaN       NaN  \n",
       "31  [0.3775, 0.4733, 0.5003, 0.5849, 0.5842, 0.725...       NaN       NaN  \n",
       "32  [0.6041, 0.4008, 0.503, 0.4479, 0.6327, 0.526,...       NaN       NaN  \n",
       "33  [0.9737, 0.9416, 0.9449, 0.9686, 0.9428, 0.938...       NaN       NaN  \n",
       "34  [0.7722, 0.495, 0.8814, 0.8389, 0.6705, 0.8308...       NaN       NaN  \n",
       "35  [0.9311, 0.8976, 0.7927, 0.8782, 0.9336, 0.817...       NaN       NaN  \n",
       "36  [0.8942, 0.9076, 0.8299, 0.8253, 0.869, 0.8474...       NaN       NaN  \n",
       "37  [0.489, 0.2846, 0.305, 0.2396, 0.4488, 0.1743,...       NaN       NaN  \n",
       "38  [0.7105, 0.7614, 0.7663, 0.7626, 0.7382, 0.584...       NaN       NaN  \n",
       "39  [0.8614, 0.9435, 0.9432, 0.9002, 0.914, 0.919,...       NaN       NaN  \n",
       "40  [0.4352, 0.359, 0.5514, 0.3225, 0.4216, 0.5575...       NaN       NaN  \n",
       "41  [0.9456, 0.9462, 0.9459, 0.9328, 0.9191, 0.856...       NaN       NaN  \n",
       "42  [0.8689, 0.8904, 0.7728, 0.8003, 0.8887, 0.790...       NaN       NaN  \n",
       "43  [0.2811, 0.2639, 0.3141, 0.2119, 0.376, 0.1939...       NaN       NaN  \n",
       "44  [0.218, 0.2214, 0.1597, 0.2, 0.315, 0.0641, 0....       NaN       NaN  \n",
       "45  [0.6628, 0.7243, 0.599, 0.6672, 0.7261, 0.6948...       NaN       NaN  \n",
       "46  [0.4387, 0.544, 0.5663, 0.5507, 0.5361, 0.4992...       NaN       NaN  \n",
       "47  [0.6611, 0.5449, 0.7386, 0.5818, 0.7393, 0.592...       NaN       NaN  \n",
       "48  [0.9165, 0.759, 0.9228, 0.9171, 0.7833, 0.8833...       NaN       NaN  \n",
       "49  [0.761, 0.1783, 0.5178, 0.567, 0.4485, 0.5482,...       NaN       NaN  \n",
       "50  [0.6478, 0.5346, 0.3581, 0.7042, 0.8133, 0.543...       NaN       NaN  \n",
       "51  [0.6093, 0.5861, 0.4624, 0.5551, 0.4638, 0.599...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 : Training: loss:  0.22995013\n",
      "202 : Training: loss:  0.22135213\n",
      "203 : Training: loss:  0.19117527\n",
      "204 : Training: loss:  0.19895793\n",
      "205 : Training: loss:  0.22415127\n",
      "206 : Training: loss:  0.181366\n",
      "207 : Training: loss:  0.19615377\n",
      "208 : Training: loss:  0.2084776\n",
      "209 : Training: loss:  0.18925752\n",
      "210 : Training: loss:  0.1858956\n",
      "211 : Training: loss:  0.1998443\n",
      "212 : Training: loss:  0.1634209\n",
      "213 : Training: loss:  0.16785623\n",
      "214 : Training: loss:  0.2099654\n",
      "215 : Training: loss:  0.22768676\n",
      "216 : Training: loss:  0.19690345\n",
      "217 : Training: loss:  0.18453884\n",
      "218 : Training: loss:  0.1723782\n",
      "219 : Training: loss:  0.18011601\n",
      "220 : Training: loss:  0.2466101\n",
      "Validation: Loss:  0.20058428  Accuracy:  0.13461539\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0691, 0.1023, 0.0497, 0.0784, 0.049, 0.0607...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.14, 0.2549, 0.1248, 0.151, 0.1056, 0.133, 0...</td>\n",
       "      <td>[0.86, 0.7451, 0.8752, 0.849, 0.8944, 0.867, 0...</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.200584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1047, 0.1402, 0.0686, 0.0997, 0.0745, 0.073...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5169, 0.4989, 0.3017, 0.3968, 0.3017, 0.277...</td>\n",
       "      <td>[0.4831, 0.5011, 0.6983, 0.6032, 0.6983, 0.722...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0681, 0.1059, 0.0517, 0.0761, 0.0501, 0.062...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2612, 0.3183, 0.1843, 0.1608, 0.1141, 0.158...</td>\n",
       "      <td>[0.7388, 0.6817, 0.8157, 0.8392, 0.8859, 0.841...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0604, 0.0888, 0.0496, 0.072, 0.0434, 0.0484...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.3647, 0.35, 0.2649, 0.3799, 0.5532, 0.3895,...</td>\n",
       "      <td>[0.6353, 0.65, 0.7351, 0.6201, 0.4468, 0.6105,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0432, 0.0704, 0.029, 0.0522, 0.0283, 0.0389...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0772, 0.0836, 0.0808, 0.0734, 0.0849, 0.049...</td>\n",
       "      <td>[0.9228, 0.9164, 0.9192, 0.9266, 0.9151, 0.950...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0733, 0.1084, 0.0589, 0.0849, 0.0517, 0.071...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0774, 0.0793, 0.0758, 0.1037, 0.0956, 0.155...</td>\n",
       "      <td>[0.9226, 0.9207, 0.9242, 0.8963, 0.9044, 0.845...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.1878, 0.1897, 0.0935, 0.1782, 0.1435, 0.103...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.7891, 0.6091, 0.7679, 0.6269, 0.5381, 0.638...</td>\n",
       "      <td>[0.2109, 0.3909, 0.2321, 0.3731, 0.4619, 0.361...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.1244, 0.1502, 0.0959, 0.1399, 0.097, 0.1176...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6004, 0.3646, 0.5376, 0.3412, 0.3412, 0.528...</td>\n",
       "      <td>[0.3996, 0.6354, 0.4624, 0.6588, 0.6588, 0.471...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1513, 0.1937, 0.1211, 0.1667, 0.127, 0.157,...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4149, 0.2935, 0.4767, 0.3107, 0.2529, 0.406...</td>\n",
       "      <td>[0.5851, 0.7065, 0.5233, 0.6893, 0.7471, 0.593...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.2339, 0.2449, 0.2466, 0.2107, 0.2259, 0.168...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.8128, 0.8828, 0.8918, 0.8082, 0.9156, 0.769...</td>\n",
       "      <td>[0.1872, 0.1172, 0.1082, 0.1918, 0.0844, 0.231...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.1093, 0.1666, 0.0731, 0.117, 0.0838, 0.0923...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.608, 0.778, 0.6519, 0.537, 0.6364, 0.5234, ...</td>\n",
       "      <td>[0.392, 0.222, 0.3481, 0.463, 0.3636, 0.4766, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0783, 0.1171, 0.0673, 0.0982, 0.0621, 0.077...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3698, 0.6075, 0.4643, 0.4033, 0.6472, 0.470...</td>\n",
       "      <td>[0.6302, 0.3925, 0.5357, 0.5967, 0.3528, 0.529...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.2192, 0.2236, 0.1599, 0.2585, 0.1999, 0.158...</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.7632, 0.8699, 0.9083, 0.8984, 0.8081, 0.867...</td>\n",
       "      <td>[0.2368, 0.1301, 0.0917, 0.1016, 0.1919, 0.132...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.3743, 0.3625, 0.3631, 0.369, 0.2698, 0.3968...</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.8313, 0.8127, 0.7325, 0.6924, 0.7896, 0.904...</td>\n",
       "      <td>[0.1687, 0.1873, 0.2675, 0.3076, 0.2104, 0.096...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0777, 0.1024, 0.0643, 0.1168, 0.0576, 0.059...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.494, 0.4139, 0.6301, 0.6386, 0.6259, 0.5371...</td>\n",
       "      <td>[0.506, 0.5861, 0.3699, 0.3614, 0.3741, 0.4629...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.082, 0.1176, 0.0706, 0.0893, 0.0686, 0.0694...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2203, 0.388, 0.4906, 0.3296, 0.373, 0.4363,...</td>\n",
       "      <td>[0.7797, 0.612, 0.5094, 0.6704, 0.627, 0.5637,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0669, 0.0994, 0.0514, 0.0802, 0.0445, 0.063...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0979, 0.0789, 0.0657, 0.153, 0.1631, 0.1656...</td>\n",
       "      <td>[0.9021, 0.9211, 0.9343, 0.847, 0.8369, 0.8344...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.1001, 0.1371, 0.0821, 0.1118, 0.0735, 0.088...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3175, 0.3424, 0.2805, 0.5054, 0.5033, 0.429...</td>\n",
       "      <td>[0.6825, 0.6576, 0.7195, 0.4946, 0.4967, 0.570...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0571, 0.088, 0.043, 0.0672, 0.0363, 0.0518,...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0972, 0.3351, 0.0845, 0.1109, 0.301, 0.1099...</td>\n",
       "      <td>[0.9028, 0.6649, 0.9155, 0.8891, 0.699, 0.8901...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0591, 0.0965, 0.0507, 0.0709, 0.0406, 0.053...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1317, 0.4932, 0.0943, 0.2199, 0.3404, 0.154...</td>\n",
       "      <td>[0.8683, 0.5068, 0.9057, 0.7801, 0.6596, 0.845...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.086, 0.122, 0.063, 0.0967, 0.0649, 0.0919, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1937, 0.366, 0.2947, 0.4346, 0.1567, 0.2581...</td>\n",
       "      <td>[0.8063, 0.634, 0.7053, 0.5654, 0.8433, 0.7419...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0395, 0.0688, 0.0272, 0.0535, 0.0252, 0.037...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1034, 0.2292, 0.135, 0.3428, 0.0929, 0.1044...</td>\n",
       "      <td>[0.8966, 0.7708, 0.865, 0.6572, 0.9071, 0.8956...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0496, 0.0832, 0.0366, 0.063, 0.0326, 0.0453...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0872, 0.2045, 0.1536, 0.2587, 0.1349, 0.240...</td>\n",
       "      <td>[0.9128, 0.7955, 0.8464, 0.7413, 0.8651, 0.759...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0945, 0.1233, 0.0664, 0.0968, 0.0575, 0.084...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4902, 0.5204, 0.3941, 0.3809, 0.3913, 0.531...</td>\n",
       "      <td>[0.5098, 0.4796, 0.6059, 0.6191, 0.6087, 0.468...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0519, 0.0803, 0.0379, 0.0629, 0.0328, 0.047...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0634, 0.0793, 0.0685, 0.0915, 0.0832, 0.082...</td>\n",
       "      <td>[0.9366, 0.9207, 0.9315, 0.9085, 0.9168, 0.917...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0939, 0.1287, 0.0743, 0.112, 0.0742, 0.089,...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1499, 0.1565, 0.5356, 0.3353, 0.2424, 0.323...</td>\n",
       "      <td>[0.8501, 0.8435, 0.4644, 0.6647, 0.7576, 0.676...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0516, 0.0801, 0.0364, 0.0647, 0.0325, 0.045...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0476, 0.0537, 0.1971, 0.1312, 0.0752, 0.135...</td>\n",
       "      <td>[0.9524, 0.9463, 0.8029, 0.8688, 0.9248, 0.864...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0764, 0.0964, 0.0557, 0.0909, 0.0516, 0.054...</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3875, 0.4347, 0.4672, 0.3718, 0.7462, 0.471...</td>\n",
       "      <td>[0.6125, 0.5653, 0.5328, 0.6282, 0.2538, 0.528...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.2232, 0.1984, 0.1856, 0.2309, 0.1278, 0.166...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.795, 0.7201, 0.7088, 0.6885, 0.9277, 0.8075...</td>\n",
       "      <td>[0.205, 0.2799, 0.2912, 0.3115, 0.0723, 0.1925...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.1527, 0.1456, 0.1255, 0.1922, 0.1104, 0.109...</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.8002, 0.6407, 0.8618, 0.7797, 0.942, 0.8075...</td>\n",
       "      <td>[0.1998, 0.3593, 0.1382, 0.2203, 0.058, 0.1925...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.4049, 0.443, 0.529, 0.4477, 0.4835, 0.4244,...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.9999, 1.0, 0.9999, 1.0, 1.0, 1.0, 1.0, 0.99...</td>\n",
       "      <td>[1e-04, 0.0, 1e-04, 0.0, 0.0, 0.0, 0.0, 0.0002...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.1354, 0.1559, 0.0886, 0.1299, 0.0795, 0.087...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6194, 0.5194, 0.495, 0.4106, 0.4106, 0.2724...</td>\n",
       "      <td>[0.3806, 0.4806, 0.505, 0.5894, 0.5894, 0.7276...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0988, 0.13, 0.0715, 0.1073, 0.0704, 0.0676,...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.392, 0.5938, 0.4928, 0.5457, 0.3614, 0.4688...</td>\n",
       "      <td>[0.608, 0.4062, 0.5072, 0.4543, 0.6386, 0.5312...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0297, 0.0533, 0.0204, 0.0377, 0.0185, 0.027...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0253, 0.0569, 0.0531, 0.03, 0.0554, 0.0597,...</td>\n",
       "      <td>[0.9747, 0.9431, 0.9469, 0.97, 0.9446, 0.9403,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0556, 0.0906, 0.0489, 0.0683, 0.0456, 0.053...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2214, 0.5033, 0.1148, 0.1567, 0.3268, 0.163...</td>\n",
       "      <td>[0.7786, 0.4967, 0.8852, 0.8433, 0.6732, 0.836...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0331, 0.0595, 0.0229, 0.0431, 0.0207, 0.030...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0666, 0.0982, 0.2025, 0.1172, 0.0642, 0.177...</td>\n",
       "      <td>[0.9334, 0.9018, 0.7975, 0.8828, 0.9358, 0.822...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0459, 0.0755, 0.0323, 0.0582, 0.0275, 0.041...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1033, 0.088, 0.166, 0.1705, 0.1252, 0.1485,...</td>\n",
       "      <td>[0.8967, 0.912, 0.834, 0.8295, 0.8748, 0.8515,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.1283, 0.1845, 0.1112, 0.1751, 0.0839, 0.115...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4909, 0.6936, 0.6804, 0.7446, 0.5243, 0.815...</td>\n",
       "      <td>[0.5091, 0.3064, 0.3196, 0.2554, 0.4757, 0.185...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0971, 0.121, 0.0712, 0.1172, 0.0686, 0.0827...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2834, 0.2313, 0.2267, 0.2308, 0.2542, 0.412...</td>\n",
       "      <td>[0.7166, 0.7687, 0.7733, 0.7692, 0.7458, 0.587...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0543, 0.084, 0.0407, 0.0717, 0.0366, 0.0519...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1354, 0.0542, 0.0552, 0.0975, 0.0825, 0.079...</td>\n",
       "      <td>[0.8646, 0.9458, 0.9448, 0.9025, 0.9175, 0.920...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.1634, 0.1765, 0.0873, 0.1672, 0.1136, 0.097...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5572, 0.6313, 0.438, 0.6695, 0.569, 0.4376,...</td>\n",
       "      <td>[0.4428, 0.3687, 0.562, 0.3305, 0.431, 0.5624,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0462, 0.0749, 0.0351, 0.0582, 0.0301, 0.045...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0531, 0.052, 0.0526, 0.0655, 0.0784, 0.1417...</td>\n",
       "      <td>[0.9469, 0.948, 0.9474, 0.9345, 0.9216, 0.8583...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0365, 0.0607, 0.0235, 0.0463, 0.0211, 0.031...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1261, 0.1035, 0.2212, 0.1923, 0.105, 0.202,...</td>\n",
       "      <td>[0.8739, 0.8965, 0.7788, 0.8077, 0.895, 0.798,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.1432, 0.1723, 0.1008, 0.1703, 0.0939, 0.099...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.695, 0.7087, 0.6669, 0.7668, 0.5931, 0.7892...</td>\n",
       "      <td>[0.305, 0.2913, 0.3331, 0.2332, 0.4069, 0.2108...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.2115, 0.215, 0.1366, 0.2208, 0.136, 0.131, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7617, 0.7517, 0.8282, 0.7789, 0.6547, 0.929...</td>\n",
       "      <td>[0.2383, 0.2483, 0.1718, 0.2211, 0.3453, 0.070...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0457, 0.0685, 0.0258, 0.0601, 0.0237, 0.033...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3229, 0.2576, 0.3885, 0.3178, 0.2564, 0.292...</td>\n",
       "      <td>[0.6771, 0.7424, 0.6115, 0.6822, 0.7436, 0.707...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1783, 0.2112, 0.1603, 0.1866, 0.124, 0.1828...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5576, 0.4499, 0.4312, 0.4443, 0.4597, 0.495...</td>\n",
       "      <td>[0.4424, 0.5501, 0.5688, 0.5557, 0.5403, 0.504...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0996, 0.1317, 0.0751, 0.0938, 0.0759, 0.078...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3357, 0.4514, 0.2583, 0.4146, 0.259, 0.4024...</td>\n",
       "      <td>[0.6643, 0.5486, 0.7417, 0.5854, 0.741, 0.5976...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0284, 0.0517, 0.0198, 0.0357, 0.018, 0.0239...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0785, 0.2344, 0.0722, 0.0772, 0.2102, 0.109...</td>\n",
       "      <td>[0.9215, 0.7656, 0.9278, 0.9228, 0.7898, 0.890...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0972, 0.1449, 0.0703, 0.1006, 0.0677, 0.073...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2275, 0.8165, 0.4651, 0.4155, 0.5417, 0.434...</td>\n",
       "      <td>[0.7725, 0.1835, 0.5349, 0.5845, 0.4583, 0.565...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1479, 0.1742, 0.107, 0.1535, 0.14, 0.1427, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3488, 0.4585, 0.6359, 0.2915, 0.1844, 0.451...</td>\n",
       "      <td>[0.6512, 0.5415, 0.3641, 0.7085, 0.8156, 0.548...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.1077, 0.1382, 0.0938, 0.1254, 0.0885, 0.067...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.3835, 0.4087, 0.529, 0.4359, 0.5292, 0.3901...</td>\n",
       "      <td>[0.6165, 0.5913, 0.471, 0.5641, 0.4708, 0.6099...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0691, 0.1023, 0.0497, 0.0784, 0.049, 0.0607...               0   \n",
       "1   [0.1047, 0.1402, 0.0686, 0.0997, 0.0745, 0.073...               0   \n",
       "2   [0.0681, 0.1059, 0.0517, 0.0761, 0.0501, 0.062...               0   \n",
       "3   [0.0604, 0.0888, 0.0496, 0.072, 0.0434, 0.0484...               1   \n",
       "4   [0.0432, 0.0704, 0.029, 0.0522, 0.0283, 0.0389...               1   \n",
       "5   [0.0733, 0.1084, 0.0589, 0.0849, 0.0517, 0.071...               2   \n",
       "6   [0.1878, 0.1897, 0.0935, 0.1782, 0.1435, 0.103...               3   \n",
       "7   [0.1244, 0.1502, 0.0959, 0.1399, 0.097, 0.1176...               3   \n",
       "8   [0.1513, 0.1937, 0.1211, 0.1667, 0.127, 0.157,...               3   \n",
       "9   [0.2339, 0.2449, 0.2466, 0.2107, 0.2259, 0.168...               4   \n",
       "10  [0.1093, 0.1666, 0.0731, 0.117, 0.0838, 0.0923...               4   \n",
       "11  [0.0783, 0.1171, 0.0673, 0.0982, 0.0621, 0.077...               5   \n",
       "12  [0.2192, 0.2236, 0.1599, 0.2585, 0.1999, 0.158...               6   \n",
       "13  [0.3743, 0.3625, 0.3631, 0.369, 0.2698, 0.3968...               7   \n",
       "14  [0.0777, 0.1024, 0.0643, 0.1168, 0.0576, 0.059...               8   \n",
       "15  [0.082, 0.1176, 0.0706, 0.0893, 0.0686, 0.0694...               8   \n",
       "16  [0.0669, 0.0994, 0.0514, 0.0802, 0.0445, 0.063...               9   \n",
       "17  [0.1001, 0.1371, 0.0821, 0.1118, 0.0735, 0.088...               9   \n",
       "18  [0.0571, 0.088, 0.043, 0.0672, 0.0363, 0.0518,...              10   \n",
       "19  [0.0591, 0.0965, 0.0507, 0.0709, 0.0406, 0.053...              10   \n",
       "20  [0.086, 0.122, 0.063, 0.0967, 0.0649, 0.0919, ...              11   \n",
       "21  [0.0395, 0.0688, 0.0272, 0.0535, 0.0252, 0.037...              11   \n",
       "22  [0.0496, 0.0832, 0.0366, 0.063, 0.0326, 0.0453...              12   \n",
       "23  [0.0945, 0.1233, 0.0664, 0.0968, 0.0575, 0.084...              13   \n",
       "24  [0.0519, 0.0803, 0.0379, 0.0629, 0.0328, 0.047...              13   \n",
       "25  [0.0939, 0.1287, 0.0743, 0.112, 0.0742, 0.089,...              14   \n",
       "26  [0.0516, 0.0801, 0.0364, 0.0647, 0.0325, 0.045...              14   \n",
       "27  [0.0764, 0.0964, 0.0557, 0.0909, 0.0516, 0.054...              15   \n",
       "28  [0.2232, 0.1984, 0.1856, 0.2309, 0.1278, 0.166...              15   \n",
       "29  [0.1527, 0.1456, 0.1255, 0.1922, 0.1104, 0.109...              15   \n",
       "30  [0.4049, 0.443, 0.529, 0.4477, 0.4835, 0.4244,...              16   \n",
       "31  [0.1354, 0.1559, 0.0886, 0.1299, 0.0795, 0.087...              17   \n",
       "32  [0.0988, 0.13, 0.0715, 0.1073, 0.0704, 0.0676,...              17   \n",
       "33  [0.0297, 0.0533, 0.0204, 0.0377, 0.0185, 0.027...              18   \n",
       "34  [0.0556, 0.0906, 0.0489, 0.0683, 0.0456, 0.053...              19   \n",
       "35  [0.0331, 0.0595, 0.0229, 0.0431, 0.0207, 0.030...              20   \n",
       "36  [0.0459, 0.0755, 0.0323, 0.0582, 0.0275, 0.041...              21   \n",
       "37  [0.1283, 0.1845, 0.1112, 0.1751, 0.0839, 0.115...              21   \n",
       "38  [0.0971, 0.121, 0.0712, 0.1172, 0.0686, 0.0827...              22   \n",
       "39  [0.0543, 0.084, 0.0407, 0.0717, 0.0366, 0.0519...              22   \n",
       "40  [0.1634, 0.1765, 0.0873, 0.1672, 0.1136, 0.097...              22   \n",
       "41  [0.0462, 0.0749, 0.0351, 0.0582, 0.0301, 0.045...              22   \n",
       "42  [0.0365, 0.0607, 0.0235, 0.0463, 0.0211, 0.031...              23   \n",
       "43  [0.1432, 0.1723, 0.1008, 0.1703, 0.0939, 0.099...              23   \n",
       "44  [0.2115, 0.215, 0.1366, 0.2208, 0.136, 0.131, ...              23   \n",
       "45  [0.0457, 0.0685, 0.0258, 0.0601, 0.0237, 0.033...              23   \n",
       "46  [0.1783, 0.2112, 0.1603, 0.1866, 0.124, 0.1828...              24   \n",
       "47  [0.0996, 0.1317, 0.0751, 0.0938, 0.0759, 0.078...              24   \n",
       "48  [0.0284, 0.0517, 0.0198, 0.0357, 0.018, 0.0239...              25   \n",
       "49  [0.0972, 0.1449, 0.0703, 0.1006, 0.0677, 0.073...              25   \n",
       "50  [0.1479, 0.1742, 0.107, 0.1535, 0.14, 0.1427, ...              26   \n",
       "51  [0.1077, 0.1382, 0.0938, 0.1254, 0.0885, 0.067...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  1  [0.14, 0.2549, 0.1248, 0.151, 0.1056, 0.133, 0...   \n",
       "1                  1  [0.5169, 0.4989, 0.3017, 0.3968, 0.3017, 0.277...   \n",
       "2                  1  [0.2612, 0.3183, 0.1843, 0.1608, 0.1141, 0.158...   \n",
       "3                 19  [0.3647, 0.35, 0.2649, 0.3799, 0.5532, 0.3895,...   \n",
       "4                  1  [0.0772, 0.0836, 0.0808, 0.0734, 0.0849, 0.049...   \n",
       "5                  1  [0.0774, 0.0793, 0.0758, 0.1037, 0.0956, 0.155...   \n",
       "6                  1  [0.7891, 0.6091, 0.7679, 0.6269, 0.5381, 0.638...   \n",
       "7                 23  [0.6004, 0.3646, 0.5376, 0.3412, 0.3412, 0.528...   \n",
       "8                  1  [0.4149, 0.2935, 0.4767, 0.3107, 0.2529, 0.406...   \n",
       "9                 26  [0.8128, 0.8828, 0.8918, 0.8082, 0.9156, 0.769...   \n",
       "10                 8  [0.608, 0.778, 0.6519, 0.537, 0.6364, 0.5234, ...   \n",
       "11                 8  [0.3698, 0.6075, 0.4643, 0.4033, 0.6472, 0.470...   \n",
       "12                20  [0.7632, 0.8699, 0.9083, 0.8984, 0.8081, 0.867...   \n",
       "13                20  [0.8313, 0.8127, 0.7325, 0.6924, 0.7896, 0.904...   \n",
       "14                 3  [0.494, 0.4139, 0.6301, 0.6386, 0.6259, 0.5371...   \n",
       "15                 1  [0.2203, 0.388, 0.4906, 0.3296, 0.373, 0.4363,...   \n",
       "16                 1  [0.0979, 0.0789, 0.0657, 0.153, 0.1631, 0.1656...   \n",
       "17                 1  [0.3175, 0.3424, 0.2805, 0.5054, 0.5033, 0.429...   \n",
       "18                 8  [0.0972, 0.3351, 0.0845, 0.1109, 0.301, 0.1099...   \n",
       "19                 8  [0.1317, 0.4932, 0.0943, 0.2199, 0.3404, 0.154...   \n",
       "20                 1  [0.1937, 0.366, 0.2947, 0.4346, 0.1567, 0.2581...   \n",
       "21                 1  [0.1034, 0.2292, 0.135, 0.3428, 0.0929, 0.1044...   \n",
       "22                 1  [0.0872, 0.2045, 0.1536, 0.2587, 0.1349, 0.240...   \n",
       "23                 1  [0.4902, 0.5204, 0.3941, 0.3809, 0.3913, 0.531...   \n",
       "24                 1  [0.0634, 0.0793, 0.0685, 0.0915, 0.0832, 0.082...   \n",
       "25                 1  [0.1499, 0.1565, 0.5356, 0.3353, 0.2424, 0.323...   \n",
       "26                 1  [0.0476, 0.0537, 0.1971, 0.1312, 0.0752, 0.135...   \n",
       "27                 8  [0.3875, 0.4347, 0.4672, 0.3718, 0.7462, 0.471...   \n",
       "28                15  [0.795, 0.7201, 0.7088, 0.6885, 0.9277, 0.8075...   \n",
       "29                20  [0.8002, 0.6407, 0.8618, 0.7797, 0.942, 0.8075...   \n",
       "30                19  [0.9999, 1.0, 0.9999, 1.0, 1.0, 1.0, 1.0, 0.99...   \n",
       "31                 1  [0.6194, 0.5194, 0.495, 0.4106, 0.4106, 0.2724...   \n",
       "32                 1  [0.392, 0.5938, 0.4928, 0.5457, 0.3614, 0.4688...   \n",
       "33                 1  [0.0253, 0.0569, 0.0531, 0.03, 0.0554, 0.0597,...   \n",
       "34                 1  [0.2214, 0.5033, 0.1148, 0.1567, 0.3268, 0.163...   \n",
       "35                 1  [0.0666, 0.0982, 0.2025, 0.1172, 0.0642, 0.177...   \n",
       "36                23  [0.1033, 0.088, 0.166, 0.1705, 0.1252, 0.1485,...   \n",
       "37                23  [0.4909, 0.6936, 0.6804, 0.7446, 0.5243, 0.815...   \n",
       "38                23  [0.2834, 0.2313, 0.2267, 0.2308, 0.2542, 0.412...   \n",
       "39                23  [0.1354, 0.0542, 0.0552, 0.0975, 0.0825, 0.079...   \n",
       "40                23  [0.5572, 0.6313, 0.438, 0.6695, 0.569, 0.4376,...   \n",
       "41                23  [0.0531, 0.052, 0.0526, 0.0655, 0.0784, 0.1417...   \n",
       "42                23  [0.1261, 0.1035, 0.2212, 0.1923, 0.105, 0.202,...   \n",
       "43                23  [0.695, 0.7087, 0.6669, 0.7668, 0.5931, 0.7892...   \n",
       "44                23  [0.7617, 0.7517, 0.8282, 0.7789, 0.6547, 0.929...   \n",
       "45                23  [0.3229, 0.2576, 0.3885, 0.3178, 0.2564, 0.292...   \n",
       "46                 9  [0.5576, 0.4499, 0.4312, 0.4443, 0.4597, 0.495...   \n",
       "47                 1  [0.3357, 0.4514, 0.2583, 0.4146, 0.259, 0.4024...   \n",
       "48                 1  [0.0785, 0.2344, 0.0722, 0.0772, 0.2102, 0.109...   \n",
       "49                 1  [0.2275, 0.8165, 0.4651, 0.4155, 0.5417, 0.434...   \n",
       "50                 8  [0.3488, 0.4585, 0.6359, 0.2915, 0.1844, 0.451...   \n",
       "51                26  [0.3835, 0.4087, 0.529, 0.4359, 0.5292, 0.3901...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.86, 0.7451, 0.8752, 0.849, 0.8944, 0.867, 0...  0.134615  0.200584  \n",
       "1   [0.4831, 0.5011, 0.6983, 0.6032, 0.6983, 0.722...       NaN       NaN  \n",
       "2   [0.7388, 0.6817, 0.8157, 0.8392, 0.8859, 0.841...       NaN       NaN  \n",
       "3   [0.6353, 0.65, 0.7351, 0.6201, 0.4468, 0.6105,...       NaN       NaN  \n",
       "4   [0.9228, 0.9164, 0.9192, 0.9266, 0.9151, 0.950...       NaN       NaN  \n",
       "5   [0.9226, 0.9207, 0.9242, 0.8963, 0.9044, 0.845...       NaN       NaN  \n",
       "6   [0.2109, 0.3909, 0.2321, 0.3731, 0.4619, 0.361...       NaN       NaN  \n",
       "7   [0.3996, 0.6354, 0.4624, 0.6588, 0.6588, 0.471...       NaN       NaN  \n",
       "8   [0.5851, 0.7065, 0.5233, 0.6893, 0.7471, 0.593...       NaN       NaN  \n",
       "9   [0.1872, 0.1172, 0.1082, 0.1918, 0.0844, 0.231...       NaN       NaN  \n",
       "10  [0.392, 0.222, 0.3481, 0.463, 0.3636, 0.4766, ...       NaN       NaN  \n",
       "11  [0.6302, 0.3925, 0.5357, 0.5967, 0.3528, 0.529...       NaN       NaN  \n",
       "12  [0.2368, 0.1301, 0.0917, 0.1016, 0.1919, 0.132...       NaN       NaN  \n",
       "13  [0.1687, 0.1873, 0.2675, 0.3076, 0.2104, 0.096...       NaN       NaN  \n",
       "14  [0.506, 0.5861, 0.3699, 0.3614, 0.3741, 0.4629...       NaN       NaN  \n",
       "15  [0.7797, 0.612, 0.5094, 0.6704, 0.627, 0.5637,...       NaN       NaN  \n",
       "16  [0.9021, 0.9211, 0.9343, 0.847, 0.8369, 0.8344...       NaN       NaN  \n",
       "17  [0.6825, 0.6576, 0.7195, 0.4946, 0.4967, 0.570...       NaN       NaN  \n",
       "18  [0.9028, 0.6649, 0.9155, 0.8891, 0.699, 0.8901...       NaN       NaN  \n",
       "19  [0.8683, 0.5068, 0.9057, 0.7801, 0.6596, 0.845...       NaN       NaN  \n",
       "20  [0.8063, 0.634, 0.7053, 0.5654, 0.8433, 0.7419...       NaN       NaN  \n",
       "21  [0.8966, 0.7708, 0.865, 0.6572, 0.9071, 0.8956...       NaN       NaN  \n",
       "22  [0.9128, 0.7955, 0.8464, 0.7413, 0.8651, 0.759...       NaN       NaN  \n",
       "23  [0.5098, 0.4796, 0.6059, 0.6191, 0.6087, 0.468...       NaN       NaN  \n",
       "24  [0.9366, 0.9207, 0.9315, 0.9085, 0.9168, 0.917...       NaN       NaN  \n",
       "25  [0.8501, 0.8435, 0.4644, 0.6647, 0.7576, 0.676...       NaN       NaN  \n",
       "26  [0.9524, 0.9463, 0.8029, 0.8688, 0.9248, 0.864...       NaN       NaN  \n",
       "27  [0.6125, 0.5653, 0.5328, 0.6282, 0.2538, 0.528...       NaN       NaN  \n",
       "28  [0.205, 0.2799, 0.2912, 0.3115, 0.0723, 0.1925...       NaN       NaN  \n",
       "29  [0.1998, 0.3593, 0.1382, 0.2203, 0.058, 0.1925...       NaN       NaN  \n",
       "30  [1e-04, 0.0, 1e-04, 0.0, 0.0, 0.0, 0.0, 0.0002...       NaN       NaN  \n",
       "31  [0.3806, 0.4806, 0.505, 0.5894, 0.5894, 0.7276...       NaN       NaN  \n",
       "32  [0.608, 0.4062, 0.5072, 0.4543, 0.6386, 0.5312...       NaN       NaN  \n",
       "33  [0.9747, 0.9431, 0.9469, 0.97, 0.9446, 0.9403,...       NaN       NaN  \n",
       "34  [0.7786, 0.4967, 0.8852, 0.8433, 0.6732, 0.836...       NaN       NaN  \n",
       "35  [0.9334, 0.9018, 0.7975, 0.8828, 0.9358, 0.822...       NaN       NaN  \n",
       "36  [0.8967, 0.912, 0.834, 0.8295, 0.8748, 0.8515,...       NaN       NaN  \n",
       "37  [0.5091, 0.3064, 0.3196, 0.2554, 0.4757, 0.185...       NaN       NaN  \n",
       "38  [0.7166, 0.7687, 0.7733, 0.7692, 0.7458, 0.587...       NaN       NaN  \n",
       "39  [0.8646, 0.9458, 0.9448, 0.9025, 0.9175, 0.920...       NaN       NaN  \n",
       "40  [0.4428, 0.3687, 0.562, 0.3305, 0.431, 0.5624,...       NaN       NaN  \n",
       "41  [0.9469, 0.948, 0.9474, 0.9345, 0.9216, 0.8583...       NaN       NaN  \n",
       "42  [0.8739, 0.8965, 0.7788, 0.8077, 0.895, 0.798,...       NaN       NaN  \n",
       "43  [0.305, 0.2913, 0.3331, 0.2332, 0.4069, 0.2108...       NaN       NaN  \n",
       "44  [0.2383, 0.2483, 0.1718, 0.2211, 0.3453, 0.070...       NaN       NaN  \n",
       "45  [0.6771, 0.7424, 0.6115, 0.6822, 0.7436, 0.707...       NaN       NaN  \n",
       "46  [0.4424, 0.5501, 0.5688, 0.5557, 0.5403, 0.504...       NaN       NaN  \n",
       "47  [0.6643, 0.5486, 0.7417, 0.5854, 0.741, 0.5976...       NaN       NaN  \n",
       "48  [0.9215, 0.7656, 0.9278, 0.9228, 0.7898, 0.890...       NaN       NaN  \n",
       "49  [0.7725, 0.1835, 0.5349, 0.5845, 0.4583, 0.565...       NaN       NaN  \n",
       "50  [0.6512, 0.5415, 0.3641, 0.7085, 0.8156, 0.548...       NaN       NaN  \n",
       "51  [0.6165, 0.5913, 0.471, 0.5641, 0.4708, 0.6099...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 : Training: loss:  0.19590816\n",
      "222 : Training: loss:  0.17054476\n",
      "223 : Training: loss:  0.19779016\n",
      "224 : Training: loss:  0.18790725\n",
      "225 : Training: loss:  0.19303128\n",
      "226 : Training: loss:  0.17659579\n",
      "227 : Training: loss:  0.17700501\n",
      "228 : Training: loss:  0.20198266\n",
      "229 : Training: loss:  0.2057299\n",
      "230 : Training: loss:  0.20912103\n",
      "231 : Training: loss:  0.16552828\n",
      "232 : Training: loss:  0.18828866\n",
      "233 : Training: loss:  0.17273448\n",
      "234 : Training: loss:  0.16813943\n",
      "235 : Training: loss:  0.17444618\n",
      "236 : Training: loss:  0.15797727\n",
      "237 : Training: loss:  0.16343866\n",
      "238 : Training: loss:  0.16054031\n",
      "239 : Training: loss:  0.15637891\n",
      "240 : Training: loss:  0.17482077\n",
      "Validation: Loss:  0.19179194  Accuracy:  0.13461539\n",
      "241 : Training: loss:  0.16246812\n",
      "242 : Training: loss:  0.20117594\n",
      "243 : Training: loss:  0.22937709\n",
      "244 : Training: loss:  0.17923136\n",
      "245 : Training: loss:  0.17286818\n",
      "246 : Training: loss:  0.16995789\n",
      "247 : Training: loss:  0.15431708\n",
      "248 : Training: loss:  0.16927698\n",
      "249 : Training: loss:  0.1715248\n",
      "250 : Training: loss:  0.21672867\n",
      "251 : Training: loss:  0.1587376\n",
      "252 : Training: loss:  0.2248746\n",
      "253 : Training: loss:  0.23802413\n",
      "254 : Training: loss:  0.17433909\n",
      "255 : Training: loss:  0.16669713\n",
      "256 : Training: loss:  0.17693904\n",
      "257 : Training: loss:  0.17350374\n",
      "258 : Training: loss:  0.17174117\n",
      "259 : Training: loss:  0.14633696\n",
      "260 : Training: loss:  0.1580509\n",
      "Validation: Loss:  0.18574776  Accuracy:  0.15384616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0542, 0.0755, 0.0356, 0.0642, 0.0351, 0.047...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1368, 0.2508, 0.1207, 0.1479, 0.1034, 0.127...</td>\n",
       "      <td>[0.8632, 0.7492, 0.8793, 0.8521, 0.8966, 0.872...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.185748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0836, 0.1088, 0.0511, 0.0835, 0.0576, 0.059...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5078, 0.4909, 0.2924, 0.3894, 0.2956, 0.265...</td>\n",
       "      <td>[0.4922, 0.5091, 0.7076, 0.6106, 0.7044, 0.734...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0529, 0.0781, 0.0368, 0.0622, 0.0359, 0.048...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2536, 0.3126, 0.1782, 0.1567, 0.1106, 0.151...</td>\n",
       "      <td>[0.7464, 0.6874, 0.8218, 0.8433, 0.8894, 0.848...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0448, 0.0634, 0.0339, 0.0568, 0.0307, 0.036...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3498, 0.3399, 0.2532, 0.3674, 0.5395, 0.371...</td>\n",
       "      <td>[0.6502, 0.6601, 0.7468, 0.6326, 0.4605, 0.628...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0325, 0.0493, 0.0198, 0.0413, 0.0192, 0.029...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0738, 0.0816, 0.0772, 0.071, 0.0824, 0.0469...</td>\n",
       "      <td>[0.9262, 0.9184, 0.9228, 0.929, 0.9176, 0.9531...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0582, 0.0813, 0.043, 0.0701, 0.0374, 0.0566...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0752, 0.0771, 0.0746, 0.1, 0.0944, 0.1516, ...</td>\n",
       "      <td>[0.9248, 0.9229, 0.9254, 0.9, 0.9056, 0.8484, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.1499, 0.1484, 0.0701, 0.1569, 0.11, 0.0792,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7796, 0.6004, 0.7623, 0.6181, 0.5135, 0.629...</td>\n",
       "      <td>[0.2204, 0.3996, 0.2377, 0.3819, 0.4865, 0.371...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0989, 0.1157, 0.0709, 0.1187, 0.072, 0.0916...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5888, 0.3512, 0.5324, 0.332, 0.3243, 0.5201...</td>\n",
       "      <td>[0.4112, 0.6488, 0.4676, 0.668, 0.6757, 0.4799...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1269, 0.1567, 0.0948, 0.1464, 0.0995, 0.128...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.4074, 0.2869, 0.4776, 0.3036, 0.2427, 0.399...</td>\n",
       "      <td>[0.5926, 0.7131, 0.5224, 0.6964, 0.7573, 0.600...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.1934, 0.2028, 0.1999, 0.1862, 0.1956, 0.137...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7999, 0.8765, 0.8837, 0.7952, 0.9107, 0.748...</td>\n",
       "      <td>[0.2001, 0.1235, 0.1163, 0.2048, 0.0893, 0.251...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0801, 0.1214, 0.0499, 0.0926, 0.0596, 0.067...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5774, 0.7645, 0.6271, 0.5065, 0.6147, 0.49,...</td>\n",
       "      <td>[0.4226, 0.2355, 0.3729, 0.4935, 0.3853, 0.51,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.057, 0.0827, 0.0455, 0.0769, 0.0429, 0.0563...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.349, 0.5896, 0.4433, 0.3818, 0.6302, 0.4441...</td>\n",
       "      <td>[0.651, 0.4104, 0.5567, 0.6182, 0.3698, 0.5559...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.1841, 0.1876, 0.1302, 0.226, 0.1655, 0.1297...</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.748, 0.8608, 0.9032, 0.8892, 0.7951, 0.8561...</td>\n",
       "      <td>[0.252, 0.1392, 0.0968, 0.1108, 0.2049, 0.1439...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.3397, 0.3307, 0.3257, 0.3413, 0.2387, 0.361...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.8133, 0.791, 0.7116, 0.6641, 0.7757, 0.897,...</td>\n",
       "      <td>[0.1867, 0.209, 0.2884, 0.3359, 0.2243, 0.103,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0569, 0.0728, 0.0434, 0.0927, 0.0395, 0.043...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4729, 0.4011, 0.6137, 0.6208, 0.6037, 0.510...</td>\n",
       "      <td>[0.5271, 0.5989, 0.3863, 0.3792, 0.3963, 0.489...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.061, 0.0844, 0.0481, 0.0702, 0.0469, 0.0509...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2069, 0.3751, 0.472, 0.3109, 0.3547, 0.4089...</td>\n",
       "      <td>[0.7931, 0.6249, 0.528, 0.6891, 0.6453, 0.5911...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0525, 0.0735, 0.037, 0.0656, 0.0317, 0.0496...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0949, 0.0771, 0.0641, 0.1484, 0.1593, 0.161...</td>\n",
       "      <td>[0.9051, 0.9229, 0.9359, 0.8516, 0.8407, 0.838...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0785, 0.1041, 0.0599, 0.0925, 0.0539, 0.068...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3078, 0.3362, 0.2738, 0.4909, 0.4908, 0.416...</td>\n",
       "      <td>[0.6922, 0.6638, 0.7262, 0.5091, 0.5092, 0.583...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0435, 0.0627, 0.0296, 0.0534, 0.0247, 0.039...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0938, 0.3242, 0.0825, 0.1069, 0.2957, 0.106...</td>\n",
       "      <td>[0.9062, 0.6758, 0.9175, 0.8931, 0.7043, 0.893...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.045, 0.069, 0.035, 0.0562, 0.0275, 0.0406, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1267, 0.4814, 0.092, 0.2126, 0.3365, 0.1479...</td>\n",
       "      <td>[0.8733, 0.5186, 0.908, 0.7874, 0.6635, 0.8521...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0674, 0.0912, 0.0455, 0.0794, 0.0461, 0.071...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1875, 0.3581, 0.287, 0.4259, 0.1501, 0.2465...</td>\n",
       "      <td>[0.8125, 0.6419, 0.713, 0.5741, 0.8499, 0.7535...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0285, 0.0463, 0.0177, 0.041, 0.0161, 0.0272...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0981, 0.2191, 0.1308, 0.3319, 0.0867, 0.099...</td>\n",
       "      <td>[0.9019, 0.7809, 0.8692, 0.6681, 0.9133, 0.901...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0372, 0.0586, 0.025, 0.0498, 0.022, 0.0339,...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0839, 0.1979, 0.1503, 0.2507, 0.127, 0.232,...</td>\n",
       "      <td>[0.9161, 0.8021, 0.8497, 0.7493, 0.873, 0.768,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0716, 0.0895, 0.0458, 0.0773, 0.0402, 0.063...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4722, 0.5027, 0.389, 0.3606, 0.3777, 0.5116...</td>\n",
       "      <td>[0.5278, 0.4973, 0.611, 0.6394, 0.6223, 0.4884...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0401, 0.0576, 0.0265, 0.0509, 0.0227, 0.036...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0625, 0.0769, 0.0698, 0.0888, 0.0808, 0.081...</td>\n",
       "      <td>[0.9375, 0.9231, 0.9302, 0.9112, 0.9192, 0.918...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0733, 0.0964, 0.0535, 0.0919, 0.0536, 0.068...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1441, 0.1536, 0.5292, 0.3248, 0.2339, 0.312...</td>\n",
       "      <td>[0.8559, 0.8464, 0.4708, 0.6752, 0.7661, 0.687...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.039, 0.0568, 0.0249, 0.0513, 0.0221, 0.0345...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0457, 0.0513, 0.1942, 0.1255, 0.0713, 0.130...</td>\n",
       "      <td>[0.9543, 0.9487, 0.8058, 0.8745, 0.9287, 0.869...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0566, 0.0695, 0.0384, 0.0719, 0.0369, 0.040...</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3704, 0.4261, 0.4534, 0.3536, 0.7346, 0.454...</td>\n",
       "      <td>[0.6296, 0.5739, 0.5466, 0.6464, 0.2654, 0.545...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.182, 0.162, 0.1462, 0.1999, 0.1026, 0.1341,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7766, 0.7032, 0.6926, 0.6641, 0.9231, 0.796...</td>\n",
       "      <td>[0.2234, 0.2968, 0.3074, 0.3359, 0.0769, 0.204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.1162, 0.1104, 0.0902, 0.1583, 0.0847, 0.081...</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7813, 0.6224, 0.8515, 0.7572, 0.9371, 0.791...</td>\n",
       "      <td>[0.2187, 0.3776, 0.1485, 0.2428, 0.0629, 0.209...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3937, 0.4327, 0.5171, 0.4391, 0.4798, 0.411...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.9998, 1.0, 0.9999, 0.9999, 1.0, 0.9999, 1.0...</td>\n",
       "      <td>[0.0002, 0.0, 1e-04, 1e-04, 0.0, 1e-04, 0.0, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.1093, 0.1225, 0.0675, 0.1108, 0.062, 0.0702...</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.6129, 0.5081, 0.4922, 0.4042, 0.4013, 0.267...</td>\n",
       "      <td>[0.3871, 0.4919, 0.5078, 0.5958, 0.5987, 0.732...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0765, 0.097, 0.0514, 0.0885, 0.0506, 0.0512...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3826, 0.5862, 0.4897, 0.536, 0.3468, 0.4586...</td>\n",
       "      <td>[0.6174, 0.4138, 0.5103, 0.464, 0.6532, 0.5414...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0218, 0.0357, 0.0133, 0.0291, 0.0119, 0.020...</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0238, 0.0557, 0.0504, 0.0282, 0.0522, 0.055...</td>\n",
       "      <td>[0.9762, 0.9443, 0.9496, 0.9718, 0.9478, 0.944...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0421, 0.0646, 0.0339, 0.0546, 0.0318, 0.040...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2135, 0.5024, 0.1097, 0.1505, 0.3205, 0.152...</td>\n",
       "      <td>[0.7865, 0.4976, 0.8903, 0.8495, 0.6795, 0.848...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0241, 0.0399, 0.0148, 0.0331, 0.0133, 0.022...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0633, 0.0936, 0.1988, 0.1119, 0.0604, 0.171...</td>\n",
       "      <td>[0.9367, 0.9064, 0.8012, 0.8881, 0.9396, 0.828...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0344, 0.0528, 0.0219, 0.0459, 0.0184, 0.031...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0989, 0.0822, 0.1621, 0.1644, 0.1164, 0.143...</td>\n",
       "      <td>[0.9011, 0.9178, 0.8379, 0.8356, 0.8836, 0.856...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0954, 0.1394, 0.0782, 0.1393, 0.0586, 0.084...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4598, 0.6604, 0.6644, 0.7189, 0.4824, 0.801...</td>\n",
       "      <td>[0.5402, 0.3396, 0.3356, 0.2811, 0.5176, 0.199...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0772, 0.0921, 0.0529, 0.0976, 0.0508, 0.065...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2769, 0.2242, 0.2195, 0.2249, 0.2443, 0.409...</td>\n",
       "      <td>[0.7231, 0.7758, 0.7805, 0.7751, 0.7557, 0.590...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0415, 0.0601, 0.0283, 0.0575, 0.0252, 0.039...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1313, 0.0515, 0.0537, 0.0952, 0.0781, 0.078...</td>\n",
       "      <td>[0.8687, 0.9485, 0.9463, 0.9048, 0.9219, 0.921...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.1292, 0.1382, 0.0659, 0.1417, 0.0885, 0.077...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5458, 0.6199, 0.4244, 0.6605, 0.5521, 0.425...</td>\n",
       "      <td>[0.4542, 0.3801, 0.5756, 0.3395, 0.4479, 0.574...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0355, 0.0532, 0.0244, 0.0467, 0.0207, 0.034...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0515, 0.0499, 0.051, 0.0639, 0.0753, 0.1406...</td>\n",
       "      <td>[0.9485, 0.9501, 0.949, 0.9361, 0.9247, 0.8594...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0262, 0.0405, 0.0151, 0.0352, 0.0134, 0.022...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1189, 0.0953, 0.2148, 0.1806, 0.0953, 0.191...</td>\n",
       "      <td>[0.8811, 0.9047, 0.7852, 0.8194, 0.9047, 0.808...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.1015, 0.1237, 0.067, 0.1301, 0.0605, 0.0692...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6596, 0.6668, 0.6442, 0.7324, 0.5405, 0.764...</td>\n",
       "      <td>[0.3404, 0.3332, 0.3558, 0.2676, 0.4595, 0.235...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.1613, 0.1654, 0.0985, 0.1784, 0.0964, 0.097...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7299, 0.7085, 0.8135, 0.7447, 0.6025, 0.920...</td>\n",
       "      <td>[0.2701, 0.2915, 0.1865, 0.2553, 0.3975, 0.079...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0313, 0.0447, 0.016, 0.0443, 0.0145, 0.0227...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3022, 0.2332, 0.3738, 0.2954, 0.2291, 0.275...</td>\n",
       "      <td>[0.6978, 0.7668, 0.6262, 0.7046, 0.7709, 0.724...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1509, 0.1747, 0.129, 0.1634, 0.0996, 0.1542...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5503, 0.4379, 0.4317, 0.435, 0.453, 0.487, ...</td>\n",
       "      <td>[0.4497, 0.5621, 0.5683, 0.565, 0.547, 0.513, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0792, 0.1014, 0.0562, 0.0791, 0.0578, 0.062...</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3299, 0.4448, 0.2551, 0.41, 0.2538, 0.3911,...</td>\n",
       "      <td>[0.6701, 0.5552, 0.7449, 0.59, 0.7462, 0.6089,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0201, 0.0339, 0.0125, 0.0268, 0.0114, 0.017...</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0726, 0.2285, 0.0665, 0.0712, 0.2023, 0.100...</td>\n",
       "      <td>[0.9274, 0.7715, 0.9335, 0.9288, 0.7977, 0.899...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0732, 0.1068, 0.0495, 0.08, 0.0483, 0.0557,...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2112, 0.8095, 0.4428, 0.394, 0.5242, 0.4077...</td>\n",
       "      <td>[0.7888, 0.1905, 0.5572, 0.606, 0.4758, 0.5923...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1199, 0.1383, 0.0816, 0.1299, 0.1076, 0.113...</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3394, 0.4476, 0.6248, 0.2828, 0.1778, 0.435...</td>\n",
       "      <td>[0.6606, 0.5524, 0.3752, 0.7172, 0.8222, 0.564...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0838, 0.1046, 0.0685, 0.1041, 0.0661, 0.052...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.3713, 0.4012, 0.5198, 0.4193, 0.5152, 0.371...</td>\n",
       "      <td>[0.6287, 0.5988, 0.4802, 0.5807, 0.4848, 0.629...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0542, 0.0755, 0.0356, 0.0642, 0.0351, 0.047...               0   \n",
       "1   [0.0836, 0.1088, 0.0511, 0.0835, 0.0576, 0.059...               0   \n",
       "2   [0.0529, 0.0781, 0.0368, 0.0622, 0.0359, 0.048...               0   \n",
       "3   [0.0448, 0.0634, 0.0339, 0.0568, 0.0307, 0.036...               1   \n",
       "4   [0.0325, 0.0493, 0.0198, 0.0413, 0.0192, 0.029...               1   \n",
       "5   [0.0582, 0.0813, 0.043, 0.0701, 0.0374, 0.0566...               2   \n",
       "6   [0.1499, 0.1484, 0.0701, 0.1569, 0.11, 0.0792,...               3   \n",
       "7   [0.0989, 0.1157, 0.0709, 0.1187, 0.072, 0.0916...               3   \n",
       "8   [0.1269, 0.1567, 0.0948, 0.1464, 0.0995, 0.128...               3   \n",
       "9   [0.1934, 0.2028, 0.1999, 0.1862, 0.1956, 0.137...               4   \n",
       "10  [0.0801, 0.1214, 0.0499, 0.0926, 0.0596, 0.067...               4   \n",
       "11  [0.057, 0.0827, 0.0455, 0.0769, 0.0429, 0.0563...               5   \n",
       "12  [0.1841, 0.1876, 0.1302, 0.226, 0.1655, 0.1297...               6   \n",
       "13  [0.3397, 0.3307, 0.3257, 0.3413, 0.2387, 0.361...               7   \n",
       "14  [0.0569, 0.0728, 0.0434, 0.0927, 0.0395, 0.043...               8   \n",
       "15  [0.061, 0.0844, 0.0481, 0.0702, 0.0469, 0.0509...               8   \n",
       "16  [0.0525, 0.0735, 0.037, 0.0656, 0.0317, 0.0496...               9   \n",
       "17  [0.0785, 0.1041, 0.0599, 0.0925, 0.0539, 0.068...               9   \n",
       "18  [0.0435, 0.0627, 0.0296, 0.0534, 0.0247, 0.039...              10   \n",
       "19  [0.045, 0.069, 0.035, 0.0562, 0.0275, 0.0406, ...              10   \n",
       "20  [0.0674, 0.0912, 0.0455, 0.0794, 0.0461, 0.071...              11   \n",
       "21  [0.0285, 0.0463, 0.0177, 0.041, 0.0161, 0.0272...              11   \n",
       "22  [0.0372, 0.0586, 0.025, 0.0498, 0.022, 0.0339,...              12   \n",
       "23  [0.0716, 0.0895, 0.0458, 0.0773, 0.0402, 0.063...              13   \n",
       "24  [0.0401, 0.0576, 0.0265, 0.0509, 0.0227, 0.036...              13   \n",
       "25  [0.0733, 0.0964, 0.0535, 0.0919, 0.0536, 0.068...              14   \n",
       "26  [0.039, 0.0568, 0.0249, 0.0513, 0.0221, 0.0345...              14   \n",
       "27  [0.0566, 0.0695, 0.0384, 0.0719, 0.0369, 0.040...              15   \n",
       "28  [0.182, 0.162, 0.1462, 0.1999, 0.1026, 0.1341,...              15   \n",
       "29  [0.1162, 0.1104, 0.0902, 0.1583, 0.0847, 0.081...              15   \n",
       "30  [0.3937, 0.4327, 0.5171, 0.4391, 0.4798, 0.411...              16   \n",
       "31  [0.1093, 0.1225, 0.0675, 0.1108, 0.062, 0.0702...              17   \n",
       "32  [0.0765, 0.097, 0.0514, 0.0885, 0.0506, 0.0512...              17   \n",
       "33  [0.0218, 0.0357, 0.0133, 0.0291, 0.0119, 0.020...              18   \n",
       "34  [0.0421, 0.0646, 0.0339, 0.0546, 0.0318, 0.040...              19   \n",
       "35  [0.0241, 0.0399, 0.0148, 0.0331, 0.0133, 0.022...              20   \n",
       "36  [0.0344, 0.0528, 0.0219, 0.0459, 0.0184, 0.031...              21   \n",
       "37  [0.0954, 0.1394, 0.0782, 0.1393, 0.0586, 0.084...              21   \n",
       "38  [0.0772, 0.0921, 0.0529, 0.0976, 0.0508, 0.065...              22   \n",
       "39  [0.0415, 0.0601, 0.0283, 0.0575, 0.0252, 0.039...              22   \n",
       "40  [0.1292, 0.1382, 0.0659, 0.1417, 0.0885, 0.077...              22   \n",
       "41  [0.0355, 0.0532, 0.0244, 0.0467, 0.0207, 0.034...              22   \n",
       "42  [0.0262, 0.0405, 0.0151, 0.0352, 0.0134, 0.022...              23   \n",
       "43  [0.1015, 0.1237, 0.067, 0.1301, 0.0605, 0.0692...              23   \n",
       "44  [0.1613, 0.1654, 0.0985, 0.1784, 0.0964, 0.097...              23   \n",
       "45  [0.0313, 0.0447, 0.016, 0.0443, 0.0145, 0.0227...              23   \n",
       "46  [0.1509, 0.1747, 0.129, 0.1634, 0.0996, 0.1542...              24   \n",
       "47  [0.0792, 0.1014, 0.0562, 0.0791, 0.0578, 0.062...              24   \n",
       "48  [0.0201, 0.0339, 0.0125, 0.0268, 0.0114, 0.017...              25   \n",
       "49  [0.0732, 0.1068, 0.0495, 0.08, 0.0483, 0.0557,...              25   \n",
       "50  [0.1199, 0.1383, 0.0816, 0.1299, 0.1076, 0.113...              26   \n",
       "51  [0.0838, 0.1046, 0.0685, 0.1041, 0.0661, 0.052...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                 23  [0.1368, 0.2508, 0.1207, 0.1479, 0.1034, 0.127...   \n",
       "1                 23  [0.5078, 0.4909, 0.2924, 0.3894, 0.2956, 0.265...   \n",
       "2                  1  [0.2536, 0.3126, 0.1782, 0.1567, 0.1106, 0.151...   \n",
       "3                 23  [0.3498, 0.3399, 0.2532, 0.3674, 0.5395, 0.371...   \n",
       "4                  8  [0.0738, 0.0816, 0.0772, 0.071, 0.0824, 0.0469...   \n",
       "5                 23  [0.0752, 0.0771, 0.0746, 0.1, 0.0944, 0.1516, ...   \n",
       "6                  3  [0.7796, 0.6004, 0.7623, 0.6181, 0.5135, 0.629...   \n",
       "7                 23  [0.5888, 0.3512, 0.5324, 0.332, 0.3243, 0.5201...   \n",
       "8                  8  [0.4074, 0.2869, 0.4776, 0.3036, 0.2427, 0.399...   \n",
       "9                 26  [0.7999, 0.8765, 0.8837, 0.7952, 0.9107, 0.748...   \n",
       "10                 8  [0.5774, 0.7645, 0.6271, 0.5065, 0.6147, 0.49,...   \n",
       "11                 8  [0.349, 0.5896, 0.4433, 0.3818, 0.6302, 0.4441...   \n",
       "12                14  [0.748, 0.8608, 0.9032, 0.8892, 0.7951, 0.8561...   \n",
       "13                23  [0.8133, 0.791, 0.7116, 0.6641, 0.7757, 0.897,...   \n",
       "14                 3  [0.4729, 0.4011, 0.6137, 0.6208, 0.6037, 0.510...   \n",
       "15                 8  [0.2069, 0.3751, 0.472, 0.3109, 0.3547, 0.4089...   \n",
       "16                10  [0.0949, 0.0771, 0.0641, 0.1484, 0.1593, 0.161...   \n",
       "17                 8  [0.3078, 0.3362, 0.2738, 0.4909, 0.4908, 0.416...   \n",
       "18                 8  [0.0938, 0.3242, 0.0825, 0.1069, 0.2957, 0.106...   \n",
       "19                 8  [0.1267, 0.4814, 0.092, 0.2126, 0.3365, 0.1479...   \n",
       "20                 1  [0.1875, 0.3581, 0.287, 0.4259, 0.1501, 0.2465...   \n",
       "21                23  [0.0981, 0.2191, 0.1308, 0.3319, 0.0867, 0.099...   \n",
       "22                 1  [0.0839, 0.1979, 0.1503, 0.2507, 0.127, 0.232,...   \n",
       "23                 1  [0.4722, 0.5027, 0.389, 0.3606, 0.3777, 0.5116...   \n",
       "24                10  [0.0625, 0.0769, 0.0698, 0.0888, 0.0808, 0.081...   \n",
       "25                10  [0.1441, 0.1536, 0.5292, 0.3248, 0.2339, 0.312...   \n",
       "26                23  [0.0457, 0.0513, 0.1942, 0.1255, 0.0713, 0.130...   \n",
       "27                 8  [0.3704, 0.4261, 0.4534, 0.3536, 0.7346, 0.454...   \n",
       "28                15  [0.7766, 0.7032, 0.6926, 0.6641, 0.9231, 0.796...   \n",
       "29                26  [0.7813, 0.6224, 0.8515, 0.7572, 0.9371, 0.791...   \n",
       "30                19  [0.9998, 1.0, 0.9999, 0.9999, 1.0, 0.9999, 1.0...   \n",
       "31                 9  [0.6129, 0.5081, 0.4922, 0.4042, 0.4013, 0.267...   \n",
       "32                 1  [0.3826, 0.5862, 0.4897, 0.536, 0.3468, 0.4586...   \n",
       "33                23  [0.0238, 0.0557, 0.0504, 0.0282, 0.0522, 0.055...   \n",
       "34                 1  [0.2135, 0.5024, 0.1097, 0.1505, 0.3205, 0.152...   \n",
       "35                23  [0.0633, 0.0936, 0.1988, 0.1119, 0.0604, 0.171...   \n",
       "36                23  [0.0989, 0.0822, 0.1621, 0.1644, 0.1164, 0.143...   \n",
       "37                23  [0.4598, 0.6604, 0.6644, 0.7189, 0.4824, 0.801...   \n",
       "38                23  [0.2769, 0.2242, 0.2195, 0.2249, 0.2443, 0.409...   \n",
       "39                23  [0.1313, 0.0515, 0.0537, 0.0952, 0.0781, 0.078...   \n",
       "40                23  [0.5458, 0.6199, 0.4244, 0.6605, 0.5521, 0.425...   \n",
       "41                23  [0.0515, 0.0499, 0.051, 0.0639, 0.0753, 0.1406...   \n",
       "42                23  [0.1189, 0.0953, 0.2148, 0.1806, 0.0953, 0.191...   \n",
       "43                23  [0.6596, 0.6668, 0.6442, 0.7324, 0.5405, 0.764...   \n",
       "44                23  [0.7299, 0.7085, 0.8135, 0.7447, 0.6025, 0.920...   \n",
       "45                23  [0.3022, 0.2332, 0.3738, 0.2954, 0.2291, 0.275...   \n",
       "46                 9  [0.5503, 0.4379, 0.4317, 0.435, 0.453, 0.487, ...   \n",
       "47                 8  [0.3299, 0.4448, 0.2551, 0.41, 0.2538, 0.3911,...   \n",
       "48                 8  [0.0726, 0.2285, 0.0665, 0.0712, 0.2023, 0.100...   \n",
       "49                 1  [0.2112, 0.8095, 0.4428, 0.394, 0.5242, 0.4077...   \n",
       "50                 8  [0.3394, 0.4476, 0.6248, 0.2828, 0.1778, 0.435...   \n",
       "51                26  [0.3713, 0.4012, 0.5198, 0.4193, 0.5152, 0.371...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8632, 0.7492, 0.8793, 0.8521, 0.8966, 0.872...  0.153846  0.185748  \n",
       "1   [0.4922, 0.5091, 0.7076, 0.6106, 0.7044, 0.734...       NaN       NaN  \n",
       "2   [0.7464, 0.6874, 0.8218, 0.8433, 0.8894, 0.848...       NaN       NaN  \n",
       "3   [0.6502, 0.6601, 0.7468, 0.6326, 0.4605, 0.628...       NaN       NaN  \n",
       "4   [0.9262, 0.9184, 0.9228, 0.929, 0.9176, 0.9531...       NaN       NaN  \n",
       "5   [0.9248, 0.9229, 0.9254, 0.9, 0.9056, 0.8484, ...       NaN       NaN  \n",
       "6   [0.2204, 0.3996, 0.2377, 0.3819, 0.4865, 0.371...       NaN       NaN  \n",
       "7   [0.4112, 0.6488, 0.4676, 0.668, 0.6757, 0.4799...       NaN       NaN  \n",
       "8   [0.5926, 0.7131, 0.5224, 0.6964, 0.7573, 0.600...       NaN       NaN  \n",
       "9   [0.2001, 0.1235, 0.1163, 0.2048, 0.0893, 0.251...       NaN       NaN  \n",
       "10  [0.4226, 0.2355, 0.3729, 0.4935, 0.3853, 0.51,...       NaN       NaN  \n",
       "11  [0.651, 0.4104, 0.5567, 0.6182, 0.3698, 0.5559...       NaN       NaN  \n",
       "12  [0.252, 0.1392, 0.0968, 0.1108, 0.2049, 0.1439...       NaN       NaN  \n",
       "13  [0.1867, 0.209, 0.2884, 0.3359, 0.2243, 0.103,...       NaN       NaN  \n",
       "14  [0.5271, 0.5989, 0.3863, 0.3792, 0.3963, 0.489...       NaN       NaN  \n",
       "15  [0.7931, 0.6249, 0.528, 0.6891, 0.6453, 0.5911...       NaN       NaN  \n",
       "16  [0.9051, 0.9229, 0.9359, 0.8516, 0.8407, 0.838...       NaN       NaN  \n",
       "17  [0.6922, 0.6638, 0.7262, 0.5091, 0.5092, 0.583...       NaN       NaN  \n",
       "18  [0.9062, 0.6758, 0.9175, 0.8931, 0.7043, 0.893...       NaN       NaN  \n",
       "19  [0.8733, 0.5186, 0.908, 0.7874, 0.6635, 0.8521...       NaN       NaN  \n",
       "20  [0.8125, 0.6419, 0.713, 0.5741, 0.8499, 0.7535...       NaN       NaN  \n",
       "21  [0.9019, 0.7809, 0.8692, 0.6681, 0.9133, 0.901...       NaN       NaN  \n",
       "22  [0.9161, 0.8021, 0.8497, 0.7493, 0.873, 0.768,...       NaN       NaN  \n",
       "23  [0.5278, 0.4973, 0.611, 0.6394, 0.6223, 0.4884...       NaN       NaN  \n",
       "24  [0.9375, 0.9231, 0.9302, 0.9112, 0.9192, 0.918...       NaN       NaN  \n",
       "25  [0.8559, 0.8464, 0.4708, 0.6752, 0.7661, 0.687...       NaN       NaN  \n",
       "26  [0.9543, 0.9487, 0.8058, 0.8745, 0.9287, 0.869...       NaN       NaN  \n",
       "27  [0.6296, 0.5739, 0.5466, 0.6464, 0.2654, 0.545...       NaN       NaN  \n",
       "28  [0.2234, 0.2968, 0.3074, 0.3359, 0.0769, 0.204...       NaN       NaN  \n",
       "29  [0.2187, 0.3776, 0.1485, 0.2428, 0.0629, 0.209...       NaN       NaN  \n",
       "30  [0.0002, 0.0, 1e-04, 1e-04, 0.0, 1e-04, 0.0, 0...       NaN       NaN  \n",
       "31  [0.3871, 0.4919, 0.5078, 0.5958, 0.5987, 0.732...       NaN       NaN  \n",
       "32  [0.6174, 0.4138, 0.5103, 0.464, 0.6532, 0.5414...       NaN       NaN  \n",
       "33  [0.9762, 0.9443, 0.9496, 0.9718, 0.9478, 0.944...       NaN       NaN  \n",
       "34  [0.7865, 0.4976, 0.8903, 0.8495, 0.6795, 0.848...       NaN       NaN  \n",
       "35  [0.9367, 0.9064, 0.8012, 0.8881, 0.9396, 0.828...       NaN       NaN  \n",
       "36  [0.9011, 0.9178, 0.8379, 0.8356, 0.8836, 0.856...       NaN       NaN  \n",
       "37  [0.5402, 0.3396, 0.3356, 0.2811, 0.5176, 0.199...       NaN       NaN  \n",
       "38  [0.7231, 0.7758, 0.7805, 0.7751, 0.7557, 0.590...       NaN       NaN  \n",
       "39  [0.8687, 0.9485, 0.9463, 0.9048, 0.9219, 0.921...       NaN       NaN  \n",
       "40  [0.4542, 0.3801, 0.5756, 0.3395, 0.4479, 0.574...       NaN       NaN  \n",
       "41  [0.9485, 0.9501, 0.949, 0.9361, 0.9247, 0.8594...       NaN       NaN  \n",
       "42  [0.8811, 0.9047, 0.7852, 0.8194, 0.9047, 0.808...       NaN       NaN  \n",
       "43  [0.3404, 0.3332, 0.3558, 0.2676, 0.4595, 0.235...       NaN       NaN  \n",
       "44  [0.2701, 0.2915, 0.1865, 0.2553, 0.3975, 0.079...       NaN       NaN  \n",
       "45  [0.6978, 0.7668, 0.6262, 0.7046, 0.7709, 0.724...       NaN       NaN  \n",
       "46  [0.4497, 0.5621, 0.5683, 0.565, 0.547, 0.513, ...       NaN       NaN  \n",
       "47  [0.6701, 0.5552, 0.7449, 0.59, 0.7462, 0.6089,...       NaN       NaN  \n",
       "48  [0.9274, 0.7715, 0.9335, 0.9288, 0.7977, 0.899...       NaN       NaN  \n",
       "49  [0.7888, 0.1905, 0.5572, 0.606, 0.4758, 0.5923...       NaN       NaN  \n",
       "50  [0.6606, 0.5524, 0.3752, 0.7172, 0.8222, 0.564...       NaN       NaN  \n",
       "51  [0.6287, 0.5988, 0.4802, 0.5807, 0.4848, 0.629...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 : Training: loss:  0.17112076\n",
      "262 : Training: loss:  0.17572947\n",
      "263 : Training: loss:  0.18033063\n",
      "264 : Training: loss:  0.17503579\n",
      "265 : Training: loss:  0.17724119\n",
      "266 : Training: loss:  0.16471781\n",
      "267 : Training: loss:  0.1674175\n",
      "268 : Training: loss:  0.15887296\n",
      "269 : Training: loss:  0.15687783\n",
      "270 : Training: loss:  0.15398782\n",
      "271 : Training: loss:  0.15259035\n",
      "272 : Training: loss:  0.189505\n",
      "273 : Training: loss:  0.15273885\n",
      "274 : Training: loss:  0.17629515\n",
      "275 : Training: loss:  0.21959063\n",
      "276 : Training: loss:  0.18038969\n",
      "277 : Training: loss:  0.16772038\n",
      "278 : Training: loss:  0.18174584\n",
      "279 : Training: loss:  0.17436236\n",
      "280 : Training: loss:  0.17370225\n",
      "Validation: Loss:  0.18062687  Accuracy:  0.17307693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.055, 0.0688, 0.032, 0.0586, 0.0318, 0.0411,...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1358, 0.2495, 0.1191, 0.1465, 0.1026, 0.125...</td>\n",
       "      <td>[0.8642, 0.7505, 0.8809, 0.8535, 0.8974, 0.874...</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.180627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0817, 0.1006, 0.0449, 0.0751, 0.052, 0.0508...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5023, 0.4867, 0.2855, 0.384, 0.2914, 0.2595...</td>\n",
       "      <td>[0.4977, 0.5133, 0.7145, 0.616, 0.7086, 0.7405...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0534, 0.0714, 0.0329, 0.0566, 0.0326, 0.041...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2517, 0.3104, 0.1762, 0.1556, 0.1096, 0.148...</td>\n",
       "      <td>[0.7483, 0.6896, 0.8238, 0.8444, 0.8904, 0.851...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0431, 0.0567, 0.0287, 0.0494, 0.0268, 0.029...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3419, 0.3338, 0.2448, 0.3596, 0.5326, 0.363...</td>\n",
       "      <td>[0.6581, 0.6662, 0.7552, 0.6404, 0.4674, 0.636...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.033, 0.044, 0.0174, 0.0369, 0.017, 0.0247, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0724, 0.0805, 0.0756, 0.07, 0.0815, 0.0462,...</td>\n",
       "      <td>[0.9276, 0.9195, 0.9244, 0.93, 0.9185, 0.9538,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0585, 0.0736, 0.0386, 0.0638, 0.0337, 0.049...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.074, 0.0755, 0.0736, 0.0978, 0.0937, 0.1506...</td>\n",
       "      <td>[0.926, 0.9245, 0.9264, 0.9022, 0.9063, 0.8494...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.1434, 0.1398, 0.0628, 0.147, 0.1, 0.0695, 0...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7768, 0.5956, 0.7627, 0.6137, 0.5047, 0.627...</td>\n",
       "      <td>[0.2232, 0.4044, 0.2373, 0.3863, 0.4953, 0.372...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0963, 0.1065, 0.0632, 0.1092, 0.0649, 0.079...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5846, 0.3442, 0.5318, 0.3269, 0.3173, 0.517...</td>\n",
       "      <td>[0.4154, 0.6558, 0.4682, 0.6731, 0.6827, 0.482...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.126, 0.1469, 0.0873, 0.1379, 0.0924, 0.1166...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.4066, 0.2844, 0.4819, 0.3006, 0.2407, 0.399...</td>\n",
       "      <td>[0.5934, 0.7156, 0.5181, 0.6994, 0.7593, 0.600...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.1805, 0.1936, 0.18, 0.1717, 0.1835, 0.1213,...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7938, 0.8735, 0.8785, 0.7882, 0.9088, 0.739...</td>\n",
       "      <td>[0.2062, 0.1265, 0.1215, 0.2118, 0.0912, 0.260...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0741, 0.1088, 0.0422, 0.0809, 0.0519, 0.055...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5645, 0.7583, 0.6124, 0.4926, 0.6065, 0.475...</td>\n",
       "      <td>[0.4355, 0.2417, 0.3876, 0.5074, 0.3935, 0.524...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0546, 0.0744, 0.0393, 0.0682, 0.0381, 0.047...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3423, 0.5828, 0.4355, 0.373, 0.6257, 0.4353...</td>\n",
       "      <td>[0.6577, 0.4172, 0.5645, 0.627, 0.3743, 0.5647...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.1724, 0.177, 0.1177, 0.2084, 0.1511, 0.1144...</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.7394, 0.8544, 0.899, 0.8839, 0.7873, 0.85, ...</td>\n",
       "      <td>[0.2606, 0.1456, 0.101, 0.1161, 0.2127, 0.15, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.331, 0.3198, 0.311, 0.3285, 0.2274, 0.3436,...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.8063, 0.7818, 0.7025, 0.6527, 0.7703, 0.894...</td>\n",
       "      <td>[0.1937, 0.2182, 0.2975, 0.3473, 0.2297, 0.106...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0543, 0.0654, 0.0373, 0.0823, 0.0347, 0.035...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4661, 0.3953, 0.6071, 0.6128, 0.5972, 0.501...</td>\n",
       "      <td>[0.5339, 0.6047, 0.3929, 0.3872, 0.4028, 0.499...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0601, 0.0763, 0.0424, 0.0631, 0.042, 0.0434...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2034, 0.3712, 0.4669, 0.3045, 0.3507, 0.399...</td>\n",
       "      <td>[0.7966, 0.6288, 0.5331, 0.6955, 0.6493, 0.601...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0525, 0.0659, 0.0328, 0.0592, 0.0283, 0.042...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0934, 0.0761, 0.0634, 0.1458, 0.1576, 0.159...</td>\n",
       "      <td>[0.9066, 0.9239, 0.9366, 0.8542, 0.8424, 0.840...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0763, 0.0943, 0.0529, 0.0837, 0.0482, 0.059...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.3032, 0.3331, 0.2708, 0.4824, 0.4865, 0.412...</td>\n",
       "      <td>[0.6968, 0.6669, 0.7292, 0.5176, 0.5135, 0.588...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0441, 0.0559, 0.0262, 0.048, 0.022, 0.0335,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0928, 0.32, 0.082, 0.1053, 0.2946, 0.1053, ...</td>\n",
       "      <td>[0.9072, 0.68, 0.918, 0.8947, 0.7054, 0.8947, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0453, 0.0612, 0.0308, 0.0504, 0.0244, 0.034...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1251, 0.4767, 0.0912, 0.2096, 0.3366, 0.145...</td>\n",
       "      <td>[0.8749, 0.5233, 0.9088, 0.7904, 0.6634, 0.854...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0679, 0.0833, 0.0412, 0.0731, 0.042, 0.0624...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1863, 0.3555, 0.2854, 0.4228, 0.1489, 0.243...</td>\n",
       "      <td>[0.8137, 0.6445, 0.7146, 0.5772, 0.8511, 0.756...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0289, 0.0409, 0.0154, 0.0364, 0.0142, 0.022...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0969, 0.2143, 0.1294, 0.3283, 0.0844, 0.098...</td>\n",
       "      <td>[0.9031, 0.7857, 0.8706, 0.6717, 0.9156, 0.901...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0378, 0.0525, 0.0221, 0.0449, 0.0197, 0.028...</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0834, 0.1945, 0.1495, 0.2474, 0.1245, 0.23,...</td>\n",
       "      <td>[0.9166, 0.8055, 0.8505, 0.7526, 0.8755, 0.77,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0697, 0.0808, 0.0399, 0.0692, 0.0356, 0.053...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4668, 0.4959, 0.3877, 0.3524, 0.3735, 0.504...</td>\n",
       "      <td>[0.5332, 0.5041, 0.6123, 0.6476, 0.6265, 0.495...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0408, 0.0515, 0.0235, 0.0459, 0.0203, 0.031...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0623, 0.0755, 0.0709, 0.0872, 0.08, 0.0821,...</td>\n",
       "      <td>[0.9377, 0.9245, 0.9291, 0.9128, 0.92, 0.9179,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0724, 0.0876, 0.0476, 0.0836, 0.0483, 0.059...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1423, 0.1524, 0.5281, 0.3202, 0.2326, 0.310...</td>\n",
       "      <td>[0.8577, 0.8476, 0.4719, 0.6798, 0.7674, 0.689...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0393, 0.0504, 0.0218, 0.0458, 0.0195, 0.029...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0451, 0.0502, 0.1926, 0.1229, 0.0698, 0.129...</td>\n",
       "      <td>[0.9549, 0.9498, 0.8074, 0.8771, 0.9302, 0.870...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0533, 0.0624, 0.0325, 0.0625, 0.0323, 0.033...</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3619, 0.42, 0.443, 0.343, 0.7305, 0.4477, 0...</td>\n",
       "      <td>[0.6381, 0.58, 0.557, 0.657, 0.2695, 0.5523, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.1678, 0.15, 0.1291, 0.1821, 0.0926, 0.1158,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7671, 0.6933, 0.6815, 0.65, 0.9214, 0.7908,...</td>\n",
       "      <td>[0.2329, 0.3067, 0.3185, 0.35, 0.0786, 0.2092,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.1043, 0.1008, 0.0759, 0.1392, 0.0742, 0.066...</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7713, 0.6116, 0.844, 0.7443, 0.9354, 0.7843...</td>\n",
       "      <td>[0.2287, 0.3884, 0.156, 0.2557, 0.0646, 0.2157...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3895, 0.4323, 0.5099, 0.4334, 0.4777, 0.402...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 1.0, 0.9998, 0.9999, 1.0, 0.9999, 1.0...</td>\n",
       "      <td>[0.0002, 0.0, 0.0002, 1e-04, 0.0, 1e-04, 0.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.1054, 0.1135, 0.06, 0.1009, 0.0562, 0.0609,...</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.6083, 0.5, 0.4874, 0.3992, 0.3941, 0.2654, ...</td>\n",
       "      <td>[0.3917, 0.5, 0.5126, 0.6008, 0.6059, 0.7346, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0742, 0.0885, 0.0452, 0.0799, 0.0451, 0.043...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3782, 0.5802, 0.4873, 0.5294, 0.3399, 0.455...</td>\n",
       "      <td>[0.6218, 0.4198, 0.5127, 0.4706, 0.6601, 0.544...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0224, 0.0314, 0.0117, 0.0259, 0.0105, 0.016...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0235, 0.0551, 0.0495, 0.0276, 0.0515, 0.054...</td>\n",
       "      <td>[0.9765, 0.9449, 0.9505, 0.9724, 0.9485, 0.945...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0423, 0.0585, 0.0301, 0.0493, 0.0288, 0.034...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2125, 0.5022, 0.1081, 0.1485, 0.3211, 0.148...</td>\n",
       "      <td>[0.7875, 0.4978, 0.8919, 0.8515, 0.6789, 0.851...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0244, 0.0349, 0.0128, 0.0292, 0.0116, 0.018...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0623, 0.0913, 0.1967, 0.1093, 0.0591, 0.169...</td>\n",
       "      <td>[0.9377, 0.9087, 0.8033, 0.8907, 0.9409, 0.830...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0349, 0.047, 0.0192, 0.0411, 0.0164, 0.0263...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0976, 0.0803, 0.1611, 0.1619, 0.1126, 0.142...</td>\n",
       "      <td>[0.9024, 0.9197, 0.8389, 0.8381, 0.8874, 0.857...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0896, 0.1259, 0.0674, 0.1237, 0.051, 0.071,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4475, 0.6456, 0.6566, 0.707, 0.4616, 0.7955...</td>\n",
       "      <td>[0.5525, 0.3544, 0.3434, 0.293, 0.5384, 0.2045...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0753, 0.0831, 0.0467, 0.0879, 0.0453, 0.055...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2733, 0.2186, 0.2137, 0.2206, 0.2377, 0.407...</td>\n",
       "      <td>[0.7267, 0.7814, 0.7863, 0.7794, 0.7623, 0.593...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0417, 0.0536, 0.0249, 0.0516, 0.0224, 0.033...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1301, 0.05, 0.0529, 0.0942, 0.0762, 0.0781,...</td>\n",
       "      <td>[0.8699, 0.95, 0.9471, 0.9058, 0.9238, 0.9219,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.1231, 0.1286, 0.0586, 0.1291, 0.0803, 0.066...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5406, 0.6131, 0.4158, 0.6552, 0.5439, 0.421...</td>\n",
       "      <td>[0.4594, 0.3869, 0.5842, 0.3448, 0.4561, 0.578...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0362, 0.0475, 0.0216, 0.0421, 0.0184, 0.029...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0511, 0.0489, 0.0504, 0.0631, 0.0739, 0.140...</td>\n",
       "      <td>[0.9489, 0.9511, 0.9496, 0.9369, 0.9261, 0.859...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0264, 0.0354, 0.013, 0.031, 0.0116, 0.0185,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.116, 0.0915, 0.2135, 0.1753, 0.0907, 0.1891...</td>\n",
       "      <td>[0.884, 0.9085, 0.7865, 0.8247, 0.9093, 0.8109...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0932, 0.1091, 0.0565, 0.1135, 0.0509, 0.056...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6444, 0.6452, 0.6349, 0.7158, 0.5131, 0.755...</td>\n",
       "      <td>[0.3556, 0.3548, 0.3651, 0.2842, 0.4869, 0.244...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.148, 0.1494, 0.0844, 0.1581, 0.0828, 0.0815...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7144, 0.6856, 0.8066, 0.7273, 0.5732, 0.916...</td>\n",
       "      <td>[0.2856, 0.3144, 0.1934, 0.2727, 0.4268, 0.083...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0306, 0.0387, 0.0136, 0.0384, 0.0124, 0.018...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2939, 0.2229, 0.3693, 0.2858, 0.2162, 0.270...</td>\n",
       "      <td>[0.7061, 0.7771, 0.6307, 0.7142, 0.7838, 0.729...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1479, 0.1642, 0.119, 0.1527, 0.0923, 0.1402...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5484, 0.4322, 0.4319, 0.4313, 0.4501, 0.483...</td>\n",
       "      <td>[0.4516, 0.5678, 0.5681, 0.5687, 0.5499, 0.516...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0782, 0.0945, 0.0506, 0.0726, 0.0531, 0.054...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3271, 0.4406, 0.2539, 0.407, 0.2511, 0.3869...</td>\n",
       "      <td>[0.6729, 0.5594, 0.7461, 0.593, 0.7489, 0.6131...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0201, 0.0293, 0.0106, 0.0231, 0.0099, 0.013...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0703, 0.2249, 0.0637, 0.0686, 0.1993, 0.096...</td>\n",
       "      <td>[0.9297, 0.7751, 0.9363, 0.9314, 0.8007, 0.903...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0706, 0.0972, 0.0435, 0.0717, 0.0434, 0.047...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2061, 0.8069, 0.4332, 0.3856, 0.5187, 0.396...</td>\n",
       "      <td>[0.7939, 0.1931, 0.5668, 0.6144, 0.4813, 0.603...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1184, 0.1288, 0.0748, 0.1209, 0.0995, 0.101...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3373, 0.4442, 0.6215, 0.2798, 0.1766, 0.430...</td>\n",
       "      <td>[0.6627, 0.5558, 0.3785, 0.7202, 0.8234, 0.569...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0811, 0.0964, 0.0609, 0.0947, 0.0602, 0.045...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3667, 0.3976, 0.5155, 0.4112, 0.5108, 0.363...</td>\n",
       "      <td>[0.6333, 0.6024, 0.4845, 0.5888, 0.4892, 0.636...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.055, 0.0688, 0.032, 0.0586, 0.0318, 0.0411,...               0   \n",
       "1   [0.0817, 0.1006, 0.0449, 0.0751, 0.052, 0.0508...               0   \n",
       "2   [0.0534, 0.0714, 0.0329, 0.0566, 0.0326, 0.041...               0   \n",
       "3   [0.0431, 0.0567, 0.0287, 0.0494, 0.0268, 0.029...               1   \n",
       "4   [0.033, 0.044, 0.0174, 0.0369, 0.017, 0.0247, ...               1   \n",
       "5   [0.0585, 0.0736, 0.0386, 0.0638, 0.0337, 0.049...               2   \n",
       "6   [0.1434, 0.1398, 0.0628, 0.147, 0.1, 0.0695, 0...               3   \n",
       "7   [0.0963, 0.1065, 0.0632, 0.1092, 0.0649, 0.079...               3   \n",
       "8   [0.126, 0.1469, 0.0873, 0.1379, 0.0924, 0.1166...               3   \n",
       "9   [0.1805, 0.1936, 0.18, 0.1717, 0.1835, 0.1213,...               4   \n",
       "10  [0.0741, 0.1088, 0.0422, 0.0809, 0.0519, 0.055...               4   \n",
       "11  [0.0546, 0.0744, 0.0393, 0.0682, 0.0381, 0.047...               5   \n",
       "12  [0.1724, 0.177, 0.1177, 0.2084, 0.1511, 0.1144...               6   \n",
       "13  [0.331, 0.3198, 0.311, 0.3285, 0.2274, 0.3436,...               7   \n",
       "14  [0.0543, 0.0654, 0.0373, 0.0823, 0.0347, 0.035...               8   \n",
       "15  [0.0601, 0.0763, 0.0424, 0.0631, 0.042, 0.0434...               8   \n",
       "16  [0.0525, 0.0659, 0.0328, 0.0592, 0.0283, 0.042...               9   \n",
       "17  [0.0763, 0.0943, 0.0529, 0.0837, 0.0482, 0.059...               9   \n",
       "18  [0.0441, 0.0559, 0.0262, 0.048, 0.022, 0.0335,...              10   \n",
       "19  [0.0453, 0.0612, 0.0308, 0.0504, 0.0244, 0.034...              10   \n",
       "20  [0.0679, 0.0833, 0.0412, 0.0731, 0.042, 0.0624...              11   \n",
       "21  [0.0289, 0.0409, 0.0154, 0.0364, 0.0142, 0.022...              11   \n",
       "22  [0.0378, 0.0525, 0.0221, 0.0449, 0.0197, 0.028...              12   \n",
       "23  [0.0697, 0.0808, 0.0399, 0.0692, 0.0356, 0.053...              13   \n",
       "24  [0.0408, 0.0515, 0.0235, 0.0459, 0.0203, 0.031...              13   \n",
       "25  [0.0724, 0.0876, 0.0476, 0.0836, 0.0483, 0.059...              14   \n",
       "26  [0.0393, 0.0504, 0.0218, 0.0458, 0.0195, 0.029...              14   \n",
       "27  [0.0533, 0.0624, 0.0325, 0.0625, 0.0323, 0.033...              15   \n",
       "28  [0.1678, 0.15, 0.1291, 0.1821, 0.0926, 0.1158,...              15   \n",
       "29  [0.1043, 0.1008, 0.0759, 0.1392, 0.0742, 0.066...              15   \n",
       "30  [0.3895, 0.4323, 0.5099, 0.4334, 0.4777, 0.402...              16   \n",
       "31  [0.1054, 0.1135, 0.06, 0.1009, 0.0562, 0.0609,...              17   \n",
       "32  [0.0742, 0.0885, 0.0452, 0.0799, 0.0451, 0.043...              17   \n",
       "33  [0.0224, 0.0314, 0.0117, 0.0259, 0.0105, 0.016...              18   \n",
       "34  [0.0423, 0.0585, 0.0301, 0.0493, 0.0288, 0.034...              19   \n",
       "35  [0.0244, 0.0349, 0.0128, 0.0292, 0.0116, 0.018...              20   \n",
       "36  [0.0349, 0.047, 0.0192, 0.0411, 0.0164, 0.0263...              21   \n",
       "37  [0.0896, 0.1259, 0.0674, 0.1237, 0.051, 0.071,...              21   \n",
       "38  [0.0753, 0.0831, 0.0467, 0.0879, 0.0453, 0.055...              22   \n",
       "39  [0.0417, 0.0536, 0.0249, 0.0516, 0.0224, 0.033...              22   \n",
       "40  [0.1231, 0.1286, 0.0586, 0.1291, 0.0803, 0.066...              22   \n",
       "41  [0.0362, 0.0475, 0.0216, 0.0421, 0.0184, 0.029...              22   \n",
       "42  [0.0264, 0.0354, 0.013, 0.031, 0.0116, 0.0185,...              23   \n",
       "43  [0.0932, 0.1091, 0.0565, 0.1135, 0.0509, 0.056...              23   \n",
       "44  [0.148, 0.1494, 0.0844, 0.1581, 0.0828, 0.0815...              23   \n",
       "45  [0.0306, 0.0387, 0.0136, 0.0384, 0.0124, 0.018...              23   \n",
       "46  [0.1479, 0.1642, 0.119, 0.1527, 0.0923, 0.1402...              24   \n",
       "47  [0.0782, 0.0945, 0.0506, 0.0726, 0.0531, 0.054...              24   \n",
       "48  [0.0201, 0.0293, 0.0106, 0.0231, 0.0099, 0.013...              25   \n",
       "49  [0.0706, 0.0972, 0.0435, 0.0717, 0.0434, 0.047...              25   \n",
       "50  [0.1184, 0.1288, 0.0748, 0.1209, 0.0995, 0.101...              26   \n",
       "51  [0.0811, 0.0964, 0.0609, 0.0947, 0.0602, 0.045...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                 23  [0.1358, 0.2495, 0.1191, 0.1465, 0.1026, 0.125...   \n",
       "1                 23  [0.5023, 0.4867, 0.2855, 0.384, 0.2914, 0.2595...   \n",
       "2                 23  [0.2517, 0.3104, 0.1762, 0.1556, 0.1096, 0.148...   \n",
       "3                 23  [0.3419, 0.3338, 0.2448, 0.3596, 0.5326, 0.363...   \n",
       "4                 23  [0.0724, 0.0805, 0.0756, 0.07, 0.0815, 0.0462,...   \n",
       "5                 10  [0.074, 0.0755, 0.0736, 0.0978, 0.0937, 0.1506...   \n",
       "6                 23  [0.7768, 0.5956, 0.7627, 0.6137, 0.5047, 0.627...   \n",
       "7                 23  [0.5846, 0.3442, 0.5318, 0.3269, 0.3173, 0.517...   \n",
       "8                 10  [0.4066, 0.2844, 0.4819, 0.3006, 0.2407, 0.399...   \n",
       "9                 26  [0.7938, 0.8735, 0.8785, 0.7882, 0.9088, 0.739...   \n",
       "10                 8  [0.5645, 0.7583, 0.6124, 0.4926, 0.6065, 0.475...   \n",
       "11                 8  [0.3423, 0.5828, 0.4355, 0.373, 0.6257, 0.4353...   \n",
       "12                14  [0.7394, 0.8544, 0.899, 0.8839, 0.7873, 0.85, ...   \n",
       "13                23  [0.8063, 0.7818, 0.7025, 0.6527, 0.7703, 0.894...   \n",
       "14                 3  [0.4661, 0.3953, 0.6071, 0.6128, 0.5972, 0.501...   \n",
       "15                10  [0.2034, 0.3712, 0.4669, 0.3045, 0.3507, 0.399...   \n",
       "16                10  [0.0934, 0.0761, 0.0634, 0.1458, 0.1576, 0.159...   \n",
       "17                10  [0.3032, 0.3331, 0.2708, 0.4824, 0.4865, 0.412...   \n",
       "18                10  [0.0928, 0.32, 0.082, 0.1053, 0.2946, 0.1053, ...   \n",
       "19                10  [0.1251, 0.4767, 0.0912, 0.2096, 0.3366, 0.145...   \n",
       "20                10  [0.1863, 0.3555, 0.2854, 0.4228, 0.1489, 0.243...   \n",
       "21                23  [0.0969, 0.2143, 0.1294, 0.3283, 0.0844, 0.098...   \n",
       "22                23  [0.0834, 0.1945, 0.1495, 0.2474, 0.1245, 0.23,...   \n",
       "23                 1  [0.4668, 0.4959, 0.3877, 0.3524, 0.3735, 0.504...   \n",
       "24                10  [0.0623, 0.0755, 0.0709, 0.0872, 0.08, 0.0821,...   \n",
       "25                10  [0.1423, 0.1524, 0.5281, 0.3202, 0.2326, 0.310...   \n",
       "26                10  [0.0451, 0.0502, 0.1926, 0.1229, 0.0698, 0.129...   \n",
       "27                 8  [0.3619, 0.42, 0.443, 0.343, 0.7305, 0.4477, 0...   \n",
       "28                15  [0.7671, 0.6933, 0.6815, 0.65, 0.9214, 0.7908,...   \n",
       "29                 3  [0.7713, 0.6116, 0.844, 0.7443, 0.9354, 0.7843...   \n",
       "30                16  [0.9998, 1.0, 0.9998, 0.9999, 1.0, 0.9999, 1.0...   \n",
       "31                 9  [0.6083, 0.5, 0.4874, 0.3992, 0.3941, 0.2654, ...   \n",
       "32                 1  [0.3782, 0.5802, 0.4873, 0.5294, 0.3399, 0.455...   \n",
       "33                10  [0.0235, 0.0551, 0.0495, 0.0276, 0.0515, 0.054...   \n",
       "34                 1  [0.2125, 0.5022, 0.1081, 0.1485, 0.3211, 0.148...   \n",
       "35                23  [0.0623, 0.0913, 0.1967, 0.1093, 0.0591, 0.169...   \n",
       "36                23  [0.0976, 0.0803, 0.1611, 0.1619, 0.1126, 0.142...   \n",
       "37                23  [0.4475, 0.6456, 0.6566, 0.707, 0.4616, 0.7955...   \n",
       "38                23  [0.2733, 0.2186, 0.2137, 0.2206, 0.2377, 0.407...   \n",
       "39                23  [0.1301, 0.05, 0.0529, 0.0942, 0.0762, 0.0781,...   \n",
       "40                23  [0.5406, 0.6131, 0.4158, 0.6552, 0.5439, 0.421...   \n",
       "41                23  [0.0511, 0.0489, 0.0504, 0.0631, 0.0739, 0.140...   \n",
       "42                23  [0.116, 0.0915, 0.2135, 0.1753, 0.0907, 0.1891...   \n",
       "43                23  [0.6444, 0.6452, 0.6349, 0.7158, 0.5131, 0.755...   \n",
       "44                23  [0.7144, 0.6856, 0.8066, 0.7273, 0.5732, 0.916...   \n",
       "45                23  [0.2939, 0.2229, 0.3693, 0.2858, 0.2162, 0.270...   \n",
       "46                 9  [0.5484, 0.4322, 0.4319, 0.4313, 0.4501, 0.483...   \n",
       "47                 1  [0.3271, 0.4406, 0.2539, 0.407, 0.2511, 0.3869...   \n",
       "48                10  [0.0703, 0.2249, 0.0637, 0.0686, 0.1993, 0.096...   \n",
       "49                 1  [0.2061, 0.8069, 0.4332, 0.3856, 0.5187, 0.396...   \n",
       "50                23  [0.3373, 0.4442, 0.6215, 0.2798, 0.1766, 0.430...   \n",
       "51                 1  [0.3667, 0.3976, 0.5155, 0.4112, 0.5108, 0.363...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8642, 0.7505, 0.8809, 0.8535, 0.8974, 0.874...  0.173077  0.180627  \n",
       "1   [0.4977, 0.5133, 0.7145, 0.616, 0.7086, 0.7405...       NaN       NaN  \n",
       "2   [0.7483, 0.6896, 0.8238, 0.8444, 0.8904, 0.851...       NaN       NaN  \n",
       "3   [0.6581, 0.6662, 0.7552, 0.6404, 0.4674, 0.636...       NaN       NaN  \n",
       "4   [0.9276, 0.9195, 0.9244, 0.93, 0.9185, 0.9538,...       NaN       NaN  \n",
       "5   [0.926, 0.9245, 0.9264, 0.9022, 0.9063, 0.8494...       NaN       NaN  \n",
       "6   [0.2232, 0.4044, 0.2373, 0.3863, 0.4953, 0.372...       NaN       NaN  \n",
       "7   [0.4154, 0.6558, 0.4682, 0.6731, 0.6827, 0.482...       NaN       NaN  \n",
       "8   [0.5934, 0.7156, 0.5181, 0.6994, 0.7593, 0.600...       NaN       NaN  \n",
       "9   [0.2062, 0.1265, 0.1215, 0.2118, 0.0912, 0.260...       NaN       NaN  \n",
       "10  [0.4355, 0.2417, 0.3876, 0.5074, 0.3935, 0.524...       NaN       NaN  \n",
       "11  [0.6577, 0.4172, 0.5645, 0.627, 0.3743, 0.5647...       NaN       NaN  \n",
       "12  [0.2606, 0.1456, 0.101, 0.1161, 0.2127, 0.15, ...       NaN       NaN  \n",
       "13  [0.1937, 0.2182, 0.2975, 0.3473, 0.2297, 0.106...       NaN       NaN  \n",
       "14  [0.5339, 0.6047, 0.3929, 0.3872, 0.4028, 0.499...       NaN       NaN  \n",
       "15  [0.7966, 0.6288, 0.5331, 0.6955, 0.6493, 0.601...       NaN       NaN  \n",
       "16  [0.9066, 0.9239, 0.9366, 0.8542, 0.8424, 0.840...       NaN       NaN  \n",
       "17  [0.6968, 0.6669, 0.7292, 0.5176, 0.5135, 0.588...       NaN       NaN  \n",
       "18  [0.9072, 0.68, 0.918, 0.8947, 0.7054, 0.8947, ...       NaN       NaN  \n",
       "19  [0.8749, 0.5233, 0.9088, 0.7904, 0.6634, 0.854...       NaN       NaN  \n",
       "20  [0.8137, 0.6445, 0.7146, 0.5772, 0.8511, 0.756...       NaN       NaN  \n",
       "21  [0.9031, 0.7857, 0.8706, 0.6717, 0.9156, 0.901...       NaN       NaN  \n",
       "22  [0.9166, 0.8055, 0.8505, 0.7526, 0.8755, 0.77,...       NaN       NaN  \n",
       "23  [0.5332, 0.5041, 0.6123, 0.6476, 0.6265, 0.495...       NaN       NaN  \n",
       "24  [0.9377, 0.9245, 0.9291, 0.9128, 0.92, 0.9179,...       NaN       NaN  \n",
       "25  [0.8577, 0.8476, 0.4719, 0.6798, 0.7674, 0.689...       NaN       NaN  \n",
       "26  [0.9549, 0.9498, 0.8074, 0.8771, 0.9302, 0.870...       NaN       NaN  \n",
       "27  [0.6381, 0.58, 0.557, 0.657, 0.2695, 0.5523, 0...       NaN       NaN  \n",
       "28  [0.2329, 0.3067, 0.3185, 0.35, 0.0786, 0.2092,...       NaN       NaN  \n",
       "29  [0.2287, 0.3884, 0.156, 0.2557, 0.0646, 0.2157...       NaN       NaN  \n",
       "30  [0.0002, 0.0, 0.0002, 1e-04, 0.0, 1e-04, 0.0, ...       NaN       NaN  \n",
       "31  [0.3917, 0.5, 0.5126, 0.6008, 0.6059, 0.7346, ...       NaN       NaN  \n",
       "32  [0.6218, 0.4198, 0.5127, 0.4706, 0.6601, 0.544...       NaN       NaN  \n",
       "33  [0.9765, 0.9449, 0.9505, 0.9724, 0.9485, 0.945...       NaN       NaN  \n",
       "34  [0.7875, 0.4978, 0.8919, 0.8515, 0.6789, 0.851...       NaN       NaN  \n",
       "35  [0.9377, 0.9087, 0.8033, 0.8907, 0.9409, 0.830...       NaN       NaN  \n",
       "36  [0.9024, 0.9197, 0.8389, 0.8381, 0.8874, 0.857...       NaN       NaN  \n",
       "37  [0.5525, 0.3544, 0.3434, 0.293, 0.5384, 0.2045...       NaN       NaN  \n",
       "38  [0.7267, 0.7814, 0.7863, 0.7794, 0.7623, 0.593...       NaN       NaN  \n",
       "39  [0.8699, 0.95, 0.9471, 0.9058, 0.9238, 0.9219,...       NaN       NaN  \n",
       "40  [0.4594, 0.3869, 0.5842, 0.3448, 0.4561, 0.578...       NaN       NaN  \n",
       "41  [0.9489, 0.9511, 0.9496, 0.9369, 0.9261, 0.859...       NaN       NaN  \n",
       "42  [0.884, 0.9085, 0.7865, 0.8247, 0.9093, 0.8109...       NaN       NaN  \n",
       "43  [0.3556, 0.3548, 0.3651, 0.2842, 0.4869, 0.244...       NaN       NaN  \n",
       "44  [0.2856, 0.3144, 0.1934, 0.2727, 0.4268, 0.083...       NaN       NaN  \n",
       "45  [0.7061, 0.7771, 0.6307, 0.7142, 0.7838, 0.729...       NaN       NaN  \n",
       "46  [0.4516, 0.5678, 0.5681, 0.5687, 0.5499, 0.516...       NaN       NaN  \n",
       "47  [0.6729, 0.5594, 0.7461, 0.593, 0.7489, 0.6131...       NaN       NaN  \n",
       "48  [0.9297, 0.7751, 0.9363, 0.9314, 0.8007, 0.903...       NaN       NaN  \n",
       "49  [0.7939, 0.1931, 0.5668, 0.6144, 0.4813, 0.603...       NaN       NaN  \n",
       "50  [0.6627, 0.5558, 0.3785, 0.7202, 0.8234, 0.569...       NaN       NaN  \n",
       "51  [0.6333, 0.6024, 0.4845, 0.5888, 0.4892, 0.636...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281 : Training: loss:  0.18764588\n",
      "282 : Training: loss:  0.15969382\n",
      "283 : Training: loss:  0.16173285\n",
      "284 : Training: loss:  0.19006579\n",
      "285 : Training: loss:  0.15830535\n",
      "286 : Training: loss:  0.15665501\n",
      "287 : Training: loss:  0.18664823\n",
      "288 : Training: loss:  0.15646966\n",
      "289 : Training: loss:  0.16338874\n",
      "290 : Training: loss:  0.15001743\n",
      "291 : Training: loss:  0.16359834\n",
      "292 : Training: loss:  0.15425538\n",
      "293 : Training: loss:  0.16903833\n",
      "294 : Training: loss:  0.1705205\n",
      "295 : Training: loss:  0.15285799\n",
      "296 : Training: loss:  0.14562303\n",
      "297 : Training: loss:  0.15250447\n",
      "298 : Training: loss:  0.1602769\n",
      "299 : Training: loss:  0.17911623\n",
      "300 : Training: loss:  0.15753931\n",
      "Validation: Loss:  0.1766721  Accuracy:  0.15384616\n",
      "301 : Training: loss:  0.18647146\n",
      "302 : Training: loss:  0.15951434\n",
      "303 : Training: loss:  0.15287904\n",
      "304 : Training: loss:  0.16392371\n",
      "305 : Training: loss:  0.15731677\n",
      "306 : Training: loss:  0.17797692\n",
      "307 : Training: loss:  0.17184795\n",
      "308 : Training: loss:  0.16615333\n",
      "309 : Training: loss:  0.1678027\n",
      "310 : Training: loss:  0.15866293\n",
      "311 : Training: loss:  0.14499195\n",
      "312 : Training: loss:  0.16477711\n",
      "313 : Training: loss:  0.19305146\n",
      "314 : Training: loss:  0.15491931\n",
      "315 : Training: loss:  0.15807064\n",
      "316 : Training: loss:  0.15001468\n",
      "317 : Training: loss:  0.16124958\n",
      "318 : Training: loss:  0.15630601\n",
      "319 : Training: loss:  0.16541338\n",
      "320 : Training: loss:  0.17578614\n",
      "Validation: Loss:  0.17403711  Accuracy:  0.15384616\n",
      "321 : Training: loss:  0.15014848\n",
      "322 : Training: loss:  0.18359192\n",
      "323 : Training: loss:  0.22627641\n",
      "324 : Training: loss:  0.16597052\n",
      "325 : Training: loss:  0.19062194\n",
      "326 : Training: loss:  0.16368611\n",
      "327 : Training: loss:  0.1567648\n",
      "328 : Training: loss:  0.17079341\n",
      "329 : Training: loss:  0.16723432\n",
      "330 : Training: loss:  0.15822294\n",
      "331 : Training: loss:  0.17150958\n",
      "332 : Training: loss:  0.16280764\n",
      "333 : Training: loss:  0.15450232\n",
      "334 : Training: loss:  0.20248029\n",
      "335 : Training: loss:  0.16719598\n",
      "336 : Training: loss:  0.15429488\n",
      "337 : Training: loss:  0.21992677\n",
      "338 : Training: loss:  0.16858193\n",
      "339 : Training: loss:  0.15715854\n",
      "340 : Training: loss:  0.16129786\n",
      "Validation: Loss:  0.17142718  Accuracy:  0.15384616\n",
      "341 : Training: loss:  0.1641775\n",
      "342 : Training: loss:  0.14972885\n",
      "343 : Training: loss:  0.15959813\n",
      "344 : Training: loss:  0.14602946\n",
      "345 : Training: loss:  0.15131414\n",
      "346 : Training: loss:  0.16275512\n",
      "347 : Training: loss:  0.16395937\n",
      "348 : Training: loss:  0.16767846\n",
      "349 : Training: loss:  0.15159644\n",
      "350 : Training: loss:  0.16928062\n",
      "351 : Training: loss:  0.16421808\n",
      "352 : Training: loss:  0.14200504\n",
      "353 : Training: loss:  0.15277302\n",
      "354 : Training: loss:  0.15824537\n",
      "355 : Training: loss:  0.15543932\n",
      "356 : Training: loss:  0.1538798\n",
      "357 : Training: loss:  0.14548163\n",
      "358 : Training: loss:  0.15219857\n",
      "359 : Training: loss:  0.14795664\n",
      "360 : Training: loss:  0.1476609\n",
      "Validation: Loss:  0.16922498  Accuracy:  0.15384616\n",
      "361 : Training: loss:  0.16682647\n",
      "362 : Training: loss:  0.15461358\n",
      "363 : Training: loss:  0.16032153\n",
      "364 : Training: loss:  0.15584162\n",
      "365 : Training: loss:  0.15328704\n",
      "366 : Training: loss:  0.1547808\n",
      "367 : Training: loss:  0.15128528\n",
      "368 : Training: loss:  0.16384313\n",
      "369 : Training: loss:  0.15775384\n",
      "370 : Training: loss:  0.14635298\n",
      "371 : Training: loss:  0.15845963\n",
      "372 : Training: loss:  0.14612696\n",
      "373 : Training: loss:  0.16019446\n",
      "374 : Training: loss:  0.16103433\n",
      "375 : Training: loss:  0.16554254\n",
      "376 : Training: loss:  0.16122437\n",
      "377 : Training: loss:  0.15216044\n",
      "378 : Training: loss:  0.1428513\n",
      "379 : Training: loss:  0.14734875\n",
      "380 : Training: loss:  0.14616287\n",
      "Validation: Loss:  0.16786638  Accuracy:  0.17307693\n",
      "381 : Training: loss:  0.15397635\n",
      "382 : Training: loss:  0.13678618\n",
      "383 : Training: loss:  0.1682605\n",
      "384 : Training: loss:  0.15321425\n",
      "385 : Training: loss:  0.15391901\n",
      "386 : Training: loss:  0.14446074\n",
      "387 : Training: loss:  0.14856796\n",
      "388 : Training: loss:  0.15393724\n",
      "389 : Training: loss:  0.17209111\n",
      "390 : Training: loss:  0.16378279\n",
      "391 : Training: loss:  0.15160514\n",
      "392 : Training: loss:  0.15868194\n",
      "393 : Training: loss:  0.16055545\n",
      "394 : Training: loss:  0.13869956\n",
      "395 : Training: loss:  0.1562167\n",
      "396 : Training: loss:  0.15847863\n",
      "397 : Training: loss:  0.14706169\n",
      "398 : Training: loss:  0.15658405\n",
      "399 : Training: loss:  0.15576592\n",
      "400 : Training: loss:  0.15131912\n",
      "Validation: Loss:  0.16649269  Accuracy:  0.1923077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0635, 0.0524, 0.0228, 0.0505, 0.0263, 0.031...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1359, 0.2524, 0.1165, 0.1441, 0.1044, 0.118...</td>\n",
       "      <td>[0.8641, 0.7476, 0.8835, 0.8559, 0.8956, 0.881...</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.166493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0851, 0.0782, 0.03, 0.0612, 0.0441, 0.0368,...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4961, 0.4835, 0.2675, 0.3749, 0.2905, 0.242...</td>\n",
       "      <td>[0.5039, 0.5165, 0.7325, 0.6251, 0.7095, 0.757...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0601, 0.0539, 0.0228, 0.0484, 0.0268, 0.031...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2498, 0.3114, 0.1705, 0.1544, 0.1095, 0.140...</td>\n",
       "      <td>[0.7502, 0.6886, 0.8295, 0.8456, 0.8905, 0.859...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0423, 0.0394, 0.017, 0.0367, 0.0209, 0.019,...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3287, 0.3172, 0.2202, 0.3425, 0.5237, 0.341...</td>\n",
       "      <td>[0.6713, 0.6828, 0.7798, 0.6575, 0.4763, 0.658...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0386, 0.0317, 0.0118, 0.0305, 0.0134, 0.017...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0697, 0.08, 0.0711, 0.068, 0.0808, 0.0431, ...</td>\n",
       "      <td>[0.9303, 0.92, 0.9289, 0.932, 0.9192, 0.9569, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0649, 0.0551, 0.0277, 0.0541, 0.0269, 0.036...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0708, 0.0698, 0.0733, 0.0921, 0.0944, 0.151...</td>\n",
       "      <td>[0.9292, 0.9302, 0.9267, 0.9079, 0.9056, 0.848...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.1173, 0.0977, 0.0367, 0.1292, 0.0696, 0.043...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.768, 0.5843, 0.7709, 0.6083, 0.464, 0.6385,...</td>\n",
       "      <td>[0.232, 0.4157, 0.2291, 0.3917, 0.536, 0.3615,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0896, 0.0763, 0.0405, 0.0929, 0.0472, 0.053...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5756, 0.3204, 0.5409, 0.3214, 0.2954, 0.530...</td>\n",
       "      <td>[0.4244, 0.6796, 0.4591, 0.6786, 0.7046, 0.469...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1254, 0.1147, 0.0635, 0.1243, 0.0741, 0.088...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.4059, 0.2781, 0.5041, 0.2988, 0.2338, 0.408...</td>\n",
       "      <td>[0.5941, 0.7219, 0.4959, 0.7012, 0.7662, 0.591...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.1416, 0.1522, 0.1201, 0.1376, 0.1722, 0.088...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7803, 0.8691, 0.8609, 0.7698, 0.9074, 0.711...</td>\n",
       "      <td>[0.2197, 0.1309, 0.1391, 0.2302, 0.0926, 0.288...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0613, 0.0736, 0.0234, 0.0568, 0.0392, 0.034...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5292, 0.7472, 0.5525, 0.4508, 0.5843, 0.434...</td>\n",
       "      <td>[0.4708, 0.2528, 0.4475, 0.5492, 0.4157, 0.565...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0496, 0.0507, 0.0236, 0.0517, 0.0296, 0.031...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.3213, 0.5662, 0.4013, 0.346, 0.6181, 0.4101...</td>\n",
       "      <td>[0.6787, 0.4338, 0.5987, 0.654, 0.3819, 0.5899...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.1243, 0.1241, 0.0706, 0.154, 0.1047, 0.0702...</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.7039, 0.823, 0.8797, 0.8659, 0.7542, 0.8152...</td>\n",
       "      <td>[0.2961, 0.177, 0.1203, 0.1341, 0.2458, 0.1848...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2918, 0.2636, 0.2443, 0.2827, 0.1788, 0.272...</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7757, 0.7242, 0.6604, 0.6059, 0.7425, 0.884...</td>\n",
       "      <td>[0.2243, 0.2758, 0.3396, 0.3941, 0.2575, 0.115...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0478, 0.043, 0.0209, 0.0619, 0.0251, 0.022,...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4495, 0.3823, 0.5805, 0.5927, 0.579, 0.4668...</td>\n",
       "      <td>[0.5505, 0.6177, 0.4195, 0.4073, 0.421, 0.5332...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0597, 0.0535, 0.0263, 0.049, 0.0319, 0.029,...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1942, 0.371, 0.445, 0.2852, 0.3454, 0.3553,...</td>\n",
       "      <td>[0.8058, 0.629, 0.555, 0.7148, 0.6546, 0.6447,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0576, 0.0477, 0.0225, 0.0488, 0.0217, 0.030...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0886, 0.0724, 0.0615, 0.137, 0.1518, 0.1574...</td>\n",
       "      <td>[0.9114, 0.9276, 0.9385, 0.863, 0.8482, 0.8426...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0736, 0.0674, 0.0343, 0.0671, 0.0367, 0.041...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2898, 0.3317, 0.2651, 0.46, 0.4726, 0.3975,...</td>\n",
       "      <td>[0.7102, 0.6683, 0.7349, 0.54, 0.5274, 0.6025,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.05, 0.0395, 0.0177, 0.0392, 0.0167, 0.024, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0901, 0.3088, 0.0808, 0.1009, 0.3012, 0.106...</td>\n",
       "      <td>[0.9099, 0.6912, 0.9192, 0.8991, 0.6988, 0.893...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0496, 0.0422, 0.0201, 0.0399, 0.0179, 0.024...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1194, 0.4657, 0.0891, 0.2013, 0.349, 0.1372...</td>\n",
       "      <td>[0.8806, 0.5343, 0.9109, 0.7987, 0.651, 0.8628...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0726, 0.0621, 0.0289, 0.0621, 0.0327, 0.045...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1843, 0.3458, 0.2906, 0.4209, 0.1474, 0.233...</td>\n",
       "      <td>[0.8157, 0.6542, 0.7094, 0.5791, 0.8526, 0.766...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0317, 0.0271, 0.0095, 0.0285, 0.01, 0.0146,...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0948, 0.1902, 0.1267, 0.3223, 0.0762, 0.099...</td>\n",
       "      <td>[0.9052, 0.8098, 0.8733, 0.6777, 0.9238, 0.900...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0422, 0.0369, 0.0146, 0.0368, 0.0148, 0.02,...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0824, 0.1805, 0.1444, 0.2424, 0.1156, 0.225...</td>\n",
       "      <td>[0.9176, 0.8195, 0.8556, 0.7576, 0.8844, 0.775...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0679, 0.0563, 0.0248, 0.0546, 0.0263, 0.036...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.4491, 0.4668, 0.3846, 0.3297, 0.3703, 0.483...</td>\n",
       "      <td>[0.5509, 0.5332, 0.6154, 0.6703, 0.6297, 0.516...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0481, 0.0374, 0.0164, 0.0391, 0.016, 0.0229...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0621, 0.0736, 0.0747, 0.0844, 0.0786, 0.084...</td>\n",
       "      <td>[0.9379, 0.9264, 0.9253, 0.9156, 0.9214, 0.915...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0719, 0.0631, 0.0317, 0.068, 0.0373, 0.0409...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1402, 0.1503, 0.529, 0.3124, 0.2331, 0.3049...</td>\n",
       "      <td>[0.8598, 0.8497, 0.471, 0.6876, 0.7669, 0.6951...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0439, 0.0354, 0.0144, 0.037, 0.0147, 0.0201...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0444, 0.0445, 0.1917, 0.119, 0.0659, 0.1297...</td>\n",
       "      <td>[0.9556, 0.9555, 0.8083, 0.881, 0.9341, 0.8703...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0473, 0.0421, 0.0189, 0.0465, 0.0249, 0.020...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3498, 0.4016, 0.4223, 0.3187, 0.7323, 0.448...</td>\n",
       "      <td>[0.6502, 0.5984, 0.5777, 0.6813, 0.2677, 0.551...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.1253, 0.1055, 0.0787, 0.1394, 0.068, 0.0741...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7387, 0.6494, 0.6497, 0.6086, 0.9169, 0.781...</td>\n",
       "      <td>[0.2613, 0.3506, 0.3503, 0.3914, 0.0831, 0.218...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0727, 0.0664, 0.0399, 0.0985, 0.0556, 0.037...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7474, 0.5721, 0.8208, 0.7074, 0.9331, 0.771...</td>\n",
       "      <td>[0.2526, 0.4279, 0.1792, 0.2926, 0.0669, 0.228...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3681, 0.4227, 0.4835, 0.413, 0.49, 0.3793, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9997, 1.0, 0.9996, 0.9999, 1.0, 0.9998, 1.0...</td>\n",
       "      <td>[0.0003, 0.0, 0.0004, 1e-04, 0.0, 0.0002, 0.0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0946, 0.0822, 0.0387, 0.082, 0.0436, 0.0415...</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5992, 0.463, 0.4847, 0.3927, 0.3707, 0.2708...</td>\n",
       "      <td>[0.4008, 0.537, 0.5153, 0.6073, 0.6293, 0.7292...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0671, 0.0595, 0.0265, 0.0627, 0.0311, 0.027...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3629, 0.5566, 0.4938, 0.5209, 0.3099, 0.448...</td>\n",
       "      <td>[0.6371, 0.4434, 0.5062, 0.4791, 0.6901, 0.551...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0272, 0.0216, 0.0076, 0.0212, 0.008, 0.0116...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0232, 0.0543, 0.0476, 0.0264, 0.0506, 0.053...</td>\n",
       "      <td>[0.9768, 0.9457, 0.9524, 0.9736, 0.9494, 0.946...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0468, 0.0427, 0.0207, 0.0412, 0.0241, 0.025...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2185, 0.5157, 0.1014, 0.1455, 0.3428, 0.137...</td>\n",
       "      <td>[0.7815, 0.4843, 0.8986, 0.8545, 0.6572, 0.862...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0277, 0.023, 0.0078, 0.0228, 0.0082, 0.0116...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0602, 0.0824, 0.1929, 0.1039, 0.0551, 0.168...</td>\n",
       "      <td>[0.9398, 0.9176, 0.8071, 0.8961, 0.9449, 0.832...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0396, 0.0328, 0.0127, 0.0334, 0.0123, 0.018...</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0923, 0.0723, 0.154, 0.1553, 0.0955, 0.1404...</td>\n",
       "      <td>[0.9077, 0.9277, 0.846, 0.8447, 0.9045, 0.8596...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0684, 0.0771, 0.0343, 0.0831, 0.0291, 0.036...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3938, 0.5634, 0.62, 0.6668, 0.3535, 0.7809,...</td>\n",
       "      <td>[0.6062, 0.4366, 0.38, 0.3332, 0.6465, 0.2191,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0729, 0.0566, 0.0304, 0.0702, 0.0327, 0.036...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2731, 0.1899, 0.1945, 0.2092, 0.2148, 0.431...</td>\n",
       "      <td>[0.7269, 0.8101, 0.8055, 0.7908, 0.7852, 0.568...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0457, 0.0371, 0.0164, 0.0417, 0.0166, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1297, 0.0434, 0.0497, 0.092, 0.0684, 0.0839...</td>\n",
       "      <td>[0.8703, 0.9566, 0.9503, 0.908, 0.9316, 0.9161...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.1057, 0.0931, 0.0371, 0.1045, 0.0628, 0.045...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5358, 0.5832, 0.3916, 0.6435, 0.5173, 0.424...</td>\n",
       "      <td>[0.4642, 0.4168, 0.6084, 0.3565, 0.4827, 0.575...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0425, 0.0337, 0.0148, 0.0354, 0.0142, 0.021...</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0508, 0.0435, 0.0481, 0.0606, 0.0692, 0.152...</td>\n",
       "      <td>[0.9492, 0.9565, 0.9519, 0.9394, 0.9308, 0.847...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0286, 0.0227, 0.0077, 0.0237, 0.0078, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1049, 0.078, 0.2088, 0.1611, 0.0718, 0.1792...</td>\n",
       "      <td>[0.8951, 0.922, 0.7912, 0.8389, 0.9282, 0.8208...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0638, 0.0593, 0.0249, 0.0693, 0.0243, 0.026...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5763, 0.5398, 0.5881, 0.6542, 0.3865, 0.717...</td>\n",
       "      <td>[0.4237, 0.4602, 0.4119, 0.3458, 0.6135, 0.282...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0959, 0.0825, 0.0369, 0.0965, 0.0386, 0.036...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6403, 0.5658, 0.7719, 0.6594, 0.4262, 0.902...</td>\n",
       "      <td>[0.3597, 0.4342, 0.2281, 0.3406, 0.5738, 0.097...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0289, 0.0229, 0.0072, 0.027, 0.0075, 0.0103...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.263, 0.18, 0.3523, 0.256, 0.1626, 0.2582, 0...</td>\n",
       "      <td>[0.737, 0.82, 0.6477, 0.744, 0.8374, 0.7418, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1372, 0.1269, 0.086, 0.1283, 0.0722, 0.1056...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5362, 0.3963, 0.4328, 0.4197, 0.4365, 0.471...</td>\n",
       "      <td>[0.4638, 0.6037, 0.5672, 0.5803, 0.5635, 0.528...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0778, 0.072, 0.0342, 0.0611, 0.0436, 0.0402...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3188, 0.4277, 0.2587, 0.4031, 0.2398, 0.372...</td>\n",
       "      <td>[0.6812, 0.5723, 0.7413, 0.5969, 0.7602, 0.627...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0222, 0.0187, 0.0062, 0.0171, 0.0071, 0.008...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0662, 0.2168, 0.0526, 0.0619, 0.1974, 0.084...</td>\n",
       "      <td>[0.9338, 0.7832, 0.9474, 0.9381, 0.8026, 0.915...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0653, 0.0683, 0.0269, 0.0549, 0.0339, 0.032...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1907, 0.8012, 0.3992, 0.3625, 0.5071, 0.361...</td>\n",
       "      <td>[0.8093, 0.1988, 0.6008, 0.6375, 0.4929, 0.638...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1121, 0.0971, 0.0516, 0.1006, 0.0769, 0.072...</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3328, 0.4324, 0.617, 0.2739, 0.1745, 0.4066...</td>\n",
       "      <td>[0.6672, 0.5676, 0.383, 0.7261, 0.8255, 0.5934...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.074, 0.0695, 0.0391, 0.0754, 0.049, 0.031, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3604, 0.3987, 0.5031, 0.3926, 0.5042, 0.338...</td>\n",
       "      <td>[0.6396, 0.6013, 0.4969, 0.6074, 0.4958, 0.661...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0635, 0.0524, 0.0228, 0.0505, 0.0263, 0.031...               0   \n",
       "1   [0.0851, 0.0782, 0.03, 0.0612, 0.0441, 0.0368,...               0   \n",
       "2   [0.0601, 0.0539, 0.0228, 0.0484, 0.0268, 0.031...               0   \n",
       "3   [0.0423, 0.0394, 0.017, 0.0367, 0.0209, 0.019,...               1   \n",
       "4   [0.0386, 0.0317, 0.0118, 0.0305, 0.0134, 0.017...               1   \n",
       "5   [0.0649, 0.0551, 0.0277, 0.0541, 0.0269, 0.036...               2   \n",
       "6   [0.1173, 0.0977, 0.0367, 0.1292, 0.0696, 0.043...               3   \n",
       "7   [0.0896, 0.0763, 0.0405, 0.0929, 0.0472, 0.053...               3   \n",
       "8   [0.1254, 0.1147, 0.0635, 0.1243, 0.0741, 0.088...               3   \n",
       "9   [0.1416, 0.1522, 0.1201, 0.1376, 0.1722, 0.088...               4   \n",
       "10  [0.0613, 0.0736, 0.0234, 0.0568, 0.0392, 0.034...               4   \n",
       "11  [0.0496, 0.0507, 0.0236, 0.0517, 0.0296, 0.031...               5   \n",
       "12  [0.1243, 0.1241, 0.0706, 0.154, 0.1047, 0.0702...               6   \n",
       "13  [0.2918, 0.2636, 0.2443, 0.2827, 0.1788, 0.272...               7   \n",
       "14  [0.0478, 0.043, 0.0209, 0.0619, 0.0251, 0.022,...               8   \n",
       "15  [0.0597, 0.0535, 0.0263, 0.049, 0.0319, 0.029,...               8   \n",
       "16  [0.0576, 0.0477, 0.0225, 0.0488, 0.0217, 0.030...               9   \n",
       "17  [0.0736, 0.0674, 0.0343, 0.0671, 0.0367, 0.041...               9   \n",
       "18  [0.05, 0.0395, 0.0177, 0.0392, 0.0167, 0.024, ...              10   \n",
       "19  [0.0496, 0.0422, 0.0201, 0.0399, 0.0179, 0.024...              10   \n",
       "20  [0.0726, 0.0621, 0.0289, 0.0621, 0.0327, 0.045...              11   \n",
       "21  [0.0317, 0.0271, 0.0095, 0.0285, 0.01, 0.0146,...              11   \n",
       "22  [0.0422, 0.0369, 0.0146, 0.0368, 0.0148, 0.02,...              12   \n",
       "23  [0.0679, 0.0563, 0.0248, 0.0546, 0.0263, 0.036...              13   \n",
       "24  [0.0481, 0.0374, 0.0164, 0.0391, 0.016, 0.0229...              13   \n",
       "25  [0.0719, 0.0631, 0.0317, 0.068, 0.0373, 0.0409...              14   \n",
       "26  [0.0439, 0.0354, 0.0144, 0.037, 0.0147, 0.0201...              14   \n",
       "27  [0.0473, 0.0421, 0.0189, 0.0465, 0.0249, 0.020...              15   \n",
       "28  [0.1253, 0.1055, 0.0787, 0.1394, 0.068, 0.0741...              15   \n",
       "29  [0.0727, 0.0664, 0.0399, 0.0985, 0.0556, 0.037...              15   \n",
       "30  [0.3681, 0.4227, 0.4835, 0.413, 0.49, 0.3793, ...              16   \n",
       "31  [0.0946, 0.0822, 0.0387, 0.082, 0.0436, 0.0415...              17   \n",
       "32  [0.0671, 0.0595, 0.0265, 0.0627, 0.0311, 0.027...              17   \n",
       "33  [0.0272, 0.0216, 0.0076, 0.0212, 0.008, 0.0116...              18   \n",
       "34  [0.0468, 0.0427, 0.0207, 0.0412, 0.0241, 0.025...              19   \n",
       "35  [0.0277, 0.023, 0.0078, 0.0228, 0.0082, 0.0116...              20   \n",
       "36  [0.0396, 0.0328, 0.0127, 0.0334, 0.0123, 0.018...              21   \n",
       "37  [0.0684, 0.0771, 0.0343, 0.0831, 0.0291, 0.036...              21   \n",
       "38  [0.0729, 0.0566, 0.0304, 0.0702, 0.0327, 0.036...              22   \n",
       "39  [0.0457, 0.0371, 0.0164, 0.0417, 0.0166, 0.023...              22   \n",
       "40  [0.1057, 0.0931, 0.0371, 0.1045, 0.0628, 0.045...              22   \n",
       "41  [0.0425, 0.0337, 0.0148, 0.0354, 0.0142, 0.021...              22   \n",
       "42  [0.0286, 0.0227, 0.0077, 0.0237, 0.0078, 0.011...              23   \n",
       "43  [0.0638, 0.0593, 0.0249, 0.0693, 0.0243, 0.026...              23   \n",
       "44  [0.0959, 0.0825, 0.0369, 0.0965, 0.0386, 0.036...              23   \n",
       "45  [0.0289, 0.0229, 0.0072, 0.027, 0.0075, 0.0103...              23   \n",
       "46  [0.1372, 0.1269, 0.086, 0.1283, 0.0722, 0.1056...              24   \n",
       "47  [0.0778, 0.072, 0.0342, 0.0611, 0.0436, 0.0402...              24   \n",
       "48  [0.0222, 0.0187, 0.0062, 0.0171, 0.0071, 0.008...              25   \n",
       "49  [0.0653, 0.0683, 0.0269, 0.0549, 0.0339, 0.032...              25   \n",
       "50  [0.1121, 0.0971, 0.0516, 0.1006, 0.0769, 0.072...              26   \n",
       "51  [0.074, 0.0695, 0.0391, 0.0754, 0.049, 0.031, ...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                 10  [0.1359, 0.2524, 0.1165, 0.1441, 0.1044, 0.118...   \n",
       "1                 23  [0.4961, 0.4835, 0.2675, 0.3749, 0.2905, 0.242...   \n",
       "2                 10  [0.2498, 0.3114, 0.1705, 0.1544, 0.1095, 0.140...   \n",
       "3                 22  [0.3287, 0.3172, 0.2202, 0.3425, 0.5237, 0.341...   \n",
       "4                 10  [0.0697, 0.08, 0.0711, 0.068, 0.0808, 0.0431, ...   \n",
       "5                 10  [0.0708, 0.0698, 0.0733, 0.0921, 0.0944, 0.151...   \n",
       "6                  3  [0.768, 0.5843, 0.7709, 0.6083, 0.464, 0.6385,...   \n",
       "7                 22  [0.5756, 0.3204, 0.5409, 0.3214, 0.2954, 0.530...   \n",
       "8                 10  [0.4059, 0.2781, 0.5041, 0.2988, 0.2338, 0.408...   \n",
       "9                 26  [0.7803, 0.8691, 0.8609, 0.7698, 0.9074, 0.711...   \n",
       "10                 1  [0.5292, 0.7472, 0.5525, 0.4508, 0.5843, 0.434...   \n",
       "11                10  [0.3213, 0.5662, 0.4013, 0.346, 0.6181, 0.4101...   \n",
       "12                14  [0.7039, 0.823, 0.8797, 0.8659, 0.7542, 0.8152...   \n",
       "13                22  [0.7757, 0.7242, 0.6604, 0.6059, 0.7425, 0.884...   \n",
       "14                 3  [0.4495, 0.3823, 0.5805, 0.5927, 0.579, 0.4668...   \n",
       "15                10  [0.1942, 0.371, 0.445, 0.2852, 0.3454, 0.3553,...   \n",
       "16                10  [0.0886, 0.0724, 0.0615, 0.137, 0.1518, 0.1574...   \n",
       "17                10  [0.2898, 0.3317, 0.2651, 0.46, 0.4726, 0.3975,...   \n",
       "18                10  [0.0901, 0.3088, 0.0808, 0.1009, 0.3012, 0.106...   \n",
       "19                10  [0.1194, 0.4657, 0.0891, 0.2013, 0.349, 0.1372...   \n",
       "20                10  [0.1843, 0.3458, 0.2906, 0.4209, 0.1474, 0.233...   \n",
       "21                10  [0.0948, 0.1902, 0.1267, 0.3223, 0.0762, 0.099...   \n",
       "22                10  [0.0824, 0.1805, 0.1444, 0.2424, 0.1156, 0.225...   \n",
       "23                10  [0.4491, 0.4668, 0.3846, 0.3297, 0.3703, 0.483...   \n",
       "24                10  [0.0621, 0.0736, 0.0747, 0.0844, 0.0786, 0.084...   \n",
       "25                10  [0.1402, 0.1503, 0.529, 0.3124, 0.2331, 0.3049...   \n",
       "26                10  [0.0444, 0.0445, 0.1917, 0.119, 0.0659, 0.1297...   \n",
       "27                22  [0.3498, 0.4016, 0.4223, 0.3187, 0.7323, 0.448...   \n",
       "28                15  [0.7387, 0.6494, 0.6497, 0.6086, 0.9169, 0.781...   \n",
       "29                15  [0.7474, 0.5721, 0.8208, 0.7074, 0.9331, 0.771...   \n",
       "30                16  [0.9997, 1.0, 0.9996, 0.9999, 1.0, 0.9998, 1.0...   \n",
       "31                22  [0.5992, 0.463, 0.4847, 0.3927, 0.3707, 0.2708...   \n",
       "32                 0  [0.3629, 0.5566, 0.4938, 0.5209, 0.3099, 0.448...   \n",
       "33                10  [0.0232, 0.0543, 0.0476, 0.0264, 0.0506, 0.053...   \n",
       "34                10  [0.2185, 0.5157, 0.1014, 0.1455, 0.3428, 0.137...   \n",
       "35                10  [0.0602, 0.0824, 0.1929, 0.1039, 0.0551, 0.168...   \n",
       "36                10  [0.0923, 0.0723, 0.154, 0.1553, 0.0955, 0.1404...   \n",
       "37                23  [0.3938, 0.5634, 0.62, 0.6668, 0.3535, 0.7809,...   \n",
       "38                22  [0.2731, 0.1899, 0.1945, 0.2092, 0.2148, 0.431...   \n",
       "39                10  [0.1297, 0.0434, 0.0497, 0.092, 0.0684, 0.0839...   \n",
       "40                23  [0.5358, 0.5832, 0.3916, 0.6435, 0.5173, 0.424...   \n",
       "41                10  [0.0508, 0.0435, 0.0481, 0.0606, 0.0692, 0.152...   \n",
       "42                10  [0.1049, 0.078, 0.2088, 0.1611, 0.0718, 0.1792...   \n",
       "43                23  [0.5763, 0.5398, 0.5881, 0.6542, 0.3865, 0.717...   \n",
       "44                23  [0.6403, 0.5658, 0.7719, 0.6594, 0.4262, 0.902...   \n",
       "45                23  [0.263, 0.18, 0.3523, 0.256, 0.1626, 0.2582, 0...   \n",
       "46                22  [0.5362, 0.3963, 0.4328, 0.4197, 0.4365, 0.471...   \n",
       "47                 0  [0.3188, 0.4277, 0.2587, 0.4031, 0.2398, 0.372...   \n",
       "48                10  [0.0662, 0.2168, 0.0526, 0.0619, 0.1974, 0.084...   \n",
       "49                 1  [0.1907, 0.8012, 0.3992, 0.3625, 0.5071, 0.361...   \n",
       "50                22  [0.3328, 0.4324, 0.617, 0.2739, 0.1745, 0.4066...   \n",
       "51                22  [0.3604, 0.3987, 0.5031, 0.3926, 0.5042, 0.338...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8641, 0.7476, 0.8835, 0.8559, 0.8956, 0.881...  0.192308  0.166493  \n",
       "1   [0.5039, 0.5165, 0.7325, 0.6251, 0.7095, 0.757...       NaN       NaN  \n",
       "2   [0.7502, 0.6886, 0.8295, 0.8456, 0.8905, 0.859...       NaN       NaN  \n",
       "3   [0.6713, 0.6828, 0.7798, 0.6575, 0.4763, 0.658...       NaN       NaN  \n",
       "4   [0.9303, 0.92, 0.9289, 0.932, 0.9192, 0.9569, ...       NaN       NaN  \n",
       "5   [0.9292, 0.9302, 0.9267, 0.9079, 0.9056, 0.848...       NaN       NaN  \n",
       "6   [0.232, 0.4157, 0.2291, 0.3917, 0.536, 0.3615,...       NaN       NaN  \n",
       "7   [0.4244, 0.6796, 0.4591, 0.6786, 0.7046, 0.469...       NaN       NaN  \n",
       "8   [0.5941, 0.7219, 0.4959, 0.7012, 0.7662, 0.591...       NaN       NaN  \n",
       "9   [0.2197, 0.1309, 0.1391, 0.2302, 0.0926, 0.288...       NaN       NaN  \n",
       "10  [0.4708, 0.2528, 0.4475, 0.5492, 0.4157, 0.565...       NaN       NaN  \n",
       "11  [0.6787, 0.4338, 0.5987, 0.654, 0.3819, 0.5899...       NaN       NaN  \n",
       "12  [0.2961, 0.177, 0.1203, 0.1341, 0.2458, 0.1848...       NaN       NaN  \n",
       "13  [0.2243, 0.2758, 0.3396, 0.3941, 0.2575, 0.115...       NaN       NaN  \n",
       "14  [0.5505, 0.6177, 0.4195, 0.4073, 0.421, 0.5332...       NaN       NaN  \n",
       "15  [0.8058, 0.629, 0.555, 0.7148, 0.6546, 0.6447,...       NaN       NaN  \n",
       "16  [0.9114, 0.9276, 0.9385, 0.863, 0.8482, 0.8426...       NaN       NaN  \n",
       "17  [0.7102, 0.6683, 0.7349, 0.54, 0.5274, 0.6025,...       NaN       NaN  \n",
       "18  [0.9099, 0.6912, 0.9192, 0.8991, 0.6988, 0.893...       NaN       NaN  \n",
       "19  [0.8806, 0.5343, 0.9109, 0.7987, 0.651, 0.8628...       NaN       NaN  \n",
       "20  [0.8157, 0.6542, 0.7094, 0.5791, 0.8526, 0.766...       NaN       NaN  \n",
       "21  [0.9052, 0.8098, 0.8733, 0.6777, 0.9238, 0.900...       NaN       NaN  \n",
       "22  [0.9176, 0.8195, 0.8556, 0.7576, 0.8844, 0.775...       NaN       NaN  \n",
       "23  [0.5509, 0.5332, 0.6154, 0.6703, 0.6297, 0.516...       NaN       NaN  \n",
       "24  [0.9379, 0.9264, 0.9253, 0.9156, 0.9214, 0.915...       NaN       NaN  \n",
       "25  [0.8598, 0.8497, 0.471, 0.6876, 0.7669, 0.6951...       NaN       NaN  \n",
       "26  [0.9556, 0.9555, 0.8083, 0.881, 0.9341, 0.8703...       NaN       NaN  \n",
       "27  [0.6502, 0.5984, 0.5777, 0.6813, 0.2677, 0.551...       NaN       NaN  \n",
       "28  [0.2613, 0.3506, 0.3503, 0.3914, 0.0831, 0.218...       NaN       NaN  \n",
       "29  [0.2526, 0.4279, 0.1792, 0.2926, 0.0669, 0.228...       NaN       NaN  \n",
       "30  [0.0003, 0.0, 0.0004, 1e-04, 0.0, 0.0002, 0.0,...       NaN       NaN  \n",
       "31  [0.4008, 0.537, 0.5153, 0.6073, 0.6293, 0.7292...       NaN       NaN  \n",
       "32  [0.6371, 0.4434, 0.5062, 0.4791, 0.6901, 0.551...       NaN       NaN  \n",
       "33  [0.9768, 0.9457, 0.9524, 0.9736, 0.9494, 0.946...       NaN       NaN  \n",
       "34  [0.7815, 0.4843, 0.8986, 0.8545, 0.6572, 0.862...       NaN       NaN  \n",
       "35  [0.9398, 0.9176, 0.8071, 0.8961, 0.9449, 0.832...       NaN       NaN  \n",
       "36  [0.9077, 0.9277, 0.846, 0.8447, 0.9045, 0.8596...       NaN       NaN  \n",
       "37  [0.6062, 0.4366, 0.38, 0.3332, 0.6465, 0.2191,...       NaN       NaN  \n",
       "38  [0.7269, 0.8101, 0.8055, 0.7908, 0.7852, 0.568...       NaN       NaN  \n",
       "39  [0.8703, 0.9566, 0.9503, 0.908, 0.9316, 0.9161...       NaN       NaN  \n",
       "40  [0.4642, 0.4168, 0.6084, 0.3565, 0.4827, 0.575...       NaN       NaN  \n",
       "41  [0.9492, 0.9565, 0.9519, 0.9394, 0.9308, 0.847...       NaN       NaN  \n",
       "42  [0.8951, 0.922, 0.7912, 0.8389, 0.9282, 0.8208...       NaN       NaN  \n",
       "43  [0.4237, 0.4602, 0.4119, 0.3458, 0.6135, 0.282...       NaN       NaN  \n",
       "44  [0.3597, 0.4342, 0.2281, 0.3406, 0.5738, 0.097...       NaN       NaN  \n",
       "45  [0.737, 0.82, 0.6477, 0.744, 0.8374, 0.7418, 0...       NaN       NaN  \n",
       "46  [0.4638, 0.6037, 0.5672, 0.5803, 0.5635, 0.528...       NaN       NaN  \n",
       "47  [0.6812, 0.5723, 0.7413, 0.5969, 0.7602, 0.627...       NaN       NaN  \n",
       "48  [0.9338, 0.7832, 0.9474, 0.9381, 0.8026, 0.915...       NaN       NaN  \n",
       "49  [0.8093, 0.1988, 0.6008, 0.6375, 0.4929, 0.638...       NaN       NaN  \n",
       "50  [0.6672, 0.5676, 0.383, 0.7261, 0.8255, 0.5934...       NaN       NaN  \n",
       "51  [0.6396, 0.6013, 0.4969, 0.6074, 0.4958, 0.661...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 : Training: loss:  0.14781295\n",
      "402 : Training: loss:  0.1609001\n",
      "403 : Training: loss:  0.17088258\n",
      "404 : Training: loss:  0.16765843\n",
      "405 : Training: loss:  0.13768004\n",
      "406 : Training: loss:  0.1429407\n",
      "407 : Training: loss:  0.15936893\n",
      "408 : Training: loss:  0.13963072\n",
      "409 : Training: loss:  0.15791976\n",
      "410 : Training: loss:  0.20776074\n",
      "411 : Training: loss:  0.17146568\n",
      "412 : Training: loss:  0.15884103\n",
      "413 : Training: loss:  0.15254034\n",
      "414 : Training: loss:  0.16814715\n",
      "415 : Training: loss:  0.14991347\n",
      "416 : Training: loss:  0.15357147\n",
      "417 : Training: loss:  0.16497084\n",
      "418 : Training: loss:  0.16513097\n",
      "419 : Training: loss:  0.15775745\n",
      "420 : Training: loss:  0.15620211\n",
      "Validation: Loss:  0.1650857  Accuracy:  0.1923077\n",
      "421 : Training: loss:  0.16645956\n",
      "422 : Training: loss:  0.15518303\n",
      "423 : Training: loss:  0.14785276\n",
      "424 : Training: loss:  0.15454231\n",
      "425 : Training: loss:  0.14840028\n",
      "426 : Training: loss:  0.14875391\n",
      "427 : Training: loss:  0.16876556\n",
      "428 : Training: loss:  0.15256456\n",
      "429 : Training: loss:  0.15470302\n",
      "430 : Training: loss:  0.14808269\n",
      "431 : Training: loss:  0.14603859\n",
      "432 : Training: loss:  0.1642224\n",
      "433 : Training: loss:  0.1452623\n",
      "434 : Training: loss:  0.15551755\n",
      "435 : Training: loss:  0.14626986\n",
      "436 : Training: loss:  0.15463088\n",
      "437 : Training: loss:  0.15401469\n",
      "438 : Training: loss:  0.14810546\n",
      "439 : Training: loss:  0.14929031\n",
      "440 : Training: loss:  0.145067\n",
      "Validation: Loss:  0.16390732  Accuracy:  0.1923077\n",
      "441 : Training: loss:  0.15862414\n",
      "442 : Training: loss:  0.14306654\n",
      "443 : Training: loss:  0.15204181\n",
      "444 : Training: loss:  0.15763\n",
      "445 : Training: loss:  0.1551457\n",
      "446 : Training: loss:  0.1376887\n",
      "447 : Training: loss:  0.15585805\n",
      "448 : Training: loss:  0.15894844\n",
      "449 : Training: loss:  0.14139608\n",
      "450 : Training: loss:  0.16781142\n",
      "451 : Training: loss:  0.15486124\n",
      "452 : Training: loss:  0.15628615\n",
      "453 : Training: loss:  0.13950938\n",
      "454 : Training: loss:  0.15674141\n",
      "455 : Training: loss:  0.14459199\n",
      "456 : Training: loss:  0.15070069\n",
      "457 : Training: loss:  0.15656841\n",
      "458 : Training: loss:  0.14099343\n",
      "459 : Training: loss:  0.14975937\n",
      "460 : Training: loss:  0.14705528\n",
      "Validation: Loss:  0.1628579  Accuracy:  0.21153846\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0614, 0.0549, 0.0223, 0.0527, 0.0262, 0.032...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1381, 0.2581, 0.1165, 0.1474, 0.1042, 0.117...</td>\n",
       "      <td>[0.8619, 0.7419, 0.8835, 0.8526, 0.8958, 0.882...</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.162858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0805, 0.0819, 0.0286, 0.0626, 0.045, 0.0379...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4991, 0.4911, 0.2671, 0.3808, 0.2923, 0.241...</td>\n",
       "      <td>[0.5009, 0.5089, 0.7329, 0.6192, 0.7077, 0.758...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0577, 0.0565, 0.0222, 0.0507, 0.0271, 0.032...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2545, 0.3201, 0.17, 0.1594, 0.1096, 0.1388,...</td>\n",
       "      <td>[0.7455, 0.6799, 0.83, 0.8406, 0.8904, 0.8612,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0381, 0.0409, 0.0158, 0.0367, 0.0212, 0.019...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.331, 0.3249, 0.2147, 0.3474, 0.528, 0.3411,...</td>\n",
       "      <td>[0.669, 0.6751, 0.7853, 0.6526, 0.472, 0.6589,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0371, 0.0338, 0.0116, 0.0319, 0.0134, 0.019...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0702, 0.0834, 0.0694, 0.0701, 0.0807, 0.042...</td>\n",
       "      <td>[0.9298, 0.9166, 0.9306, 0.9299, 0.9193, 0.957...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0617, 0.0564, 0.0268, 0.0552, 0.0261, 0.038...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.07, 0.0683, 0.0741, 0.091, 0.095, 0.1519, 0...</td>\n",
       "      <td>[0.93, 0.9317, 0.9259, 0.909, 0.905, 0.8481, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0986, 0.0897, 0.0309, 0.1318, 0.0613, 0.038...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.771, 0.5853, 0.7802, 0.6164, 0.4494, 0.6486...</td>\n",
       "      <td>[0.229, 0.4147, 0.2198, 0.3836, 0.5506, 0.3514...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0806, 0.0737, 0.0365, 0.0944, 0.0433, 0.050...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5788, 0.3161, 0.5523, 0.3261, 0.2885, 0.541...</td>\n",
       "      <td>[0.4212, 0.6839, 0.4477, 0.6739, 0.7115, 0.459...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1163, 0.1111, 0.0585, 0.1257, 0.0689, 0.085...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.4097, 0.2766, 0.5203, 0.3017, 0.2309, 0.413...</td>\n",
       "      <td>[0.5903, 0.7234, 0.4797, 0.6983, 0.7691, 0.586...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.1195, 0.1464, 0.1072, 0.1323, 0.1737, 0.084...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7819, 0.8728, 0.8573, 0.7713, 0.9099, 0.710...</td>\n",
       "      <td>[0.2181, 0.1272, 0.1427, 0.2287, 0.0901, 0.289...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0539, 0.0741, 0.0215, 0.0556, 0.0399, 0.035...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5304, 0.7548, 0.5328, 0.4507, 0.5831, 0.437...</td>\n",
       "      <td>[0.4696, 0.2452, 0.4672, 0.5493, 0.4169, 0.562...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0442, 0.0508, 0.022, 0.0519, 0.0298, 0.0328...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.3235, 0.5734, 0.391, 0.3452, 0.6213, 0.4122...</td>\n",
       "      <td>[0.6765, 0.4266, 0.609, 0.6548, 0.3787, 0.5878...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.1007, 0.1117, 0.0588, 0.1389, 0.0909, 0.06,...</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.6938, 0.8122, 0.8727, 0.8611, 0.7443, 0.805...</td>\n",
       "      <td>[0.3062, 0.1878, 0.1273, 0.1389, 0.2557, 0.195...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2665, 0.2476, 0.2243, 0.2703, 0.1619, 0.255...</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7658, 0.7022, 0.645, 0.5894, 0.735, 0.8854,...</td>\n",
       "      <td>[0.2342, 0.2978, 0.355, 0.4106, 0.265, 0.1146,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0414, 0.0419, 0.0185, 0.0614, 0.024, 0.021,...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4538, 0.389, 0.5748, 0.5986, 0.5761, 0.4669...</td>\n",
       "      <td>[0.5462, 0.611, 0.4252, 0.4014, 0.4239, 0.5331...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.055, 0.0544, 0.0246, 0.0494, 0.0316, 0.0292...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1951, 0.3864, 0.4398, 0.2882, 0.3468, 0.345...</td>\n",
       "      <td>[0.8049, 0.6136, 0.5602, 0.7118, 0.6532, 0.654...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0545, 0.0484, 0.0215, 0.0496, 0.0208, 0.032...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0873, 0.0718, 0.0619, 0.1351, 0.15, 0.1567,...</td>\n",
       "      <td>[0.9127, 0.9282, 0.9381, 0.8649, 0.85, 0.8433,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0667, 0.0661, 0.0314, 0.0665, 0.0346, 0.040...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2868, 0.3326, 0.2681, 0.4562, 0.4691, 0.395...</td>\n",
       "      <td>[0.7132, 0.6674, 0.7319, 0.5438, 0.5309, 0.604...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0484, 0.0411, 0.0173, 0.0407, 0.0165, 0.025...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0904, 0.311, 0.0809, 0.1003, 0.3075, 0.1084...</td>\n",
       "      <td>[0.9096, 0.689, 0.9191, 0.8997, 0.6925, 0.8916...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0473, 0.0432, 0.0194, 0.0407, 0.0174, 0.025...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1188, 0.4695, 0.0888, 0.2008, 0.358, 0.1367...</td>\n",
       "      <td>[0.8812, 0.5305, 0.9112, 0.7992, 0.642, 0.8633...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0686, 0.0632, 0.0276, 0.0635, 0.0314, 0.046...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1853, 0.342, 0.2968, 0.4275, 0.145, 0.2304,...</td>\n",
       "      <td>[0.8147, 0.658, 0.7032, 0.5725, 0.855, 0.7696,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0298, 0.0281, 0.0092, 0.0298, 0.0097, 0.015...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0965, 0.1839, 0.1284, 0.3278, 0.0727, 0.102...</td>\n",
       "      <td>[0.9035, 0.8161, 0.8716, 0.6722, 0.9273, 0.897...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0399, 0.038, 0.014, 0.038, 0.0144, 0.0209, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.084, 0.1802, 0.1439, 0.2439, 0.1141, 0.2257...</td>\n",
       "      <td>[0.916, 0.8198, 0.8561, 0.7561, 0.8859, 0.7743...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0625, 0.0559, 0.0232, 0.0555, 0.0256, 0.036...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.4512, 0.4654, 0.3906, 0.3271, 0.3744, 0.484...</td>\n",
       "      <td>[0.5488, 0.5346, 0.6094, 0.6729, 0.6256, 0.515...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0462, 0.0387, 0.016, 0.0406, 0.0156, 0.0241...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0626, 0.0729, 0.0783, 0.083, 0.0784, 0.086,...</td>\n",
       "      <td>[0.9374, 0.9271, 0.9217, 0.917, 0.9216, 0.914,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0654, 0.0626, 0.0293, 0.0677, 0.0355, 0.040...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1406, 0.1528, 0.5374, 0.3158, 0.2346, 0.305...</td>\n",
       "      <td>[0.8594, 0.8472, 0.4626, 0.6842, 0.7654, 0.694...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0409, 0.0362, 0.0136, 0.0375, 0.014, 0.0205...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0448, 0.0432, 0.1938, 0.1192, 0.0648, 0.131...</td>\n",
       "      <td>[0.9552, 0.9568, 0.8062, 0.8808, 0.9352, 0.868...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0401, 0.0409, 0.0167, 0.0447, 0.0239, 0.02,...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3531, 0.4033, 0.4169, 0.317, 0.7374, 0.4597...</td>\n",
       "      <td>[0.6469, 0.5967, 0.5831, 0.683, 0.2626, 0.5403...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.1023, 0.0954, 0.0668, 0.1293, 0.061, 0.0654...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7329, 0.6355, 0.6424, 0.5992, 0.9171, 0.791...</td>\n",
       "      <td>[0.2671, 0.3645, 0.3576, 0.4008, 0.0829, 0.209...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0563, 0.0597, 0.0321, 0.0897, 0.051, 0.0324...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7445, 0.5603, 0.8145, 0.701, 0.9335, 0.7808...</td>\n",
       "      <td>[0.2555, 0.4397, 0.1855, 0.299, 0.0665, 0.2192...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3561, 0.4292, 0.4784, 0.4076, 0.5022, 0.381...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9997, 1.0, 0.9995, 0.9998, 1.0, 0.9998, 1.0...</td>\n",
       "      <td>[0.0003, 0.0, 0.0005, 0.0002, 0.0, 0.0002, 0.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0821, 0.0784, 0.0344, 0.0801, 0.0405, 0.039...</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6008, 0.4479, 0.493, 0.3941, 0.3626, 0.2756...</td>\n",
       "      <td>[0.3992, 0.5521, 0.507, 0.6059, 0.6374, 0.7244...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0577, 0.0562, 0.0228, 0.061, 0.0277, 0.025,...</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3618, 0.5477, 0.5054, 0.5241, 0.2995, 0.448...</td>\n",
       "      <td>[0.6382, 0.4523, 0.4946, 0.4759, 0.7005, 0.551...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.026, 0.0229, 0.0074, 0.0223, 0.0079, 0.0124...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0238, 0.0564, 0.0469, 0.027, 0.0505, 0.0541...</td>\n",
       "      <td>[0.9762, 0.9436, 0.9531, 0.973, 0.9495, 0.9459...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0446, 0.045, 0.0206, 0.0433, 0.025, 0.0274,...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2275, 0.537, 0.0996, 0.1505, 0.3569, 0.1377...</td>\n",
       "      <td>[0.7725, 0.463, 0.9004, 0.8495, 0.6431, 0.8623...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0257, 0.0236, 0.0073, 0.0234, 0.0078, 0.012...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0608, 0.0813, 0.1965, 0.1047, 0.0539, 0.171...</td>\n",
       "      <td>[0.9392, 0.9187, 0.8035, 0.8953, 0.9461, 0.828...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0378, 0.0341, 0.0122, 0.0347, 0.0119, 0.019...</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0917, 0.0707, 0.1566, 0.1545, 0.0901, 0.140...</td>\n",
       "      <td>[0.9083, 0.9293, 0.8434, 0.8455, 0.9099, 0.859...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0575, 0.0714, 0.0289, 0.0791, 0.025, 0.033,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3831, 0.5349, 0.6163, 0.6561, 0.3165, 0.783...</td>\n",
       "      <td>[0.6169, 0.4651, 0.3837, 0.3439, 0.6835, 0.216...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.066, 0.0547, 0.028, 0.0699, 0.0303, 0.0359,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2784, 0.1831, 0.1886, 0.2054, 0.2107, 0.447...</td>\n",
       "      <td>[0.7216, 0.8169, 0.8114, 0.7946, 0.7893, 0.552...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0431, 0.0378, 0.0157, 0.0428, 0.016, 0.0237...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1324, 0.0419, 0.0494, 0.0927, 0.066, 0.0885...</td>\n",
       "      <td>[0.8676, 0.9581, 0.9506, 0.9073, 0.934, 0.9115...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0912, 0.0896, 0.0329, 0.1033, 0.0594, 0.042...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5406, 0.5749, 0.3857, 0.6443, 0.5089, 0.431...</td>\n",
       "      <td>[0.4594, 0.4251, 0.6143, 0.3557, 0.4911, 0.568...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0409, 0.0349, 0.0144, 0.0369, 0.0139, 0.022...</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0512, 0.0428, 0.0482, 0.0599, 0.0686, 0.158...</td>\n",
       "      <td>[0.9488, 0.9572, 0.9518, 0.9401, 0.9314, 0.842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0263, 0.0231, 0.0072, 0.0243, 0.0073, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1028, 0.0739, 0.2121, 0.1588, 0.0652, 0.179...</td>\n",
       "      <td>[0.8972, 0.9261, 0.7879, 0.8412, 0.9348, 0.820...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0514, 0.0531, 0.02, 0.0638, 0.0195, 0.0226,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5623, 0.505, 0.5767, 0.6379, 0.3432, 0.717,...</td>\n",
       "      <td>[0.4377, 0.495, 0.4233, 0.3621, 0.6568, 0.283,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0742, 0.0698, 0.0279, 0.0851, 0.0291, 0.029...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6198, 0.5188, 0.7637, 0.6364, 0.3734, 0.902...</td>\n",
       "      <td>[0.3802, 0.4812, 0.2363, 0.3636, 0.6266, 0.097...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0255, 0.0226, 0.0064, 0.0271, 0.0067, 0.010...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2587, 0.1672, 0.3526, 0.2498, 0.1445, 0.261...</td>\n",
       "      <td>[0.7413, 0.8328, 0.6474, 0.7502, 0.8555, 0.738...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1227, 0.1194, 0.0774, 0.1227, 0.0651, 0.099...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5331, 0.3816, 0.4389, 0.4144, 0.433, 0.4655...</td>\n",
       "      <td>[0.4669, 0.6184, 0.5611, 0.5856, 0.567, 0.5345...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0698, 0.0712, 0.0312, 0.0609, 0.0416, 0.039...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3192, 0.4226, 0.2638, 0.4057, 0.235, 0.3674...</td>\n",
       "      <td>[0.6808, 0.5774, 0.7362, 0.5943, 0.765, 0.6326...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0205, 0.0196, 0.0058, 0.0174, 0.007, 0.0092...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0679, 0.229, 0.0483, 0.062, 0.2048, 0.0832,...</td>\n",
       "      <td>[0.9321, 0.771, 0.9517, 0.938, 0.7952, 0.9168,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0593, 0.0694, 0.0256, 0.0553, 0.0345, 0.033...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1923, 0.8115, 0.3867, 0.3662, 0.5114, 0.356...</td>\n",
       "      <td>[0.8077, 0.1885, 0.6133, 0.6338, 0.4886, 0.643...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1026, 0.0949, 0.0476, 0.0995, 0.0726, 0.070...</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3355, 0.4317, 0.6197, 0.2768, 0.1729, 0.403...</td>\n",
       "      <td>[0.6645, 0.5683, 0.3803, 0.7232, 0.8271, 0.596...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0654, 0.0683, 0.0358, 0.0749, 0.0479, 0.03,...</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3649, 0.4097, 0.5074, 0.3958, 0.5062, 0.336...</td>\n",
       "      <td>[0.6351, 0.5903, 0.4926, 0.6042, 0.4938, 0.663...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0614, 0.0549, 0.0223, 0.0527, 0.0262, 0.032...               0   \n",
       "1   [0.0805, 0.0819, 0.0286, 0.0626, 0.045, 0.0379...               0   \n",
       "2   [0.0577, 0.0565, 0.0222, 0.0507, 0.0271, 0.032...               0   \n",
       "3   [0.0381, 0.0409, 0.0158, 0.0367, 0.0212, 0.019...               1   \n",
       "4   [0.0371, 0.0338, 0.0116, 0.0319, 0.0134, 0.019...               1   \n",
       "5   [0.0617, 0.0564, 0.0268, 0.0552, 0.0261, 0.038...               2   \n",
       "6   [0.0986, 0.0897, 0.0309, 0.1318, 0.0613, 0.038...               3   \n",
       "7   [0.0806, 0.0737, 0.0365, 0.0944, 0.0433, 0.050...               3   \n",
       "8   [0.1163, 0.1111, 0.0585, 0.1257, 0.0689, 0.085...               3   \n",
       "9   [0.1195, 0.1464, 0.1072, 0.1323, 0.1737, 0.084...               4   \n",
       "10  [0.0539, 0.0741, 0.0215, 0.0556, 0.0399, 0.035...               4   \n",
       "11  [0.0442, 0.0508, 0.022, 0.0519, 0.0298, 0.0328...               5   \n",
       "12  [0.1007, 0.1117, 0.0588, 0.1389, 0.0909, 0.06,...               6   \n",
       "13  [0.2665, 0.2476, 0.2243, 0.2703, 0.1619, 0.255...               7   \n",
       "14  [0.0414, 0.0419, 0.0185, 0.0614, 0.024, 0.021,...               8   \n",
       "15  [0.055, 0.0544, 0.0246, 0.0494, 0.0316, 0.0292...               8   \n",
       "16  [0.0545, 0.0484, 0.0215, 0.0496, 0.0208, 0.032...               9   \n",
       "17  [0.0667, 0.0661, 0.0314, 0.0665, 0.0346, 0.040...               9   \n",
       "18  [0.0484, 0.0411, 0.0173, 0.0407, 0.0165, 0.025...              10   \n",
       "19  [0.0473, 0.0432, 0.0194, 0.0407, 0.0174, 0.025...              10   \n",
       "20  [0.0686, 0.0632, 0.0276, 0.0635, 0.0314, 0.046...              11   \n",
       "21  [0.0298, 0.0281, 0.0092, 0.0298, 0.0097, 0.015...              11   \n",
       "22  [0.0399, 0.038, 0.014, 0.038, 0.0144, 0.0209, ...              12   \n",
       "23  [0.0625, 0.0559, 0.0232, 0.0555, 0.0256, 0.036...              13   \n",
       "24  [0.0462, 0.0387, 0.016, 0.0406, 0.0156, 0.0241...              13   \n",
       "25  [0.0654, 0.0626, 0.0293, 0.0677, 0.0355, 0.040...              14   \n",
       "26  [0.0409, 0.0362, 0.0136, 0.0375, 0.014, 0.0205...              14   \n",
       "27  [0.0401, 0.0409, 0.0167, 0.0447, 0.0239, 0.02,...              15   \n",
       "28  [0.1023, 0.0954, 0.0668, 0.1293, 0.061, 0.0654...              15   \n",
       "29  [0.0563, 0.0597, 0.0321, 0.0897, 0.051, 0.0324...              15   \n",
       "30  [0.3561, 0.4292, 0.4784, 0.4076, 0.5022, 0.381...              16   \n",
       "31  [0.0821, 0.0784, 0.0344, 0.0801, 0.0405, 0.039...              17   \n",
       "32  [0.0577, 0.0562, 0.0228, 0.061, 0.0277, 0.025,...              17   \n",
       "33  [0.026, 0.0229, 0.0074, 0.0223, 0.0079, 0.0124...              18   \n",
       "34  [0.0446, 0.045, 0.0206, 0.0433, 0.025, 0.0274,...              19   \n",
       "35  [0.0257, 0.0236, 0.0073, 0.0234, 0.0078, 0.012...              20   \n",
       "36  [0.0378, 0.0341, 0.0122, 0.0347, 0.0119, 0.019...              21   \n",
       "37  [0.0575, 0.0714, 0.0289, 0.0791, 0.025, 0.033,...              21   \n",
       "38  [0.066, 0.0547, 0.028, 0.0699, 0.0303, 0.0359,...              22   \n",
       "39  [0.0431, 0.0378, 0.0157, 0.0428, 0.016, 0.0237...              22   \n",
       "40  [0.0912, 0.0896, 0.0329, 0.1033, 0.0594, 0.042...              22   \n",
       "41  [0.0409, 0.0349, 0.0144, 0.0369, 0.0139, 0.022...              22   \n",
       "42  [0.0263, 0.0231, 0.0072, 0.0243, 0.0073, 0.011...              23   \n",
       "43  [0.0514, 0.0531, 0.02, 0.0638, 0.0195, 0.0226,...              23   \n",
       "44  [0.0742, 0.0698, 0.0279, 0.0851, 0.0291, 0.029...              23   \n",
       "45  [0.0255, 0.0226, 0.0064, 0.0271, 0.0067, 0.010...              23   \n",
       "46  [0.1227, 0.1194, 0.0774, 0.1227, 0.0651, 0.099...              24   \n",
       "47  [0.0698, 0.0712, 0.0312, 0.0609, 0.0416, 0.039...              24   \n",
       "48  [0.0205, 0.0196, 0.0058, 0.0174, 0.007, 0.0092...              25   \n",
       "49  [0.0593, 0.0694, 0.0256, 0.0553, 0.0345, 0.033...              25   \n",
       "50  [0.1026, 0.0949, 0.0476, 0.0995, 0.0726, 0.070...              26   \n",
       "51  [0.0654, 0.0683, 0.0358, 0.0749, 0.0479, 0.03,...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                 10  [0.1381, 0.2581, 0.1165, 0.1474, 0.1042, 0.117...   \n",
       "1                 23  [0.4991, 0.4911, 0.2671, 0.3808, 0.2923, 0.241...   \n",
       "2                 10  [0.2545, 0.3201, 0.17, 0.1594, 0.1096, 0.1388,...   \n",
       "3                 22  [0.331, 0.3249, 0.2147, 0.3474, 0.528, 0.3411,...   \n",
       "4                 10  [0.0702, 0.0834, 0.0694, 0.0701, 0.0807, 0.042...   \n",
       "5                 10  [0.07, 0.0683, 0.0741, 0.091, 0.095, 0.1519, 0...   \n",
       "6                  3  [0.771, 0.5853, 0.7802, 0.6164, 0.4494, 0.6486...   \n",
       "7                 22  [0.5788, 0.3161, 0.5523, 0.3261, 0.2885, 0.541...   \n",
       "8                 10  [0.4097, 0.2766, 0.5203, 0.3017, 0.2309, 0.413...   \n",
       "9                 26  [0.7819, 0.8728, 0.8573, 0.7713, 0.9099, 0.710...   \n",
       "10                 1  [0.5304, 0.7548, 0.5328, 0.4507, 0.5831, 0.437...   \n",
       "11                10  [0.3235, 0.5734, 0.391, 0.3452, 0.6213, 0.4122...   \n",
       "12                14  [0.6938, 0.8122, 0.8727, 0.8611, 0.7443, 0.805...   \n",
       "13                22  [0.7658, 0.7022, 0.645, 0.5894, 0.735, 0.8854,...   \n",
       "14                 3  [0.4538, 0.389, 0.5748, 0.5986, 0.5761, 0.4669...   \n",
       "15                10  [0.1951, 0.3864, 0.4398, 0.2882, 0.3468, 0.345...   \n",
       "16                10  [0.0873, 0.0718, 0.0619, 0.1351, 0.15, 0.1567,...   \n",
       "17                10  [0.2868, 0.3326, 0.2681, 0.4562, 0.4691, 0.395...   \n",
       "18                10  [0.0904, 0.311, 0.0809, 0.1003, 0.3075, 0.1084...   \n",
       "19                10  [0.1188, 0.4695, 0.0888, 0.2008, 0.358, 0.1367...   \n",
       "20                10  [0.1853, 0.342, 0.2968, 0.4275, 0.145, 0.2304,...   \n",
       "21                10  [0.0965, 0.1839, 0.1284, 0.3278, 0.0727, 0.102...   \n",
       "22                10  [0.084, 0.1802, 0.1439, 0.2439, 0.1141, 0.2257...   \n",
       "23                10  [0.4512, 0.4654, 0.3906, 0.3271, 0.3744, 0.484...   \n",
       "24                10  [0.0626, 0.0729, 0.0783, 0.083, 0.0784, 0.086,...   \n",
       "25                10  [0.1406, 0.1528, 0.5374, 0.3158, 0.2346, 0.305...   \n",
       "26                10  [0.0448, 0.0432, 0.1938, 0.1192, 0.0648, 0.131...   \n",
       "27                22  [0.3531, 0.4033, 0.4169, 0.317, 0.7374, 0.4597...   \n",
       "28                15  [0.7329, 0.6355, 0.6424, 0.5992, 0.9171, 0.791...   \n",
       "29                15  [0.7445, 0.5603, 0.8145, 0.701, 0.9335, 0.7808...   \n",
       "30                16  [0.9997, 1.0, 0.9995, 0.9998, 1.0, 0.9998, 1.0...   \n",
       "31                22  [0.6008, 0.4479, 0.493, 0.3941, 0.3626, 0.2756...   \n",
       "32                22  [0.3618, 0.5477, 0.5054, 0.5241, 0.2995, 0.448...   \n",
       "33                10  [0.0238, 0.0564, 0.0469, 0.027, 0.0505, 0.0541...   \n",
       "34                10  [0.2275, 0.537, 0.0996, 0.1505, 0.3569, 0.1377...   \n",
       "35                10  [0.0608, 0.0813, 0.1965, 0.1047, 0.0539, 0.171...   \n",
       "36                10  [0.0917, 0.0707, 0.1566, 0.1545, 0.0901, 0.140...   \n",
       "37                23  [0.3831, 0.5349, 0.6163, 0.6561, 0.3165, 0.783...   \n",
       "38                22  [0.2784, 0.1831, 0.1886, 0.2054, 0.2107, 0.447...   \n",
       "39                22  [0.1324, 0.0419, 0.0494, 0.0927, 0.066, 0.0885...   \n",
       "40                23  [0.5406, 0.5749, 0.3857, 0.6443, 0.5089, 0.431...   \n",
       "41                10  [0.0512, 0.0428, 0.0482, 0.0599, 0.0686, 0.158...   \n",
       "42                10  [0.1028, 0.0739, 0.2121, 0.1588, 0.0652, 0.179...   \n",
       "43                23  [0.5623, 0.505, 0.5767, 0.6379, 0.3432, 0.717,...   \n",
       "44                23  [0.6198, 0.5188, 0.7637, 0.6364, 0.3734, 0.902...   \n",
       "45                23  [0.2587, 0.1672, 0.3526, 0.2498, 0.1445, 0.261...   \n",
       "46                22  [0.5331, 0.3816, 0.4389, 0.4144, 0.433, 0.4655...   \n",
       "47                22  [0.3192, 0.4226, 0.2638, 0.4057, 0.235, 0.3674...   \n",
       "48                10  [0.0679, 0.229, 0.0483, 0.062, 0.2048, 0.0832,...   \n",
       "49                 1  [0.1923, 0.8115, 0.3867, 0.3662, 0.5114, 0.356...   \n",
       "50                22  [0.3355, 0.4317, 0.6197, 0.2768, 0.1729, 0.403...   \n",
       "51                22  [0.3649, 0.4097, 0.5074, 0.3958, 0.5062, 0.336...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8619, 0.7419, 0.8835, 0.8526, 0.8958, 0.882...  0.211538  0.162858  \n",
       "1   [0.5009, 0.5089, 0.7329, 0.6192, 0.7077, 0.758...       NaN       NaN  \n",
       "2   [0.7455, 0.6799, 0.83, 0.8406, 0.8904, 0.8612,...       NaN       NaN  \n",
       "3   [0.669, 0.6751, 0.7853, 0.6526, 0.472, 0.6589,...       NaN       NaN  \n",
       "4   [0.9298, 0.9166, 0.9306, 0.9299, 0.9193, 0.957...       NaN       NaN  \n",
       "5   [0.93, 0.9317, 0.9259, 0.909, 0.905, 0.8481, 0...       NaN       NaN  \n",
       "6   [0.229, 0.4147, 0.2198, 0.3836, 0.5506, 0.3514...       NaN       NaN  \n",
       "7   [0.4212, 0.6839, 0.4477, 0.6739, 0.7115, 0.459...       NaN       NaN  \n",
       "8   [0.5903, 0.7234, 0.4797, 0.6983, 0.7691, 0.586...       NaN       NaN  \n",
       "9   [0.2181, 0.1272, 0.1427, 0.2287, 0.0901, 0.289...       NaN       NaN  \n",
       "10  [0.4696, 0.2452, 0.4672, 0.5493, 0.4169, 0.562...       NaN       NaN  \n",
       "11  [0.6765, 0.4266, 0.609, 0.6548, 0.3787, 0.5878...       NaN       NaN  \n",
       "12  [0.3062, 0.1878, 0.1273, 0.1389, 0.2557, 0.195...       NaN       NaN  \n",
       "13  [0.2342, 0.2978, 0.355, 0.4106, 0.265, 0.1146,...       NaN       NaN  \n",
       "14  [0.5462, 0.611, 0.4252, 0.4014, 0.4239, 0.5331...       NaN       NaN  \n",
       "15  [0.8049, 0.6136, 0.5602, 0.7118, 0.6532, 0.654...       NaN       NaN  \n",
       "16  [0.9127, 0.9282, 0.9381, 0.8649, 0.85, 0.8433,...       NaN       NaN  \n",
       "17  [0.7132, 0.6674, 0.7319, 0.5438, 0.5309, 0.604...       NaN       NaN  \n",
       "18  [0.9096, 0.689, 0.9191, 0.8997, 0.6925, 0.8916...       NaN       NaN  \n",
       "19  [0.8812, 0.5305, 0.9112, 0.7992, 0.642, 0.8633...       NaN       NaN  \n",
       "20  [0.8147, 0.658, 0.7032, 0.5725, 0.855, 0.7696,...       NaN       NaN  \n",
       "21  [0.9035, 0.8161, 0.8716, 0.6722, 0.9273, 0.897...       NaN       NaN  \n",
       "22  [0.916, 0.8198, 0.8561, 0.7561, 0.8859, 0.7743...       NaN       NaN  \n",
       "23  [0.5488, 0.5346, 0.6094, 0.6729, 0.6256, 0.515...       NaN       NaN  \n",
       "24  [0.9374, 0.9271, 0.9217, 0.917, 0.9216, 0.914,...       NaN       NaN  \n",
       "25  [0.8594, 0.8472, 0.4626, 0.6842, 0.7654, 0.694...       NaN       NaN  \n",
       "26  [0.9552, 0.9568, 0.8062, 0.8808, 0.9352, 0.868...       NaN       NaN  \n",
       "27  [0.6469, 0.5967, 0.5831, 0.683, 0.2626, 0.5403...       NaN       NaN  \n",
       "28  [0.2671, 0.3645, 0.3576, 0.4008, 0.0829, 0.209...       NaN       NaN  \n",
       "29  [0.2555, 0.4397, 0.1855, 0.299, 0.0665, 0.2192...       NaN       NaN  \n",
       "30  [0.0003, 0.0, 0.0005, 0.0002, 0.0, 0.0002, 0.0...       NaN       NaN  \n",
       "31  [0.3992, 0.5521, 0.507, 0.6059, 0.6374, 0.7244...       NaN       NaN  \n",
       "32  [0.6382, 0.4523, 0.4946, 0.4759, 0.7005, 0.551...       NaN       NaN  \n",
       "33  [0.9762, 0.9436, 0.9531, 0.973, 0.9495, 0.9459...       NaN       NaN  \n",
       "34  [0.7725, 0.463, 0.9004, 0.8495, 0.6431, 0.8623...       NaN       NaN  \n",
       "35  [0.9392, 0.9187, 0.8035, 0.8953, 0.9461, 0.828...       NaN       NaN  \n",
       "36  [0.9083, 0.9293, 0.8434, 0.8455, 0.9099, 0.859...       NaN       NaN  \n",
       "37  [0.6169, 0.4651, 0.3837, 0.3439, 0.6835, 0.216...       NaN       NaN  \n",
       "38  [0.7216, 0.8169, 0.8114, 0.7946, 0.7893, 0.552...       NaN       NaN  \n",
       "39  [0.8676, 0.9581, 0.9506, 0.9073, 0.934, 0.9115...       NaN       NaN  \n",
       "40  [0.4594, 0.4251, 0.6143, 0.3557, 0.4911, 0.568...       NaN       NaN  \n",
       "41  [0.9488, 0.9572, 0.9518, 0.9401, 0.9314, 0.842...       NaN       NaN  \n",
       "42  [0.8972, 0.9261, 0.7879, 0.8412, 0.9348, 0.820...       NaN       NaN  \n",
       "43  [0.4377, 0.495, 0.4233, 0.3621, 0.6568, 0.283,...       NaN       NaN  \n",
       "44  [0.3802, 0.4812, 0.2363, 0.3636, 0.6266, 0.097...       NaN       NaN  \n",
       "45  [0.7413, 0.8328, 0.6474, 0.7502, 0.8555, 0.738...       NaN       NaN  \n",
       "46  [0.4669, 0.6184, 0.5611, 0.5856, 0.567, 0.5345...       NaN       NaN  \n",
       "47  [0.6808, 0.5774, 0.7362, 0.5943, 0.765, 0.6326...       NaN       NaN  \n",
       "48  [0.9321, 0.771, 0.9517, 0.938, 0.7952, 0.9168,...       NaN       NaN  \n",
       "49  [0.8077, 0.1885, 0.6133, 0.6338, 0.4886, 0.643...       NaN       NaN  \n",
       "50  [0.6645, 0.5683, 0.3803, 0.7232, 0.8271, 0.596...       NaN       NaN  \n",
       "51  [0.6351, 0.5903, 0.4926, 0.6042, 0.4938, 0.663...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 : Training: loss:  0.15202287\n",
      "462 : Training: loss:  0.14639585\n",
      "463 : Training: loss:  0.14567788\n",
      "464 : Training: loss:  0.13430992\n",
      "465 : Training: loss:  0.14555773\n",
      "466 : Training: loss:  0.14204739\n",
      "467 : Training: loss:  0.15807562\n",
      "468 : Training: loss:  0.17123997\n",
      "469 : Training: loss:  0.13493873\n",
      "470 : Training: loss:  0.16020234\n",
      "471 : Training: loss:  0.15226486\n",
      "472 : Training: loss:  0.14599356\n",
      "473 : Training: loss:  0.14841408\n",
      "474 : Training: loss:  0.14287464\n",
      "475 : Training: loss:  0.15146154\n",
      "476 : Training: loss:  0.15229282\n",
      "477 : Training: loss:  0.15505093\n",
      "478 : Training: loss:  0.15872338\n",
      "479 : Training: loss:  0.15866973\n",
      "480 : Training: loss:  0.17544791\n",
      "Validation: Loss:  0.16172394  Accuracy:  0.21153846\n",
      "481 : Training: loss:  0.151644\n",
      "482 : Training: loss:  0.13778944\n",
      "483 : Training: loss:  0.14303145\n",
      "484 : Training: loss:  0.14031364\n",
      "485 : Training: loss:  0.14918448\n",
      "486 : Training: loss:  0.15079458\n",
      "487 : Training: loss:  0.18559703\n",
      "488 : Training: loss:  0.14409971\n",
      "489 : Training: loss:  0.14670129\n",
      "490 : Training: loss:  0.14783752\n",
      "491 : Training: loss:  0.15508479\n",
      "492 : Training: loss:  0.15531395\n",
      "493 : Training: loss:  0.15209725\n",
      "494 : Training: loss:  0.14951941\n",
      "495 : Training: loss:  0.1517127\n",
      "496 : Training: loss:  0.13665888\n",
      "497 : Training: loss:  0.15646479\n",
      "498 : Training: loss:  0.14858943\n",
      "499 : Training: loss:  0.15088739\n",
      "500 : Training: loss:  0.1432717\n",
      "Validation: Loss:  0.16070618  Accuracy:  0.25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0656, 0.0529, 0.0199, 0.053, 0.0258, 0.0332...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1391, 0.2635, 0.1157, 0.1491, 0.1047, 0.115...</td>\n",
       "      <td>[0.8609, 0.7365, 0.8843, 0.8509, 0.8953, 0.884...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.160706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0828, 0.0775, 0.0247, 0.0612, 0.0438, 0.037...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4989, 0.4977, 0.2623, 0.3821, 0.2937, 0.234...</td>\n",
       "      <td>[0.5011, 0.5023, 0.7377, 0.6179, 0.7063, 0.765...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0613, 0.0541, 0.0195, 0.0508, 0.0265, 0.033...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2556, 0.3271, 0.168, 0.1619, 0.1102, 0.1359...</td>\n",
       "      <td>[0.7444, 0.6729, 0.832, 0.8381, 0.8898, 0.8641...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0391, 0.0384, 0.0133, 0.0355, 0.0206, 0.019...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3298, 0.3282, 0.2105, 0.348, 0.5289, 0.3373...</td>\n",
       "      <td>[0.6702, 0.6718, 0.7895, 0.652, 0.4711, 0.6627...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.04, 0.0324, 0.0101, 0.0319, 0.0132, 0.0194,...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0701, 0.0854, 0.0686, 0.0707, 0.0812, 0.041...</td>\n",
       "      <td>[0.9299, 0.9146, 0.9314, 0.9293, 0.9188, 0.958...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0656, 0.0544, 0.024, 0.0555, 0.0255, 0.0387...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.07, 0.0679, 0.0758, 0.091, 0.0958, 0.1524, ...</td>\n",
       "      <td>[0.93, 0.9321, 0.9242, 0.909, 0.9042, 0.8476, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0947, 0.0801, 0.0257, 0.1327, 0.0552, 0.036...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7728, 0.5847, 0.7869, 0.6205, 0.4405, 0.655...</td>\n",
       "      <td>[0.2272, 0.4153, 0.2131, 0.3795, 0.5595, 0.344...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0821, 0.069, 0.032, 0.0955, 0.041, 0.0498, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5822, 0.313, 0.56, 0.3299, 0.2833, 0.5512, ...</td>\n",
       "      <td>[0.4178, 0.687, 0.44, 0.6701, 0.7167, 0.4488, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1191, 0.1055, 0.0526, 0.1269, 0.0662, 0.084...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.4153, 0.2774, 0.5318, 0.3042, 0.2298, 0.421...</td>\n",
       "      <td>[0.5847, 0.7226, 0.4682, 0.6958, 0.7702, 0.578...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.1106, 0.1327, 0.0916, 0.1254, 0.1663, 0.084...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.778, 0.8756, 0.8532, 0.7682, 0.9112, 0.7033...</td>\n",
       "      <td>[0.222, 0.1244, 0.1468, 0.2318, 0.0888, 0.2967...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0525, 0.0669, 0.0177, 0.0524, 0.0377, 0.034...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5211, 0.7599, 0.5161, 0.4424, 0.5825, 0.431...</td>\n",
       "      <td>[0.4789, 0.2401, 0.4839, 0.5576, 0.4175, 0.569...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0442, 0.0464, 0.0185, 0.0502, 0.0285, 0.033...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.316, 0.5792, 0.3809, 0.3401, 0.6215, 0.4065...</td>\n",
       "      <td>[0.684, 0.4208, 0.6191, 0.6599, 0.3785, 0.5935...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0923, 0.1005, 0.0494, 0.1289, 0.083, 0.0566...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.6868, 0.8088, 0.8703, 0.8576, 0.7411, 0.795...</td>\n",
       "      <td>[0.3132, 0.1912, 0.1297, 0.1424, 0.2589, 0.204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.269, 0.2414, 0.2128, 0.2697, 0.1568, 0.2536...</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7581, 0.6919, 0.6382, 0.5833, 0.7314, 0.886...</td>\n",
       "      <td>[0.2419, 0.3081, 0.3618, 0.4167, 0.2686, 0.113...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0409, 0.038, 0.0152, 0.0596, 0.0225, 0.0204...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4523, 0.3949, 0.5687, 0.5996, 0.575, 0.4624...</td>\n",
       "      <td>[0.5477, 0.6051, 0.4313, 0.4004, 0.425, 0.5376...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0565, 0.0511, 0.0211, 0.0481, 0.0306, 0.028...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1926, 0.4007, 0.4323, 0.2881, 0.3507, 0.334...</td>\n",
       "      <td>[0.8074, 0.5993, 0.5677, 0.7119, 0.6493, 0.666...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0577, 0.0461, 0.019, 0.0494, 0.0201, 0.0322...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0867, 0.0718, 0.0628, 0.1342, 0.1491, 0.155...</td>\n",
       "      <td>[0.9133, 0.9282, 0.9372, 0.8658, 0.8509, 0.844...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0677, 0.0616, 0.027, 0.0652, 0.0329, 0.0402...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2856, 0.336, 0.2721, 0.4537, 0.4693, 0.3927...</td>\n",
       "      <td>[0.7144, 0.664, 0.7279, 0.5463, 0.5307, 0.6073...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0517, 0.039, 0.0151, 0.0403, 0.0159, 0.0258...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0891, 0.3119, 0.0806, 0.0989, 0.309, 0.109,...</td>\n",
       "      <td>[0.9109, 0.6881, 0.9194, 0.9011, 0.691, 0.891,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0502, 0.0409, 0.0168, 0.0399, 0.0167, 0.025...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1169, 0.473, 0.0886, 0.1991, 0.3618, 0.1351...</td>\n",
       "      <td>[0.8831, 0.527, 0.9114, 0.8009, 0.6382, 0.8649...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0717, 0.0604, 0.0244, 0.0637, 0.0304, 0.046...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1883, 0.3422, 0.3029, 0.4335, 0.1448, 0.230...</td>\n",
       "      <td>[0.8117, 0.6578, 0.6971, 0.5665, 0.8552, 0.769...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0319, 0.0265, 0.0078, 0.0298, 0.0093, 0.015...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0982, 0.1786, 0.1299, 0.3303, 0.0701, 0.105...</td>\n",
       "      <td>[0.9018, 0.8214, 0.8701, 0.6697, 0.9299, 0.894...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0426, 0.0362, 0.0122, 0.038, 0.0139, 0.021,...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0847, 0.1799, 0.1442, 0.2465, 0.1122, 0.226...</td>\n",
       "      <td>[0.9153, 0.8201, 0.8558, 0.7535, 0.8878, 0.774...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0648, 0.0523, 0.02, 0.0549, 0.0245, 0.0367,...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.4482, 0.469, 0.3921, 0.3242, 0.3768, 0.4828...</td>\n",
       "      <td>[0.5518, 0.531, 0.6079, 0.6758, 0.6232, 0.5172...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.05, 0.037, 0.0141, 0.0409, 0.0152, 0.0244, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.063, 0.0729, 0.0809, 0.0822, 0.0782, 0.0872...</td>\n",
       "      <td>[0.937, 0.9271, 0.9191, 0.9178, 0.9218, 0.9128...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0663, 0.0587, 0.0254, 0.0664, 0.034, 0.0393...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1427, 0.1549, 0.545, 0.3177, 0.2381, 0.3072...</td>\n",
       "      <td>[0.8573, 0.8451, 0.455, 0.6823, 0.7619, 0.6928...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0436, 0.0344, 0.0119, 0.0372, 0.0135, 0.020...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0457, 0.0424, 0.1971, 0.1202, 0.0643, 0.133...</td>\n",
       "      <td>[0.9543, 0.9576, 0.8029, 0.8798, 0.9357, 0.866...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0395, 0.037, 0.0139, 0.0427, 0.0226, 0.0197...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3542, 0.4032, 0.4148, 0.3141, 0.7408, 0.463...</td>\n",
       "      <td>[0.6458, 0.5968, 0.5852, 0.6859, 0.2592, 0.536...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0964, 0.0863, 0.0566, 0.1238, 0.0561, 0.062...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.728, 0.6269, 0.6397, 0.5928, 0.9171, 0.7946...</td>\n",
       "      <td>[0.272, 0.3731, 0.3603, 0.4072, 0.0829, 0.2054...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0508, 0.0515, 0.0254, 0.083, 0.0462, 0.0305...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7425, 0.5543, 0.8118, 0.6922, 0.9347, 0.784...</td>\n",
       "      <td>[0.2575, 0.4457, 0.1882, 0.3078, 0.0653, 0.215...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3515, 0.4283, 0.4719, 0.4041, 0.5073, 0.389...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9996, 1.0, 0.9995, 0.9998, 1.0, 0.9997, 1.0...</td>\n",
       "      <td>[0.0004, 0.0, 0.0005, 0.0002, 0.0, 0.0003, 0.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0817, 0.0725, 0.0298, 0.0786, 0.0383, 0.038...</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6033, 0.4387, 0.4982, 0.3942, 0.3572, 0.277...</td>\n",
       "      <td>[0.3967, 0.5613, 0.5018, 0.6058, 0.6428, 0.723...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0578, 0.0515, 0.0191, 0.0598, 0.0257, 0.024...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3637, 0.5442, 0.5168, 0.5282, 0.2951, 0.447...</td>\n",
       "      <td>[0.6363, 0.4558, 0.4832, 0.4718, 0.7049, 0.552...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0286, 0.0219, 0.0064, 0.0224, 0.0078, 0.012...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0243, 0.058, 0.0469, 0.0273, 0.0511, 0.0545...</td>\n",
       "      <td>[0.9757, 0.942, 0.9531, 0.9727, 0.9489, 0.9455...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.047, 0.043, 0.0182, 0.0432, 0.0249, 0.0281,...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2325, 0.5521, 0.0974, 0.1531, 0.3676, 0.135...</td>\n",
       "      <td>[0.7675, 0.4479, 0.9026, 0.8469, 0.6324, 0.864...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0278, 0.0223, 0.0062, 0.0233, 0.0075, 0.011...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0615, 0.0807, 0.1983, 0.1055, 0.0529, 0.174...</td>\n",
       "      <td>[0.9385, 0.9193, 0.8017, 0.8945, 0.9471, 0.825...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0406, 0.0325, 0.0107, 0.0347, 0.0115, 0.019...</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0906, 0.0691, 0.1578, 0.1544, 0.0857, 0.141...</td>\n",
       "      <td>[0.9094, 0.9309, 0.8422, 0.8456, 0.9143, 0.858...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0565, 0.065, 0.0239, 0.0759, 0.0226, 0.0306...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3708, 0.5145, 0.6166, 0.6486, 0.2923, 0.786...</td>\n",
       "      <td>[0.6292, 0.4855, 0.3834, 0.3514, 0.7077, 0.213...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0669, 0.0505, 0.0242, 0.0683, 0.0285, 0.034...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2819, 0.1757, 0.1846, 0.2036, 0.2043, 0.458...</td>\n",
       "      <td>[0.7181, 0.8243, 0.8154, 0.7964, 0.7957, 0.541...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0455, 0.0356, 0.0136, 0.0424, 0.0153, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1345, 0.0403, 0.0493, 0.0927, 0.0637, 0.092...</td>\n",
       "      <td>[0.8655, 0.9597, 0.9507, 0.9073, 0.9363, 0.907...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0884, 0.0821, 0.0279, 0.101, 0.056, 0.0417,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5438, 0.5665, 0.3822, 0.644, 0.5016, 0.4357...</td>\n",
       "      <td>[0.4562, 0.4335, 0.6178, 0.356, 0.4984, 0.5643...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0442, 0.0333, 0.0127, 0.0372, 0.0135, 0.022...</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0515, 0.0418, 0.0481, 0.0597, 0.0669, 0.162...</td>\n",
       "      <td>[0.9485, 0.9582, 0.9519, 0.9403, 0.9331, 0.837...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0284, 0.0218, 0.0061, 0.0243, 0.007, 0.0116...</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1015, 0.0715, 0.2168, 0.1569, 0.0614, 0.180...</td>\n",
       "      <td>[0.8985, 0.9285, 0.7832, 0.8431, 0.9386, 0.819...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0502, 0.0478, 0.0161, 0.0609, 0.0173, 0.021...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5496, 0.4821, 0.5772, 0.626, 0.3154, 0.719,...</td>\n",
       "      <td>[0.4504, 0.5179, 0.4228, 0.374, 0.6846, 0.281,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0707, 0.0617, 0.0222, 0.08, 0.0252, 0.0261,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.604, 0.4897, 0.7651, 0.6211, 0.3406, 0.9039...</td>\n",
       "      <td>[0.396, 0.5103, 0.2349, 0.3789, 0.6594, 0.0961...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0269, 0.0209, 0.0054, 0.0269, 0.0063, 0.01,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2547, 0.1586, 0.357, 0.2449, 0.1332, 0.2662...</td>\n",
       "      <td>[0.7453, 0.8414, 0.643, 0.7551, 0.8668, 0.7338...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1245, 0.114, 0.0705, 0.1221, 0.0627, 0.0998...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5334, 0.3778, 0.4447, 0.4146, 0.4319, 0.464...</td>\n",
       "      <td>[0.4666, 0.6222, 0.5553, 0.5854, 0.5681, 0.535...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0716, 0.0673, 0.0271, 0.0607, 0.0403, 0.040...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3211, 0.4225, 0.2672, 0.4098, 0.232, 0.3614...</td>\n",
       "      <td>[0.6789, 0.5775, 0.7328, 0.5902, 0.768, 0.6386...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0219, 0.0183, 0.0049, 0.0169, 0.0068, 0.009...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0677, 0.2372, 0.0457, 0.0618, 0.2096, 0.080...</td>\n",
       "      <td>[0.9323, 0.7628, 0.9543, 0.9382, 0.7904, 0.919...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0598, 0.0648, 0.0219, 0.0534, 0.0336, 0.034...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1876, 0.8191, 0.3741, 0.3662, 0.5133, 0.344...</td>\n",
       "      <td>[0.8124, 0.1809, 0.6259, 0.6338, 0.4867, 0.655...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.1015, 0.0884, 0.0413, 0.0967, 0.0682, 0.067...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3372, 0.4329, 0.6196, 0.2804, 0.1728, 0.398...</td>\n",
       "      <td>[0.6628, 0.5671, 0.3804, 0.7196, 0.8272, 0.601...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0642, 0.0629, 0.0306, 0.0725, 0.0456, 0.029...</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3656, 0.4189, 0.508, 0.3966, 0.5093, 0.3318...</td>\n",
       "      <td>[0.6344, 0.5811, 0.492, 0.6034, 0.4907, 0.6682...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0656, 0.0529, 0.0199, 0.053, 0.0258, 0.0332...               0   \n",
       "1   [0.0828, 0.0775, 0.0247, 0.0612, 0.0438, 0.037...               0   \n",
       "2   [0.0613, 0.0541, 0.0195, 0.0508, 0.0265, 0.033...               0   \n",
       "3   [0.0391, 0.0384, 0.0133, 0.0355, 0.0206, 0.019...               1   \n",
       "4   [0.04, 0.0324, 0.0101, 0.0319, 0.0132, 0.0194,...               1   \n",
       "5   [0.0656, 0.0544, 0.024, 0.0555, 0.0255, 0.0387...               2   \n",
       "6   [0.0947, 0.0801, 0.0257, 0.1327, 0.0552, 0.036...               3   \n",
       "7   [0.0821, 0.069, 0.032, 0.0955, 0.041, 0.0498, ...               3   \n",
       "8   [0.1191, 0.1055, 0.0526, 0.1269, 0.0662, 0.084...               3   \n",
       "9   [0.1106, 0.1327, 0.0916, 0.1254, 0.1663, 0.084...               4   \n",
       "10  [0.0525, 0.0669, 0.0177, 0.0524, 0.0377, 0.034...               4   \n",
       "11  [0.0442, 0.0464, 0.0185, 0.0502, 0.0285, 0.033...               5   \n",
       "12  [0.0923, 0.1005, 0.0494, 0.1289, 0.083, 0.0566...               6   \n",
       "13  [0.269, 0.2414, 0.2128, 0.2697, 0.1568, 0.2536...               7   \n",
       "14  [0.0409, 0.038, 0.0152, 0.0596, 0.0225, 0.0204...               8   \n",
       "15  [0.0565, 0.0511, 0.0211, 0.0481, 0.0306, 0.028...               8   \n",
       "16  [0.0577, 0.0461, 0.019, 0.0494, 0.0201, 0.0322...               9   \n",
       "17  [0.0677, 0.0616, 0.027, 0.0652, 0.0329, 0.0402...               9   \n",
       "18  [0.0517, 0.039, 0.0151, 0.0403, 0.0159, 0.0258...              10   \n",
       "19  [0.0502, 0.0409, 0.0168, 0.0399, 0.0167, 0.025...              10   \n",
       "20  [0.0717, 0.0604, 0.0244, 0.0637, 0.0304, 0.046...              11   \n",
       "21  [0.0319, 0.0265, 0.0078, 0.0298, 0.0093, 0.015...              11   \n",
       "22  [0.0426, 0.0362, 0.0122, 0.038, 0.0139, 0.021,...              12   \n",
       "23  [0.0648, 0.0523, 0.02, 0.0549, 0.0245, 0.0367,...              13   \n",
       "24  [0.05, 0.037, 0.0141, 0.0409, 0.0152, 0.0244, ...              13   \n",
       "25  [0.0663, 0.0587, 0.0254, 0.0664, 0.034, 0.0393...              14   \n",
       "26  [0.0436, 0.0344, 0.0119, 0.0372, 0.0135, 0.020...              14   \n",
       "27  [0.0395, 0.037, 0.0139, 0.0427, 0.0226, 0.0197...              15   \n",
       "28  [0.0964, 0.0863, 0.0566, 0.1238, 0.0561, 0.062...              15   \n",
       "29  [0.0508, 0.0515, 0.0254, 0.083, 0.0462, 0.0305...              15   \n",
       "30  [0.3515, 0.4283, 0.4719, 0.4041, 0.5073, 0.389...              16   \n",
       "31  [0.0817, 0.0725, 0.0298, 0.0786, 0.0383, 0.038...              17   \n",
       "32  [0.0578, 0.0515, 0.0191, 0.0598, 0.0257, 0.024...              17   \n",
       "33  [0.0286, 0.0219, 0.0064, 0.0224, 0.0078, 0.012...              18   \n",
       "34  [0.047, 0.043, 0.0182, 0.0432, 0.0249, 0.0281,...              19   \n",
       "35  [0.0278, 0.0223, 0.0062, 0.0233, 0.0075, 0.011...              20   \n",
       "36  [0.0406, 0.0325, 0.0107, 0.0347, 0.0115, 0.019...              21   \n",
       "37  [0.0565, 0.065, 0.0239, 0.0759, 0.0226, 0.0306...              21   \n",
       "38  [0.0669, 0.0505, 0.0242, 0.0683, 0.0285, 0.034...              22   \n",
       "39  [0.0455, 0.0356, 0.0136, 0.0424, 0.0153, 0.023...              22   \n",
       "40  [0.0884, 0.0821, 0.0279, 0.101, 0.056, 0.0417,...              22   \n",
       "41  [0.0442, 0.0333, 0.0127, 0.0372, 0.0135, 0.022...              22   \n",
       "42  [0.0284, 0.0218, 0.0061, 0.0243, 0.007, 0.0116...              23   \n",
       "43  [0.0502, 0.0478, 0.0161, 0.0609, 0.0173, 0.021...              23   \n",
       "44  [0.0707, 0.0617, 0.0222, 0.08, 0.0252, 0.0261,...              23   \n",
       "45  [0.0269, 0.0209, 0.0054, 0.0269, 0.0063, 0.01,...              23   \n",
       "46  [0.1245, 0.114, 0.0705, 0.1221, 0.0627, 0.0998...              24   \n",
       "47  [0.0716, 0.0673, 0.0271, 0.0607, 0.0403, 0.040...              24   \n",
       "48  [0.0219, 0.0183, 0.0049, 0.0169, 0.0068, 0.009...              25   \n",
       "49  [0.0598, 0.0648, 0.0219, 0.0534, 0.0336, 0.034...              25   \n",
       "50  [0.1015, 0.0884, 0.0413, 0.0967, 0.0682, 0.067...              26   \n",
       "51  [0.0642, 0.0629, 0.0306, 0.0725, 0.0456, 0.029...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                 10  [0.1391, 0.2635, 0.1157, 0.1491, 0.1047, 0.115...   \n",
       "1                 23  [0.4989, 0.4977, 0.2623, 0.3821, 0.2937, 0.234...   \n",
       "2                 10  [0.2556, 0.3271, 0.168, 0.1619, 0.1102, 0.1359...   \n",
       "3                 22  [0.3298, 0.3282, 0.2105, 0.348, 0.5289, 0.3373...   \n",
       "4                 10  [0.0701, 0.0854, 0.0686, 0.0707, 0.0812, 0.041...   \n",
       "5                 10  [0.07, 0.0679, 0.0758, 0.091, 0.0958, 0.1524, ...   \n",
       "6                  3  [0.7728, 0.5847, 0.7869, 0.6205, 0.4405, 0.655...   \n",
       "7                 22  [0.5822, 0.313, 0.56, 0.3299, 0.2833, 0.5512, ...   \n",
       "8                 10  [0.4153, 0.2774, 0.5318, 0.3042, 0.2298, 0.421...   \n",
       "9                 26  [0.778, 0.8756, 0.8532, 0.7682, 0.9112, 0.7033...   \n",
       "10                 1  [0.5211, 0.7599, 0.5161, 0.4424, 0.5825, 0.431...   \n",
       "11                10  [0.316, 0.5792, 0.3809, 0.3401, 0.6215, 0.4065...   \n",
       "12                 3  [0.6868, 0.8088, 0.8703, 0.8576, 0.7411, 0.795...   \n",
       "13                22  [0.7581, 0.6919, 0.6382, 0.5833, 0.7314, 0.886...   \n",
       "14                 3  [0.4523, 0.3949, 0.5687, 0.5996, 0.575, 0.4624...   \n",
       "15                10  [0.1926, 0.4007, 0.4323, 0.2881, 0.3507, 0.334...   \n",
       "16                10  [0.0867, 0.0718, 0.0628, 0.1342, 0.1491, 0.155...   \n",
       "17                10  [0.2856, 0.336, 0.2721, 0.4537, 0.4693, 0.3927...   \n",
       "18                10  [0.0891, 0.3119, 0.0806, 0.0989, 0.309, 0.109,...   \n",
       "19                10  [0.1169, 0.473, 0.0886, 0.1991, 0.3618, 0.1351...   \n",
       "20                10  [0.1883, 0.3422, 0.3029, 0.4335, 0.1448, 0.230...   \n",
       "21                10  [0.0982, 0.1786, 0.1299, 0.3303, 0.0701, 0.105...   \n",
       "22                10  [0.0847, 0.1799, 0.1442, 0.2465, 0.1122, 0.226...   \n",
       "23                10  [0.4482, 0.469, 0.3921, 0.3242, 0.3768, 0.4828...   \n",
       "24                10  [0.063, 0.0729, 0.0809, 0.0822, 0.0782, 0.0872...   \n",
       "25                10  [0.1427, 0.1549, 0.545, 0.3177, 0.2381, 0.3072...   \n",
       "26                10  [0.0457, 0.0424, 0.1971, 0.1202, 0.0643, 0.133...   \n",
       "27                22  [0.3542, 0.4032, 0.4148, 0.3141, 0.7408, 0.463...   \n",
       "28                15  [0.728, 0.6269, 0.6397, 0.5928, 0.9171, 0.7946...   \n",
       "29                15  [0.7425, 0.5543, 0.8118, 0.6922, 0.9347, 0.784...   \n",
       "30                16  [0.9996, 1.0, 0.9995, 0.9998, 1.0, 0.9997, 1.0...   \n",
       "31                22  [0.6033, 0.4387, 0.4982, 0.3942, 0.3572, 0.277...   \n",
       "32                23  [0.3637, 0.5442, 0.5168, 0.5282, 0.2951, 0.447...   \n",
       "33                10  [0.0243, 0.058, 0.0469, 0.0273, 0.0511, 0.0545...   \n",
       "34                10  [0.2325, 0.5521, 0.0974, 0.1531, 0.3676, 0.135...   \n",
       "35                10  [0.0615, 0.0807, 0.1983, 0.1055, 0.0529, 0.174...   \n",
       "36                10  [0.0906, 0.0691, 0.1578, 0.1544, 0.0857, 0.141...   \n",
       "37                23  [0.3708, 0.5145, 0.6166, 0.6486, 0.2923, 0.786...   \n",
       "38                22  [0.2819, 0.1757, 0.1846, 0.2036, 0.2043, 0.458...   \n",
       "39                22  [0.1345, 0.0403, 0.0493, 0.0927, 0.0637, 0.092...   \n",
       "40                22  [0.5438, 0.5665, 0.3822, 0.644, 0.5016, 0.4357...   \n",
       "41                10  [0.0515, 0.0418, 0.0481, 0.0597, 0.0669, 0.162...   \n",
       "42                10  [0.1015, 0.0715, 0.2168, 0.1569, 0.0614, 0.180...   \n",
       "43                23  [0.5496, 0.4821, 0.5772, 0.626, 0.3154, 0.719,...   \n",
       "44                23  [0.604, 0.4897, 0.7651, 0.6211, 0.3406, 0.9039...   \n",
       "45                23  [0.2547, 0.1586, 0.357, 0.2449, 0.1332, 0.2662...   \n",
       "46                22  [0.5334, 0.3778, 0.4447, 0.4146, 0.4319, 0.464...   \n",
       "47                 0  [0.3211, 0.4225, 0.2672, 0.4098, 0.232, 0.3614...   \n",
       "48                10  [0.0677, 0.2372, 0.0457, 0.0618, 0.2096, 0.080...   \n",
       "49                25  [0.1876, 0.8191, 0.3741, 0.3662, 0.5133, 0.344...   \n",
       "50                23  [0.3372, 0.4329, 0.6196, 0.2804, 0.1728, 0.398...   \n",
       "51                22  [0.3656, 0.4189, 0.508, 0.3966, 0.5093, 0.3318...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8609, 0.7365, 0.8843, 0.8509, 0.8953, 0.884...      0.25  0.160706  \n",
       "1   [0.5011, 0.5023, 0.7377, 0.6179, 0.7063, 0.765...       NaN       NaN  \n",
       "2   [0.7444, 0.6729, 0.832, 0.8381, 0.8898, 0.8641...       NaN       NaN  \n",
       "3   [0.6702, 0.6718, 0.7895, 0.652, 0.4711, 0.6627...       NaN       NaN  \n",
       "4   [0.9299, 0.9146, 0.9314, 0.9293, 0.9188, 0.958...       NaN       NaN  \n",
       "5   [0.93, 0.9321, 0.9242, 0.909, 0.9042, 0.8476, ...       NaN       NaN  \n",
       "6   [0.2272, 0.4153, 0.2131, 0.3795, 0.5595, 0.344...       NaN       NaN  \n",
       "7   [0.4178, 0.687, 0.44, 0.6701, 0.7167, 0.4488, ...       NaN       NaN  \n",
       "8   [0.5847, 0.7226, 0.4682, 0.6958, 0.7702, 0.578...       NaN       NaN  \n",
       "9   [0.222, 0.1244, 0.1468, 0.2318, 0.0888, 0.2967...       NaN       NaN  \n",
       "10  [0.4789, 0.2401, 0.4839, 0.5576, 0.4175, 0.569...       NaN       NaN  \n",
       "11  [0.684, 0.4208, 0.6191, 0.6599, 0.3785, 0.5935...       NaN       NaN  \n",
       "12  [0.3132, 0.1912, 0.1297, 0.1424, 0.2589, 0.204...       NaN       NaN  \n",
       "13  [0.2419, 0.3081, 0.3618, 0.4167, 0.2686, 0.113...       NaN       NaN  \n",
       "14  [0.5477, 0.6051, 0.4313, 0.4004, 0.425, 0.5376...       NaN       NaN  \n",
       "15  [0.8074, 0.5993, 0.5677, 0.7119, 0.6493, 0.666...       NaN       NaN  \n",
       "16  [0.9133, 0.9282, 0.9372, 0.8658, 0.8509, 0.844...       NaN       NaN  \n",
       "17  [0.7144, 0.664, 0.7279, 0.5463, 0.5307, 0.6073...       NaN       NaN  \n",
       "18  [0.9109, 0.6881, 0.9194, 0.9011, 0.691, 0.891,...       NaN       NaN  \n",
       "19  [0.8831, 0.527, 0.9114, 0.8009, 0.6382, 0.8649...       NaN       NaN  \n",
       "20  [0.8117, 0.6578, 0.6971, 0.5665, 0.8552, 0.769...       NaN       NaN  \n",
       "21  [0.9018, 0.8214, 0.8701, 0.6697, 0.9299, 0.894...       NaN       NaN  \n",
       "22  [0.9153, 0.8201, 0.8558, 0.7535, 0.8878, 0.774...       NaN       NaN  \n",
       "23  [0.5518, 0.531, 0.6079, 0.6758, 0.6232, 0.5172...       NaN       NaN  \n",
       "24  [0.937, 0.9271, 0.9191, 0.9178, 0.9218, 0.9128...       NaN       NaN  \n",
       "25  [0.8573, 0.8451, 0.455, 0.6823, 0.7619, 0.6928...       NaN       NaN  \n",
       "26  [0.9543, 0.9576, 0.8029, 0.8798, 0.9357, 0.866...       NaN       NaN  \n",
       "27  [0.6458, 0.5968, 0.5852, 0.6859, 0.2592, 0.536...       NaN       NaN  \n",
       "28  [0.272, 0.3731, 0.3603, 0.4072, 0.0829, 0.2054...       NaN       NaN  \n",
       "29  [0.2575, 0.4457, 0.1882, 0.3078, 0.0653, 0.215...       NaN       NaN  \n",
       "30  [0.0004, 0.0, 0.0005, 0.0002, 0.0, 0.0003, 0.0...       NaN       NaN  \n",
       "31  [0.3967, 0.5613, 0.5018, 0.6058, 0.6428, 0.723...       NaN       NaN  \n",
       "32  [0.6363, 0.4558, 0.4832, 0.4718, 0.7049, 0.552...       NaN       NaN  \n",
       "33  [0.9757, 0.942, 0.9531, 0.9727, 0.9489, 0.9455...       NaN       NaN  \n",
       "34  [0.7675, 0.4479, 0.9026, 0.8469, 0.6324, 0.864...       NaN       NaN  \n",
       "35  [0.9385, 0.9193, 0.8017, 0.8945, 0.9471, 0.825...       NaN       NaN  \n",
       "36  [0.9094, 0.9309, 0.8422, 0.8456, 0.9143, 0.858...       NaN       NaN  \n",
       "37  [0.6292, 0.4855, 0.3834, 0.3514, 0.7077, 0.213...       NaN       NaN  \n",
       "38  [0.7181, 0.8243, 0.8154, 0.7964, 0.7957, 0.541...       NaN       NaN  \n",
       "39  [0.8655, 0.9597, 0.9507, 0.9073, 0.9363, 0.907...       NaN       NaN  \n",
       "40  [0.4562, 0.4335, 0.6178, 0.356, 0.4984, 0.5643...       NaN       NaN  \n",
       "41  [0.9485, 0.9582, 0.9519, 0.9403, 0.9331, 0.837...       NaN       NaN  \n",
       "42  [0.8985, 0.9285, 0.7832, 0.8431, 0.9386, 0.819...       NaN       NaN  \n",
       "43  [0.4504, 0.5179, 0.4228, 0.374, 0.6846, 0.281,...       NaN       NaN  \n",
       "44  [0.396, 0.5103, 0.2349, 0.3789, 0.6594, 0.0961...       NaN       NaN  \n",
       "45  [0.7453, 0.8414, 0.643, 0.7551, 0.8668, 0.7338...       NaN       NaN  \n",
       "46  [0.4666, 0.6222, 0.5553, 0.5854, 0.5681, 0.535...       NaN       NaN  \n",
       "47  [0.6789, 0.5775, 0.7328, 0.5902, 0.768, 0.6386...       NaN       NaN  \n",
       "48  [0.9323, 0.7628, 0.9543, 0.9382, 0.7904, 0.919...       NaN       NaN  \n",
       "49  [0.8124, 0.1809, 0.6259, 0.6338, 0.4867, 0.655...       NaN       NaN  \n",
       "50  [0.6628, 0.5671, 0.3804, 0.7196, 0.8272, 0.601...       NaN       NaN  \n",
       "51  [0.6344, 0.5811, 0.492, 0.6034, 0.4907, 0.6682...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 : Training: loss:  0.15546694\n",
      "502 : Training: loss:  0.16176984\n",
      "503 : Training: loss:  0.15493919\n",
      "504 : Training: loss:  0.15342529\n",
      "505 : Training: loss:  0.15061016\n",
      "506 : Training: loss:  0.16899496\n",
      "507 : Training: loss:  0.18374743\n",
      "508 : Training: loss:  0.15137154\n",
      "509 : Training: loss:  0.15125188\n",
      "510 : Training: loss:  0.14266658\n",
      "511 : Training: loss:  0.14085352\n",
      "512 : Training: loss:  0.14159305\n",
      "513 : Training: loss:  0.13996512\n",
      "514 : Training: loss:  0.14799105\n",
      "515 : Training: loss:  0.13689028\n",
      "516 : Training: loss:  0.15456964\n",
      "517 : Training: loss:  0.15114519\n",
      "518 : Training: loss:  0.14623298\n",
      "519 : Training: loss:  0.15058401\n",
      "520 : Training: loss:  0.15291755\n",
      "Validation: Loss:  0.15991363  Accuracy:  0.25\n",
      "521 : Training: loss:  0.16361214\n",
      "522 : Training: loss:  0.15362109\n",
      "523 : Training: loss:  0.14475732\n",
      "524 : Training: loss:  0.15255998\n",
      "525 : Training: loss:  0.16314012\n",
      "526 : Training: loss:  0.15015401\n",
      "527 : Training: loss:  0.15721458\n",
      "528 : Training: loss:  0.16284622\n",
      "529 : Training: loss:  0.16811413\n",
      "530 : Training: loss:  0.15635757\n",
      "531 : Training: loss:  0.15742427\n",
      "532 : Training: loss:  0.13765045\n",
      "533 : Training: loss:  0.14255655\n",
      "534 : Training: loss:  0.16118658\n",
      "535 : Training: loss:  0.13233505\n",
      "536 : Training: loss:  0.15800169\n",
      "537 : Training: loss:  0.16267549\n",
      "538 : Training: loss:  0.16359818\n",
      "539 : Training: loss:  0.14210321\n",
      "540 : Training: loss:  0.14563741\n",
      "Validation: Loss:  0.15920916  Accuracy:  0.25\n",
      "541 : Training: loss:  0.15520449\n",
      "542 : Training: loss:  0.15142946\n",
      "543 : Training: loss:  0.150028\n",
      "544 : Training: loss:  0.14750788\n",
      "545 : Training: loss:  0.17754059\n",
      "546 : Training: loss:  0.13833489\n",
      "547 : Training: loss:  0.15626372\n",
      "548 : Training: loss:  0.14667301\n",
      "549 : Training: loss:  0.14416169\n",
      "550 : Training: loss:  0.13603155\n",
      "551 : Training: loss:  0.14620806\n",
      "552 : Training: loss:  0.15150182\n",
      "553 : Training: loss:  0.14696011\n",
      "554 : Training: loss:  0.13526879\n",
      "555 : Training: loss:  0.14074723\n",
      "556 : Training: loss:  0.14729813\n",
      "557 : Training: loss:  0.15670203\n",
      "558 : Training: loss:  0.14511114\n",
      "559 : Training: loss:  0.15750322\n",
      "560 : Training: loss:  0.15091458\n",
      "Validation: Loss:  0.15797722  Accuracy:  0.26923078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0658, 0.0479, 0.0182, 0.0548, 0.0245, 0.037...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1412, 0.2681, 0.1154, 0.153, 0.1054, 0.1124...</td>\n",
       "      <td>[0.8588, 0.7319, 0.8846, 0.847, 0.8946, 0.8876...</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.157977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0802, 0.0713, 0.0222, 0.0613, 0.043, 0.0412...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5017, 0.5024, 0.262, 0.3878, 0.2967, 0.2281...</td>\n",
       "      <td>[0.4983, 0.4976, 0.738, 0.6122, 0.7033, 0.7719...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0606, 0.0488, 0.0177, 0.0524, 0.0252, 0.038...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2606, 0.3336, 0.1673, 0.1682, 0.1105, 0.131...</td>\n",
       "      <td>[0.7394, 0.6664, 0.8327, 0.8318, 0.8895, 0.868...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0367, 0.034, 0.0117, 0.0354, 0.0199, 0.0219...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3341, 0.33, 0.2095, 0.3546, 0.5344, 0.333, ...</td>\n",
       "      <td>[0.6659, 0.67, 0.7905, 0.6454, 0.4656, 0.667, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0404, 0.029, 0.0092, 0.0332, 0.0124, 0.0229...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0708, 0.0875, 0.0676, 0.073, 0.0821, 0.0405...</td>\n",
       "      <td>[0.9292, 0.9125, 0.9324, 0.927, 0.9179, 0.9595...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.066, 0.049, 0.0223, 0.0575, 0.0239, 0.0442,...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0704, 0.066, 0.0775, 0.0916, 0.0971, 0.1561...</td>\n",
       "      <td>[0.9296, 0.934, 0.9225, 0.9084, 0.9029, 0.8439...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0784, 0.0667, 0.0203, 0.1326, 0.0474, 0.034...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7785, 0.5873, 0.798, 0.6341, 0.4251, 0.6658...</td>\n",
       "      <td>[0.2215, 0.4127, 0.202, 0.3659, 0.5749, 0.3342...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0759, 0.0601, 0.0274, 0.0975, 0.0367, 0.050...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5872, 0.3073, 0.5737, 0.337, 0.2762, 0.5621...</td>\n",
       "      <td>[0.4128, 0.6927, 0.4263, 0.663, 0.7238, 0.4379...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1134, 0.0942, 0.0465, 0.1296, 0.0606, 0.086...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4227, 0.2799, 0.5498, 0.3099, 0.2273, 0.429...</td>\n",
       "      <td>[0.5773, 0.7201, 0.4502, 0.6901, 0.7727, 0.570...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0903, 0.1163, 0.0763, 0.1152, 0.1655, 0.083...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7775, 0.8779, 0.8515, 0.7665, 0.9128, 0.695...</td>\n",
       "      <td>[0.2225, 0.1221, 0.1485, 0.2335, 0.0872, 0.304...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0469, 0.0585, 0.0148, 0.0496, 0.0361, 0.038...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.514, 0.7643, 0.501, 0.4379, 0.5818, 0.4252,...</td>\n",
       "      <td>[0.486, 0.2357, 0.499, 0.5621, 0.4182, 0.5748,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0406, 0.0406, 0.016, 0.0494, 0.0273, 0.038,...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3122, 0.5845, 0.3738, 0.3377, 0.6234, 0.399...</td>\n",
       "      <td>[0.6878, 0.4155, 0.6262, 0.6623, 0.3766, 0.600...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0763, 0.0863, 0.0404, 0.1175, 0.0747, 0.053...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6855, 0.8005, 0.8691, 0.86, 0.7395, 0.7858,...</td>\n",
       "      <td>[0.3145, 0.1995, 0.1309, 0.14, 0.2605, 0.2142,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2523, 0.2209, 0.1911, 0.2616, 0.1403, 0.246...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7455, 0.6543, 0.6213, 0.5665, 0.7293, 0.888...</td>\n",
       "      <td>[0.2545, 0.3457, 0.3787, 0.4335, 0.2707, 0.111...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0357, 0.0317, 0.0123, 0.0581, 0.0205, 0.021...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4555, 0.4026, 0.5667, 0.6079, 0.5703, 0.448...</td>\n",
       "      <td>[0.5445, 0.5974, 0.4333, 0.3921, 0.4297, 0.551...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.052, 0.0441, 0.0178, 0.0468, 0.0281, 0.0306...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1919, 0.4131, 0.4285, 0.2915, 0.3534, 0.313...</td>\n",
       "      <td>[0.8081, 0.5869, 0.5715, 0.7085, 0.6466, 0.686...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0578, 0.041, 0.0175, 0.0509, 0.0186, 0.0369...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0867, 0.0714, 0.0631, 0.1347, 0.1484, 0.157...</td>\n",
       "      <td>[0.9133, 0.9286, 0.9369, 0.8653, 0.8516, 0.842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0633, 0.0536, 0.0234, 0.0648, 0.0303, 0.042...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2865, 0.3389, 0.2764, 0.4554, 0.4667, 0.393...</td>\n",
       "      <td>[0.7135, 0.6611, 0.7236, 0.5446, 0.5333, 0.606...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0529, 0.0348, 0.0139, 0.0419, 0.0147, 0.030...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0887, 0.3126, 0.0811, 0.0981, 0.3122, 0.110...</td>\n",
       "      <td>[0.9113, 0.6874, 0.9189, 0.9019, 0.6878, 0.889...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0507, 0.0361, 0.0152, 0.041, 0.0153, 0.0297...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1163, 0.4781, 0.0897, 0.1999, 0.3684, 0.133...</td>\n",
       "      <td>[0.8837, 0.5219, 0.9103, 0.8001, 0.6316, 0.866...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0695, 0.0537, 0.0217, 0.0647, 0.0281, 0.050...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1932, 0.3416, 0.3089, 0.4468, 0.1439, 0.225...</td>\n",
       "      <td>[0.8068, 0.6584, 0.6911, 0.5532, 0.8561, 0.775...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0319, 0.023, 0.007, 0.0313, 0.0084, 0.0176,...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1012, 0.1737, 0.133, 0.3393, 0.0679, 0.1075...</td>\n",
       "      <td>[0.8988, 0.8263, 0.867, 0.6607, 0.9321, 0.8925...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0424, 0.0319, 0.011, 0.0395, 0.0127, 0.0242...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0871, 0.1775, 0.1446, 0.2513, 0.1104, 0.224...</td>\n",
       "      <td>[0.9129, 0.8225, 0.8554, 0.7487, 0.8896, 0.775...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0626, 0.0464, 0.0178, 0.0562, 0.0229, 0.041...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.4495, 0.4659, 0.4025, 0.3222, 0.3824, 0.482...</td>\n",
       "      <td>[0.5505, 0.5341, 0.5975, 0.6778, 0.6176, 0.517...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0511, 0.0328, 0.013, 0.043, 0.0141, 0.0285,...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.064, 0.0734, 0.0848, 0.0818, 0.0773, 0.0902...</td>\n",
       "      <td>[0.936, 0.9266, 0.9152, 0.9182, 0.9227, 0.9098...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0619, 0.051, 0.0221, 0.0661, 0.0311, 0.0415...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.146, 0.1579, 0.5534, 0.3246, 0.2419, 0.3072...</td>\n",
       "      <td>[0.854, 0.8421, 0.4466, 0.6754, 0.7581, 0.6928...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0431, 0.0299, 0.0105, 0.0381, 0.0122, 0.023...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0469, 0.0409, 0.2016, 0.1225, 0.064, 0.1346...</td>\n",
       "      <td>[0.9531, 0.9591, 0.7984, 0.8775, 0.936, 0.8654...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.035, 0.0315, 0.0117, 0.0414, 0.0212, 0.0211...</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3605, 0.4001, 0.4166, 0.316, 0.7456, 0.4719...</td>\n",
       "      <td>[0.6395, 0.5999, 0.5834, 0.684, 0.2544, 0.5281...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0806, 0.0727, 0.0459, 0.1159, 0.0494, 0.059...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7233, 0.6051, 0.6368, 0.5834, 0.9182, 0.802...</td>\n",
       "      <td>[0.2767, 0.3949, 0.3632, 0.4166, 0.0818, 0.197...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0401, 0.0423, 0.0196, 0.0758, 0.0425, 0.028...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7426, 0.5435, 0.8095, 0.6887, 0.9359, 0.789...</td>\n",
       "      <td>[0.2574, 0.4565, 0.1905, 0.3113, 0.0641, 0.210...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3383, 0.4274, 0.4658, 0.3963, 0.5222, 0.395...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9996, 1.0, 0.9993, 0.9998, 1.0, 0.9996, 1.0...</td>\n",
       "      <td>[0.0004, 0.0, 0.0007, 0.0002, 0.0, 0.0004, 0.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0745, 0.0637, 0.0259, 0.0777, 0.0353, 0.040...</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6107, 0.4253, 0.5112, 0.3984, 0.3517, 0.283...</td>\n",
       "      <td>[0.3893, 0.5747, 0.4888, 0.6016, 0.6483, 0.716...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0521, 0.0438, 0.0159, 0.0594, 0.0229, 0.025...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3721, 0.5395, 0.5357, 0.5424, 0.2888, 0.453...</td>\n",
       "      <td>[0.6279, 0.4605, 0.4643, 0.4576, 0.7112, 0.546...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0291, 0.0191, 0.0058, 0.0236, 0.0072, 0.015...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0253, 0.0596, 0.0473, 0.0285, 0.0516, 0.054...</td>\n",
       "      <td>[0.9747, 0.9404, 0.9527, 0.9715, 0.9484, 0.945...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0461, 0.0384, 0.0167, 0.0447, 0.0243, 0.032...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2405, 0.5686, 0.0973, 0.1582, 0.3798, 0.130...</td>\n",
       "      <td>[0.7595, 0.4314, 0.9027, 0.8418, 0.6202, 0.869...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0276, 0.019, 0.0054, 0.0241, 0.0067, 0.0138...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.063, 0.0794, 0.2028, 0.1076, 0.0524, 0.176,...</td>\n",
       "      <td>[0.937, 0.9206, 0.7972, 0.8924, 0.9476, 0.824,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0408, 0.0284, 0.0096, 0.0361, 0.0104, 0.022...</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0894, 0.0669, 0.1574, 0.1535, 0.0803, 0.141...</td>\n",
       "      <td>[0.9106, 0.9331, 0.8426, 0.8465, 0.9197, 0.858...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0508, 0.0546, 0.0195, 0.075, 0.0189, 0.0305...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3598, 0.483, 0.6181, 0.6431, 0.267, 0.7926,...</td>\n",
       "      <td>[0.6402, 0.517, 0.3819, 0.3569, 0.733, 0.2074,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0634, 0.0431, 0.0212, 0.069, 0.0253, 0.0364...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2888, 0.1646, 0.1796, 0.203, 0.1994, 0.478,...</td>\n",
       "      <td>[0.7112, 0.8354, 0.8204, 0.797, 0.8006, 0.522,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0452, 0.0309, 0.0122, 0.044, 0.0139, 0.0266...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1379, 0.0388, 0.0489, 0.0946, 0.0614, 0.096...</td>\n",
       "      <td>[0.8621, 0.9612, 0.9511, 0.9054, 0.9386, 0.903...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0776, 0.0714, 0.0234, 0.0981, 0.0515, 0.042...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5507, 0.5535, 0.3779, 0.648, 0.493, 0.4409,...</td>\n",
       "      <td>[0.4493, 0.4465, 0.6221, 0.352, 0.507, 0.5591,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.045, 0.0292, 0.0116, 0.0391, 0.0123, 0.0261...</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0523, 0.0402, 0.0477, 0.0596, 0.066, 0.1703...</td>\n",
       "      <td>[0.9477, 0.9598, 0.9523, 0.9404, 0.934, 0.8297...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0279, 0.0185, 0.0053, 0.0252, 0.0061, 0.013...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1007, 0.0685, 0.2225, 0.1578, 0.0571, 0.180...</td>\n",
       "      <td>[0.8993, 0.9315, 0.7775, 0.8422, 0.9429, 0.819...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0438, 0.039, 0.0126, 0.0588, 0.014, 0.0207,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5379, 0.4473, 0.5812, 0.618, 0.2859, 0.7205...</td>\n",
       "      <td>[0.4621, 0.5527, 0.4188, 0.382, 0.7141, 0.2795...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0601, 0.05, 0.017, 0.0754, 0.0199, 0.0241, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5879, 0.4456, 0.7693, 0.609, 0.308, 0.9061,...</td>\n",
       "      <td>[0.4121, 0.5544, 0.2307, 0.391, 0.692, 0.0939,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0254, 0.0174, 0.0045, 0.0273, 0.0054, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2512, 0.1468, 0.3616, 0.2424, 0.1208, 0.268...</td>\n",
       "      <td>[0.7488, 0.8532, 0.6384, 0.7576, 0.8792, 0.731...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1165, 0.1016, 0.0629, 0.12, 0.0568, 0.1024,...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5368, 0.3665, 0.456, 0.4133, 0.4323, 0.4638...</td>\n",
       "      <td>[0.4632, 0.6335, 0.544, 0.5867, 0.5677, 0.5362...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0673, 0.0605, 0.0239, 0.0607, 0.0384, 0.043...</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3272, 0.4238, 0.2771, 0.4184, 0.2311, 0.354...</td>\n",
       "      <td>[0.6728, 0.5762, 0.7229, 0.5816, 0.7689, 0.645...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0214, 0.0156, 0.0042, 0.0171, 0.0062, 0.010...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0687, 0.2412, 0.0437, 0.0624, 0.2173, 0.077...</td>\n",
       "      <td>[0.9313, 0.7588, 0.9563, 0.9376, 0.7827, 0.922...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0565, 0.0583, 0.0194, 0.0529, 0.0326, 0.038...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.186, 0.8229, 0.3694, 0.37, 0.5171, 0.3335, ...</td>\n",
       "      <td>[0.814, 0.1771, 0.6306, 0.63, 0.4829, 0.6665, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0911, 0.0765, 0.0346, 0.0922, 0.0608, 0.066...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3417, 0.4326, 0.6207, 0.2869, 0.1743, 0.383...</td>\n",
       "      <td>[0.6583, 0.5674, 0.3793, 0.7131, 0.8257, 0.616...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0562, 0.0539, 0.0257, 0.0699, 0.0424, 0.03,...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3706, 0.4273, 0.515, 0.4013, 0.513, 0.3233,...</td>\n",
       "      <td>[0.6294, 0.5727, 0.485, 0.5987, 0.487, 0.6767,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0658, 0.0479, 0.0182, 0.0548, 0.0245, 0.037...               0   \n",
       "1   [0.0802, 0.0713, 0.0222, 0.0613, 0.043, 0.0412...               0   \n",
       "2   [0.0606, 0.0488, 0.0177, 0.0524, 0.0252, 0.038...               0   \n",
       "3   [0.0367, 0.034, 0.0117, 0.0354, 0.0199, 0.0219...               1   \n",
       "4   [0.0404, 0.029, 0.0092, 0.0332, 0.0124, 0.0229...               1   \n",
       "5   [0.066, 0.049, 0.0223, 0.0575, 0.0239, 0.0442,...               2   \n",
       "6   [0.0784, 0.0667, 0.0203, 0.1326, 0.0474, 0.034...               3   \n",
       "7   [0.0759, 0.0601, 0.0274, 0.0975, 0.0367, 0.050...               3   \n",
       "8   [0.1134, 0.0942, 0.0465, 0.1296, 0.0606, 0.086...               3   \n",
       "9   [0.0903, 0.1163, 0.0763, 0.1152, 0.1655, 0.083...               4   \n",
       "10  [0.0469, 0.0585, 0.0148, 0.0496, 0.0361, 0.038...               4   \n",
       "11  [0.0406, 0.0406, 0.016, 0.0494, 0.0273, 0.038,...               5   \n",
       "12  [0.0763, 0.0863, 0.0404, 0.1175, 0.0747, 0.053...               6   \n",
       "13  [0.2523, 0.2209, 0.1911, 0.2616, 0.1403, 0.246...               7   \n",
       "14  [0.0357, 0.0317, 0.0123, 0.0581, 0.0205, 0.021...               8   \n",
       "15  [0.052, 0.0441, 0.0178, 0.0468, 0.0281, 0.0306...               8   \n",
       "16  [0.0578, 0.041, 0.0175, 0.0509, 0.0186, 0.0369...               9   \n",
       "17  [0.0633, 0.0536, 0.0234, 0.0648, 0.0303, 0.042...               9   \n",
       "18  [0.0529, 0.0348, 0.0139, 0.0419, 0.0147, 0.030...              10   \n",
       "19  [0.0507, 0.0361, 0.0152, 0.041, 0.0153, 0.0297...              10   \n",
       "20  [0.0695, 0.0537, 0.0217, 0.0647, 0.0281, 0.050...              11   \n",
       "21  [0.0319, 0.023, 0.007, 0.0313, 0.0084, 0.0176,...              11   \n",
       "22  [0.0424, 0.0319, 0.011, 0.0395, 0.0127, 0.0242...              12   \n",
       "23  [0.0626, 0.0464, 0.0178, 0.0562, 0.0229, 0.041...              13   \n",
       "24  [0.0511, 0.0328, 0.013, 0.043, 0.0141, 0.0285,...              13   \n",
       "25  [0.0619, 0.051, 0.0221, 0.0661, 0.0311, 0.0415...              14   \n",
       "26  [0.0431, 0.0299, 0.0105, 0.0381, 0.0122, 0.023...              14   \n",
       "27  [0.035, 0.0315, 0.0117, 0.0414, 0.0212, 0.0211...              15   \n",
       "28  [0.0806, 0.0727, 0.0459, 0.1159, 0.0494, 0.059...              15   \n",
       "29  [0.0401, 0.0423, 0.0196, 0.0758, 0.0425, 0.028...              15   \n",
       "30  [0.3383, 0.4274, 0.4658, 0.3963, 0.5222, 0.395...              16   \n",
       "31  [0.0745, 0.0637, 0.0259, 0.0777, 0.0353, 0.040...              17   \n",
       "32  [0.0521, 0.0438, 0.0159, 0.0594, 0.0229, 0.025...              17   \n",
       "33  [0.0291, 0.0191, 0.0058, 0.0236, 0.0072, 0.015...              18   \n",
       "34  [0.0461, 0.0384, 0.0167, 0.0447, 0.0243, 0.032...              19   \n",
       "35  [0.0276, 0.019, 0.0054, 0.0241, 0.0067, 0.0138...              20   \n",
       "36  [0.0408, 0.0284, 0.0096, 0.0361, 0.0104, 0.022...              21   \n",
       "37  [0.0508, 0.0546, 0.0195, 0.075, 0.0189, 0.0305...              21   \n",
       "38  [0.0634, 0.0431, 0.0212, 0.069, 0.0253, 0.0364...              22   \n",
       "39  [0.0452, 0.0309, 0.0122, 0.044, 0.0139, 0.0266...              22   \n",
       "40  [0.0776, 0.0714, 0.0234, 0.0981, 0.0515, 0.042...              22   \n",
       "41  [0.045, 0.0292, 0.0116, 0.0391, 0.0123, 0.0261...              22   \n",
       "42  [0.0279, 0.0185, 0.0053, 0.0252, 0.0061, 0.013...              23   \n",
       "43  [0.0438, 0.039, 0.0126, 0.0588, 0.014, 0.0207,...              23   \n",
       "44  [0.0601, 0.05, 0.017, 0.0754, 0.0199, 0.0241, ...              23   \n",
       "45  [0.0254, 0.0174, 0.0045, 0.0273, 0.0054, 0.011...              23   \n",
       "46  [0.1165, 0.1016, 0.0629, 0.12, 0.0568, 0.1024,...              24   \n",
       "47  [0.0673, 0.0605, 0.0239, 0.0607, 0.0384, 0.043...              24   \n",
       "48  [0.0214, 0.0156, 0.0042, 0.0171, 0.0062, 0.010...              25   \n",
       "49  [0.0565, 0.0583, 0.0194, 0.0529, 0.0326, 0.038...              25   \n",
       "50  [0.0911, 0.0765, 0.0346, 0.0922, 0.0608, 0.066...              26   \n",
       "51  [0.0562, 0.0539, 0.0257, 0.0699, 0.0424, 0.03,...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                 10  [0.1412, 0.2681, 0.1154, 0.153, 0.1054, 0.1124...   \n",
       "1                 23  [0.5017, 0.5024, 0.262, 0.3878, 0.2967, 0.2281...   \n",
       "2                 10  [0.2606, 0.3336, 0.1673, 0.1682, 0.1105, 0.131...   \n",
       "3                 22  [0.3341, 0.33, 0.2095, 0.3546, 0.5344, 0.333, ...   \n",
       "4                 10  [0.0708, 0.0875, 0.0676, 0.073, 0.0821, 0.0405...   \n",
       "5                 10  [0.0704, 0.066, 0.0775, 0.0916, 0.0971, 0.1561...   \n",
       "6                  3  [0.7785, 0.5873, 0.798, 0.6341, 0.4251, 0.6658...   \n",
       "7                 22  [0.5872, 0.3073, 0.5737, 0.337, 0.2762, 0.5621...   \n",
       "8                  3  [0.4227, 0.2799, 0.5498, 0.3099, 0.2273, 0.429...   \n",
       "9                 26  [0.7775, 0.8779, 0.8515, 0.7665, 0.9128, 0.695...   \n",
       "10                 8  [0.514, 0.7643, 0.501, 0.4379, 0.5818, 0.4252,...   \n",
       "11                 3  [0.3122, 0.5845, 0.3738, 0.3377, 0.6234, 0.399...   \n",
       "12                17  [0.6855, 0.8005, 0.8691, 0.86, 0.7395, 0.7858,...   \n",
       "13                23  [0.7455, 0.6543, 0.6213, 0.5665, 0.7293, 0.888...   \n",
       "14                 3  [0.4555, 0.4026, 0.5667, 0.6079, 0.5703, 0.448...   \n",
       "15                10  [0.1919, 0.4131, 0.4285, 0.2915, 0.3534, 0.313...   \n",
       "16                10  [0.0867, 0.0714, 0.0631, 0.1347, 0.1484, 0.157...   \n",
       "17                23  [0.2865, 0.3389, 0.2764, 0.4554, 0.4667, 0.393...   \n",
       "18                10  [0.0887, 0.3126, 0.0811, 0.0981, 0.3122, 0.110...   \n",
       "19                10  [0.1163, 0.4781, 0.0897, 0.1999, 0.3684, 0.133...   \n",
       "20                10  [0.1932, 0.3416, 0.3089, 0.4468, 0.1439, 0.225...   \n",
       "21                10  [0.1012, 0.1737, 0.133, 0.3393, 0.0679, 0.1075...   \n",
       "22                10  [0.0871, 0.1775, 0.1446, 0.2513, 0.1104, 0.224...   \n",
       "23                10  [0.4495, 0.4659, 0.4025, 0.3222, 0.3824, 0.482...   \n",
       "24                10  [0.064, 0.0734, 0.0848, 0.0818, 0.0773, 0.0902...   \n",
       "25                10  [0.146, 0.1579, 0.5534, 0.3246, 0.2419, 0.3072...   \n",
       "26                10  [0.0469, 0.0409, 0.2016, 0.1225, 0.064, 0.1346...   \n",
       "27                22  [0.3605, 0.4001, 0.4166, 0.316, 0.7456, 0.4719...   \n",
       "28                15  [0.7233, 0.6051, 0.6368, 0.5834, 0.9182, 0.802...   \n",
       "29                15  [0.7426, 0.5435, 0.8095, 0.6887, 0.9359, 0.789...   \n",
       "30                16  [0.9996, 1.0, 0.9993, 0.9998, 1.0, 0.9996, 1.0...   \n",
       "31                22  [0.6107, 0.4253, 0.5112, 0.3984, 0.3517, 0.283...   \n",
       "32                23  [0.3721, 0.5395, 0.5357, 0.5424, 0.2888, 0.453...   \n",
       "33                10  [0.0253, 0.0596, 0.0473, 0.0285, 0.0516, 0.054...   \n",
       "34                10  [0.2405, 0.5686, 0.0973, 0.1582, 0.3798, 0.130...   \n",
       "35                10  [0.063, 0.0794, 0.2028, 0.1076, 0.0524, 0.176,...   \n",
       "36                10  [0.0894, 0.0669, 0.1574, 0.1535, 0.0803, 0.141...   \n",
       "37                23  [0.3598, 0.483, 0.6181, 0.6431, 0.267, 0.7926,...   \n",
       "38                22  [0.2888, 0.1646, 0.1796, 0.203, 0.1994, 0.478,...   \n",
       "39                22  [0.1379, 0.0388, 0.0489, 0.0946, 0.0614, 0.096...   \n",
       "40                23  [0.5507, 0.5535, 0.3779, 0.648, 0.493, 0.4409,...   \n",
       "41                10  [0.0523, 0.0402, 0.0477, 0.0596, 0.066, 0.1703...   \n",
       "42                23  [0.1007, 0.0685, 0.2225, 0.1578, 0.0571, 0.180...   \n",
       "43                23  [0.5379, 0.4473, 0.5812, 0.618, 0.2859, 0.7205...   \n",
       "44                23  [0.5879, 0.4456, 0.7693, 0.609, 0.308, 0.9061,...   \n",
       "45                23  [0.2512, 0.1468, 0.3616, 0.2424, 0.1208, 0.268...   \n",
       "46                22  [0.5368, 0.3665, 0.456, 0.4133, 0.4323, 0.4638...   \n",
       "47                23  [0.3272, 0.4238, 0.2771, 0.4184, 0.2311, 0.354...   \n",
       "48                10  [0.0687, 0.2412, 0.0437, 0.0624, 0.2173, 0.077...   \n",
       "49                25  [0.186, 0.8229, 0.3694, 0.37, 0.5171, 0.3335, ...   \n",
       "50                23  [0.3417, 0.4326, 0.6207, 0.2869, 0.1743, 0.383...   \n",
       "51                 3  [0.3706, 0.4273, 0.515, 0.4013, 0.513, 0.3233,...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8588, 0.7319, 0.8846, 0.847, 0.8946, 0.8876...  0.269231  0.157977  \n",
       "1   [0.4983, 0.4976, 0.738, 0.6122, 0.7033, 0.7719...       NaN       NaN  \n",
       "2   [0.7394, 0.6664, 0.8327, 0.8318, 0.8895, 0.868...       NaN       NaN  \n",
       "3   [0.6659, 0.67, 0.7905, 0.6454, 0.4656, 0.667, ...       NaN       NaN  \n",
       "4   [0.9292, 0.9125, 0.9324, 0.927, 0.9179, 0.9595...       NaN       NaN  \n",
       "5   [0.9296, 0.934, 0.9225, 0.9084, 0.9029, 0.8439...       NaN       NaN  \n",
       "6   [0.2215, 0.4127, 0.202, 0.3659, 0.5749, 0.3342...       NaN       NaN  \n",
       "7   [0.4128, 0.6927, 0.4263, 0.663, 0.7238, 0.4379...       NaN       NaN  \n",
       "8   [0.5773, 0.7201, 0.4502, 0.6901, 0.7727, 0.570...       NaN       NaN  \n",
       "9   [0.2225, 0.1221, 0.1485, 0.2335, 0.0872, 0.304...       NaN       NaN  \n",
       "10  [0.486, 0.2357, 0.499, 0.5621, 0.4182, 0.5748,...       NaN       NaN  \n",
       "11  [0.6878, 0.4155, 0.6262, 0.6623, 0.3766, 0.600...       NaN       NaN  \n",
       "12  [0.3145, 0.1995, 0.1309, 0.14, 0.2605, 0.2142,...       NaN       NaN  \n",
       "13  [0.2545, 0.3457, 0.3787, 0.4335, 0.2707, 0.111...       NaN       NaN  \n",
       "14  [0.5445, 0.5974, 0.4333, 0.3921, 0.4297, 0.551...       NaN       NaN  \n",
       "15  [0.8081, 0.5869, 0.5715, 0.7085, 0.6466, 0.686...       NaN       NaN  \n",
       "16  [0.9133, 0.9286, 0.9369, 0.8653, 0.8516, 0.842...       NaN       NaN  \n",
       "17  [0.7135, 0.6611, 0.7236, 0.5446, 0.5333, 0.606...       NaN       NaN  \n",
       "18  [0.9113, 0.6874, 0.9189, 0.9019, 0.6878, 0.889...       NaN       NaN  \n",
       "19  [0.8837, 0.5219, 0.9103, 0.8001, 0.6316, 0.866...       NaN       NaN  \n",
       "20  [0.8068, 0.6584, 0.6911, 0.5532, 0.8561, 0.775...       NaN       NaN  \n",
       "21  [0.8988, 0.8263, 0.867, 0.6607, 0.9321, 0.8925...       NaN       NaN  \n",
       "22  [0.9129, 0.8225, 0.8554, 0.7487, 0.8896, 0.775...       NaN       NaN  \n",
       "23  [0.5505, 0.5341, 0.5975, 0.6778, 0.6176, 0.517...       NaN       NaN  \n",
       "24  [0.936, 0.9266, 0.9152, 0.9182, 0.9227, 0.9098...       NaN       NaN  \n",
       "25  [0.854, 0.8421, 0.4466, 0.6754, 0.7581, 0.6928...       NaN       NaN  \n",
       "26  [0.9531, 0.9591, 0.7984, 0.8775, 0.936, 0.8654...       NaN       NaN  \n",
       "27  [0.6395, 0.5999, 0.5834, 0.684, 0.2544, 0.5281...       NaN       NaN  \n",
       "28  [0.2767, 0.3949, 0.3632, 0.4166, 0.0818, 0.197...       NaN       NaN  \n",
       "29  [0.2574, 0.4565, 0.1905, 0.3113, 0.0641, 0.210...       NaN       NaN  \n",
       "30  [0.0004, 0.0, 0.0007, 0.0002, 0.0, 0.0004, 0.0...       NaN       NaN  \n",
       "31  [0.3893, 0.5747, 0.4888, 0.6016, 0.6483, 0.716...       NaN       NaN  \n",
       "32  [0.6279, 0.4605, 0.4643, 0.4576, 0.7112, 0.546...       NaN       NaN  \n",
       "33  [0.9747, 0.9404, 0.9527, 0.9715, 0.9484, 0.945...       NaN       NaN  \n",
       "34  [0.7595, 0.4314, 0.9027, 0.8418, 0.6202, 0.869...       NaN       NaN  \n",
       "35  [0.937, 0.9206, 0.7972, 0.8924, 0.9476, 0.824,...       NaN       NaN  \n",
       "36  [0.9106, 0.9331, 0.8426, 0.8465, 0.9197, 0.858...       NaN       NaN  \n",
       "37  [0.6402, 0.517, 0.3819, 0.3569, 0.733, 0.2074,...       NaN       NaN  \n",
       "38  [0.7112, 0.8354, 0.8204, 0.797, 0.8006, 0.522,...       NaN       NaN  \n",
       "39  [0.8621, 0.9612, 0.9511, 0.9054, 0.9386, 0.903...       NaN       NaN  \n",
       "40  [0.4493, 0.4465, 0.6221, 0.352, 0.507, 0.5591,...       NaN       NaN  \n",
       "41  [0.9477, 0.9598, 0.9523, 0.9404, 0.934, 0.8297...       NaN       NaN  \n",
       "42  [0.8993, 0.9315, 0.7775, 0.8422, 0.9429, 0.819...       NaN       NaN  \n",
       "43  [0.4621, 0.5527, 0.4188, 0.382, 0.7141, 0.2795...       NaN       NaN  \n",
       "44  [0.4121, 0.5544, 0.2307, 0.391, 0.692, 0.0939,...       NaN       NaN  \n",
       "45  [0.7488, 0.8532, 0.6384, 0.7576, 0.8792, 0.731...       NaN       NaN  \n",
       "46  [0.4632, 0.6335, 0.544, 0.5867, 0.5677, 0.5362...       NaN       NaN  \n",
       "47  [0.6728, 0.5762, 0.7229, 0.5816, 0.7689, 0.645...       NaN       NaN  \n",
       "48  [0.9313, 0.7588, 0.9563, 0.9376, 0.7827, 0.922...       NaN       NaN  \n",
       "49  [0.814, 0.1771, 0.6306, 0.63, 0.4829, 0.6665, ...       NaN       NaN  \n",
       "50  [0.6583, 0.5674, 0.3793, 0.7131, 0.8257, 0.616...       NaN       NaN  \n",
       "51  [0.6294, 0.5727, 0.485, 0.5987, 0.487, 0.6767,...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 : Training: loss:  0.15751143\n",
      "562 : Training: loss:  0.16751292\n",
      "563 : Training: loss:  0.13640057\n",
      "564 : Training: loss:  0.18584287\n",
      "565 : Training: loss:  0.1532032\n",
      "566 : Training: loss:  0.1541816\n",
      "567 : Training: loss:  0.15179203\n",
      "568 : Training: loss:  0.14684394\n",
      "569 : Training: loss:  0.15694435\n",
      "570 : Training: loss:  0.1511197\n",
      "571 : Training: loss:  0.14390701\n",
      "572 : Training: loss:  0.1532633\n",
      "573 : Training: loss:  0.13551547\n",
      "574 : Training: loss:  0.14400668\n",
      "575 : Training: loss:  0.12678792\n",
      "576 : Training: loss:  0.15052506\n",
      "577 : Training: loss:  0.15348595\n",
      "578 : Training: loss:  0.14459795\n",
      "579 : Training: loss:  0.131054\n",
      "580 : Training: loss:  0.1436846\n",
      "Validation: Loss:  0.15697184  Accuracy:  0.28846154\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0648, 0.0493, 0.0186, 0.0554, 0.0243, 0.036...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1429, 0.2727, 0.1151, 0.1552, 0.1062, 0.111...</td>\n",
       "      <td>[0.8571, 0.7273, 0.8849, 0.8448, 0.8938, 0.888...</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.156972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0786, 0.0734, 0.0223, 0.0613, 0.0427, 0.039...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5051, 0.5079, 0.2624, 0.3916, 0.2985, 0.228...</td>\n",
       "      <td>[0.4949, 0.4921, 0.7376, 0.6084, 0.7015, 0.772...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0593, 0.0501, 0.0179, 0.0529, 0.0249, 0.036...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2641, 0.3377, 0.1671, 0.1717, 0.1109, 0.131...</td>\n",
       "      <td>[0.7359, 0.6623, 0.8329, 0.8283, 0.8891, 0.868...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0353, 0.0349, 0.0118, 0.0351, 0.0197, 0.020...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3378, 0.3341, 0.2082, 0.3587, 0.5355, 0.334...</td>\n",
       "      <td>[0.6622, 0.6659, 0.7918, 0.6413, 0.4645, 0.665...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0397, 0.0302, 0.0095, 0.0338, 0.0122, 0.022...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0717, 0.0893, 0.0671, 0.0746, 0.0825, 0.040...</td>\n",
       "      <td>[0.9283, 0.9107, 0.9329, 0.9254, 0.9175, 0.959...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0646, 0.0499, 0.0227, 0.0578, 0.0234, 0.042...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0707, 0.0653, 0.0779, 0.0917, 0.098, 0.1573...</td>\n",
       "      <td>[0.9293, 0.9347, 0.9221, 0.9083, 0.902, 0.8427...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0724, 0.0653, 0.0194, 0.1304, 0.0441, 0.031...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7816, 0.5876, 0.8035, 0.6403, 0.4216, 0.670...</td>\n",
       "      <td>[0.2184, 0.4124, 0.1965, 0.3597, 0.5784, 0.329...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.073, 0.0601, 0.0271, 0.0977, 0.0352, 0.0483...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5897, 0.3055, 0.579, 0.3398, 0.2738, 0.5672...</td>\n",
       "      <td>[0.4103, 0.6945, 0.421, 0.6602, 0.7262, 0.4328...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.11, 0.0942, 0.0461, 0.1299, 0.0586, 0.0831,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4264, 0.2801, 0.558, 0.3123, 0.227, 0.4332,...</td>\n",
       "      <td>[0.5736, 0.7199, 0.442, 0.6877, 0.773, 0.5668,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0838, 0.115, 0.0742, 0.1106, 0.1621, 0.0801...</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7808, 0.8807, 0.8503, 0.7679, 0.914, 0.6955...</td>\n",
       "      <td>[0.2192, 0.1193, 0.1497, 0.2321, 0.086, 0.3045...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0445, 0.0599, 0.0146, 0.0484, 0.0352, 0.036...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5149, 0.7668, 0.4941, 0.4409, 0.5807, 0.426...</td>\n",
       "      <td>[0.4851, 0.2332, 0.5059, 0.5591, 0.4193, 0.574...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0388, 0.0416, 0.0161, 0.0489, 0.0269, 0.036...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3144, 0.5889, 0.3694, 0.3397, 0.624, 0.4001...</td>\n",
       "      <td>[0.6856, 0.4111, 0.6306, 0.6603, 0.376, 0.5999...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0696, 0.0832, 0.0384, 0.111, 0.0696, 0.0492...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6842, 0.797, 0.8677, 0.861, 0.7374, 0.7823,...</td>\n",
       "      <td>[0.3158, 0.203, 0.1323, 0.139, 0.2626, 0.2177,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2444, 0.2171, 0.1859, 0.2559, 0.1338, 0.237...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7387, 0.6388, 0.6113, 0.5611, 0.7242, 0.888...</td>\n",
       "      <td>[0.2613, 0.3612, 0.3887, 0.4389, 0.2758, 0.111...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0334, 0.0317, 0.0122, 0.0572, 0.0198, 0.019...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4608, 0.4082, 0.5652, 0.6133, 0.5697, 0.448...</td>\n",
       "      <td>[0.5392, 0.5918, 0.4348, 0.3867, 0.4303, 0.551...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0499, 0.0448, 0.0178, 0.0463, 0.0276, 0.029...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1945, 0.4223, 0.4262, 0.2947, 0.3552, 0.309...</td>\n",
       "      <td>[0.8055, 0.5777, 0.5738, 0.7053, 0.6448, 0.690...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0564, 0.0417, 0.0177, 0.0512, 0.0181, 0.035...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0865, 0.0709, 0.0637, 0.1356, 0.1483, 0.157...</td>\n",
       "      <td>[0.9135, 0.9291, 0.9363, 0.8644, 0.8517, 0.842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0604, 0.0536, 0.0232, 0.064, 0.0291, 0.0404...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2876, 0.3381, 0.2791, 0.4582, 0.466, 0.3946...</td>\n",
       "      <td>[0.7124, 0.6619, 0.7209, 0.5418, 0.534, 0.6054...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0522, 0.0358, 0.0143, 0.0425, 0.0145, 0.029...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0893, 0.3146, 0.0808, 0.0982, 0.313, 0.1112...</td>\n",
       "      <td>[0.9107, 0.6854, 0.9192, 0.9018, 0.687, 0.8888...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0496, 0.037, 0.0156, 0.0413, 0.015, 0.0286,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1172, 0.4829, 0.0892, 0.2006, 0.3704, 0.133...</td>\n",
       "      <td>[0.8828, 0.5171, 0.9108, 0.7994, 0.6296, 0.866...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0674, 0.0545, 0.0218, 0.065, 0.0274, 0.0483...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1966, 0.3438, 0.312, 0.4529, 0.1451, 0.2249...</td>\n",
       "      <td>[0.8034, 0.6562, 0.688, 0.5471, 0.8549, 0.7751...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0311, 0.0237, 0.0072, 0.0319, 0.0082, 0.016...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1032, 0.1744, 0.1341, 0.3444, 0.0676, 0.109...</td>\n",
       "      <td>[0.8968, 0.8256, 0.8659, 0.6556, 0.9324, 0.891...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0415, 0.0328, 0.0112, 0.0401, 0.0125, 0.023...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0883, 0.1776, 0.1445, 0.2542, 0.1096, 0.225...</td>\n",
       "      <td>[0.9117, 0.8224, 0.8555, 0.7458, 0.8904, 0.774...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.061, 0.0474, 0.018, 0.0564, 0.0224, 0.0396,...</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4518, 0.4656, 0.405, 0.3224, 0.3819, 0.4848...</td>\n",
       "      <td>[0.5482, 0.5344, 0.595, 0.6776, 0.6181, 0.5152...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0502, 0.0337, 0.0133, 0.0436, 0.0138, 0.027...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0646, 0.0734, 0.0861, 0.0818, 0.0773, 0.091...</td>\n",
       "      <td>[0.9354, 0.9266, 0.9139, 0.9182, 0.9227, 0.909...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.059, 0.0512, 0.022, 0.0655, 0.0301, 0.0392,...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1481, 0.1605, 0.5578, 0.3278, 0.2457, 0.308...</td>\n",
       "      <td>[0.8519, 0.8395, 0.4422, 0.6722, 0.7543, 0.691...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0421, 0.0306, 0.0107, 0.0384, 0.0119, 0.022...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0476, 0.0408, 0.2032, 0.1234, 0.0641, 0.135...</td>\n",
       "      <td>[0.9524, 0.9592, 0.7968, 0.8766, 0.9359, 0.864...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0329, 0.0315, 0.0116, 0.0404, 0.0206, 0.019...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.365, 0.4009, 0.4175, 0.3185, 0.7489, 0.4777...</td>\n",
       "      <td>[0.635, 0.5991, 0.5825, 0.6815, 0.2511, 0.5223...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0748, 0.0709, 0.0442, 0.1115, 0.0467, 0.055...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7216, 0.5979, 0.6337, 0.5828, 0.9184, 0.805...</td>\n",
       "      <td>[0.2784, 0.4021, 0.3663, 0.4172, 0.0816, 0.194...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0363, 0.0412, 0.0187, 0.0719, 0.0403, 0.026...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7449, 0.5417, 0.8081, 0.6894, 0.9368, 0.792...</td>\n",
       "      <td>[0.2551, 0.4583, 0.1919, 0.3106, 0.0632, 0.207...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3345, 0.4296, 0.4671, 0.3931, 0.5249, 0.396...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9996, 1.0, 0.9993, 0.9998, 1.0, 0.9996, 1.0...</td>\n",
       "      <td>[0.0004, 0.0, 0.0007, 0.0002, 0.0, 0.0004, 0.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0711, 0.0637, 0.0258, 0.0768, 0.0341, 0.038...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.615, 0.4229, 0.5171, 0.4012, 0.3525, 0.2866...</td>\n",
       "      <td>[0.385, 0.5771, 0.4829, 0.5988, 0.6475, 0.7134...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0491, 0.0434, 0.0156, 0.0584, 0.0217, 0.023...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3764, 0.5374, 0.5433, 0.5479, 0.2882, 0.454...</td>\n",
       "      <td>[0.6236, 0.4626, 0.4567, 0.4521, 0.7118, 0.545...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0286, 0.0198, 0.006, 0.0241, 0.007, 0.0145,...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.026, 0.0607, 0.0474, 0.0291, 0.0522, 0.0551...</td>\n",
       "      <td>[0.974, 0.9393, 0.9526, 0.9709, 0.9478, 0.9449...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0452, 0.0397, 0.0173, 0.0453, 0.0245, 0.031...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.247, 0.5802, 0.0967, 0.1614, 0.3866, 0.1308...</td>\n",
       "      <td>[0.753, 0.4198, 0.9033, 0.8386, 0.6134, 0.8692...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0268, 0.0195, 0.0055, 0.0244, 0.0065, 0.013...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0637, 0.0798, 0.2045, 0.1088, 0.0525, 0.177...</td>\n",
       "      <td>[0.9363, 0.9202, 0.7955, 0.8912, 0.9475, 0.822...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0397, 0.029, 0.0097, 0.0364, 0.0101, 0.0215...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.089, 0.0658, 0.1575, 0.1543, 0.0779, 0.1421...</td>\n",
       "      <td>[0.911, 0.9342, 0.8425, 0.8457, 0.9221, 0.8579...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0472, 0.0533, 0.0185, 0.0727, 0.0173, 0.027...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.354, 0.4677, 0.6161, 0.6421, 0.2523, 0.7936...</td>\n",
       "      <td>[0.646, 0.5323, 0.3839, 0.3579, 0.7477, 0.2064...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0611, 0.0431, 0.0212, 0.0688, 0.0243, 0.034...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.29, 0.1621, 0.1776, 0.2029, 0.1981, 0.4834,...</td>\n",
       "      <td>[0.71, 0.8379, 0.8224, 0.7971, 0.8019, 0.5166,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0441, 0.0316, 0.0124, 0.0445, 0.0135, 0.025...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1398, 0.0386, 0.0488, 0.0956, 0.0609, 0.098...</td>\n",
       "      <td>[0.8602, 0.9614, 0.9512, 0.9044, 0.9391, 0.902...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0733, 0.0712, 0.0229, 0.0962, 0.0496, 0.039...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.555, 0.553, 0.3763, 0.6515, 0.4925, 0.4438,...</td>\n",
       "      <td>[0.445, 0.447, 0.6237, 0.3485, 0.5075, 0.5562,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0442, 0.0299, 0.0119, 0.0398, 0.012, 0.025,...</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0523, 0.0397, 0.0478, 0.0595, 0.0656, 0.173...</td>\n",
       "      <td>[0.9477, 0.9603, 0.9522, 0.9405, 0.9344, 0.826...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0268, 0.0188, 0.0053, 0.0253, 0.0058, 0.012...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1008, 0.0673, 0.2239, 0.1586, 0.0554, 0.180...</td>\n",
       "      <td>[0.8992, 0.9327, 0.7761, 0.8414, 0.9446, 0.819...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0405, 0.0383, 0.012, 0.0568, 0.0127, 0.0187...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5341, 0.4309, 0.5775, 0.6162, 0.2695, 0.720...</td>\n",
       "      <td>[0.4659, 0.5691, 0.4225, 0.3838, 0.7305, 0.279...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0549, 0.0482, 0.0159, 0.0718, 0.0178, 0.021...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5815, 0.4248, 0.7671, 0.6048, 0.2895, 0.906...</td>\n",
       "      <td>[0.4185, 0.5752, 0.2329, 0.3952, 0.7105, 0.093...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0241, 0.0176, 0.0045, 0.0272, 0.0051, 0.010...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2509, 0.1422, 0.362, 0.2428, 0.1153, 0.2696...</td>\n",
       "      <td>[0.7491, 0.8578, 0.638, 0.7572, 0.8847, 0.7304...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1128, 0.1014, 0.0626, 0.1188, 0.055, 0.0988...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5379, 0.363, 0.46, 0.4128, 0.4319, 0.4634, ...</td>\n",
       "      <td>[0.4621, 0.637, 0.54, 0.5872, 0.5681, 0.5366, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0651, 0.0619, 0.0241, 0.0605, 0.0378, 0.041...</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3318, 0.4272, 0.2805, 0.4227, 0.2324, 0.354...</td>\n",
       "      <td>[0.6682, 0.5728, 0.7195, 0.5773, 0.7676, 0.645...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0208, 0.0161, 0.0043, 0.0171, 0.006, 0.0103...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0699, 0.2456, 0.0426, 0.0633, 0.2195, 0.077...</td>\n",
       "      <td>[0.9301, 0.7544, 0.9574, 0.9367, 0.7805, 0.922...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.055, 0.0602, 0.0198, 0.0528, 0.0325, 0.038,...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1875, 0.8261, 0.3664, 0.3746, 0.5182, 0.332...</td>\n",
       "      <td>[0.8125, 0.1739, 0.6336, 0.6254, 0.4818, 0.667...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0855, 0.0754, 0.0335, 0.0897, 0.0581, 0.062...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3456, 0.4372, 0.6204, 0.2904, 0.1764, 0.380...</td>\n",
       "      <td>[0.6544, 0.5628, 0.3796, 0.7096, 0.8236, 0.619...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0525, 0.0536, 0.0253, 0.0682, 0.0411, 0.028...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3766, 0.4346, 0.5162, 0.4055, 0.5162, 0.322...</td>\n",
       "      <td>[0.6234, 0.5654, 0.4838, 0.5945, 0.4838, 0.677...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0648, 0.0493, 0.0186, 0.0554, 0.0243, 0.036...               0   \n",
       "1   [0.0786, 0.0734, 0.0223, 0.0613, 0.0427, 0.039...               0   \n",
       "2   [0.0593, 0.0501, 0.0179, 0.0529, 0.0249, 0.036...               0   \n",
       "3   [0.0353, 0.0349, 0.0118, 0.0351, 0.0197, 0.020...               1   \n",
       "4   [0.0397, 0.0302, 0.0095, 0.0338, 0.0122, 0.022...               1   \n",
       "5   [0.0646, 0.0499, 0.0227, 0.0578, 0.0234, 0.042...               2   \n",
       "6   [0.0724, 0.0653, 0.0194, 0.1304, 0.0441, 0.031...               3   \n",
       "7   [0.073, 0.0601, 0.0271, 0.0977, 0.0352, 0.0483...               3   \n",
       "8   [0.11, 0.0942, 0.0461, 0.1299, 0.0586, 0.0831,...               3   \n",
       "9   [0.0838, 0.115, 0.0742, 0.1106, 0.1621, 0.0801...               4   \n",
       "10  [0.0445, 0.0599, 0.0146, 0.0484, 0.0352, 0.036...               4   \n",
       "11  [0.0388, 0.0416, 0.0161, 0.0489, 0.0269, 0.036...               5   \n",
       "12  [0.0696, 0.0832, 0.0384, 0.111, 0.0696, 0.0492...               6   \n",
       "13  [0.2444, 0.2171, 0.1859, 0.2559, 0.1338, 0.237...               7   \n",
       "14  [0.0334, 0.0317, 0.0122, 0.0572, 0.0198, 0.019...               8   \n",
       "15  [0.0499, 0.0448, 0.0178, 0.0463, 0.0276, 0.029...               8   \n",
       "16  [0.0564, 0.0417, 0.0177, 0.0512, 0.0181, 0.035...               9   \n",
       "17  [0.0604, 0.0536, 0.0232, 0.064, 0.0291, 0.0404...               9   \n",
       "18  [0.0522, 0.0358, 0.0143, 0.0425, 0.0145, 0.029...              10   \n",
       "19  [0.0496, 0.037, 0.0156, 0.0413, 0.015, 0.0286,...              10   \n",
       "20  [0.0674, 0.0545, 0.0218, 0.065, 0.0274, 0.0483...              11   \n",
       "21  [0.0311, 0.0237, 0.0072, 0.0319, 0.0082, 0.016...              11   \n",
       "22  [0.0415, 0.0328, 0.0112, 0.0401, 0.0125, 0.023...              12   \n",
       "23  [0.061, 0.0474, 0.018, 0.0564, 0.0224, 0.0396,...              13   \n",
       "24  [0.0502, 0.0337, 0.0133, 0.0436, 0.0138, 0.027...              13   \n",
       "25  [0.059, 0.0512, 0.022, 0.0655, 0.0301, 0.0392,...              14   \n",
       "26  [0.0421, 0.0306, 0.0107, 0.0384, 0.0119, 0.022...              14   \n",
       "27  [0.0329, 0.0315, 0.0116, 0.0404, 0.0206, 0.019...              15   \n",
       "28  [0.0748, 0.0709, 0.0442, 0.1115, 0.0467, 0.055...              15   \n",
       "29  [0.0363, 0.0412, 0.0187, 0.0719, 0.0403, 0.026...              15   \n",
       "30  [0.3345, 0.4296, 0.4671, 0.3931, 0.5249, 0.396...              16   \n",
       "31  [0.0711, 0.0637, 0.0258, 0.0768, 0.0341, 0.038...              17   \n",
       "32  [0.0491, 0.0434, 0.0156, 0.0584, 0.0217, 0.023...              17   \n",
       "33  [0.0286, 0.0198, 0.006, 0.0241, 0.007, 0.0145,...              18   \n",
       "34  [0.0452, 0.0397, 0.0173, 0.0453, 0.0245, 0.031...              19   \n",
       "35  [0.0268, 0.0195, 0.0055, 0.0244, 0.0065, 0.013...              20   \n",
       "36  [0.0397, 0.029, 0.0097, 0.0364, 0.0101, 0.0215...              21   \n",
       "37  [0.0472, 0.0533, 0.0185, 0.0727, 0.0173, 0.027...              21   \n",
       "38  [0.0611, 0.0431, 0.0212, 0.0688, 0.0243, 0.034...              22   \n",
       "39  [0.0441, 0.0316, 0.0124, 0.0445, 0.0135, 0.025...              22   \n",
       "40  [0.0733, 0.0712, 0.0229, 0.0962, 0.0496, 0.039...              22   \n",
       "41  [0.0442, 0.0299, 0.0119, 0.0398, 0.012, 0.025,...              22   \n",
       "42  [0.0268, 0.0188, 0.0053, 0.0253, 0.0058, 0.012...              23   \n",
       "43  [0.0405, 0.0383, 0.012, 0.0568, 0.0127, 0.0187...              23   \n",
       "44  [0.0549, 0.0482, 0.0159, 0.0718, 0.0178, 0.021...              23   \n",
       "45  [0.0241, 0.0176, 0.0045, 0.0272, 0.0051, 0.010...              23   \n",
       "46  [0.1128, 0.1014, 0.0626, 0.1188, 0.055, 0.0988...              24   \n",
       "47  [0.0651, 0.0619, 0.0241, 0.0605, 0.0378, 0.041...              24   \n",
       "48  [0.0208, 0.0161, 0.0043, 0.0171, 0.006, 0.0103...              25   \n",
       "49  [0.055, 0.0602, 0.0198, 0.0528, 0.0325, 0.038,...              25   \n",
       "50  [0.0855, 0.0754, 0.0335, 0.0897, 0.0581, 0.062...              26   \n",
       "51  [0.0525, 0.0536, 0.0253, 0.0682, 0.0411, 0.028...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                 10  [0.1429, 0.2727, 0.1151, 0.1552, 0.1062, 0.111...   \n",
       "1                 23  [0.5051, 0.5079, 0.2624, 0.3916, 0.2985, 0.228...   \n",
       "2                 23  [0.2641, 0.3377, 0.1671, 0.1717, 0.1109, 0.131...   \n",
       "3                 22  [0.3378, 0.3341, 0.2082, 0.3587, 0.5355, 0.334...   \n",
       "4                 10  [0.0717, 0.0893, 0.0671, 0.0746, 0.0825, 0.040...   \n",
       "5                 10  [0.0707, 0.0653, 0.0779, 0.0917, 0.098, 0.1573...   \n",
       "6                  3  [0.7816, 0.5876, 0.8035, 0.6403, 0.4216, 0.670...   \n",
       "7                 23  [0.5897, 0.3055, 0.579, 0.3398, 0.2738, 0.5672...   \n",
       "8                  3  [0.4264, 0.2801, 0.558, 0.3123, 0.227, 0.4332,...   \n",
       "9                 26  [0.7808, 0.8807, 0.8503, 0.7679, 0.914, 0.6955...   \n",
       "10                 8  [0.5149, 0.7668, 0.4941, 0.4409, 0.5807, 0.426...   \n",
       "11                 3  [0.3144, 0.5889, 0.3694, 0.3397, 0.624, 0.4001...   \n",
       "12                17  [0.6842, 0.797, 0.8677, 0.861, 0.7374, 0.7823,...   \n",
       "13                23  [0.7387, 0.6388, 0.6113, 0.5611, 0.7242, 0.888...   \n",
       "14                 3  [0.4608, 0.4082, 0.5652, 0.6133, 0.5697, 0.448...   \n",
       "15                10  [0.1945, 0.4223, 0.4262, 0.2947, 0.3552, 0.309...   \n",
       "16                10  [0.0865, 0.0709, 0.0637, 0.1356, 0.1483, 0.157...   \n",
       "17                23  [0.2876, 0.3381, 0.2791, 0.4582, 0.466, 0.3946...   \n",
       "18                10  [0.0893, 0.3146, 0.0808, 0.0982, 0.313, 0.1112...   \n",
       "19                10  [0.1172, 0.4829, 0.0892, 0.2006, 0.3704, 0.133...   \n",
       "20                10  [0.1966, 0.3438, 0.312, 0.4529, 0.1451, 0.2249...   \n",
       "21                22  [0.1032, 0.1744, 0.1341, 0.3444, 0.0676, 0.109...   \n",
       "22                10  [0.0883, 0.1776, 0.1445, 0.2542, 0.1096, 0.225...   \n",
       "23                23  [0.4518, 0.4656, 0.405, 0.3224, 0.3819, 0.4848...   \n",
       "24                10  [0.0646, 0.0734, 0.0861, 0.0818, 0.0773, 0.091...   \n",
       "25                10  [0.1481, 0.1605, 0.5578, 0.3278, 0.2457, 0.308...   \n",
       "26                10  [0.0476, 0.0408, 0.2032, 0.1234, 0.0641, 0.135...   \n",
       "27                15  [0.365, 0.4009, 0.4175, 0.3185, 0.7489, 0.4777...   \n",
       "28                15  [0.7216, 0.5979, 0.6337, 0.5828, 0.9184, 0.805...   \n",
       "29                15  [0.7449, 0.5417, 0.8081, 0.6894, 0.9368, 0.792...   \n",
       "30                16  [0.9996, 1.0, 0.9993, 0.9998, 1.0, 0.9996, 1.0...   \n",
       "31                23  [0.615, 0.4229, 0.5171, 0.4012, 0.3525, 0.2866...   \n",
       "32                23  [0.3764, 0.5374, 0.5433, 0.5479, 0.2882, 0.454...   \n",
       "33                10  [0.026, 0.0607, 0.0474, 0.0291, 0.0522, 0.0551...   \n",
       "34                10  [0.247, 0.5802, 0.0967, 0.1614, 0.3866, 0.1308...   \n",
       "35                10  [0.0637, 0.0798, 0.2045, 0.1088, 0.0525, 0.177...   \n",
       "36                23  [0.089, 0.0658, 0.1575, 0.1543, 0.0779, 0.1421...   \n",
       "37                23  [0.354, 0.4677, 0.6161, 0.6421, 0.2523, 0.7936...   \n",
       "38                22  [0.29, 0.1621, 0.1776, 0.2029, 0.1981, 0.4834,...   \n",
       "39                22  [0.1398, 0.0386, 0.0488, 0.0956, 0.0609, 0.098...   \n",
       "40                23  [0.555, 0.553, 0.3763, 0.6515, 0.4925, 0.4438,...   \n",
       "41                10  [0.0523, 0.0397, 0.0478, 0.0595, 0.0656, 0.173...   \n",
       "42                23  [0.1008, 0.0673, 0.2239, 0.1586, 0.0554, 0.180...   \n",
       "43                23  [0.5341, 0.4309, 0.5775, 0.6162, 0.2695, 0.720...   \n",
       "44                23  [0.5815, 0.4248, 0.7671, 0.6048, 0.2895, 0.906...   \n",
       "45                23  [0.2509, 0.1422, 0.362, 0.2428, 0.1153, 0.2696...   \n",
       "46                22  [0.5379, 0.363, 0.46, 0.4128, 0.4319, 0.4634, ...   \n",
       "47                23  [0.3318, 0.4272, 0.2805, 0.4227, 0.2324, 0.354...   \n",
       "48                10  [0.0699, 0.2456, 0.0426, 0.0633, 0.2195, 0.077...   \n",
       "49                25  [0.1875, 0.8261, 0.3664, 0.3746, 0.5182, 0.332...   \n",
       "50                23  [0.3456, 0.4372, 0.6204, 0.2904, 0.1764, 0.380...   \n",
       "51                 3  [0.3766, 0.4346, 0.5162, 0.4055, 0.5162, 0.322...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8571, 0.7273, 0.8849, 0.8448, 0.8938, 0.888...  0.288462  0.156972  \n",
       "1   [0.4949, 0.4921, 0.7376, 0.6084, 0.7015, 0.772...       NaN       NaN  \n",
       "2   [0.7359, 0.6623, 0.8329, 0.8283, 0.8891, 0.868...       NaN       NaN  \n",
       "3   [0.6622, 0.6659, 0.7918, 0.6413, 0.4645, 0.665...       NaN       NaN  \n",
       "4   [0.9283, 0.9107, 0.9329, 0.9254, 0.9175, 0.959...       NaN       NaN  \n",
       "5   [0.9293, 0.9347, 0.9221, 0.9083, 0.902, 0.8427...       NaN       NaN  \n",
       "6   [0.2184, 0.4124, 0.1965, 0.3597, 0.5784, 0.329...       NaN       NaN  \n",
       "7   [0.4103, 0.6945, 0.421, 0.6602, 0.7262, 0.4328...       NaN       NaN  \n",
       "8   [0.5736, 0.7199, 0.442, 0.6877, 0.773, 0.5668,...       NaN       NaN  \n",
       "9   [0.2192, 0.1193, 0.1497, 0.2321, 0.086, 0.3045...       NaN       NaN  \n",
       "10  [0.4851, 0.2332, 0.5059, 0.5591, 0.4193, 0.574...       NaN       NaN  \n",
       "11  [0.6856, 0.4111, 0.6306, 0.6603, 0.376, 0.5999...       NaN       NaN  \n",
       "12  [0.3158, 0.203, 0.1323, 0.139, 0.2626, 0.2177,...       NaN       NaN  \n",
       "13  [0.2613, 0.3612, 0.3887, 0.4389, 0.2758, 0.111...       NaN       NaN  \n",
       "14  [0.5392, 0.5918, 0.4348, 0.3867, 0.4303, 0.551...       NaN       NaN  \n",
       "15  [0.8055, 0.5777, 0.5738, 0.7053, 0.6448, 0.690...       NaN       NaN  \n",
       "16  [0.9135, 0.9291, 0.9363, 0.8644, 0.8517, 0.842...       NaN       NaN  \n",
       "17  [0.7124, 0.6619, 0.7209, 0.5418, 0.534, 0.6054...       NaN       NaN  \n",
       "18  [0.9107, 0.6854, 0.9192, 0.9018, 0.687, 0.8888...       NaN       NaN  \n",
       "19  [0.8828, 0.5171, 0.9108, 0.7994, 0.6296, 0.866...       NaN       NaN  \n",
       "20  [0.8034, 0.6562, 0.688, 0.5471, 0.8549, 0.7751...       NaN       NaN  \n",
       "21  [0.8968, 0.8256, 0.8659, 0.6556, 0.9324, 0.891...       NaN       NaN  \n",
       "22  [0.9117, 0.8224, 0.8555, 0.7458, 0.8904, 0.774...       NaN       NaN  \n",
       "23  [0.5482, 0.5344, 0.595, 0.6776, 0.6181, 0.5152...       NaN       NaN  \n",
       "24  [0.9354, 0.9266, 0.9139, 0.9182, 0.9227, 0.909...       NaN       NaN  \n",
       "25  [0.8519, 0.8395, 0.4422, 0.6722, 0.7543, 0.691...       NaN       NaN  \n",
       "26  [0.9524, 0.9592, 0.7968, 0.8766, 0.9359, 0.864...       NaN       NaN  \n",
       "27  [0.635, 0.5991, 0.5825, 0.6815, 0.2511, 0.5223...       NaN       NaN  \n",
       "28  [0.2784, 0.4021, 0.3663, 0.4172, 0.0816, 0.194...       NaN       NaN  \n",
       "29  [0.2551, 0.4583, 0.1919, 0.3106, 0.0632, 0.207...       NaN       NaN  \n",
       "30  [0.0004, 0.0, 0.0007, 0.0002, 0.0, 0.0004, 0.0...       NaN       NaN  \n",
       "31  [0.385, 0.5771, 0.4829, 0.5988, 0.6475, 0.7134...       NaN       NaN  \n",
       "32  [0.6236, 0.4626, 0.4567, 0.4521, 0.7118, 0.545...       NaN       NaN  \n",
       "33  [0.974, 0.9393, 0.9526, 0.9709, 0.9478, 0.9449...       NaN       NaN  \n",
       "34  [0.753, 0.4198, 0.9033, 0.8386, 0.6134, 0.8692...       NaN       NaN  \n",
       "35  [0.9363, 0.9202, 0.7955, 0.8912, 0.9475, 0.822...       NaN       NaN  \n",
       "36  [0.911, 0.9342, 0.8425, 0.8457, 0.9221, 0.8579...       NaN       NaN  \n",
       "37  [0.646, 0.5323, 0.3839, 0.3579, 0.7477, 0.2064...       NaN       NaN  \n",
       "38  [0.71, 0.8379, 0.8224, 0.7971, 0.8019, 0.5166,...       NaN       NaN  \n",
       "39  [0.8602, 0.9614, 0.9512, 0.9044, 0.9391, 0.902...       NaN       NaN  \n",
       "40  [0.445, 0.447, 0.6237, 0.3485, 0.5075, 0.5562,...       NaN       NaN  \n",
       "41  [0.9477, 0.9603, 0.9522, 0.9405, 0.9344, 0.826...       NaN       NaN  \n",
       "42  [0.8992, 0.9327, 0.7761, 0.8414, 0.9446, 0.819...       NaN       NaN  \n",
       "43  [0.4659, 0.5691, 0.4225, 0.3838, 0.7305, 0.279...       NaN       NaN  \n",
       "44  [0.4185, 0.5752, 0.2329, 0.3952, 0.7105, 0.093...       NaN       NaN  \n",
       "45  [0.7491, 0.8578, 0.638, 0.7572, 0.8847, 0.7304...       NaN       NaN  \n",
       "46  [0.4621, 0.637, 0.54, 0.5872, 0.5681, 0.5366, ...       NaN       NaN  \n",
       "47  [0.6682, 0.5728, 0.7195, 0.5773, 0.7676, 0.645...       NaN       NaN  \n",
       "48  [0.9301, 0.7544, 0.9574, 0.9367, 0.7805, 0.922...       NaN       NaN  \n",
       "49  [0.8125, 0.1739, 0.6336, 0.6254, 0.4818, 0.667...       NaN       NaN  \n",
       "50  [0.6544, 0.5628, 0.3796, 0.7096, 0.8236, 0.619...       NaN       NaN  \n",
       "51  [0.6234, 0.5654, 0.4838, 0.5945, 0.4838, 0.677...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581 : Training: loss:  0.14156649\n",
      "582 : Training: loss:  0.13164029\n",
      "583 : Training: loss:  0.15458116\n",
      "584 : Training: loss:  0.13943318\n",
      "585 : Training: loss:  0.13927907\n",
      "586 : Training: loss:  0.14635222\n",
      "587 : Training: loss:  0.1658489\n",
      "588 : Training: loss:  0.14009646\n",
      "589 : Training: loss:  0.14028099\n",
      "590 : Training: loss:  0.13349934\n",
      "591 : Training: loss:  0.14465448\n",
      "592 : Training: loss:  0.14268865\n",
      "593 : Training: loss:  0.13327639\n",
      "594 : Training: loss:  0.16294858\n",
      "595 : Training: loss:  0.14751856\n",
      "596 : Training: loss:  0.13968714\n",
      "597 : Training: loss:  0.15380184\n",
      "598 : Training: loss:  0.13664274\n",
      "599 : Training: loss:  0.12990683\n",
      "600 : Training: loss:  0.13439691\n",
      "Validation: Loss:  0.15617159  Accuracy:  0.28846154\n",
      "601 : Training: loss:  0.15772034\n",
      "602 : Training: loss:  0.14057872\n",
      "603 : Training: loss:  0.15849197\n",
      "604 : Training: loss:  0.15645903\n",
      "605 : Training: loss:  0.14173707\n",
      "606 : Training: loss:  0.13575743\n",
      "607 : Training: loss:  0.14129649\n",
      "608 : Training: loss:  0.1409956\n",
      "609 : Training: loss:  0.14200154\n",
      "610 : Training: loss:  0.13591057\n",
      "611 : Training: loss:  0.15751289\n",
      "612 : Training: loss:  0.1569616\n",
      "613 : Training: loss:  0.15434612\n",
      "614 : Training: loss:  0.15504904\n",
      "615 : Training: loss:  0.15759777\n",
      "616 : Training: loss:  0.14436196\n",
      "617 : Training: loss:  0.13985501\n",
      "618 : Training: loss:  0.1456231\n",
      "619 : Training: loss:  0.14174798\n",
      "620 : Training: loss:  0.1362647\n",
      "Validation: Loss:  0.15522283  Accuracy:  0.23076923\n",
      "621 : Training: loss:  0.14780062\n",
      "622 : Training: loss:  0.15348008\n",
      "623 : Training: loss:  0.16993964\n",
      "624 : Training: loss:  0.15974405\n",
      "625 : Training: loss:  0.12652752\n",
      "626 : Training: loss:  0.14856645\n",
      "627 : Training: loss:  0.1335234\n",
      "628 : Training: loss:  0.14996903\n",
      "629 : Training: loss:  0.1451807\n",
      "630 : Training: loss:  0.1475981\n",
      "631 : Training: loss:  0.14794609\n",
      "632 : Training: loss:  0.13835849\n",
      "633 : Training: loss:  0.14732172\n",
      "634 : Training: loss:  0.13773452\n",
      "635 : Training: loss:  0.15078576\n",
      "636 : Training: loss:  0.13482535\n",
      "637 : Training: loss:  0.16299613\n",
      "638 : Training: loss:  0.14170648\n",
      "639 : Training: loss:  0.15217039\n",
      "640 : Training: loss:  0.13434932\n",
      "Validation: Loss:  0.15461819  Accuracy:  0.26923078\n",
      "641 : Training: loss:  0.16382621\n",
      "642 : Training: loss:  0.12282712\n",
      "643 : Training: loss:  0.15625727\n",
      "644 : Training: loss:  0.14060396\n",
      "645 : Training: loss:  0.14505865\n",
      "646 : Training: loss:  0.1331829\n",
      "647 : Training: loss:  0.1598263\n",
      "648 : Training: loss:  0.12546629\n",
      "649 : Training: loss:  0.14628261\n",
      "650 : Training: loss:  0.16491722\n",
      "651 : Training: loss:  0.12341298\n",
      "652 : Training: loss:  0.13543802\n",
      "653 : Training: loss:  0.13098425\n",
      "654 : Training: loss:  0.1429968\n",
      "655 : Training: loss:  0.14697127\n",
      "656 : Training: loss:  0.14491884\n",
      "657 : Training: loss:  0.13506696\n",
      "658 : Training: loss:  0.14389604\n",
      "659 : Training: loss:  0.18335053\n",
      "660 : Training: loss:  0.15384234\n",
      "Validation: Loss:  0.15352412  Accuracy:  0.26923078\n",
      "661 : Training: loss:  0.15217817\n",
      "662 : Training: loss:  0.13813837\n",
      "663 : Training: loss:  0.14841212\n",
      "664 : Training: loss:  0.15427646\n",
      "665 : Training: loss:  0.15504286\n",
      "666 : Training: loss:  0.15060204\n",
      "667 : Training: loss:  0.15032396\n",
      "668 : Training: loss:  0.14305766\n",
      "669 : Training: loss:  0.13675134\n",
      "670 : Training: loss:  0.14539038\n",
      "671 : Training: loss:  0.14777516\n",
      "672 : Training: loss:  0.1527544\n",
      "673 : Training: loss:  0.1523693\n",
      "674 : Training: loss:  0.15081944\n",
      "675 : Training: loss:  0.15152435\n",
      "676 : Training: loss:  0.1548218\n",
      "677 : Training: loss:  0.1428011\n",
      "678 : Training: loss:  0.15718845\n",
      "679 : Training: loss:  0.14420801\n",
      "680 : Training: loss:  0.14512746\n",
      "Validation: Loss:  0.15280095  Accuracy:  0.3653846\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0846, 0.0477, 0.019, 0.056, 0.0317, 0.0363,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1502, 0.2812, 0.1152, 0.161, 0.1124, 0.1089...</td>\n",
       "      <td>[0.8498, 0.7188, 0.8848, 0.839, 0.8876, 0.8911...</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.152801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0966, 0.076, 0.0218, 0.0593, 0.0586, 0.0384...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5218, 0.5274, 0.2601, 0.4055, 0.3196, 0.226...</td>\n",
       "      <td>[0.4782, 0.4726, 0.7399, 0.5945, 0.6804, 0.773...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0762, 0.0489, 0.018, 0.0536, 0.0329, 0.0366...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2781, 0.3501, 0.1661, 0.1835, 0.1166, 0.128...</td>\n",
       "      <td>[0.7219, 0.6499, 0.8339, 0.8165, 0.8834, 0.871...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0423, 0.035, 0.0114, 0.0329, 0.029, 0.0198,...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3586, 0.347, 0.205, 0.3743, 0.5651, 0.339, ...</td>\n",
       "      <td>[0.6414, 0.653, 0.795, 0.6257, 0.4349, 0.661, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0545, 0.0292, 0.0099, 0.0339, 0.0168, 0.022...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.076, 0.0968, 0.0653, 0.0797, 0.089, 0.0403,...</td>\n",
       "      <td>[0.924, 0.9032, 0.9347, 0.9203, 0.911, 0.9597,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0822, 0.0472, 0.0234, 0.058, 0.0296, 0.0425...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0719, 0.0625, 0.0807, 0.0926, 0.1026, 0.166...</td>\n",
       "      <td>[0.9281, 0.9375, 0.9193, 0.9074, 0.8974, 0.833...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0687, 0.0576, 0.0157, 0.1348, 0.046, 0.0251...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7971, 0.6048, 0.8236, 0.6663, 0.4061, 0.702...</td>\n",
       "      <td>[0.2029, 0.3952, 0.1764, 0.3337, 0.5939, 0.297...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0817, 0.0561, 0.0252, 0.1016, 0.0404, 0.043...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6103, 0.3036, 0.6039, 0.3562, 0.2709, 0.598...</td>\n",
       "      <td>[0.3897, 0.6964, 0.3961, 0.6438, 0.7291, 0.401...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1219, 0.0871, 0.0431, 0.1319, 0.0652, 0.076...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4449, 0.2831, 0.588, 0.3219, 0.228, 0.4536,...</td>\n",
       "      <td>[0.5551, 0.7169, 0.412, 0.6781, 0.772, 0.5464,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0706, 0.1117, 0.0614, 0.095, 0.2093, 0.071,...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.7932, 0.8907, 0.8459, 0.7727, 0.9235, 0.697...</td>\n",
       "      <td>[0.2068, 0.1093, 0.1541, 0.2273, 0.0765, 0.302...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0491, 0.0613, 0.0135, 0.0435, 0.0536, 0.035...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5284, 0.7911, 0.4616, 0.4503, 0.6102, 0.445...</td>\n",
       "      <td>[0.4716, 0.2089, 0.5384, 0.5497, 0.3898, 0.554...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0433, 0.0399, 0.0151, 0.0459, 0.0379, 0.035...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3275, 0.6081, 0.3563, 0.3455, 0.6462, 0.406...</td>\n",
       "      <td>[0.6725, 0.3919, 0.6437, 0.6545, 0.3538, 0.593...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0602, 0.0734, 0.0318, 0.0941, 0.0727, 0.040...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6845, 0.7834, 0.8619, 0.8678, 0.7424, 0.775...</td>\n",
       "      <td>[0.3155, 0.2166, 0.1381, 0.1322, 0.2576, 0.224...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2456, 0.2016, 0.17, 0.2442, 0.1289, 0.2191,...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7287, 0.5914, 0.5909, 0.5458, 0.7306, 0.895...</td>\n",
       "      <td>[0.2713, 0.4086, 0.4091, 0.4542, 0.2694, 0.104...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0359, 0.0293, 0.0108, 0.0545, 0.0265, 0.017...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4857, 0.4256, 0.5597, 0.6344, 0.5834, 0.456...</td>\n",
       "      <td>[0.5143, 0.5744, 0.4403, 0.3656, 0.4166, 0.543...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0592, 0.043, 0.0169, 0.0436, 0.0373, 0.0271...</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2058, 0.4525, 0.4193, 0.3055, 0.3802, 0.292...</td>\n",
       "      <td>[0.7942, 0.5475, 0.5807, 0.6945, 0.6198, 0.707...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0723, 0.0385, 0.018, 0.0507, 0.0225, 0.0348...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0866, 0.071, 0.066, 0.1367, 0.1502, 0.1644,...</td>\n",
       "      <td>[0.9134, 0.929, 0.934, 0.8633, 0.8498, 0.8356,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0682, 0.0489, 0.0215, 0.0608, 0.0354, 0.037...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2915, 0.3445, 0.2888, 0.4621, 0.468, 0.4103...</td>\n",
       "      <td>[0.7085, 0.6555, 0.7112, 0.5379, 0.532, 0.5897...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0697, 0.0328, 0.0145, 0.0422, 0.0185, 0.028...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0924, 0.3212, 0.081, 0.0986, 0.324, 0.115, ...</td>\n",
       "      <td>[0.9076, 0.6788, 0.919, 0.9014, 0.676, 0.885, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0649, 0.0335, 0.0155, 0.0401, 0.0192, 0.027...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1211, 0.492, 0.0886, 0.2051, 0.3878, 0.1317...</td>\n",
       "      <td>[0.8789, 0.508, 0.9114, 0.7949, 0.6122, 0.8683...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0815, 0.0502, 0.021, 0.0644, 0.033, 0.0455,...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2059, 0.3333, 0.3241, 0.4726, 0.1451, 0.223...</td>\n",
       "      <td>[0.7941, 0.6667, 0.6759, 0.5274, 0.8549, 0.776...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0423, 0.0217, 0.0072, 0.0324, 0.0107, 0.016...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1127, 0.164, 0.1398, 0.3633, 0.0658, 0.1173...</td>\n",
       "      <td>[0.8873, 0.836, 0.8602, 0.6367, 0.9342, 0.8827...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0545, 0.0304, 0.0113, 0.0403, 0.0162, 0.022...</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0962, 0.1736, 0.1433, 0.2667, 0.109, 0.2288...</td>\n",
       "      <td>[0.9038, 0.8264, 0.8567, 0.7333, 0.891, 0.7712...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0729, 0.0445, 0.0173, 0.0551, 0.0288, 0.038...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4681, 0.4694, 0.4119, 0.3257, 0.4056, 0.491...</td>\n",
       "      <td>[0.5319, 0.5306, 0.5881, 0.6743, 0.5944, 0.508...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.068, 0.0313, 0.0138, 0.0443, 0.018, 0.0272,...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0668, 0.0735, 0.0925, 0.0812, 0.0765, 0.097...</td>\n",
       "      <td>[0.9332, 0.9265, 0.9075, 0.9188, 0.9235, 0.902...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0669, 0.0466, 0.0204, 0.0617, 0.0371, 0.035...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1551, 0.1666, 0.5729, 0.3378, 0.2599, 0.315...</td>\n",
       "      <td>[0.8449, 0.8334, 0.4271, 0.6622, 0.7401, 0.685...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.055, 0.0276, 0.0105, 0.0372, 0.015, 0.0206,...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0506, 0.0387, 0.2087, 0.1278, 0.0653, 0.139...</td>\n",
       "      <td>[0.9494, 0.9613, 0.7913, 0.8722, 0.9347, 0.860...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0344, 0.0288, 0.0104, 0.0366, 0.0268, 0.017...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.3863, 0.407, 0.424, 0.3261, 0.7714, 0.5106,...</td>\n",
       "      <td>[0.6137, 0.593, 0.576, 0.6739, 0.2286, 0.4894,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0668, 0.0628, 0.0368, 0.1005, 0.0508, 0.046...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7273, 0.5785, 0.6378, 0.5796, 0.9258, 0.833...</td>\n",
       "      <td>[0.2727, 0.4215, 0.3622, 0.4204, 0.0742, 0.167...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0306, 0.0365, 0.0145, 0.0604, 0.0486, 0.021...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7557, 0.5323, 0.8065, 0.6906, 0.9443, 0.819...</td>\n",
       "      <td>[0.2443, 0.4677, 0.1935, 0.3094, 0.0557, 0.180...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3307, 0.4575, 0.4666, 0.3827, 0.5741, 0.406...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9997, 0.9999, 0.999, 0.9998, 1.0, 0.9993, 1...</td>\n",
       "      <td>[0.0003, 1e-04, 0.001, 0.0002, 0.0, 0.0007, 0....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0748, 0.0583, 0.0234, 0.0731, 0.0395, 0.034...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6302, 0.4115, 0.5415, 0.4134, 0.352, 0.3047...</td>\n",
       "      <td>[0.3698, 0.5885, 0.4585, 0.5866, 0.648, 0.6953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0527, 0.0381, 0.0135, 0.0554, 0.025, 0.02, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3894, 0.5309, 0.5744, 0.5717, 0.2817, 0.470...</td>\n",
       "      <td>[0.6106, 0.4691, 0.4256, 0.4283, 0.7183, 0.529...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0412, 0.0185, 0.0063, 0.0245, 0.0098, 0.014...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.029, 0.0649, 0.0478, 0.0312, 0.056, 0.058, ...</td>\n",
       "      <td>[0.971, 0.9351, 0.9522, 0.9688, 0.944, 0.942, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.057, 0.0392, 0.0181, 0.0454, 0.0354, 0.0325...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2784, 0.6066, 0.0951, 0.1729, 0.4258, 0.13,...</td>\n",
       "      <td>[0.7216, 0.3934, 0.9049, 0.8271, 0.5742, 0.87,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0371, 0.0177, 0.0054, 0.0241, 0.0086, 0.012...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0686, 0.0791, 0.2127, 0.1133, 0.0547, 0.187...</td>\n",
       "      <td>[0.9314, 0.9209, 0.7873, 0.8867, 0.9453, 0.812...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0539, 0.0267, 0.0099, 0.0367, 0.013, 0.0209...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0875, 0.0637, 0.1607, 0.1556, 0.0702, 0.144...</td>\n",
       "      <td>[0.9125, 0.9363, 0.8393, 0.8444, 0.9298, 0.855...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0518, 0.0459, 0.016, 0.0684, 0.0187, 0.0227...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3442, 0.4315, 0.6202, 0.6428, 0.2191, 0.807...</td>\n",
       "      <td>[0.6558, 0.5685, 0.3798, 0.3572, 0.7809, 0.192...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0706, 0.0371, 0.02, 0.0669, 0.0274, 0.0307,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3071, 0.151, 0.1713, 0.2037, 0.1947, 0.5134...</td>\n",
       "      <td>[0.6929, 0.849, 0.8287, 0.7963, 0.8053, 0.4866...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0574, 0.0283, 0.0123, 0.0443, 0.0169, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.15, 0.0367, 0.0485, 0.1001, 0.0577, 0.1089,...</td>\n",
       "      <td>[0.85, 0.9633, 0.9515, 0.8999, 0.9423, 0.8911,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.074, 0.0646, 0.0196, 0.0908, 0.0565, 0.0341...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5744, 0.5378, 0.3745, 0.6626, 0.486, 0.462,...</td>\n",
       "      <td>[0.4256, 0.4622, 0.6255, 0.3374, 0.514, 0.538,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0606, 0.0274, 0.0123, 0.0407, 0.0155, 0.024...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0546, 0.0383, 0.0482, 0.0593, 0.0659, 0.185...</td>\n",
       "      <td>[0.9454, 0.9617, 0.9518, 0.9407, 0.9341, 0.814...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0369, 0.017, 0.0053, 0.0253, 0.0076, 0.0116...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1014, 0.0641, 0.2364, 0.1615, 0.0503, 0.187...</td>\n",
       "      <td>[0.8986, 0.9359, 0.7636, 0.8385, 0.9497, 0.812...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.045, 0.0334, 0.0104, 0.0536, 0.0139, 0.0154...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5357, 0.3874, 0.5852, 0.6176, 0.2368, 0.740...</td>\n",
       "      <td>[0.4643, 0.6126, 0.4148, 0.3824, 0.7632, 0.259...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0567, 0.0409, 0.0131, 0.0655, 0.0177, 0.016...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5708, 0.3733, 0.7757, 0.5987, 0.2529, 0.916...</td>\n",
       "      <td>[0.4292, 0.6267, 0.2243, 0.4013, 0.7471, 0.083...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0315, 0.0155, 0.0043, 0.0269, 0.0063, 0.009...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2522, 0.1306, 0.3765, 0.2446, 0.1007, 0.284...</td>\n",
       "      <td>[0.7478, 0.8694, 0.6235, 0.7554, 0.8993, 0.715...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1183, 0.0909, 0.0583, 0.1124, 0.0589, 0.091...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5454, 0.3468, 0.4762, 0.4133, 0.4358, 0.462...</td>\n",
       "      <td>[0.4546, 0.6532, 0.5238, 0.5867, 0.5642, 0.537...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0742, 0.0609, 0.0228, 0.0582, 0.0481, 0.039...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3476, 0.4291, 0.2942, 0.4372, 0.2401, 0.351...</td>\n",
       "      <td>[0.6524, 0.5709, 0.7058, 0.5628, 0.7599, 0.648...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0285, 0.0148, 0.0042, 0.016, 0.0088, 0.0098...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0772, 0.2677, 0.0377, 0.0669, 0.2514, 0.076...</td>\n",
       "      <td>[0.9228, 0.7323, 0.9623, 0.9331, 0.7486, 0.924...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0634, 0.0595, 0.0192, 0.0501, 0.0457, 0.037...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.198, 0.8437, 0.3511, 0.3943, 0.55, 0.3267, ...</td>\n",
       "      <td>[0.802, 0.1563, 0.6489, 0.6057, 0.45, 0.6733, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0908, 0.0681, 0.0296, 0.083, 0.065, 0.0545,...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3591, 0.4369, 0.6257, 0.3007, 0.1833, 0.365...</td>\n",
       "      <td>[0.6409, 0.5631, 0.3743, 0.6993, 0.8167, 0.634...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0533, 0.0496, 0.0224, 0.0622, 0.0529, 0.024...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.3989, 0.4568, 0.5279, 0.4199, 0.5363, 0.324...</td>\n",
       "      <td>[0.6011, 0.5432, 0.4721, 0.5801, 0.4637, 0.675...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0846, 0.0477, 0.019, 0.056, 0.0317, 0.0363,...               0   \n",
       "1   [0.0966, 0.076, 0.0218, 0.0593, 0.0586, 0.0384...               0   \n",
       "2   [0.0762, 0.0489, 0.018, 0.0536, 0.0329, 0.0366...               0   \n",
       "3   [0.0423, 0.035, 0.0114, 0.0329, 0.029, 0.0198,...               1   \n",
       "4   [0.0545, 0.0292, 0.0099, 0.0339, 0.0168, 0.022...               1   \n",
       "5   [0.0822, 0.0472, 0.0234, 0.058, 0.0296, 0.0425...               2   \n",
       "6   [0.0687, 0.0576, 0.0157, 0.1348, 0.046, 0.0251...               3   \n",
       "7   [0.0817, 0.0561, 0.0252, 0.1016, 0.0404, 0.043...               3   \n",
       "8   [0.1219, 0.0871, 0.0431, 0.1319, 0.0652, 0.076...               3   \n",
       "9   [0.0706, 0.1117, 0.0614, 0.095, 0.2093, 0.071,...               4   \n",
       "10  [0.0491, 0.0613, 0.0135, 0.0435, 0.0536, 0.035...               4   \n",
       "11  [0.0433, 0.0399, 0.0151, 0.0459, 0.0379, 0.035...               5   \n",
       "12  [0.0602, 0.0734, 0.0318, 0.0941, 0.0727, 0.040...               6   \n",
       "13  [0.2456, 0.2016, 0.17, 0.2442, 0.1289, 0.2191,...               7   \n",
       "14  [0.0359, 0.0293, 0.0108, 0.0545, 0.0265, 0.017...               8   \n",
       "15  [0.0592, 0.043, 0.0169, 0.0436, 0.0373, 0.0271...               8   \n",
       "16  [0.0723, 0.0385, 0.018, 0.0507, 0.0225, 0.0348...               9   \n",
       "17  [0.0682, 0.0489, 0.0215, 0.0608, 0.0354, 0.037...               9   \n",
       "18  [0.0697, 0.0328, 0.0145, 0.0422, 0.0185, 0.028...              10   \n",
       "19  [0.0649, 0.0335, 0.0155, 0.0401, 0.0192, 0.027...              10   \n",
       "20  [0.0815, 0.0502, 0.021, 0.0644, 0.033, 0.0455,...              11   \n",
       "21  [0.0423, 0.0217, 0.0072, 0.0324, 0.0107, 0.016...              11   \n",
       "22  [0.0545, 0.0304, 0.0113, 0.0403, 0.0162, 0.022...              12   \n",
       "23  [0.0729, 0.0445, 0.0173, 0.0551, 0.0288, 0.038...              13   \n",
       "24  [0.068, 0.0313, 0.0138, 0.0443, 0.018, 0.0272,...              13   \n",
       "25  [0.0669, 0.0466, 0.0204, 0.0617, 0.0371, 0.035...              14   \n",
       "26  [0.055, 0.0276, 0.0105, 0.0372, 0.015, 0.0206,...              14   \n",
       "27  [0.0344, 0.0288, 0.0104, 0.0366, 0.0268, 0.017...              15   \n",
       "28  [0.0668, 0.0628, 0.0368, 0.1005, 0.0508, 0.046...              15   \n",
       "29  [0.0306, 0.0365, 0.0145, 0.0604, 0.0486, 0.021...              15   \n",
       "30  [0.3307, 0.4575, 0.4666, 0.3827, 0.5741, 0.406...              16   \n",
       "31  [0.0748, 0.0583, 0.0234, 0.0731, 0.0395, 0.034...              17   \n",
       "32  [0.0527, 0.0381, 0.0135, 0.0554, 0.025, 0.02, ...              17   \n",
       "33  [0.0412, 0.0185, 0.0063, 0.0245, 0.0098, 0.014...              18   \n",
       "34  [0.057, 0.0392, 0.0181, 0.0454, 0.0354, 0.0325...              19   \n",
       "35  [0.0371, 0.0177, 0.0054, 0.0241, 0.0086, 0.012...              20   \n",
       "36  [0.0539, 0.0267, 0.0099, 0.0367, 0.013, 0.0209...              21   \n",
       "37  [0.0518, 0.0459, 0.016, 0.0684, 0.0187, 0.0227...              21   \n",
       "38  [0.0706, 0.0371, 0.02, 0.0669, 0.0274, 0.0307,...              22   \n",
       "39  [0.0574, 0.0283, 0.0123, 0.0443, 0.0169, 0.023...              22   \n",
       "40  [0.074, 0.0646, 0.0196, 0.0908, 0.0565, 0.0341...              22   \n",
       "41  [0.0606, 0.0274, 0.0123, 0.0407, 0.0155, 0.024...              22   \n",
       "42  [0.0369, 0.017, 0.0053, 0.0253, 0.0076, 0.0116...              23   \n",
       "43  [0.045, 0.0334, 0.0104, 0.0536, 0.0139, 0.0154...              23   \n",
       "44  [0.0567, 0.0409, 0.0131, 0.0655, 0.0177, 0.016...              23   \n",
       "45  [0.0315, 0.0155, 0.0043, 0.0269, 0.0063, 0.009...              23   \n",
       "46  [0.1183, 0.0909, 0.0583, 0.1124, 0.0589, 0.091...              24   \n",
       "47  [0.0742, 0.0609, 0.0228, 0.0582, 0.0481, 0.039...              24   \n",
       "48  [0.0285, 0.0148, 0.0042, 0.016, 0.0088, 0.0098...              25   \n",
       "49  [0.0634, 0.0595, 0.0192, 0.0501, 0.0457, 0.037...              25   \n",
       "50  [0.0908, 0.0681, 0.0296, 0.083, 0.065, 0.0545,...              26   \n",
       "51  [0.0533, 0.0496, 0.0224, 0.0622, 0.0529, 0.024...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1502, 0.2812, 0.1152, 0.161, 0.1124, 0.1089...   \n",
       "1                 23  [0.5218, 0.5274, 0.2601, 0.4055, 0.3196, 0.226...   \n",
       "2                  0  [0.2781, 0.3501, 0.1661, 0.1835, 0.1166, 0.128...   \n",
       "3                 23  [0.3586, 0.347, 0.205, 0.3743, 0.5651, 0.339, ...   \n",
       "4                 23  [0.076, 0.0968, 0.0653, 0.0797, 0.089, 0.0403,...   \n",
       "5                 23  [0.0719, 0.0625, 0.0807, 0.0926, 0.1026, 0.166...   \n",
       "6                  3  [0.7971, 0.6048, 0.8236, 0.6663, 0.4061, 0.702...   \n",
       "7                 23  [0.6103, 0.3036, 0.6039, 0.3562, 0.2709, 0.598...   \n",
       "8                 23  [0.4449, 0.2831, 0.588, 0.3219, 0.228, 0.4536,...   \n",
       "9                  4  [0.7932, 0.8907, 0.8459, 0.7727, 0.9235, 0.697...   \n",
       "10                 8  [0.5284, 0.7911, 0.4616, 0.4503, 0.6102, 0.445...   \n",
       "11                 3  [0.3275, 0.6081, 0.3563, 0.3455, 0.6462, 0.406...   \n",
       "12                17  [0.6845, 0.7834, 0.8619, 0.8678, 0.7424, 0.775...   \n",
       "13                23  [0.7287, 0.5914, 0.5909, 0.5458, 0.7306, 0.895...   \n",
       "14                 3  [0.4857, 0.4256, 0.5597, 0.6344, 0.5834, 0.456...   \n",
       "15                23  [0.2058, 0.4525, 0.4193, 0.3055, 0.3802, 0.292...   \n",
       "16                23  [0.0866, 0.071, 0.066, 0.1367, 0.1502, 0.1644,...   \n",
       "17                23  [0.2915, 0.3445, 0.2888, 0.4621, 0.468, 0.4103...   \n",
       "18                10  [0.0924, 0.3212, 0.081, 0.0986, 0.324, 0.115, ...   \n",
       "19                10  [0.1211, 0.492, 0.0886, 0.2051, 0.3878, 0.1317...   \n",
       "20                 0  [0.2059, 0.3333, 0.3241, 0.4726, 0.1451, 0.223...   \n",
       "21                23  [0.1127, 0.164, 0.1398, 0.3633, 0.0658, 0.1173...   \n",
       "22                23  [0.0962, 0.1736, 0.1433, 0.2667, 0.109, 0.2288...   \n",
       "23                 0  [0.4681, 0.4694, 0.4119, 0.3257, 0.4056, 0.491...   \n",
       "24                10  [0.0668, 0.0735, 0.0925, 0.0812, 0.0765, 0.097...   \n",
       "25                23  [0.1551, 0.1666, 0.5729, 0.3378, 0.2599, 0.315...   \n",
       "26                23  [0.0506, 0.0387, 0.2087, 0.1278, 0.0653, 0.139...   \n",
       "27                15  [0.3863, 0.407, 0.424, 0.3261, 0.7714, 0.5106,...   \n",
       "28                15  [0.7273, 0.5785, 0.6378, 0.5796, 0.9258, 0.833...   \n",
       "29                15  [0.7557, 0.5323, 0.8065, 0.6906, 0.9443, 0.819...   \n",
       "30                16  [0.9997, 0.9999, 0.999, 0.9998, 1.0, 0.9993, 1...   \n",
       "31                23  [0.6302, 0.4115, 0.5415, 0.4134, 0.352, 0.3047...   \n",
       "32                23  [0.3894, 0.5309, 0.5744, 0.5717, 0.2817, 0.470...   \n",
       "33                10  [0.029, 0.0649, 0.0478, 0.0312, 0.056, 0.058, ...   \n",
       "34                 0  [0.2784, 0.6066, 0.0951, 0.1729, 0.4258, 0.13,...   \n",
       "35                23  [0.0686, 0.0791, 0.2127, 0.1133, 0.0547, 0.187...   \n",
       "36                23  [0.0875, 0.0637, 0.1607, 0.1556, 0.0702, 0.144...   \n",
       "37                23  [0.3442, 0.4315, 0.6202, 0.6428, 0.2191, 0.807...   \n",
       "38                22  [0.3071, 0.151, 0.1713, 0.2037, 0.1947, 0.5134...   \n",
       "39                22  [0.15, 0.0367, 0.0485, 0.1001, 0.0577, 0.1089,...   \n",
       "40                23  [0.5744, 0.5378, 0.3745, 0.6626, 0.486, 0.462,...   \n",
       "41                22  [0.0546, 0.0383, 0.0482, 0.0593, 0.0659, 0.185...   \n",
       "42                23  [0.1014, 0.0641, 0.2364, 0.1615, 0.0503, 0.187...   \n",
       "43                23  [0.5357, 0.3874, 0.5852, 0.6176, 0.2368, 0.740...   \n",
       "44                23  [0.5708, 0.3733, 0.7757, 0.5987, 0.2529, 0.916...   \n",
       "45                23  [0.2522, 0.1306, 0.3765, 0.2446, 0.1007, 0.284...   \n",
       "46                22  [0.5454, 0.3468, 0.4762, 0.4133, 0.4358, 0.462...   \n",
       "47                 0  [0.3476, 0.4291, 0.2942, 0.4372, 0.2401, 0.351...   \n",
       "48                 0  [0.0772, 0.2677, 0.0377, 0.0669, 0.2514, 0.076...   \n",
       "49                25  [0.198, 0.8437, 0.3511, 0.3943, 0.55, 0.3267, ...   \n",
       "50                23  [0.3591, 0.4369, 0.6257, 0.3007, 0.1833, 0.365...   \n",
       "51                26  [0.3989, 0.4568, 0.5279, 0.4199, 0.5363, 0.324...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8498, 0.7188, 0.8848, 0.839, 0.8876, 0.8911...  0.365385  0.152801  \n",
       "1   [0.4782, 0.4726, 0.7399, 0.5945, 0.6804, 0.773...       NaN       NaN  \n",
       "2   [0.7219, 0.6499, 0.8339, 0.8165, 0.8834, 0.871...       NaN       NaN  \n",
       "3   [0.6414, 0.653, 0.795, 0.6257, 0.4349, 0.661, ...       NaN       NaN  \n",
       "4   [0.924, 0.9032, 0.9347, 0.9203, 0.911, 0.9597,...       NaN       NaN  \n",
       "5   [0.9281, 0.9375, 0.9193, 0.9074, 0.8974, 0.833...       NaN       NaN  \n",
       "6   [0.2029, 0.3952, 0.1764, 0.3337, 0.5939, 0.297...       NaN       NaN  \n",
       "7   [0.3897, 0.6964, 0.3961, 0.6438, 0.7291, 0.401...       NaN       NaN  \n",
       "8   [0.5551, 0.7169, 0.412, 0.6781, 0.772, 0.5464,...       NaN       NaN  \n",
       "9   [0.2068, 0.1093, 0.1541, 0.2273, 0.0765, 0.302...       NaN       NaN  \n",
       "10  [0.4716, 0.2089, 0.5384, 0.5497, 0.3898, 0.554...       NaN       NaN  \n",
       "11  [0.6725, 0.3919, 0.6437, 0.6545, 0.3538, 0.593...       NaN       NaN  \n",
       "12  [0.3155, 0.2166, 0.1381, 0.1322, 0.2576, 0.224...       NaN       NaN  \n",
       "13  [0.2713, 0.4086, 0.4091, 0.4542, 0.2694, 0.104...       NaN       NaN  \n",
       "14  [0.5143, 0.5744, 0.4403, 0.3656, 0.4166, 0.543...       NaN       NaN  \n",
       "15  [0.7942, 0.5475, 0.5807, 0.6945, 0.6198, 0.707...       NaN       NaN  \n",
       "16  [0.9134, 0.929, 0.934, 0.8633, 0.8498, 0.8356,...       NaN       NaN  \n",
       "17  [0.7085, 0.6555, 0.7112, 0.5379, 0.532, 0.5897...       NaN       NaN  \n",
       "18  [0.9076, 0.6788, 0.919, 0.9014, 0.676, 0.885, ...       NaN       NaN  \n",
       "19  [0.8789, 0.508, 0.9114, 0.7949, 0.6122, 0.8683...       NaN       NaN  \n",
       "20  [0.7941, 0.6667, 0.6759, 0.5274, 0.8549, 0.776...       NaN       NaN  \n",
       "21  [0.8873, 0.836, 0.8602, 0.6367, 0.9342, 0.8827...       NaN       NaN  \n",
       "22  [0.9038, 0.8264, 0.8567, 0.7333, 0.891, 0.7712...       NaN       NaN  \n",
       "23  [0.5319, 0.5306, 0.5881, 0.6743, 0.5944, 0.508...       NaN       NaN  \n",
       "24  [0.9332, 0.9265, 0.9075, 0.9188, 0.9235, 0.902...       NaN       NaN  \n",
       "25  [0.8449, 0.8334, 0.4271, 0.6622, 0.7401, 0.685...       NaN       NaN  \n",
       "26  [0.9494, 0.9613, 0.7913, 0.8722, 0.9347, 0.860...       NaN       NaN  \n",
       "27  [0.6137, 0.593, 0.576, 0.6739, 0.2286, 0.4894,...       NaN       NaN  \n",
       "28  [0.2727, 0.4215, 0.3622, 0.4204, 0.0742, 0.167...       NaN       NaN  \n",
       "29  [0.2443, 0.4677, 0.1935, 0.3094, 0.0557, 0.180...       NaN       NaN  \n",
       "30  [0.0003, 1e-04, 0.001, 0.0002, 0.0, 0.0007, 0....       NaN       NaN  \n",
       "31  [0.3698, 0.5885, 0.4585, 0.5866, 0.648, 0.6953...       NaN       NaN  \n",
       "32  [0.6106, 0.4691, 0.4256, 0.4283, 0.7183, 0.529...       NaN       NaN  \n",
       "33  [0.971, 0.9351, 0.9522, 0.9688, 0.944, 0.942, ...       NaN       NaN  \n",
       "34  [0.7216, 0.3934, 0.9049, 0.8271, 0.5742, 0.87,...       NaN       NaN  \n",
       "35  [0.9314, 0.9209, 0.7873, 0.8867, 0.9453, 0.812...       NaN       NaN  \n",
       "36  [0.9125, 0.9363, 0.8393, 0.8444, 0.9298, 0.855...       NaN       NaN  \n",
       "37  [0.6558, 0.5685, 0.3798, 0.3572, 0.7809, 0.192...       NaN       NaN  \n",
       "38  [0.6929, 0.849, 0.8287, 0.7963, 0.8053, 0.4866...       NaN       NaN  \n",
       "39  [0.85, 0.9633, 0.9515, 0.8999, 0.9423, 0.8911,...       NaN       NaN  \n",
       "40  [0.4256, 0.4622, 0.6255, 0.3374, 0.514, 0.538,...       NaN       NaN  \n",
       "41  [0.9454, 0.9617, 0.9518, 0.9407, 0.9341, 0.814...       NaN       NaN  \n",
       "42  [0.8986, 0.9359, 0.7636, 0.8385, 0.9497, 0.812...       NaN       NaN  \n",
       "43  [0.4643, 0.6126, 0.4148, 0.3824, 0.7632, 0.259...       NaN       NaN  \n",
       "44  [0.4292, 0.6267, 0.2243, 0.4013, 0.7471, 0.083...       NaN       NaN  \n",
       "45  [0.7478, 0.8694, 0.6235, 0.7554, 0.8993, 0.715...       NaN       NaN  \n",
       "46  [0.4546, 0.6532, 0.5238, 0.5867, 0.5642, 0.537...       NaN       NaN  \n",
       "47  [0.6524, 0.5709, 0.7058, 0.5628, 0.7599, 0.648...       NaN       NaN  \n",
       "48  [0.9228, 0.7323, 0.9623, 0.9331, 0.7486, 0.924...       NaN       NaN  \n",
       "49  [0.802, 0.1563, 0.6489, 0.6057, 0.45, 0.6733, ...       NaN       NaN  \n",
       "50  [0.6409, 0.5631, 0.3743, 0.6993, 0.8167, 0.634...       NaN       NaN  \n",
       "51  [0.6011, 0.5432, 0.4721, 0.5801, 0.4637, 0.675...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681 : Training: loss:  0.13301648\n",
      "682 : Training: loss:  0.14418355\n",
      "683 : Training: loss:  0.13139927\n",
      "684 : Training: loss:  0.14421603\n",
      "685 : Training: loss:  0.13520165\n",
      "686 : Training: loss:  0.14826997\n",
      "687 : Training: loss:  0.15249777\n",
      "688 : Training: loss:  0.14652838\n",
      "689 : Training: loss:  0.12569691\n",
      "690 : Training: loss:  0.15405732\n",
      "691 : Training: loss:  0.14647566\n",
      "692 : Training: loss:  0.14334506\n",
      "693 : Training: loss:  0.15071909\n",
      "694 : Training: loss:  0.14683454\n",
      "695 : Training: loss:  0.13918008\n",
      "696 : Training: loss:  0.15945837\n",
      "697 : Training: loss:  0.15847807\n",
      "698 : Training: loss:  0.14693087\n",
      "699 : Training: loss:  0.15145232\n",
      "700 : Training: loss:  0.14568241\n",
      "Validation: Loss:  0.15204081  Accuracy:  0.40384614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.086, 0.047, 0.0192, 0.0577, 0.0317, 0.0357,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1511, 0.2835, 0.1158, 0.1625, 0.1135, 0.108...</td>\n",
       "      <td>[0.8489, 0.7165, 0.8842, 0.8375, 0.8865, 0.891...</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.152041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0978, 0.0752, 0.0218, 0.061, 0.0594, 0.0378...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5224, 0.5295, 0.2598, 0.4071, 0.3232, 0.226...</td>\n",
       "      <td>[0.4776, 0.4705, 0.7402, 0.5929, 0.6768, 0.773...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0773, 0.0481, 0.0181, 0.0553, 0.0329, 0.036...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2795, 0.3519, 0.1667, 0.186, 0.1174, 0.1284...</td>\n",
       "      <td>[0.7205, 0.6481, 0.8333, 0.814, 0.8826, 0.8716...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0425, 0.0346, 0.0116, 0.0339, 0.0298, 0.019...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.361, 0.3499, 0.2045, 0.3765, 0.5698, 0.339,...</td>\n",
       "      <td>[0.639, 0.6501, 0.7955, 0.6235, 0.4302, 0.661,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0555, 0.0287, 0.0101, 0.035, 0.0169, 0.0217...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0763, 0.0985, 0.065, 0.0802, 0.0902, 0.0402...</td>\n",
       "      <td>[0.9237, 0.9015, 0.935, 0.9198, 0.9098, 0.9598...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0827, 0.0463, 0.0236, 0.0593, 0.0294, 0.041...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0717, 0.0618, 0.0806, 0.0932, 0.1037, 0.167...</td>\n",
       "      <td>[0.9283, 0.9382, 0.9194, 0.9068, 0.8963, 0.832...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0673, 0.0552, 0.0152, 0.1434, 0.0448, 0.023...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8012, 0.6092, 0.8284, 0.6736, 0.4028, 0.709...</td>\n",
       "      <td>[0.1988, 0.3908, 0.1716, 0.3264, 0.5972, 0.290...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0809, 0.0544, 0.025, 0.1061, 0.0396, 0.042,...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6152, 0.3029, 0.6083, 0.3621, 0.2704, 0.605...</td>\n",
       "      <td>[0.3848, 0.6971, 0.3917, 0.6379, 0.7296, 0.394...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1213, 0.085, 0.0427, 0.1361, 0.0642, 0.0746...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4496, 0.284, 0.5952, 0.3275, 0.228, 0.4585,...</td>\n",
       "      <td>[0.5504, 0.716, 0.4048, 0.6725, 0.772, 0.5415,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0681, 0.1085, 0.0597, 0.0961, 0.2148, 0.07,...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.7933, 0.8917, 0.8447, 0.7723, 0.9248, 0.696...</td>\n",
       "      <td>[0.2067, 0.1083, 0.1553, 0.2277, 0.0752, 0.303...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0483, 0.0597, 0.0133, 0.0437, 0.0551, 0.034...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.525, 0.7931, 0.4508, 0.4485, 0.6146, 0.4435...</td>\n",
       "      <td>[0.475, 0.2069, 0.5492, 0.5515, 0.3854, 0.5565...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0427, 0.0387, 0.015, 0.0468, 0.0385, 0.0352...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3266, 0.6099, 0.353, 0.3459, 0.6495, 0.4055...</td>\n",
       "      <td>[0.6734, 0.3901, 0.647, 0.6541, 0.3505, 0.5945...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0583, 0.0712, 0.0314, 0.0947, 0.0724, 0.039...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6828, 0.7817, 0.8599, 0.8689, 0.7461, 0.772...</td>\n",
       "      <td>[0.3172, 0.2183, 0.1401, 0.1311, 0.2539, 0.227...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2366, 0.1926, 0.1626, 0.2395, 0.1223, 0.210...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7205, 0.5744, 0.573, 0.5402, 0.7305, 0.8941...</td>\n",
       "      <td>[0.2795, 0.4256, 0.427, 0.4598, 0.2695, 0.1059...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0355, 0.0284, 0.0107, 0.0566, 0.0269, 0.016...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4899, 0.4298, 0.5602, 0.6394, 0.5855, 0.457...</td>\n",
       "      <td>[0.5101, 0.5702, 0.4398, 0.3606, 0.4145, 0.542...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0595, 0.0423, 0.0169, 0.0445, 0.0379, 0.026...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2059, 0.4586, 0.4189, 0.3069, 0.3857, 0.288...</td>\n",
       "      <td>[0.7941, 0.5414, 0.5811, 0.6931, 0.6143, 0.711...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0728, 0.0375, 0.0182, 0.0518, 0.0223, 0.034...</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.086, 0.0709, 0.0656, 0.1369, 0.1509, 0.1648...</td>\n",
       "      <td>[0.914, 0.9291, 0.9344, 0.8631, 0.8491, 0.8352...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0676, 0.0474, 0.0213, 0.062, 0.0351, 0.0361...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2908, 0.3467, 0.2905, 0.4635, 0.4691, 0.411...</td>\n",
       "      <td>[0.7092, 0.6533, 0.7095, 0.5365, 0.5309, 0.588...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0707, 0.0322, 0.0148, 0.0434, 0.0184, 0.028...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0923, 0.3214, 0.0807, 0.0983, 0.3263, 0.115...</td>\n",
       "      <td>[0.9077, 0.6786, 0.9193, 0.9017, 0.6737, 0.884...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0657, 0.0328, 0.0158, 0.041, 0.0191, 0.0273...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.121, 0.4935, 0.0882, 0.2056, 0.3921, 0.131,...</td>\n",
       "      <td>[0.879, 0.5065, 0.9118, 0.7944, 0.6079, 0.869,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0819, 0.0493, 0.0211, 0.0663, 0.0327, 0.044...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2076, 0.3334, 0.3287, 0.4797, 0.1447, 0.224...</td>\n",
       "      <td>[0.7924, 0.6666, 0.6713, 0.5203, 0.8553, 0.775...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.043, 0.0212, 0.0074, 0.0338, 0.0107, 0.0156...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1152, 0.1632, 0.142, 0.37, 0.0654, 0.1201, ...</td>\n",
       "      <td>[0.8848, 0.8368, 0.858, 0.63, 0.9346, 0.8799, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0551, 0.0298, 0.0115, 0.0418, 0.0161, 0.021...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0979, 0.173, 0.1437, 0.2706, 0.1087, 0.2314...</td>\n",
       "      <td>[0.9021, 0.827, 0.8563, 0.7294, 0.8913, 0.7686...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0725, 0.0433, 0.0174, 0.0566, 0.0288, 0.037...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4687, 0.4678, 0.4104, 0.3271, 0.4114, 0.490...</td>\n",
       "      <td>[0.5313, 0.5322, 0.5896, 0.6729, 0.5886, 0.509...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.069, 0.0306, 0.014, 0.0458, 0.0179, 0.0266,...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0672, 0.0733, 0.094, 0.0819, 0.0765, 0.0989...</td>\n",
       "      <td>[0.9328, 0.9267, 0.906, 0.9181, 0.9235, 0.9011...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0663, 0.0452, 0.0203, 0.0628, 0.0367, 0.034...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1559, 0.1675, 0.5777, 0.3415, 0.2612, 0.316...</td>\n",
       "      <td>[0.8441, 0.8325, 0.4223, 0.6585, 0.7388, 0.683...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0554, 0.0269, 0.0107, 0.0382, 0.0149, 0.02,...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0513, 0.0384, 0.2095, 0.1292, 0.0657, 0.141...</td>\n",
       "      <td>[0.9487, 0.9616, 0.7905, 0.8708, 0.9343, 0.858...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0337, 0.0278, 0.0104, 0.0373, 0.0271, 0.017...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.3891, 0.4077, 0.4216, 0.3268, 0.7741, 0.514...</td>\n",
       "      <td>[0.6109, 0.5923, 0.5784, 0.6732, 0.2259, 0.485...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0634, 0.0595, 0.0353, 0.1001, 0.0496, 0.043...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7253, 0.5711, 0.6297, 0.5769, 0.9264, 0.834...</td>\n",
       "      <td>[0.2747, 0.4289, 0.3703, 0.4231, 0.0736, 0.165...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0291, 0.0347, 0.014, 0.0611, 0.0486, 0.0201...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7551, 0.5279, 0.803, 0.6914, 0.9449, 0.8207...</td>\n",
       "      <td>[0.2449, 0.4721, 0.197, 0.3086, 0.0551, 0.1793...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3314, 0.4602, 0.4681, 0.3824, 0.5828, 0.412...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9997, 0.9999, 0.9988, 0.9998, 1.0, 0.9993, ...</td>\n",
       "      <td>[0.0003, 1e-04, 0.0012, 0.0002, 0.0, 0.0007, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0735, 0.0563, 0.0232, 0.0753, 0.0389, 0.033...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.633, 0.4075, 0.5462, 0.4171, 0.3513, 0.3087...</td>\n",
       "      <td>[0.367, 0.5925, 0.4538, 0.5829, 0.6487, 0.6913...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0521, 0.0369, 0.0133, 0.0575, 0.0247, 0.019...</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3927, 0.5313, 0.5831, 0.5786, 0.2813, 0.474...</td>\n",
       "      <td>[0.6073, 0.4687, 0.4169, 0.4214, 0.7187, 0.525...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.042, 0.0181, 0.0065, 0.0254, 0.0098, 0.0141...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0295, 0.0661, 0.0478, 0.0317, 0.0567, 0.058...</td>\n",
       "      <td>[0.9705, 0.9339, 0.9522, 0.9683, 0.9433, 0.941...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0577, 0.0388, 0.0185, 0.0469, 0.0363, 0.032...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2851, 0.6125, 0.0954, 0.1759, 0.432, 0.1304...</td>\n",
       "      <td>[0.7149, 0.3875, 0.9046, 0.8241, 0.568, 0.8696...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0376, 0.0173, 0.0055, 0.025, 0.0085, 0.0119...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0693, 0.0788, 0.2144, 0.1148, 0.0549, 0.189...</td>\n",
       "      <td>[0.9307, 0.9212, 0.7856, 0.8852, 0.9451, 0.810...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0544, 0.026, 0.0101, 0.0378, 0.0128, 0.0204...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0864, 0.063, 0.1608, 0.1554, 0.0691, 0.1456...</td>\n",
       "      <td>[0.9136, 0.937, 0.8392, 0.8446, 0.9309, 0.8544...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0512, 0.0442, 0.0159, 0.0706, 0.0181, 0.021...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3394, 0.4251, 0.6188, 0.643, 0.2156, 0.8091...</td>\n",
       "      <td>[0.6606, 0.5749, 0.3812, 0.357, 0.7844, 0.1909...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0701, 0.0357, 0.0199, 0.0686, 0.0267, 0.029...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3108, 0.1478, 0.1687, 0.204, 0.1926, 0.5226...</td>\n",
       "      <td>[0.6892, 0.8522, 0.8313, 0.796, 0.8074, 0.4774...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0579, 0.0275, 0.0125, 0.0457, 0.0167, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1522, 0.0361, 0.0485, 0.1015, 0.057, 0.112,...</td>\n",
       "      <td>[0.8478, 0.9639, 0.9515, 0.8985, 0.943, 0.888,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.073, 0.0624, 0.0193, 0.0938, 0.0557, 0.0328...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5785, 0.5348, 0.3747, 0.6651, 0.4827, 0.468...</td>\n",
       "      <td>[0.4215, 0.4652, 0.6253, 0.3349, 0.5173, 0.532...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0614, 0.0267, 0.0125, 0.0421, 0.0153, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0548, 0.0377, 0.0479, 0.0595, 0.0655, 0.189...</td>\n",
       "      <td>[0.9452, 0.9623, 0.9521, 0.9405, 0.9345, 0.810...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0374, 0.0166, 0.0054, 0.0263, 0.0075, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1013, 0.0641, 0.2393, 0.1632, 0.05, 0.1891,...</td>\n",
       "      <td>[0.8987, 0.9359, 0.7607, 0.8368, 0.95, 0.8109,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0448, 0.0323, 0.0104, 0.0556, 0.0137, 0.014...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5364, 0.3844, 0.5831, 0.6194, 0.2369, 0.742...</td>\n",
       "      <td>[0.4636, 0.6156, 0.4169, 0.3806, 0.7631, 0.257...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.056, 0.0394, 0.013, 0.0675, 0.0172, 0.016, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5675, 0.3672, 0.7736, 0.5986, 0.2526, 0.917...</td>\n",
       "      <td>[0.4325, 0.6328, 0.2264, 0.4014, 0.7474, 0.083...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0317, 0.015, 0.0043, 0.028, 0.0062, 0.0088,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2521, 0.1297, 0.3783, 0.2457, 0.0997, 0.287...</td>\n",
       "      <td>[0.7479, 0.8703, 0.6217, 0.7543, 0.9003, 0.712...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.116, 0.0879, 0.0577, 0.1136, 0.0577, 0.0887...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5455, 0.3415, 0.4759, 0.4147, 0.4379, 0.462...</td>\n",
       "      <td>[0.4545, 0.6585, 0.5241, 0.5853, 0.5621, 0.538...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0741, 0.0599, 0.0229, 0.06, 0.0483, 0.0391,...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.351, 0.4304, 0.2991, 0.4424, 0.2415, 0.3519...</td>\n",
       "      <td>[0.649, 0.5696, 0.7009, 0.5576, 0.7585, 0.6481...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0287, 0.0145, 0.0043, 0.0163, 0.0089, 0.009...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0779, 0.2713, 0.0364, 0.0669, 0.2578, 0.075...</td>\n",
       "      <td>[0.9221, 0.7287, 0.9636, 0.9331, 0.7422, 0.924...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0633, 0.0585, 0.0193, 0.0509, 0.047, 0.0376...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1976, 0.8467, 0.3471, 0.3954, 0.5563, 0.323...</td>\n",
       "      <td>[0.8024, 0.1533, 0.6529, 0.6046, 0.4437, 0.676...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0906, 0.0665, 0.0293, 0.0843, 0.0644, 0.053...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3602, 0.4371, 0.6281, 0.3024, 0.184, 0.3636...</td>\n",
       "      <td>[0.6398, 0.5629, 0.3719, 0.6976, 0.816, 0.6364...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0527, 0.0484, 0.0223, 0.0639, 0.0539, 0.024...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4022, 0.4621, 0.5323, 0.4223, 0.5407, 0.323...</td>\n",
       "      <td>[0.5978, 0.5379, 0.4677, 0.5777, 0.4593, 0.676...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.086, 0.047, 0.0192, 0.0577, 0.0317, 0.0357,...               0   \n",
       "1   [0.0978, 0.0752, 0.0218, 0.061, 0.0594, 0.0378...               0   \n",
       "2   [0.0773, 0.0481, 0.0181, 0.0553, 0.0329, 0.036...               0   \n",
       "3   [0.0425, 0.0346, 0.0116, 0.0339, 0.0298, 0.019...               1   \n",
       "4   [0.0555, 0.0287, 0.0101, 0.035, 0.0169, 0.0217...               1   \n",
       "5   [0.0827, 0.0463, 0.0236, 0.0593, 0.0294, 0.041...               2   \n",
       "6   [0.0673, 0.0552, 0.0152, 0.1434, 0.0448, 0.023...               3   \n",
       "7   [0.0809, 0.0544, 0.025, 0.1061, 0.0396, 0.042,...               3   \n",
       "8   [0.1213, 0.085, 0.0427, 0.1361, 0.0642, 0.0746...               3   \n",
       "9   [0.0681, 0.1085, 0.0597, 0.0961, 0.2148, 0.07,...               4   \n",
       "10  [0.0483, 0.0597, 0.0133, 0.0437, 0.0551, 0.034...               4   \n",
       "11  [0.0427, 0.0387, 0.015, 0.0468, 0.0385, 0.0352...               5   \n",
       "12  [0.0583, 0.0712, 0.0314, 0.0947, 0.0724, 0.039...               6   \n",
       "13  [0.2366, 0.1926, 0.1626, 0.2395, 0.1223, 0.210...               7   \n",
       "14  [0.0355, 0.0284, 0.0107, 0.0566, 0.0269, 0.016...               8   \n",
       "15  [0.0595, 0.0423, 0.0169, 0.0445, 0.0379, 0.026...               8   \n",
       "16  [0.0728, 0.0375, 0.0182, 0.0518, 0.0223, 0.034...               9   \n",
       "17  [0.0676, 0.0474, 0.0213, 0.062, 0.0351, 0.0361...               9   \n",
       "18  [0.0707, 0.0322, 0.0148, 0.0434, 0.0184, 0.028...              10   \n",
       "19  [0.0657, 0.0328, 0.0158, 0.041, 0.0191, 0.0273...              10   \n",
       "20  [0.0819, 0.0493, 0.0211, 0.0663, 0.0327, 0.044...              11   \n",
       "21  [0.043, 0.0212, 0.0074, 0.0338, 0.0107, 0.0156...              11   \n",
       "22  [0.0551, 0.0298, 0.0115, 0.0418, 0.0161, 0.021...              12   \n",
       "23  [0.0725, 0.0433, 0.0174, 0.0566, 0.0288, 0.037...              13   \n",
       "24  [0.069, 0.0306, 0.014, 0.0458, 0.0179, 0.0266,...              13   \n",
       "25  [0.0663, 0.0452, 0.0203, 0.0628, 0.0367, 0.034...              14   \n",
       "26  [0.0554, 0.0269, 0.0107, 0.0382, 0.0149, 0.02,...              14   \n",
       "27  [0.0337, 0.0278, 0.0104, 0.0373, 0.0271, 0.017...              15   \n",
       "28  [0.0634, 0.0595, 0.0353, 0.1001, 0.0496, 0.043...              15   \n",
       "29  [0.0291, 0.0347, 0.014, 0.0611, 0.0486, 0.0201...              15   \n",
       "30  [0.3314, 0.4602, 0.4681, 0.3824, 0.5828, 0.412...              16   \n",
       "31  [0.0735, 0.0563, 0.0232, 0.0753, 0.0389, 0.033...              17   \n",
       "32  [0.0521, 0.0369, 0.0133, 0.0575, 0.0247, 0.019...              17   \n",
       "33  [0.042, 0.0181, 0.0065, 0.0254, 0.0098, 0.0141...              18   \n",
       "34  [0.0577, 0.0388, 0.0185, 0.0469, 0.0363, 0.032...              19   \n",
       "35  [0.0376, 0.0173, 0.0055, 0.025, 0.0085, 0.0119...              20   \n",
       "36  [0.0544, 0.026, 0.0101, 0.0378, 0.0128, 0.0204...              21   \n",
       "37  [0.0512, 0.0442, 0.0159, 0.0706, 0.0181, 0.021...              21   \n",
       "38  [0.0701, 0.0357, 0.0199, 0.0686, 0.0267, 0.029...              22   \n",
       "39  [0.0579, 0.0275, 0.0125, 0.0457, 0.0167, 0.023...              22   \n",
       "40  [0.073, 0.0624, 0.0193, 0.0938, 0.0557, 0.0328...              22   \n",
       "41  [0.0614, 0.0267, 0.0125, 0.0421, 0.0153, 0.023...              22   \n",
       "42  [0.0374, 0.0166, 0.0054, 0.0263, 0.0075, 0.011...              23   \n",
       "43  [0.0448, 0.0323, 0.0104, 0.0556, 0.0137, 0.014...              23   \n",
       "44  [0.056, 0.0394, 0.013, 0.0675, 0.0172, 0.016, ...              23   \n",
       "45  [0.0317, 0.015, 0.0043, 0.028, 0.0062, 0.0088,...              23   \n",
       "46  [0.116, 0.0879, 0.0577, 0.1136, 0.0577, 0.0887...              24   \n",
       "47  [0.0741, 0.0599, 0.0229, 0.06, 0.0483, 0.0391,...              24   \n",
       "48  [0.0287, 0.0145, 0.0043, 0.0163, 0.0089, 0.009...              25   \n",
       "49  [0.0633, 0.0585, 0.0193, 0.0509, 0.047, 0.0376...              25   \n",
       "50  [0.0906, 0.0665, 0.0293, 0.0843, 0.0644, 0.053...              26   \n",
       "51  [0.0527, 0.0484, 0.0223, 0.0639, 0.0539, 0.024...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1511, 0.2835, 0.1158, 0.1625, 0.1135, 0.108...   \n",
       "1                  0  [0.5224, 0.5295, 0.2598, 0.4071, 0.3232, 0.226...   \n",
       "2                  0  [0.2795, 0.3519, 0.1667, 0.186, 0.1174, 0.1284...   \n",
       "3                 22  [0.361, 0.3499, 0.2045, 0.3765, 0.5698, 0.339,...   \n",
       "4                  0  [0.0763, 0.0985, 0.065, 0.0802, 0.0902, 0.0402...   \n",
       "5                  0  [0.0717, 0.0618, 0.0806, 0.0932, 0.1037, 0.167...   \n",
       "6                  3  [0.8012, 0.6092, 0.8284, 0.6736, 0.4028, 0.709...   \n",
       "7                 23  [0.6152, 0.3029, 0.6083, 0.3621, 0.2704, 0.605...   \n",
       "8                  3  [0.4496, 0.284, 0.5952, 0.3275, 0.228, 0.4585,...   \n",
       "9                  4  [0.7933, 0.8917, 0.8447, 0.7723, 0.9248, 0.696...   \n",
       "10                 1  [0.525, 0.7931, 0.4508, 0.4485, 0.6146, 0.4435...   \n",
       "11                 3  [0.3266, 0.6099, 0.353, 0.3459, 0.6495, 0.4055...   \n",
       "12                17  [0.6828, 0.7817, 0.8599, 0.8689, 0.7461, 0.772...   \n",
       "13                23  [0.7205, 0.5744, 0.573, 0.5402, 0.7305, 0.8941...   \n",
       "14                 3  [0.4899, 0.4298, 0.5602, 0.6394, 0.5855, 0.457...   \n",
       "15                 0  [0.2059, 0.4586, 0.4189, 0.3069, 0.3857, 0.288...   \n",
       "16                22  [0.086, 0.0709, 0.0656, 0.1369, 0.1509, 0.1648...   \n",
       "17                23  [0.2908, 0.3467, 0.2905, 0.4635, 0.4691, 0.411...   \n",
       "18                10  [0.0923, 0.3214, 0.0807, 0.0983, 0.3263, 0.115...   \n",
       "19                10  [0.121, 0.4935, 0.0882, 0.2056, 0.3921, 0.131,...   \n",
       "20                 0  [0.2076, 0.3334, 0.3287, 0.4797, 0.1447, 0.224...   \n",
       "21                22  [0.1152, 0.1632, 0.142, 0.37, 0.0654, 0.1201, ...   \n",
       "22                22  [0.0979, 0.173, 0.1437, 0.2706, 0.1087, 0.2314...   \n",
       "23                 0  [0.4687, 0.4678, 0.4104, 0.3271, 0.4114, 0.490...   \n",
       "24                10  [0.0672, 0.0733, 0.094, 0.0819, 0.0765, 0.0989...   \n",
       "25                22  [0.1559, 0.1675, 0.5777, 0.3415, 0.2612, 0.316...   \n",
       "26                22  [0.0513, 0.0384, 0.2095, 0.1292, 0.0657, 0.141...   \n",
       "27                15  [0.3891, 0.4077, 0.4216, 0.3268, 0.7741, 0.514...   \n",
       "28                15  [0.7253, 0.5711, 0.6297, 0.5769, 0.9264, 0.834...   \n",
       "29                15  [0.7551, 0.5279, 0.803, 0.6914, 0.9449, 0.8207...   \n",
       "30                16  [0.9997, 0.9999, 0.9988, 0.9998, 1.0, 0.9993, ...   \n",
       "31                23  [0.633, 0.4075, 0.5462, 0.4171, 0.3513, 0.3087...   \n",
       "32                 3  [0.3927, 0.5313, 0.5831, 0.5786, 0.2813, 0.474...   \n",
       "33                 0  [0.0295, 0.0661, 0.0478, 0.0317, 0.0567, 0.058...   \n",
       "34                 0  [0.2851, 0.6125, 0.0954, 0.1759, 0.432, 0.1304...   \n",
       "35                23  [0.0693, 0.0788, 0.2144, 0.1148, 0.0549, 0.189...   \n",
       "36                23  [0.0864, 0.063, 0.1608, 0.1554, 0.0691, 0.1456...   \n",
       "37                23  [0.3394, 0.4251, 0.6188, 0.643, 0.2156, 0.8091...   \n",
       "38                22  [0.3108, 0.1478, 0.1687, 0.204, 0.1926, 0.5226...   \n",
       "39                22  [0.1522, 0.0361, 0.0485, 0.1015, 0.057, 0.112,...   \n",
       "40                23  [0.5785, 0.5348, 0.3747, 0.6651, 0.4827, 0.468...   \n",
       "41                22  [0.0548, 0.0377, 0.0479, 0.0595, 0.0655, 0.189...   \n",
       "42                23  [0.1013, 0.0641, 0.2393, 0.1632, 0.05, 0.1891,...   \n",
       "43                23  [0.5364, 0.3844, 0.5831, 0.6194, 0.2369, 0.742...   \n",
       "44                23  [0.5675, 0.3672, 0.7736, 0.5986, 0.2526, 0.917...   \n",
       "45                23  [0.2521, 0.1297, 0.3783, 0.2457, 0.0997, 0.287...   \n",
       "46                22  [0.5455, 0.3415, 0.4759, 0.4147, 0.4379, 0.462...   \n",
       "47                 0  [0.351, 0.4304, 0.2991, 0.4424, 0.2415, 0.3519...   \n",
       "48                 0  [0.0779, 0.2713, 0.0364, 0.0669, 0.2578, 0.075...   \n",
       "49                25  [0.1976, 0.8467, 0.3471, 0.3954, 0.5563, 0.323...   \n",
       "50                23  [0.3602, 0.4371, 0.6281, 0.3024, 0.184, 0.3636...   \n",
       "51                26  [0.4022, 0.4621, 0.5323, 0.4223, 0.5407, 0.323...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8489, 0.7165, 0.8842, 0.8375, 0.8865, 0.891...  0.403846  0.152041  \n",
       "1   [0.4776, 0.4705, 0.7402, 0.5929, 0.6768, 0.773...       NaN       NaN  \n",
       "2   [0.7205, 0.6481, 0.8333, 0.814, 0.8826, 0.8716...       NaN       NaN  \n",
       "3   [0.639, 0.6501, 0.7955, 0.6235, 0.4302, 0.661,...       NaN       NaN  \n",
       "4   [0.9237, 0.9015, 0.935, 0.9198, 0.9098, 0.9598...       NaN       NaN  \n",
       "5   [0.9283, 0.9382, 0.9194, 0.9068, 0.8963, 0.832...       NaN       NaN  \n",
       "6   [0.1988, 0.3908, 0.1716, 0.3264, 0.5972, 0.290...       NaN       NaN  \n",
       "7   [0.3848, 0.6971, 0.3917, 0.6379, 0.7296, 0.394...       NaN       NaN  \n",
       "8   [0.5504, 0.716, 0.4048, 0.6725, 0.772, 0.5415,...       NaN       NaN  \n",
       "9   [0.2067, 0.1083, 0.1553, 0.2277, 0.0752, 0.303...       NaN       NaN  \n",
       "10  [0.475, 0.2069, 0.5492, 0.5515, 0.3854, 0.5565...       NaN       NaN  \n",
       "11  [0.6734, 0.3901, 0.647, 0.6541, 0.3505, 0.5945...       NaN       NaN  \n",
       "12  [0.3172, 0.2183, 0.1401, 0.1311, 0.2539, 0.227...       NaN       NaN  \n",
       "13  [0.2795, 0.4256, 0.427, 0.4598, 0.2695, 0.1059...       NaN       NaN  \n",
       "14  [0.5101, 0.5702, 0.4398, 0.3606, 0.4145, 0.542...       NaN       NaN  \n",
       "15  [0.7941, 0.5414, 0.5811, 0.6931, 0.6143, 0.711...       NaN       NaN  \n",
       "16  [0.914, 0.9291, 0.9344, 0.8631, 0.8491, 0.8352...       NaN       NaN  \n",
       "17  [0.7092, 0.6533, 0.7095, 0.5365, 0.5309, 0.588...       NaN       NaN  \n",
       "18  [0.9077, 0.6786, 0.9193, 0.9017, 0.6737, 0.884...       NaN       NaN  \n",
       "19  [0.879, 0.5065, 0.9118, 0.7944, 0.6079, 0.869,...       NaN       NaN  \n",
       "20  [0.7924, 0.6666, 0.6713, 0.5203, 0.8553, 0.775...       NaN       NaN  \n",
       "21  [0.8848, 0.8368, 0.858, 0.63, 0.9346, 0.8799, ...       NaN       NaN  \n",
       "22  [0.9021, 0.827, 0.8563, 0.7294, 0.8913, 0.7686...       NaN       NaN  \n",
       "23  [0.5313, 0.5322, 0.5896, 0.6729, 0.5886, 0.509...       NaN       NaN  \n",
       "24  [0.9328, 0.9267, 0.906, 0.9181, 0.9235, 0.9011...       NaN       NaN  \n",
       "25  [0.8441, 0.8325, 0.4223, 0.6585, 0.7388, 0.683...       NaN       NaN  \n",
       "26  [0.9487, 0.9616, 0.7905, 0.8708, 0.9343, 0.858...       NaN       NaN  \n",
       "27  [0.6109, 0.5923, 0.5784, 0.6732, 0.2259, 0.485...       NaN       NaN  \n",
       "28  [0.2747, 0.4289, 0.3703, 0.4231, 0.0736, 0.165...       NaN       NaN  \n",
       "29  [0.2449, 0.4721, 0.197, 0.3086, 0.0551, 0.1793...       NaN       NaN  \n",
       "30  [0.0003, 1e-04, 0.0012, 0.0002, 0.0, 0.0007, 0...       NaN       NaN  \n",
       "31  [0.367, 0.5925, 0.4538, 0.5829, 0.6487, 0.6913...       NaN       NaN  \n",
       "32  [0.6073, 0.4687, 0.4169, 0.4214, 0.7187, 0.525...       NaN       NaN  \n",
       "33  [0.9705, 0.9339, 0.9522, 0.9683, 0.9433, 0.941...       NaN       NaN  \n",
       "34  [0.7149, 0.3875, 0.9046, 0.8241, 0.568, 0.8696...       NaN       NaN  \n",
       "35  [0.9307, 0.9212, 0.7856, 0.8852, 0.9451, 0.810...       NaN       NaN  \n",
       "36  [0.9136, 0.937, 0.8392, 0.8446, 0.9309, 0.8544...       NaN       NaN  \n",
       "37  [0.6606, 0.5749, 0.3812, 0.357, 0.7844, 0.1909...       NaN       NaN  \n",
       "38  [0.6892, 0.8522, 0.8313, 0.796, 0.8074, 0.4774...       NaN       NaN  \n",
       "39  [0.8478, 0.9639, 0.9515, 0.8985, 0.943, 0.888,...       NaN       NaN  \n",
       "40  [0.4215, 0.4652, 0.6253, 0.3349, 0.5173, 0.532...       NaN       NaN  \n",
       "41  [0.9452, 0.9623, 0.9521, 0.9405, 0.9345, 0.810...       NaN       NaN  \n",
       "42  [0.8987, 0.9359, 0.7607, 0.8368, 0.95, 0.8109,...       NaN       NaN  \n",
       "43  [0.4636, 0.6156, 0.4169, 0.3806, 0.7631, 0.257...       NaN       NaN  \n",
       "44  [0.4325, 0.6328, 0.2264, 0.4014, 0.7474, 0.083...       NaN       NaN  \n",
       "45  [0.7479, 0.8703, 0.6217, 0.7543, 0.9003, 0.712...       NaN       NaN  \n",
       "46  [0.4545, 0.6585, 0.5241, 0.5853, 0.5621, 0.538...       NaN       NaN  \n",
       "47  [0.649, 0.5696, 0.7009, 0.5576, 0.7585, 0.6481...       NaN       NaN  \n",
       "48  [0.9221, 0.7287, 0.9636, 0.9331, 0.7422, 0.924...       NaN       NaN  \n",
       "49  [0.8024, 0.1533, 0.6529, 0.6046, 0.4437, 0.676...       NaN       NaN  \n",
       "50  [0.6398, 0.5629, 0.3719, 0.6976, 0.816, 0.6364...       NaN       NaN  \n",
       "51  [0.5978, 0.5379, 0.4677, 0.5777, 0.4593, 0.676...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 : Training: loss:  0.13296013\n",
      "702 : Training: loss:  0.16127457\n",
      "703 : Training: loss:  0.15296644\n",
      "704 : Training: loss:  0.13701041\n",
      "705 : Training: loss:  0.13310117\n",
      "706 : Training: loss:  0.14402835\n",
      "707 : Training: loss:  0.13899331\n",
      "708 : Training: loss:  0.14094593\n",
      "709 : Training: loss:  0.13560416\n",
      "710 : Training: loss:  0.1361327\n",
      "711 : Training: loss:  0.13246211\n",
      "712 : Training: loss:  0.13142481\n",
      "713 : Training: loss:  0.15494691\n",
      "714 : Training: loss:  0.13223948\n",
      "715 : Training: loss:  0.1292294\n",
      "716 : Training: loss:  0.13703059\n",
      "717 : Training: loss:  0.1380307\n",
      "718 : Training: loss:  0.15996054\n",
      "719 : Training: loss:  0.15121743\n",
      "720 : Training: loss:  0.12462002\n",
      "Validation: Loss:  0.15147802  Accuracy:  0.42307693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0912, 0.0462, 0.02, 0.0564, 0.0308, 0.0364,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1519, 0.286, 0.1157, 0.1634, 0.114, 0.1087,...</td>\n",
       "      <td>[0.8481, 0.714, 0.8843, 0.8366, 0.886, 0.8913,...</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.151478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1035, 0.0752, 0.0225, 0.06, 0.059, 0.0387, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5248, 0.5345, 0.2592, 0.4084, 0.3265, 0.226...</td>\n",
       "      <td>[0.4752, 0.4655, 0.7408, 0.5916, 0.6735, 0.774...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0818, 0.0475, 0.0188, 0.0542, 0.0322, 0.036...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2811, 0.3556, 0.1665, 0.1877, 0.1183, 0.128...</td>\n",
       "      <td>[0.7189, 0.6444, 0.8335, 0.8123, 0.8817, 0.871...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0448, 0.0342, 0.012, 0.0328, 0.0293, 0.0201...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3631, 0.3532, 0.204, 0.378, 0.5738, 0.3399,...</td>\n",
       "      <td>[0.6369, 0.6468, 0.796, 0.622, 0.4262, 0.6601,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0593, 0.0283, 0.0106, 0.0339, 0.0164, 0.022...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0769, 0.101, 0.0647, 0.081, 0.0915, 0.04, 0...</td>\n",
       "      <td>[0.9231, 0.899, 0.9353, 0.919, 0.9085, 0.96, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0868, 0.0454, 0.0247, 0.0578, 0.0285, 0.042...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0717, 0.0616, 0.0806, 0.0935, 0.1042, 0.169...</td>\n",
       "      <td>[0.9283, 0.9384, 0.9194, 0.9065, 0.8958, 0.830...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0688, 0.054, 0.0151, 0.1436, 0.0433, 0.0235...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8041, 0.6128, 0.831, 0.6772, 0.4005, 0.7146...</td>\n",
       "      <td>[0.1959, 0.3872, 0.169, 0.3228, 0.5995, 0.2854...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0834, 0.0531, 0.0254, 0.1049, 0.0382, 0.042...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6187, 0.3026, 0.6105, 0.3647, 0.2695, 0.609...</td>\n",
       "      <td>[0.3813, 0.6974, 0.3895, 0.6353, 0.7305, 0.390...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1249, 0.0836, 0.0436, 0.1346, 0.0628, 0.075...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4525, 0.2862, 0.599, 0.3292, 0.2286, 0.46, ...</td>\n",
       "      <td>[0.5475, 0.7138, 0.401, 0.6708, 0.7714, 0.54, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0683, 0.1077, 0.0596, 0.0937, 0.2158, 0.071...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.7937, 0.8935, 0.8436, 0.7726, 0.9262, 0.697...</td>\n",
       "      <td>[0.2063, 0.1065, 0.1564, 0.2274, 0.0738, 0.303...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0503, 0.0595, 0.0137, 0.0423, 0.0557, 0.035...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5266, 0.7987, 0.4432, 0.4515, 0.6198, 0.446...</td>\n",
       "      <td>[0.4734, 0.2013, 0.5568, 0.5485, 0.3802, 0.553...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0445, 0.0382, 0.0155, 0.0455, 0.0382, 0.036...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3263, 0.615, 0.3505, 0.3472, 0.6532, 0.406,...</td>\n",
       "      <td>[0.6737, 0.385, 0.6495, 0.6528, 0.3468, 0.594,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0586, 0.0705, 0.0317, 0.0922, 0.0714, 0.039...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6847, 0.7844, 0.8602, 0.8697, 0.749, 0.7717...</td>\n",
       "      <td>[0.3153, 0.2156, 0.1398, 0.1303, 0.251, 0.2283...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2366, 0.1889, 0.1627, 0.236, 0.1181, 0.2089...</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7177, 0.5626, 0.5656, 0.5378, 0.7321, 0.896...</td>\n",
       "      <td>[0.2823, 0.4374, 0.4344, 0.4622, 0.2679, 0.103...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0369, 0.0279, 0.011, 0.0553, 0.0265, 0.0171...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.4935, 0.4349, 0.5598, 0.6434, 0.5875, 0.459...</td>\n",
       "      <td>[0.5065, 0.5651, 0.4402, 0.3566, 0.4125, 0.540...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0628, 0.0418, 0.0176, 0.0433, 0.0375, 0.027...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2063, 0.4675, 0.4186, 0.3085, 0.3916, 0.284...</td>\n",
       "      <td>[0.7937, 0.5325, 0.5814, 0.6915, 0.6084, 0.715...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0766, 0.0367, 0.019, 0.0503, 0.0215, 0.0348...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.086, 0.0714, 0.0654, 0.1367, 0.1513, 0.1658...</td>\n",
       "      <td>[0.914, 0.9286, 0.9346, 0.8633, 0.8487, 0.8342...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0704, 0.0466, 0.0219, 0.0604, 0.0344, 0.036...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2908, 0.3512, 0.2912, 0.4635, 0.4713, 0.412...</td>\n",
       "      <td>[0.7092, 0.6488, 0.7088, 0.5365, 0.5287, 0.587...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0753, 0.0314, 0.0155, 0.0422, 0.0179, 0.028...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0921, 0.3218, 0.0807, 0.0985, 0.329, 0.1166...</td>\n",
       "      <td>[0.9079, 0.6782, 0.9193, 0.9015, 0.671, 0.8834...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.07, 0.032, 0.0167, 0.0398, 0.0186, 0.0281, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1208, 0.4956, 0.0884, 0.2069, 0.3971, 0.131...</td>\n",
       "      <td>[0.8792, 0.5044, 0.9116, 0.7931, 0.6029, 0.868...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0858, 0.0484, 0.0218, 0.0649, 0.0318, 0.045...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2088, 0.334, 0.3312, 0.4842, 0.1437, 0.2242...</td>\n",
       "      <td>[0.7912, 0.666, 0.6688, 0.5158, 0.8563, 0.7758...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0459, 0.0206, 0.0077, 0.0328, 0.0102, 0.015...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1175, 0.1621, 0.1428, 0.374, 0.0643, 0.1224...</td>\n",
       "      <td>[0.8825, 0.8379, 0.8572, 0.626, 0.9357, 0.8776...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0585, 0.0291, 0.012, 0.0405, 0.0155, 0.0224...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0986, 0.1737, 0.1439, 0.272, 0.1084, 0.2323...</td>\n",
       "      <td>[0.9014, 0.8263, 0.8561, 0.728, 0.8916, 0.7677...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0759, 0.0426, 0.0181, 0.0555, 0.0283, 0.038...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4695, 0.4687, 0.411, 0.328, 0.4173, 0.4935,...</td>\n",
       "      <td>[0.5305, 0.5313, 0.589, 0.672, 0.5827, 0.5065,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0734, 0.0299, 0.0147, 0.0446, 0.0172, 0.027...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.067, 0.0734, 0.095, 0.0817, 0.0768, 0.1002,...</td>\n",
       "      <td>[0.933, 0.9266, 0.905, 0.9183, 0.9232, 0.8998,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0689, 0.0442, 0.0208, 0.0608, 0.0358, 0.035...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1566, 0.1697, 0.5802, 0.3423, 0.2626, 0.316...</td>\n",
       "      <td>[0.8434, 0.8303, 0.4198, 0.6577, 0.7374, 0.683...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0586, 0.0262, 0.0111, 0.0368, 0.0143, 0.020...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0518, 0.0383, 0.2109, 0.1298, 0.0657, 0.142...</td>\n",
       "      <td>[0.9482, 0.9617, 0.7891, 0.8702, 0.9343, 0.857...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0348, 0.0272, 0.0106, 0.0361, 0.0264, 0.017...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.3925, 0.4096, 0.4218, 0.3274, 0.7763, 0.520...</td>\n",
       "      <td>[0.6075, 0.5904, 0.5782, 0.6726, 0.2237, 0.479...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0633, 0.0582, 0.0355, 0.0978, 0.0482, 0.044...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7262, 0.5675, 0.6279, 0.5768, 0.9275, 0.839...</td>\n",
       "      <td>[0.2738, 0.4325, 0.3721, 0.4232, 0.0725, 0.160...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0291, 0.034, 0.014, 0.0591, 0.0477, 0.0202,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7568, 0.5277, 0.8024, 0.6922, 0.9458, 0.824...</td>\n",
       "      <td>[0.2432, 0.4723, 0.1976, 0.3078, 0.0542, 0.175...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3338, 0.4634, 0.47, 0.3813, 0.5876, 0.4185,...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9997, 0.9999, 0.9987, 0.9998, 1.0, 0.9992, ...</td>\n",
       "      <td>[0.0003, 1e-04, 0.0013, 0.0002, 0.0, 0.0008, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0753, 0.0551, 0.0237, 0.0737, 0.0378, 0.033...</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6366, 0.4076, 0.5492, 0.4179, 0.3516, 0.311...</td>\n",
       "      <td>[0.3634, 0.5924, 0.4508, 0.5821, 0.6484, 0.688...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0541, 0.0361, 0.0137, 0.0564, 0.0239, 0.019...</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.3945, 0.5344, 0.5895, 0.5816, 0.2814, 0.477...</td>\n",
       "      <td>[0.6055, 0.4656, 0.4105, 0.4184, 0.7186, 0.522...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0452, 0.0177, 0.0068, 0.0246, 0.0095, 0.014...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.03, 0.068, 0.0479, 0.0321, 0.0573, 0.0587, ...</td>\n",
       "      <td>[0.97, 0.932, 0.9521, 0.9679, 0.9427, 0.9413, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.061, 0.0383, 0.0193, 0.0458, 0.0359, 0.0331...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2905, 0.6189, 0.0951, 0.1769, 0.4371, 0.130...</td>\n",
       "      <td>[0.7095, 0.3811, 0.9049, 0.8231, 0.5629, 0.869...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0405, 0.0168, 0.0058, 0.0242, 0.0082, 0.012...</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0699, 0.0792, 0.2157, 0.1158, 0.0549, 0.191...</td>\n",
       "      <td>[0.9301, 0.9208, 0.7843, 0.8842, 0.9451, 0.808...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0579, 0.0253, 0.0105, 0.0365, 0.0123, 0.020...</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0853, 0.0629, 0.1606, 0.1555, 0.0684, 0.146...</td>\n",
       "      <td>[0.9147, 0.9371, 0.8394, 0.8445, 0.9316, 0.853...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.053, 0.0429, 0.0162, 0.0688, 0.0172, 0.0216...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3358, 0.4201, 0.6192, 0.6435, 0.2124, 0.812...</td>\n",
       "      <td>[0.6642, 0.5799, 0.3808, 0.3565, 0.7876, 0.187...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.072, 0.0341, 0.0203, 0.0663, 0.0253, 0.0292...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3148, 0.1441, 0.1662, 0.2026, 0.1889, 0.532...</td>\n",
       "      <td>[0.6852, 0.8559, 0.8338, 0.7974, 0.8111, 0.468...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0609, 0.0265, 0.0129, 0.0442, 0.0159, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1544, 0.0354, 0.0483, 0.1022, 0.0558, 0.115...</td>\n",
       "      <td>[0.8456, 0.9646, 0.9517, 0.8978, 0.9442, 0.884...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0745, 0.0607, 0.0193, 0.0916, 0.0536, 0.032...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5826, 0.5307, 0.3736, 0.6655, 0.4783, 0.473...</td>\n",
       "      <td>[0.4174, 0.4693, 0.6264, 0.3345, 0.5217, 0.526...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0651, 0.0259, 0.013, 0.0407, 0.0146, 0.0241...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0551, 0.0373, 0.0475, 0.0592, 0.0648, 0.192...</td>\n",
       "      <td>[0.9449, 0.9627, 0.9525, 0.9408, 0.9352, 0.808...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0401, 0.0162, 0.0057, 0.0255, 0.0072, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1011, 0.0646, 0.2412, 0.1643, 0.0497, 0.189...</td>\n",
       "      <td>[0.8989, 0.9354, 0.7588, 0.8357, 0.9503, 0.810...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0468, 0.0316, 0.0108, 0.0543, 0.0131, 0.015...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5372, 0.3826, 0.5839, 0.6211, 0.2363, 0.745...</td>\n",
       "      <td>[0.4628, 0.6174, 0.4161, 0.3789, 0.7637, 0.254...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0579, 0.0386, 0.0133, 0.0661, 0.0164, 0.016...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5662, 0.3641, 0.7752, 0.5991, 0.2522, 0.918...</td>\n",
       "      <td>[0.4338, 0.6359, 0.2248, 0.4009, 0.7478, 0.081...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0337, 0.0146, 0.0045, 0.0271, 0.0059, 0.008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2518, 0.1294, 0.3796, 0.2468, 0.0986, 0.289...</td>\n",
       "      <td>[0.7482, 0.8706, 0.6204, 0.7532, 0.9014, 0.710...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1176, 0.0862, 0.0589, 0.1109, 0.056, 0.0893...</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5468, 0.3399, 0.4775, 0.4143, 0.4408, 0.462...</td>\n",
       "      <td>[0.4532, 0.6601, 0.5225, 0.5857, 0.5592, 0.537...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0769, 0.0593, 0.0235, 0.0584, 0.0473, 0.039...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3536, 0.4345, 0.3025, 0.4439, 0.2431, 0.348...</td>\n",
       "      <td>[0.6464, 0.5655, 0.6975, 0.5561, 0.7569, 0.651...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0308, 0.0142, 0.0045, 0.0156, 0.0087, 0.009...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0791, 0.2794, 0.0357, 0.0669, 0.2652, 0.075...</td>\n",
       "      <td>[0.9209, 0.7206, 0.9643, 0.9331, 0.7348, 0.924...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0663, 0.0584, 0.0201, 0.0498, 0.0473, 0.039...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1987, 0.8526, 0.345, 0.3983, 0.5639, 0.3219...</td>\n",
       "      <td>[0.8013, 0.1474, 0.655, 0.6017, 0.4361, 0.6781...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0941, 0.0657, 0.03, 0.0827, 0.063, 0.0536, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3609, 0.4386, 0.6313, 0.3047, 0.1843, 0.362...</td>\n",
       "      <td>[0.6391, 0.5614, 0.3687, 0.6953, 0.8157, 0.637...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0545, 0.0477, 0.0229, 0.0624, 0.0533, 0.024...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.404, 0.4675, 0.5351, 0.4237, 0.5446, 0.3253...</td>\n",
       "      <td>[0.596, 0.5325, 0.4649, 0.5763, 0.4554, 0.6747...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.0912, 0.0462, 0.02, 0.0564, 0.0308, 0.0364,...               0   \n",
       "1   [0.1035, 0.0752, 0.0225, 0.06, 0.059, 0.0387, ...               0   \n",
       "2   [0.0818, 0.0475, 0.0188, 0.0542, 0.0322, 0.036...               0   \n",
       "3   [0.0448, 0.0342, 0.012, 0.0328, 0.0293, 0.0201...               1   \n",
       "4   [0.0593, 0.0283, 0.0106, 0.0339, 0.0164, 0.022...               1   \n",
       "5   [0.0868, 0.0454, 0.0247, 0.0578, 0.0285, 0.042...               2   \n",
       "6   [0.0688, 0.054, 0.0151, 0.1436, 0.0433, 0.0235...               3   \n",
       "7   [0.0834, 0.0531, 0.0254, 0.1049, 0.0382, 0.042...               3   \n",
       "8   [0.1249, 0.0836, 0.0436, 0.1346, 0.0628, 0.075...               3   \n",
       "9   [0.0683, 0.1077, 0.0596, 0.0937, 0.2158, 0.071...               4   \n",
       "10  [0.0503, 0.0595, 0.0137, 0.0423, 0.0557, 0.035...               4   \n",
       "11  [0.0445, 0.0382, 0.0155, 0.0455, 0.0382, 0.036...               5   \n",
       "12  [0.0586, 0.0705, 0.0317, 0.0922, 0.0714, 0.039...               6   \n",
       "13  [0.2366, 0.1889, 0.1627, 0.236, 0.1181, 0.2089...               7   \n",
       "14  [0.0369, 0.0279, 0.011, 0.0553, 0.0265, 0.0171...               8   \n",
       "15  [0.0628, 0.0418, 0.0176, 0.0433, 0.0375, 0.027...               8   \n",
       "16  [0.0766, 0.0367, 0.019, 0.0503, 0.0215, 0.0348...               9   \n",
       "17  [0.0704, 0.0466, 0.0219, 0.0604, 0.0344, 0.036...               9   \n",
       "18  [0.0753, 0.0314, 0.0155, 0.0422, 0.0179, 0.028...              10   \n",
       "19  [0.07, 0.032, 0.0167, 0.0398, 0.0186, 0.0281, ...              10   \n",
       "20  [0.0858, 0.0484, 0.0218, 0.0649, 0.0318, 0.045...              11   \n",
       "21  [0.0459, 0.0206, 0.0077, 0.0328, 0.0102, 0.015...              11   \n",
       "22  [0.0585, 0.0291, 0.012, 0.0405, 0.0155, 0.0224...              12   \n",
       "23  [0.0759, 0.0426, 0.0181, 0.0555, 0.0283, 0.038...              13   \n",
       "24  [0.0734, 0.0299, 0.0147, 0.0446, 0.0172, 0.027...              13   \n",
       "25  [0.0689, 0.0442, 0.0208, 0.0608, 0.0358, 0.035...              14   \n",
       "26  [0.0586, 0.0262, 0.0111, 0.0368, 0.0143, 0.020...              14   \n",
       "27  [0.0348, 0.0272, 0.0106, 0.0361, 0.0264, 0.017...              15   \n",
       "28  [0.0633, 0.0582, 0.0355, 0.0978, 0.0482, 0.044...              15   \n",
       "29  [0.0291, 0.034, 0.014, 0.0591, 0.0477, 0.0202,...              15   \n",
       "30  [0.3338, 0.4634, 0.47, 0.3813, 0.5876, 0.4185,...              16   \n",
       "31  [0.0753, 0.0551, 0.0237, 0.0737, 0.0378, 0.033...              17   \n",
       "32  [0.0541, 0.0361, 0.0137, 0.0564, 0.0239, 0.019...              17   \n",
       "33  [0.0452, 0.0177, 0.0068, 0.0246, 0.0095, 0.014...              18   \n",
       "34  [0.061, 0.0383, 0.0193, 0.0458, 0.0359, 0.0331...              19   \n",
       "35  [0.0405, 0.0168, 0.0058, 0.0242, 0.0082, 0.012...              20   \n",
       "36  [0.0579, 0.0253, 0.0105, 0.0365, 0.0123, 0.020...              21   \n",
       "37  [0.053, 0.0429, 0.0162, 0.0688, 0.0172, 0.0216...              21   \n",
       "38  [0.072, 0.0341, 0.0203, 0.0663, 0.0253, 0.0292...              22   \n",
       "39  [0.0609, 0.0265, 0.0129, 0.0442, 0.0159, 0.023...              22   \n",
       "40  [0.0745, 0.0607, 0.0193, 0.0916, 0.0536, 0.032...              22   \n",
       "41  [0.0651, 0.0259, 0.013, 0.0407, 0.0146, 0.0241...              22   \n",
       "42  [0.0401, 0.0162, 0.0057, 0.0255, 0.0072, 0.011...              23   \n",
       "43  [0.0468, 0.0316, 0.0108, 0.0543, 0.0131, 0.015...              23   \n",
       "44  [0.0579, 0.0386, 0.0133, 0.0661, 0.0164, 0.016...              23   \n",
       "45  [0.0337, 0.0146, 0.0045, 0.0271, 0.0059, 0.008...              23   \n",
       "46  [0.1176, 0.0862, 0.0589, 0.1109, 0.056, 0.0893...              24   \n",
       "47  [0.0769, 0.0593, 0.0235, 0.0584, 0.0473, 0.039...              24   \n",
       "48  [0.0308, 0.0142, 0.0045, 0.0156, 0.0087, 0.009...              25   \n",
       "49  [0.0663, 0.0584, 0.0201, 0.0498, 0.0473, 0.039...              25   \n",
       "50  [0.0941, 0.0657, 0.03, 0.0827, 0.063, 0.0536, ...              26   \n",
       "51  [0.0545, 0.0477, 0.0229, 0.0624, 0.0533, 0.024...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1519, 0.286, 0.1157, 0.1634, 0.114, 0.1087,...   \n",
       "1                  0  [0.5248, 0.5345, 0.2592, 0.4084, 0.3265, 0.226...   \n",
       "2                  0  [0.2811, 0.3556, 0.1665, 0.1877, 0.1183, 0.128...   \n",
       "3                  0  [0.3631, 0.3532, 0.204, 0.378, 0.5738, 0.3399,...   \n",
       "4                  0  [0.0769, 0.101, 0.0647, 0.081, 0.0915, 0.04, 0...   \n",
       "5                  0  [0.0717, 0.0616, 0.0806, 0.0935, 0.1042, 0.169...   \n",
       "6                  3  [0.8041, 0.6128, 0.831, 0.6772, 0.4005, 0.7146...   \n",
       "7                 22  [0.6187, 0.3026, 0.6105, 0.3647, 0.2695, 0.609...   \n",
       "8                  3  [0.4525, 0.2862, 0.599, 0.3292, 0.2286, 0.46, ...   \n",
       "9                  4  [0.7937, 0.8935, 0.8436, 0.7726, 0.9262, 0.697...   \n",
       "10                 8  [0.5266, 0.7987, 0.4432, 0.4515, 0.6198, 0.446...   \n",
       "11                 3  [0.3263, 0.615, 0.3505, 0.3472, 0.6532, 0.406,...   \n",
       "12                17  [0.6847, 0.7844, 0.8602, 0.8697, 0.749, 0.7717...   \n",
       "13                22  [0.7177, 0.5626, 0.5656, 0.5378, 0.7321, 0.896...   \n",
       "14                 3  [0.4935, 0.4349, 0.5598, 0.6434, 0.5875, 0.459...   \n",
       "15                 0  [0.2063, 0.4675, 0.4186, 0.3085, 0.3916, 0.284...   \n",
       "16                10  [0.086, 0.0714, 0.0654, 0.1367, 0.1513, 0.1658...   \n",
       "17                 0  [0.2908, 0.3512, 0.2912, 0.4635, 0.4713, 0.412...   \n",
       "18                10  [0.0921, 0.3218, 0.0807, 0.0985, 0.329, 0.1166...   \n",
       "19                10  [0.1208, 0.4956, 0.0884, 0.2069, 0.3971, 0.131...   \n",
       "20                 0  [0.2088, 0.334, 0.3312, 0.4842, 0.1437, 0.2242...   \n",
       "21                22  [0.1175, 0.1621, 0.1428, 0.374, 0.0643, 0.1224...   \n",
       "22                22  [0.0986, 0.1737, 0.1439, 0.272, 0.1084, 0.2323...   \n",
       "23                 0  [0.4695, 0.4687, 0.411, 0.328, 0.4173, 0.4935,...   \n",
       "24                10  [0.067, 0.0734, 0.095, 0.0817, 0.0768, 0.1002,...   \n",
       "25                22  [0.1566, 0.1697, 0.5802, 0.3423, 0.2626, 0.316...   \n",
       "26                22  [0.0518, 0.0383, 0.2109, 0.1298, 0.0657, 0.142...   \n",
       "27                15  [0.3925, 0.4096, 0.4218, 0.3274, 0.7763, 0.520...   \n",
       "28                15  [0.7262, 0.5675, 0.6279, 0.5768, 0.9275, 0.839...   \n",
       "29                15  [0.7568, 0.5277, 0.8024, 0.6922, 0.9458, 0.824...   \n",
       "30                16  [0.9997, 0.9999, 0.9987, 0.9998, 1.0, 0.9992, ...   \n",
       "31                22  [0.6366, 0.4076, 0.5492, 0.4179, 0.3516, 0.311...   \n",
       "32                 3  [0.3945, 0.5344, 0.5895, 0.5816, 0.2814, 0.477...   \n",
       "33                10  [0.03, 0.068, 0.0479, 0.0321, 0.0573, 0.0587, ...   \n",
       "34                 0  [0.2905, 0.6189, 0.0951, 0.1769, 0.4371, 0.130...   \n",
       "35                22  [0.0699, 0.0792, 0.2157, 0.1158, 0.0549, 0.191...   \n",
       "36                22  [0.0853, 0.0629, 0.1606, 0.1555, 0.0684, 0.146...   \n",
       "37                23  [0.3358, 0.4201, 0.6192, 0.6435, 0.2124, 0.812...   \n",
       "38                22  [0.3148, 0.1441, 0.1662, 0.2026, 0.1889, 0.532...   \n",
       "39                22  [0.1544, 0.0354, 0.0483, 0.1022, 0.0558, 0.115...   \n",
       "40                22  [0.5826, 0.5307, 0.3736, 0.6655, 0.4783, 0.473...   \n",
       "41                22  [0.0551, 0.0373, 0.0475, 0.0592, 0.0648, 0.192...   \n",
       "42                23  [0.1011, 0.0646, 0.2412, 0.1643, 0.0497, 0.189...   \n",
       "43                23  [0.5372, 0.3826, 0.5839, 0.6211, 0.2363, 0.745...   \n",
       "44                23  [0.5662, 0.3641, 0.7752, 0.5991, 0.2522, 0.918...   \n",
       "45                23  [0.2518, 0.1294, 0.3796, 0.2468, 0.0986, 0.289...   \n",
       "46                22  [0.5468, 0.3399, 0.4775, 0.4143, 0.4408, 0.462...   \n",
       "47                 0  [0.3536, 0.4345, 0.3025, 0.4439, 0.2431, 0.348...   \n",
       "48                 0  [0.0791, 0.2794, 0.0357, 0.0669, 0.2652, 0.075...   \n",
       "49                25  [0.1987, 0.8526, 0.345, 0.3983, 0.5639, 0.3219...   \n",
       "50                23  [0.3609, 0.4386, 0.6313, 0.3047, 0.1843, 0.362...   \n",
       "51                26  [0.404, 0.4675, 0.5351, 0.4237, 0.5446, 0.3253...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8481, 0.714, 0.8843, 0.8366, 0.886, 0.8913,...  0.423077  0.151478  \n",
       "1   [0.4752, 0.4655, 0.7408, 0.5916, 0.6735, 0.774...       NaN       NaN  \n",
       "2   [0.7189, 0.6444, 0.8335, 0.8123, 0.8817, 0.871...       NaN       NaN  \n",
       "3   [0.6369, 0.6468, 0.796, 0.622, 0.4262, 0.6601,...       NaN       NaN  \n",
       "4   [0.9231, 0.899, 0.9353, 0.919, 0.9085, 0.96, 0...       NaN       NaN  \n",
       "5   [0.9283, 0.9384, 0.9194, 0.9065, 0.8958, 0.830...       NaN       NaN  \n",
       "6   [0.1959, 0.3872, 0.169, 0.3228, 0.5995, 0.2854...       NaN       NaN  \n",
       "7   [0.3813, 0.6974, 0.3895, 0.6353, 0.7305, 0.390...       NaN       NaN  \n",
       "8   [0.5475, 0.7138, 0.401, 0.6708, 0.7714, 0.54, ...       NaN       NaN  \n",
       "9   [0.2063, 0.1065, 0.1564, 0.2274, 0.0738, 0.303...       NaN       NaN  \n",
       "10  [0.4734, 0.2013, 0.5568, 0.5485, 0.3802, 0.553...       NaN       NaN  \n",
       "11  [0.6737, 0.385, 0.6495, 0.6528, 0.3468, 0.594,...       NaN       NaN  \n",
       "12  [0.3153, 0.2156, 0.1398, 0.1303, 0.251, 0.2283...       NaN       NaN  \n",
       "13  [0.2823, 0.4374, 0.4344, 0.4622, 0.2679, 0.103...       NaN       NaN  \n",
       "14  [0.5065, 0.5651, 0.4402, 0.3566, 0.4125, 0.540...       NaN       NaN  \n",
       "15  [0.7937, 0.5325, 0.5814, 0.6915, 0.6084, 0.715...       NaN       NaN  \n",
       "16  [0.914, 0.9286, 0.9346, 0.8633, 0.8487, 0.8342...       NaN       NaN  \n",
       "17  [0.7092, 0.6488, 0.7088, 0.5365, 0.5287, 0.587...       NaN       NaN  \n",
       "18  [0.9079, 0.6782, 0.9193, 0.9015, 0.671, 0.8834...       NaN       NaN  \n",
       "19  [0.8792, 0.5044, 0.9116, 0.7931, 0.6029, 0.868...       NaN       NaN  \n",
       "20  [0.7912, 0.666, 0.6688, 0.5158, 0.8563, 0.7758...       NaN       NaN  \n",
       "21  [0.8825, 0.8379, 0.8572, 0.626, 0.9357, 0.8776...       NaN       NaN  \n",
       "22  [0.9014, 0.8263, 0.8561, 0.728, 0.8916, 0.7677...       NaN       NaN  \n",
       "23  [0.5305, 0.5313, 0.589, 0.672, 0.5827, 0.5065,...       NaN       NaN  \n",
       "24  [0.933, 0.9266, 0.905, 0.9183, 0.9232, 0.8998,...       NaN       NaN  \n",
       "25  [0.8434, 0.8303, 0.4198, 0.6577, 0.7374, 0.683...       NaN       NaN  \n",
       "26  [0.9482, 0.9617, 0.7891, 0.8702, 0.9343, 0.857...       NaN       NaN  \n",
       "27  [0.6075, 0.5904, 0.5782, 0.6726, 0.2237, 0.479...       NaN       NaN  \n",
       "28  [0.2738, 0.4325, 0.3721, 0.4232, 0.0725, 0.160...       NaN       NaN  \n",
       "29  [0.2432, 0.4723, 0.1976, 0.3078, 0.0542, 0.175...       NaN       NaN  \n",
       "30  [0.0003, 1e-04, 0.0013, 0.0002, 0.0, 0.0008, 0...       NaN       NaN  \n",
       "31  [0.3634, 0.5924, 0.4508, 0.5821, 0.6484, 0.688...       NaN       NaN  \n",
       "32  [0.6055, 0.4656, 0.4105, 0.4184, 0.7186, 0.522...       NaN       NaN  \n",
       "33  [0.97, 0.932, 0.9521, 0.9679, 0.9427, 0.9413, ...       NaN       NaN  \n",
       "34  [0.7095, 0.3811, 0.9049, 0.8231, 0.5629, 0.869...       NaN       NaN  \n",
       "35  [0.9301, 0.9208, 0.7843, 0.8842, 0.9451, 0.808...       NaN       NaN  \n",
       "36  [0.9147, 0.9371, 0.8394, 0.8445, 0.9316, 0.853...       NaN       NaN  \n",
       "37  [0.6642, 0.5799, 0.3808, 0.3565, 0.7876, 0.187...       NaN       NaN  \n",
       "38  [0.6852, 0.8559, 0.8338, 0.7974, 0.8111, 0.468...       NaN       NaN  \n",
       "39  [0.8456, 0.9646, 0.9517, 0.8978, 0.9442, 0.884...       NaN       NaN  \n",
       "40  [0.4174, 0.4693, 0.6264, 0.3345, 0.5217, 0.526...       NaN       NaN  \n",
       "41  [0.9449, 0.9627, 0.9525, 0.9408, 0.9352, 0.808...       NaN       NaN  \n",
       "42  [0.8989, 0.9354, 0.7588, 0.8357, 0.9503, 0.810...       NaN       NaN  \n",
       "43  [0.4628, 0.6174, 0.4161, 0.3789, 0.7637, 0.254...       NaN       NaN  \n",
       "44  [0.4338, 0.6359, 0.2248, 0.4009, 0.7478, 0.081...       NaN       NaN  \n",
       "45  [0.7482, 0.8706, 0.6204, 0.7532, 0.9014, 0.710...       NaN       NaN  \n",
       "46  [0.4532, 0.6601, 0.5225, 0.5857, 0.5592, 0.537...       NaN       NaN  \n",
       "47  [0.6464, 0.5655, 0.6975, 0.5561, 0.7569, 0.651...       NaN       NaN  \n",
       "48  [0.9209, 0.7206, 0.9643, 0.9331, 0.7348, 0.924...       NaN       NaN  \n",
       "49  [0.8013, 0.1474, 0.655, 0.6017, 0.4361, 0.6781...       NaN       NaN  \n",
       "50  [0.6391, 0.5614, 0.3687, 0.6953, 0.8157, 0.637...       NaN       NaN  \n",
       "51  [0.596, 0.5325, 0.4649, 0.5763, 0.4554, 0.6747...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721 : Training: loss:  0.13931638\n",
      "722 : Training: loss:  0.12682764\n",
      "723 : Training: loss:  0.13480993\n",
      "724 : Training: loss:  0.13903575\n",
      "725 : Training: loss:  0.1315113\n",
      "726 : Training: loss:  0.13557424\n",
      "727 : Training: loss:  0.14992088\n",
      "728 : Training: loss:  0.13047543\n",
      "729 : Training: loss:  0.13831913\n",
      "730 : Training: loss:  0.16530068\n",
      "731 : Training: loss:  0.13901034\n",
      "732 : Training: loss:  0.1431861\n",
      "733 : Training: loss:  0.1234145\n",
      "734 : Training: loss:  0.14505105\n",
      "735 : Training: loss:  0.13370508\n",
      "736 : Training: loss:  0.1466032\n",
      "737 : Training: loss:  0.14963904\n",
      "738 : Training: loss:  0.12826364\n",
      "739 : Training: loss:  0.13963924\n",
      "740 : Training: loss:  0.15826073\n",
      "Validation: Loss:  0.15063451  Accuracy:  0.40384614\n",
      "741 : Training: loss:  0.15032108\n",
      "742 : Training: loss:  0.1416218\n",
      "743 : Training: loss:  0.12161595\n",
      "744 : Training: loss:  0.11908402\n",
      "745 : Training: loss:  0.1437131\n",
      "746 : Training: loss:  0.11628543\n",
      "747 : Training: loss:  0.14092956\n",
      "748 : Training: loss:  0.13653778\n",
      "749 : Training: loss:  0.13778746\n",
      "750 : Training: loss:  0.1444106\n",
      "751 : Training: loss:  0.12973002\n",
      "752 : Training: loss:  0.14734878\n",
      "753 : Training: loss:  0.11747284\n",
      "754 : Training: loss:  0.14883623\n",
      "755 : Training: loss:  0.14301468\n",
      "756 : Training: loss:  0.13682058\n",
      "757 : Training: loss:  0.14322822\n",
      "758 : Training: loss:  0.1479316\n",
      "759 : Training: loss:  0.12263705\n",
      "760 : Training: loss:  0.13224186\n",
      "Validation: Loss:  0.14957236  Accuracy:  0.34615386\n",
      "761 : Training: loss:  0.14237386\n",
      "762 : Training: loss:  0.12194368\n",
      "763 : Training: loss:  0.13726072\n",
      "764 : Training: loss:  0.12818207\n",
      "765 : Training: loss:  0.12831463\n",
      "766 : Training: loss:  0.15975818\n",
      "767 : Training: loss:  0.11787298\n",
      "768 : Training: loss:  0.13707754\n",
      "769 : Training: loss:  0.13303316\n",
      "770 : Training: loss:  0.12393493\n",
      "771 : Training: loss:  0.13962089\n",
      "772 : Training: loss:  0.14037818\n",
      "773 : Training: loss:  0.1327097\n",
      "774 : Training: loss:  0.16163537\n",
      "775 : Training: loss:  0.14303273\n",
      "776 : Training: loss:  0.1364775\n",
      "777 : Training: loss:  0.13112845\n",
      "778 : Training: loss:  0.13179757\n",
      "779 : Training: loss:  0.114966206\n",
      "780 : Training: loss:  0.15429375\n",
      "Validation: Loss:  0.14864153  Accuracy:  0.3846154\n",
      "781 : Training: loss:  0.11952133\n",
      "782 : Training: loss:  0.1421575\n",
      "783 : Training: loss:  0.13050406\n",
      "784 : Training: loss:  0.16052566\n",
      "785 : Training: loss:  0.14118971\n",
      "786 : Training: loss:  0.1452669\n",
      "787 : Training: loss:  0.13417123\n",
      "788 : Training: loss:  0.13612348\n",
      "789 : Training: loss:  0.1761323\n",
      "790 : Training: loss:  0.14150706\n",
      "791 : Training: loss:  0.11451646\n",
      "792 : Training: loss:  0.12525798\n",
      "793 : Training: loss:  0.13060051\n",
      "794 : Training: loss:  0.14817184\n",
      "795 : Training: loss:  0.13367584\n",
      "796 : Training: loss:  0.11963755\n",
      "797 : Training: loss:  0.13004172\n",
      "798 : Training: loss:  0.13627338\n",
      "799 : Training: loss:  0.14171602\n",
      "800 : Training: loss:  0.1372556\n",
      "Validation: Loss:  0.14773966  Accuracy:  0.40384614\n",
      "801 : Training: loss:  0.12424188\n",
      "802 : Training: loss:  0.15204361\n",
      "803 : Training: loss:  0.12425941\n",
      "804 : Training: loss:  0.13740705\n",
      "805 : Training: loss:  0.13500413\n",
      "806 : Training: loss:  0.11447725\n",
      "807 : Training: loss:  0.12890323\n",
      "808 : Training: loss:  0.13000561\n",
      "809 : Training: loss:  0.1299991\n",
      "810 : Training: loss:  0.15226828\n",
      "811 : Training: loss:  0.1429097\n",
      "812 : Training: loss:  0.1288201\n",
      "813 : Training: loss:  0.12069642\n",
      "814 : Training: loss:  0.12410998\n",
      "815 : Training: loss:  0.12631647\n",
      "816 : Training: loss:  0.14356937\n",
      "817 : Training: loss:  0.12976336\n",
      "818 : Training: loss:  0.10804702\n",
      "819 : Training: loss:  0.13228749\n",
      "820 : Training: loss:  0.12721536\n",
      "Validation: Loss:  0.14663455  Accuracy:  0.3846154\n",
      "821 : Training: loss:  0.12566704\n",
      "822 : Training: loss:  0.1295049\n",
      "823 : Training: loss:  0.12379147\n",
      "824 : Training: loss:  0.14743398\n",
      "825 : Training: loss:  0.121158935\n",
      "826 : Training: loss:  0.14005479\n",
      "827 : Training: loss:  0.1173296\n",
      "828 : Training: loss:  0.14818342\n",
      "829 : Training: loss:  0.14545488\n",
      "830 : Training: loss:  0.13347146\n",
      "831 : Training: loss:  0.13998455\n",
      "832 : Training: loss:  0.12990777\n",
      "833 : Training: loss:  0.12305769\n",
      "834 : Training: loss:  0.13774289\n",
      "835 : Training: loss:  0.14292999\n",
      "836 : Training: loss:  0.1382324\n",
      "837 : Training: loss:  0.14782736\n",
      "838 : Training: loss:  0.13235724\n",
      "839 : Training: loss:  0.14167301\n",
      "840 : Training: loss:  0.13038856\n",
      "Validation: Loss:  0.14582324  Accuracy:  0.3846154\n",
      "841 : Training: loss:  0.13684794\n",
      "842 : Training: loss:  0.12604424\n",
      "843 : Training: loss:  0.12825836\n",
      "844 : Training: loss:  0.12183588\n",
      "845 : Training: loss:  0.13060601\n",
      "846 : Training: loss:  0.12810266\n",
      "847 : Training: loss:  0.11507511\n",
      "848 : Training: loss:  0.13026287\n",
      "849 : Training: loss:  0.1227431\n",
      "850 : Training: loss:  0.14403425\n",
      "851 : Training: loss:  0.1273081\n",
      "852 : Training: loss:  0.14513208\n",
      "853 : Training: loss:  0.14012301\n",
      "854 : Training: loss:  0.12094559\n",
      "855 : Training: loss:  0.14642568\n",
      "856 : Training: loss:  0.16813257\n",
      "857 : Training: loss:  0.121809475\n",
      "858 : Training: loss:  0.12858683\n",
      "859 : Training: loss:  0.13994743\n",
      "860 : Training: loss:  0.1553517\n",
      "Validation: Loss:  0.1448915  Accuracy:  0.3846154\n",
      "861 : Training: loss:  0.12105398\n",
      "862 : Training: loss:  0.13392754\n",
      "863 : Training: loss:  0.1367566\n",
      "864 : Training: loss:  0.1384765\n",
      "865 : Training: loss:  0.11069124\n",
      "866 : Training: loss:  0.1430044\n",
      "867 : Training: loss:  0.14581726\n",
      "868 : Training: loss:  0.14146584\n",
      "869 : Training: loss:  0.14327796\n",
      "870 : Training: loss:  0.14622252\n",
      "871 : Training: loss:  0.14816082\n",
      "872 : Training: loss:  0.14183258\n",
      "873 : Training: loss:  0.13509418\n",
      "874 : Training: loss:  0.12894082\n",
      "875 : Training: loss:  0.13954127\n",
      "876 : Training: loss:  0.14556585\n",
      "877 : Training: loss:  0.12494878\n",
      "878 : Training: loss:  0.13814847\n",
      "879 : Training: loss:  0.14383031\n",
      "880 : Training: loss:  0.12341019\n",
      "Validation: Loss:  0.1437442  Accuracy:  0.46153846\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1071, 0.0456, 0.0231, 0.0546, 0.0297, 0.036...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1636, 0.3045, 0.1154, 0.1774, 0.1247, 0.104...</td>\n",
       "      <td>[0.8364, 0.6955, 0.8846, 0.8226, 0.8753, 0.895...</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.143744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1215, 0.0861, 0.0242, 0.0588, 0.0674, 0.039...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.551, 0.5674, 0.2633, 0.4378, 0.3613, 0.2189...</td>\n",
       "      <td>[0.449, 0.4326, 0.7367, 0.5622, 0.6387, 0.7811...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0931, 0.0479, 0.0209, 0.0538, 0.032, 0.0376...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3091, 0.3833, 0.1693, 0.2143, 0.1303, 0.126...</td>\n",
       "      <td>[0.6909, 0.6167, 0.8307, 0.7857, 0.8697, 0.873...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0456, 0.0367, 0.0132, 0.0302, 0.0332, 0.020...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4011, 0.3763, 0.2037, 0.4128, 0.6147, 0.349...</td>\n",
       "      <td>[0.5989, 0.6237, 0.7963, 0.5872, 0.3853, 0.650...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0702, 0.0283, 0.013, 0.0315, 0.0158, 0.0231...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0822, 0.1124, 0.0617, 0.0915, 0.1022, 0.039...</td>\n",
       "      <td>[0.9178, 0.8876, 0.9383, 0.9085, 0.8978, 0.960...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0956, 0.0423, 0.0296, 0.0545, 0.0258, 0.043...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0777, 0.059, 0.088, 0.0969, 0.1126, 0.1937,...</td>\n",
       "      <td>[0.9223, 0.941, 0.912, 0.9031, 0.8874, 0.8063,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0521, 0.045, 0.0109, 0.1752, 0.033, 0.0157,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8374, 0.6377, 0.8609, 0.7369, 0.3732, 0.771...</td>\n",
       "      <td>[0.1626, 0.3623, 0.1391, 0.2631, 0.6268, 0.228...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0755, 0.046, 0.0233, 0.1147, 0.0301, 0.0342...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6641, 0.3005, 0.6579, 0.4068, 0.26, 0.6656,...</td>\n",
       "      <td>[0.3359, 0.6995, 0.3421, 0.5932, 0.74, 0.3344,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1175, 0.0732, 0.0407, 0.141, 0.0524, 0.065,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.497, 0.3004, 0.6593, 0.3582, 0.2315, 0.5028...</td>\n",
       "      <td>[0.503, 0.6996, 0.3407, 0.6418, 0.7685, 0.4972...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0488, 0.1109, 0.0488, 0.0833, 0.2617, 0.067...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8109, 0.9102, 0.8395, 0.7896, 0.939, 0.698,...</td>\n",
       "      <td>[0.1891, 0.0898, 0.1605, 0.2104, 0.061, 0.302,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0482, 0.0726, 0.0143, 0.0371, 0.0741, 0.040...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.541, 0.8286, 0.4022, 0.4823, 0.6637, 0.4652...</td>\n",
       "      <td>[0.459, 0.1714, 0.5978, 0.5177, 0.3363, 0.5348...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0402, 0.0397, 0.0163, 0.042, 0.044, 0.0401,...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.349, 0.6488, 0.3381, 0.3681, 0.687, 0.4193,...</td>\n",
       "      <td>[0.651, 0.3512, 0.6619, 0.6319, 0.313, 0.5807,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.041, 0.0632, 0.027, 0.0773, 0.0611, 0.0313,...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6948, 0.7672, 0.8615, 0.8797, 0.7488, 0.762...</td>\n",
       "      <td>[0.3052, 0.2328, 0.1385, 0.1203, 0.2512, 0.237...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2092, 0.1623, 0.1487, 0.2213, 0.0886, 0.185...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7065, 0.4687, 0.5362, 0.5207, 0.727, 0.9118...</td>\n",
       "      <td>[0.2935, 0.5313, 0.4638, 0.4793, 0.273, 0.0882...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0318, 0.0258, 0.01, 0.0545, 0.0264, 0.0143,...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5355, 0.4691, 0.5678, 0.689, 0.6041, 0.4643...</td>\n",
       "      <td>[0.4645, 0.5309, 0.4322, 0.311, 0.3959, 0.5357...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0642, 0.0416, 0.0181, 0.0389, 0.039, 0.0259...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.216, 0.5202, 0.4153, 0.3321, 0.4261, 0.2472...</td>\n",
       "      <td>[0.784, 0.4798, 0.5847, 0.6679, 0.5739, 0.7528...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0851, 0.0335, 0.023, 0.0472, 0.019, 0.0356,...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0901, 0.0728, 0.0707, 0.1408, 0.1566, 0.181...</td>\n",
       "      <td>[0.9099, 0.9272, 0.9293, 0.8592, 0.8434, 0.818...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0689, 0.044, 0.0229, 0.058, 0.0322, 0.0347,...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3031, 0.3665, 0.3178, 0.4818, 0.4793, 0.437...</td>\n",
       "      <td>[0.6969, 0.6335, 0.6822, 0.5182, 0.5207, 0.562...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0893, 0.0289, 0.0193, 0.0397, 0.0162, 0.029...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0984, 0.336, 0.0823, 0.0984, 0.3507, 0.1261...</td>\n",
       "      <td>[0.9016, 0.664, 0.9177, 0.9016, 0.6493, 0.8739...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0808, 0.0293, 0.0205, 0.0363, 0.0171, 0.029...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1288, 0.5197, 0.0914, 0.213, 0.4295, 0.131,...</td>\n",
       "      <td>[0.8712, 0.4803, 0.9086, 0.787, 0.5705, 0.869,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0901, 0.0456, 0.0234, 0.0638, 0.0284, 0.042...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2316, 0.332, 0.3589, 0.5288, 0.1443, 0.2247...</td>\n",
       "      <td>[0.7684, 0.668, 0.6411, 0.4712, 0.8557, 0.7753...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.052, 0.0188, 0.0095, 0.0327, 0.0088, 0.0152...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1441, 0.1506, 0.1572, 0.4151, 0.0599, 0.144...</td>\n",
       "      <td>[0.8559, 0.8494, 0.8428, 0.5849, 0.9401, 0.855...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0645, 0.0264, 0.0139, 0.0384, 0.0132, 0.021...</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.119, 0.1714, 0.1512, 0.2936, 0.1062, 0.2473...</td>\n",
       "      <td>[0.881, 0.8286, 0.8488, 0.7064, 0.8938, 0.7527...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0773, 0.0412, 0.0201, 0.0545, 0.0278, 0.039...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5087, 0.4877, 0.4551, 0.3367, 0.4607, 0.513...</td>\n",
       "      <td>[0.4913, 0.5123, 0.5449, 0.6633, 0.5393, 0.486...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0858, 0.0268, 0.0178, 0.0424, 0.0151, 0.027...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0758, 0.0763, 0.1142, 0.0816, 0.0787, 0.117...</td>\n",
       "      <td>[0.9242, 0.9237, 0.8858, 0.9184, 0.9213, 0.882...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0659, 0.0402, 0.0215, 0.0567, 0.0337, 0.031...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.175, 0.1858, 0.6155, 0.3709, 0.2845, 0.3375...</td>\n",
       "      <td>[0.825, 0.8142, 0.3845, 0.6291, 0.7155, 0.6625...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0653, 0.0229, 0.0129, 0.033, 0.0119, 0.0189...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0597, 0.0361, 0.2316, 0.1365, 0.0663, 0.155...</td>\n",
       "      <td>[0.9403, 0.9639, 0.7684, 0.8635, 0.9337, 0.844...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0283, 0.0246, 0.0105, 0.0328, 0.0258, 0.015...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.439, 0.4281, 0.4462, 0.3518, 0.8076, 0.5783...</td>\n",
       "      <td>[0.561, 0.5719, 0.5538, 0.6482, 0.1924, 0.4217...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0447, 0.0499, 0.0305, 0.0905, 0.0398, 0.035...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7425, 0.533, 0.6445, 0.5903, 0.935, 0.8762,...</td>\n",
       "      <td>[0.2575, 0.467, 0.3555, 0.4097, 0.065, 0.1238,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0191, 0.0309, 0.0113, 0.0529, 0.0464, 0.016...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7821, 0.5195, 0.8152, 0.7124, 0.9549, 0.863...</td>\n",
       "      <td>[0.2179, 0.4805, 0.1848, 0.2876, 0.0451, 0.136...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3274, 0.5048, 0.4737, 0.3741, 0.643, 0.448,...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9999, 0.9978, 0.9997, 1.0, 0.9985, ...</td>\n",
       "      <td>[0.0002, 1e-04, 0.0022, 0.0003, 0.0, 0.0015, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0639, 0.0489, 0.0226, 0.0714, 0.0327, 0.028...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.678, 0.3968, 0.5974, 0.4475, 0.3514, 0.3549...</td>\n",
       "      <td>[0.322, 0.6032, 0.4026, 0.5525, 0.6486, 0.6451...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0472, 0.0312, 0.0127, 0.0573, 0.0197, 0.015...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4372, 0.5371, 0.6543, 0.6337, 0.2724, 0.519...</td>\n",
       "      <td>[0.5628, 0.4629, 0.3457, 0.3663, 0.7276, 0.480...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.055, 0.0163, 0.0087, 0.0231, 0.0084, 0.0147...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.036, 0.0773, 0.0498, 0.0366, 0.0622, 0.0634...</td>\n",
       "      <td>[0.964, 0.9227, 0.9502, 0.9634, 0.9378, 0.9366...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0655, 0.0383, 0.0231, 0.0448, 0.0397, 0.035...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3488, 0.6754, 0.0975, 0.1989, 0.4971, 0.126...</td>\n",
       "      <td>[0.6512, 0.3246, 0.9025, 0.8011, 0.5029, 0.873...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0474, 0.0153, 0.007, 0.0227, 0.007, 0.0118,...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0821, 0.079, 0.2366, 0.1281, 0.058, 0.2141,...</td>\n",
       "      <td>[0.9179, 0.921, 0.7634, 0.8719, 0.942, 0.7859,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0662, 0.0231, 0.0129, 0.0347, 0.0105, 0.020...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0859, 0.0591, 0.1664, 0.1602, 0.0596, 0.159...</td>\n",
       "      <td>[0.9141, 0.9409, 0.8336, 0.8398, 0.9404, 0.840...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0463, 0.0357, 0.0155, 0.0666, 0.0123, 0.016...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3355, 0.3486, 0.6344, 0.6558, 0.1608, 0.849...</td>\n",
       "      <td>[0.6645, 0.6514, 0.3656, 0.3442, 0.8392, 0.150...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0666, 0.0249, 0.0201, 0.0615, 0.0181, 0.022...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.367, 0.1265, 0.1546, 0.2057, 0.1742, 0.6057...</td>\n",
       "      <td>[0.633, 0.8735, 0.8454, 0.7943, 0.8258, 0.3943...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0651, 0.0216, 0.0143, 0.0406, 0.0126, 0.020...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1844, 0.0326, 0.0478, 0.1123, 0.0504, 0.145...</td>\n",
       "      <td>[0.8156, 0.9674, 0.9522, 0.8877, 0.9496, 0.854...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0621, 0.0531, 0.0164, 0.0911, 0.0459, 0.025...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6288, 0.5097, 0.3666, 0.6931, 0.4615, 0.520...</td>\n",
       "      <td>[0.3712, 0.4903, 0.6334, 0.3069, 0.5385, 0.479...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0753, 0.0219, 0.0156, 0.0388, 0.012, 0.0229...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0635, 0.0345, 0.0473, 0.0593, 0.0627, 0.23,...</td>\n",
       "      <td>[0.9365, 0.9655, 0.9527, 0.9407, 0.9373, 0.77,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0457, 0.0149, 0.007, 0.0249, 0.006, 0.0109,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1043, 0.0583, 0.2668, 0.174, 0.0413, 0.2074...</td>\n",
       "      <td>[0.8957, 0.9417, 0.7332, 0.826, 0.9587, 0.7926...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0415, 0.0274, 0.0105, 0.0526, 0.0093, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5493, 0.3015, 0.6056, 0.6313, 0.1767, 0.788...</td>\n",
       "      <td>[0.4507, 0.6985, 0.3944, 0.3687, 0.8233, 0.211...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0474, 0.0311, 0.0116, 0.0605, 0.0106, 0.010...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5599, 0.2666, 0.7978, 0.5983, 0.1843, 0.937...</td>\n",
       "      <td>[0.4401, 0.7334, 0.2022, 0.4017, 0.8157, 0.062...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0351, 0.0132, 0.0054, 0.0268, 0.0047, 0.007...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2595, 0.1078, 0.4033, 0.2579, 0.0761, 0.329...</td>\n",
       "      <td>[0.7405, 0.8922, 0.5967, 0.7421, 0.9239, 0.670...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.1019, 0.0727, 0.0577, 0.1003, 0.0457, 0.079...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5767, 0.327, 0.5247, 0.4136, 0.4525, 0.4724...</td>\n",
       "      <td>[0.4233, 0.673, 0.4753, 0.5864, 0.5475, 0.5276...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0729, 0.0598, 0.0229, 0.054, 0.0446, 0.0376...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.3842, 0.4467, 0.3494, 0.4789, 0.2509, 0.336...</td>\n",
       "      <td>[0.6158, 0.5533, 0.6506, 0.5211, 0.7491, 0.663...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0356, 0.0136, 0.0055, 0.0134, 0.0086, 0.010...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.093, 0.3185, 0.032, 0.072, 0.3117, 0.0734, ...</td>\n",
       "      <td>[0.907, 0.6815, 0.968, 0.928, 0.6883, 0.9266, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0669, 0.0627, 0.0222, 0.0469, 0.054, 0.0433...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2129, 0.8762, 0.3351, 0.4364, 0.6046, 0.307...</td>\n",
       "      <td>[0.7871, 0.1238, 0.6649, 0.5636, 0.3954, 0.692...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0874, 0.0605, 0.0282, 0.0754, 0.0576, 0.047...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3807, 0.4504, 0.6415, 0.3331, 0.1925, 0.338...</td>\n",
       "      <td>[0.6193, 0.5496, 0.3585, 0.6669, 0.8075, 0.661...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0447, 0.0445, 0.021, 0.0574, 0.0551, 0.0206...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4402, 0.5158, 0.5636, 0.4601, 0.5771, 0.325...</td>\n",
       "      <td>[0.5598, 0.4842, 0.4364, 0.5399, 0.4229, 0.674...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1071, 0.0456, 0.0231, 0.0546, 0.0297, 0.036...               0   \n",
       "1   [0.1215, 0.0861, 0.0242, 0.0588, 0.0674, 0.039...               0   \n",
       "2   [0.0931, 0.0479, 0.0209, 0.0538, 0.032, 0.0376...               0   \n",
       "3   [0.0456, 0.0367, 0.0132, 0.0302, 0.0332, 0.020...               1   \n",
       "4   [0.0702, 0.0283, 0.013, 0.0315, 0.0158, 0.0231...               1   \n",
       "5   [0.0956, 0.0423, 0.0296, 0.0545, 0.0258, 0.043...               2   \n",
       "6   [0.0521, 0.045, 0.0109, 0.1752, 0.033, 0.0157,...               3   \n",
       "7   [0.0755, 0.046, 0.0233, 0.1147, 0.0301, 0.0342...               3   \n",
       "8   [0.1175, 0.0732, 0.0407, 0.141, 0.0524, 0.065,...               3   \n",
       "9   [0.0488, 0.1109, 0.0488, 0.0833, 0.2617, 0.067...               4   \n",
       "10  [0.0482, 0.0726, 0.0143, 0.0371, 0.0741, 0.040...               4   \n",
       "11  [0.0402, 0.0397, 0.0163, 0.042, 0.044, 0.0401,...               5   \n",
       "12  [0.041, 0.0632, 0.027, 0.0773, 0.0611, 0.0313,...               6   \n",
       "13  [0.2092, 0.1623, 0.1487, 0.2213, 0.0886, 0.185...               7   \n",
       "14  [0.0318, 0.0258, 0.01, 0.0545, 0.0264, 0.0143,...               8   \n",
       "15  [0.0642, 0.0416, 0.0181, 0.0389, 0.039, 0.0259...               8   \n",
       "16  [0.0851, 0.0335, 0.023, 0.0472, 0.019, 0.0356,...               9   \n",
       "17  [0.0689, 0.044, 0.0229, 0.058, 0.0322, 0.0347,...               9   \n",
       "18  [0.0893, 0.0289, 0.0193, 0.0397, 0.0162, 0.029...              10   \n",
       "19  [0.0808, 0.0293, 0.0205, 0.0363, 0.0171, 0.029...              10   \n",
       "20  [0.0901, 0.0456, 0.0234, 0.0638, 0.0284, 0.042...              11   \n",
       "21  [0.052, 0.0188, 0.0095, 0.0327, 0.0088, 0.0152...              11   \n",
       "22  [0.0645, 0.0264, 0.0139, 0.0384, 0.0132, 0.021...              12   \n",
       "23  [0.0773, 0.0412, 0.0201, 0.0545, 0.0278, 0.039...              13   \n",
       "24  [0.0858, 0.0268, 0.0178, 0.0424, 0.0151, 0.027...              13   \n",
       "25  [0.0659, 0.0402, 0.0215, 0.0567, 0.0337, 0.031...              14   \n",
       "26  [0.0653, 0.0229, 0.0129, 0.033, 0.0119, 0.0189...              14   \n",
       "27  [0.0283, 0.0246, 0.0105, 0.0328, 0.0258, 0.015...              15   \n",
       "28  [0.0447, 0.0499, 0.0305, 0.0905, 0.0398, 0.035...              15   \n",
       "29  [0.0191, 0.0309, 0.0113, 0.0529, 0.0464, 0.016...              15   \n",
       "30  [0.3274, 0.5048, 0.4737, 0.3741, 0.643, 0.448,...              16   \n",
       "31  [0.0639, 0.0489, 0.0226, 0.0714, 0.0327, 0.028...              17   \n",
       "32  [0.0472, 0.0312, 0.0127, 0.0573, 0.0197, 0.015...              17   \n",
       "33  [0.055, 0.0163, 0.0087, 0.0231, 0.0084, 0.0147...              18   \n",
       "34  [0.0655, 0.0383, 0.0231, 0.0448, 0.0397, 0.035...              19   \n",
       "35  [0.0474, 0.0153, 0.007, 0.0227, 0.007, 0.0118,...              20   \n",
       "36  [0.0662, 0.0231, 0.0129, 0.0347, 0.0105, 0.020...              21   \n",
       "37  [0.0463, 0.0357, 0.0155, 0.0666, 0.0123, 0.016...              21   \n",
       "38  [0.0666, 0.0249, 0.0201, 0.0615, 0.0181, 0.022...              22   \n",
       "39  [0.0651, 0.0216, 0.0143, 0.0406, 0.0126, 0.020...              22   \n",
       "40  [0.0621, 0.0531, 0.0164, 0.0911, 0.0459, 0.025...              22   \n",
       "41  [0.0753, 0.0219, 0.0156, 0.0388, 0.012, 0.0229...              22   \n",
       "42  [0.0457, 0.0149, 0.007, 0.0249, 0.006, 0.0109,...              23   \n",
       "43  [0.0415, 0.0274, 0.0105, 0.0526, 0.0093, 0.011...              23   \n",
       "44  [0.0474, 0.0311, 0.0116, 0.0605, 0.0106, 0.010...              23   \n",
       "45  [0.0351, 0.0132, 0.0054, 0.0268, 0.0047, 0.007...              23   \n",
       "46  [0.1019, 0.0727, 0.0577, 0.1003, 0.0457, 0.079...              24   \n",
       "47  [0.0729, 0.0598, 0.0229, 0.054, 0.0446, 0.0376...              24   \n",
       "48  [0.0356, 0.0136, 0.0055, 0.0134, 0.0086, 0.010...              25   \n",
       "49  [0.0669, 0.0627, 0.0222, 0.0469, 0.054, 0.0433...              25   \n",
       "50  [0.0874, 0.0605, 0.0282, 0.0754, 0.0576, 0.047...              26   \n",
       "51  [0.0447, 0.0445, 0.021, 0.0574, 0.0551, 0.0206...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1636, 0.3045, 0.1154, 0.1774, 0.1247, 0.104...   \n",
       "1                  0  [0.551, 0.5674, 0.2633, 0.4378, 0.3613, 0.2189...   \n",
       "2                  0  [0.3091, 0.3833, 0.1693, 0.2143, 0.1303, 0.126...   \n",
       "3                  0  [0.4011, 0.3763, 0.2037, 0.4128, 0.6147, 0.349...   \n",
       "4                  0  [0.0822, 0.1124, 0.0617, 0.0915, 0.1022, 0.039...   \n",
       "5                  0  [0.0777, 0.059, 0.088, 0.0969, 0.1126, 0.1937,...   \n",
       "6                  3  [0.8374, 0.6377, 0.8609, 0.7369, 0.3732, 0.771...   \n",
       "7                 23  [0.6641, 0.3005, 0.6579, 0.4068, 0.26, 0.6656,...   \n",
       "8                  3  [0.497, 0.3004, 0.6593, 0.3582, 0.2315, 0.5028...   \n",
       "9                  4  [0.8109, 0.9102, 0.8395, 0.7896, 0.939, 0.698,...   \n",
       "10                 8  [0.541, 0.8286, 0.4022, 0.4823, 0.6637, 0.4652...   \n",
       "11                 8  [0.349, 0.6488, 0.3381, 0.3681, 0.687, 0.4193,...   \n",
       "12                17  [0.6948, 0.7672, 0.8615, 0.8797, 0.7488, 0.762...   \n",
       "13                23  [0.7065, 0.4687, 0.5362, 0.5207, 0.727, 0.9118...   \n",
       "14                 3  [0.5355, 0.4691, 0.5678, 0.689, 0.6041, 0.4643...   \n",
       "15                 8  [0.216, 0.5202, 0.4153, 0.3321, 0.4261, 0.2472...   \n",
       "16                23  [0.0901, 0.0728, 0.0707, 0.1408, 0.1566, 0.181...   \n",
       "17                23  [0.3031, 0.3665, 0.3178, 0.4818, 0.4793, 0.437...   \n",
       "18                10  [0.0984, 0.336, 0.0823, 0.0984, 0.3507, 0.1261...   \n",
       "19                10  [0.1288, 0.5197, 0.0914, 0.213, 0.4295, 0.131,...   \n",
       "20                 0  [0.2316, 0.332, 0.3589, 0.5288, 0.1443, 0.2247...   \n",
       "21                23  [0.1441, 0.1506, 0.1572, 0.4151, 0.0599, 0.144...   \n",
       "22                23  [0.119, 0.1714, 0.1512, 0.2936, 0.1062, 0.2473...   \n",
       "23                 0  [0.5087, 0.4877, 0.4551, 0.3367, 0.4607, 0.513...   \n",
       "24                 0  [0.0758, 0.0763, 0.1142, 0.0816, 0.0787, 0.117...   \n",
       "25                 0  [0.175, 0.1858, 0.6155, 0.3709, 0.2845, 0.3375...   \n",
       "26                23  [0.0597, 0.0361, 0.2316, 0.1365, 0.0663, 0.155...   \n",
       "27                15  [0.439, 0.4281, 0.4462, 0.3518, 0.8076, 0.5783...   \n",
       "28                15  [0.7425, 0.533, 0.6445, 0.5903, 0.935, 0.8762,...   \n",
       "29                15  [0.7821, 0.5195, 0.8152, 0.7124, 0.9549, 0.863...   \n",
       "30                16  [0.9998, 0.9999, 0.9978, 0.9997, 1.0, 0.9985, ...   \n",
       "31                17  [0.678, 0.3968, 0.5974, 0.4475, 0.3514, 0.3549...   \n",
       "32                23  [0.4372, 0.5371, 0.6543, 0.6337, 0.2724, 0.519...   \n",
       "33                 0  [0.036, 0.0773, 0.0498, 0.0366, 0.0622, 0.0634...   \n",
       "34                 0  [0.3488, 0.6754, 0.0975, 0.1989, 0.4971, 0.126...   \n",
       "35                23  [0.0821, 0.079, 0.2366, 0.1281, 0.058, 0.2141,...   \n",
       "36                23  [0.0859, 0.0591, 0.1664, 0.1602, 0.0596, 0.159...   \n",
       "37                23  [0.3355, 0.3486, 0.6344, 0.6558, 0.1608, 0.849...   \n",
       "38                22  [0.367, 0.1265, 0.1546, 0.2057, 0.1742, 0.6057...   \n",
       "39                22  [0.1844, 0.0326, 0.0478, 0.1123, 0.0504, 0.145...   \n",
       "40                23  [0.6288, 0.5097, 0.3666, 0.6931, 0.4615, 0.520...   \n",
       "41                22  [0.0635, 0.0345, 0.0473, 0.0593, 0.0627, 0.23,...   \n",
       "42                23  [0.1043, 0.0583, 0.2668, 0.174, 0.0413, 0.2074...   \n",
       "43                23  [0.5493, 0.3015, 0.6056, 0.6313, 0.1767, 0.788...   \n",
       "44                23  [0.5599, 0.2666, 0.7978, 0.5983, 0.1843, 0.937...   \n",
       "45                23  [0.2595, 0.1078, 0.4033, 0.2579, 0.0761, 0.329...   \n",
       "46                 9  [0.5767, 0.327, 0.5247, 0.4136, 0.4525, 0.4724...   \n",
       "47                24  [0.3842, 0.4467, 0.3494, 0.4789, 0.2509, 0.336...   \n",
       "48                 0  [0.093, 0.3185, 0.032, 0.072, 0.3117, 0.0734, ...   \n",
       "49                25  [0.2129, 0.8762, 0.3351, 0.4364, 0.6046, 0.307...   \n",
       "50                23  [0.3807, 0.4504, 0.6415, 0.3331, 0.1925, 0.338...   \n",
       "51                26  [0.4402, 0.5158, 0.5636, 0.4601, 0.5771, 0.325...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8364, 0.6955, 0.8846, 0.8226, 0.8753, 0.895...  0.461538  0.143744  \n",
       "1   [0.449, 0.4326, 0.7367, 0.5622, 0.6387, 0.7811...       NaN       NaN  \n",
       "2   [0.6909, 0.6167, 0.8307, 0.7857, 0.8697, 0.873...       NaN       NaN  \n",
       "3   [0.5989, 0.6237, 0.7963, 0.5872, 0.3853, 0.650...       NaN       NaN  \n",
       "4   [0.9178, 0.8876, 0.9383, 0.9085, 0.8978, 0.960...       NaN       NaN  \n",
       "5   [0.9223, 0.941, 0.912, 0.9031, 0.8874, 0.8063,...       NaN       NaN  \n",
       "6   [0.1626, 0.3623, 0.1391, 0.2631, 0.6268, 0.228...       NaN       NaN  \n",
       "7   [0.3359, 0.6995, 0.3421, 0.5932, 0.74, 0.3344,...       NaN       NaN  \n",
       "8   [0.503, 0.6996, 0.3407, 0.6418, 0.7685, 0.4972...       NaN       NaN  \n",
       "9   [0.1891, 0.0898, 0.1605, 0.2104, 0.061, 0.302,...       NaN       NaN  \n",
       "10  [0.459, 0.1714, 0.5978, 0.5177, 0.3363, 0.5348...       NaN       NaN  \n",
       "11  [0.651, 0.3512, 0.6619, 0.6319, 0.313, 0.5807,...       NaN       NaN  \n",
       "12  [0.3052, 0.2328, 0.1385, 0.1203, 0.2512, 0.237...       NaN       NaN  \n",
       "13  [0.2935, 0.5313, 0.4638, 0.4793, 0.273, 0.0882...       NaN       NaN  \n",
       "14  [0.4645, 0.5309, 0.4322, 0.311, 0.3959, 0.5357...       NaN       NaN  \n",
       "15  [0.784, 0.4798, 0.5847, 0.6679, 0.5739, 0.7528...       NaN       NaN  \n",
       "16  [0.9099, 0.9272, 0.9293, 0.8592, 0.8434, 0.818...       NaN       NaN  \n",
       "17  [0.6969, 0.6335, 0.6822, 0.5182, 0.5207, 0.562...       NaN       NaN  \n",
       "18  [0.9016, 0.664, 0.9177, 0.9016, 0.6493, 0.8739...       NaN       NaN  \n",
       "19  [0.8712, 0.4803, 0.9086, 0.787, 0.5705, 0.869,...       NaN       NaN  \n",
       "20  [0.7684, 0.668, 0.6411, 0.4712, 0.8557, 0.7753...       NaN       NaN  \n",
       "21  [0.8559, 0.8494, 0.8428, 0.5849, 0.9401, 0.855...       NaN       NaN  \n",
       "22  [0.881, 0.8286, 0.8488, 0.7064, 0.8938, 0.7527...       NaN       NaN  \n",
       "23  [0.4913, 0.5123, 0.5449, 0.6633, 0.5393, 0.486...       NaN       NaN  \n",
       "24  [0.9242, 0.9237, 0.8858, 0.9184, 0.9213, 0.882...       NaN       NaN  \n",
       "25  [0.825, 0.8142, 0.3845, 0.6291, 0.7155, 0.6625...       NaN       NaN  \n",
       "26  [0.9403, 0.9639, 0.7684, 0.8635, 0.9337, 0.844...       NaN       NaN  \n",
       "27  [0.561, 0.5719, 0.5538, 0.6482, 0.1924, 0.4217...       NaN       NaN  \n",
       "28  [0.2575, 0.467, 0.3555, 0.4097, 0.065, 0.1238,...       NaN       NaN  \n",
       "29  [0.2179, 0.4805, 0.1848, 0.2876, 0.0451, 0.136...       NaN       NaN  \n",
       "30  [0.0002, 1e-04, 0.0022, 0.0003, 0.0, 0.0015, 0...       NaN       NaN  \n",
       "31  [0.322, 0.6032, 0.4026, 0.5525, 0.6486, 0.6451...       NaN       NaN  \n",
       "32  [0.5628, 0.4629, 0.3457, 0.3663, 0.7276, 0.480...       NaN       NaN  \n",
       "33  [0.964, 0.9227, 0.9502, 0.9634, 0.9378, 0.9366...       NaN       NaN  \n",
       "34  [0.6512, 0.3246, 0.9025, 0.8011, 0.5029, 0.873...       NaN       NaN  \n",
       "35  [0.9179, 0.921, 0.7634, 0.8719, 0.942, 0.7859,...       NaN       NaN  \n",
       "36  [0.9141, 0.9409, 0.8336, 0.8398, 0.9404, 0.840...       NaN       NaN  \n",
       "37  [0.6645, 0.6514, 0.3656, 0.3442, 0.8392, 0.150...       NaN       NaN  \n",
       "38  [0.633, 0.8735, 0.8454, 0.7943, 0.8258, 0.3943...       NaN       NaN  \n",
       "39  [0.8156, 0.9674, 0.9522, 0.8877, 0.9496, 0.854...       NaN       NaN  \n",
       "40  [0.3712, 0.4903, 0.6334, 0.3069, 0.5385, 0.479...       NaN       NaN  \n",
       "41  [0.9365, 0.9655, 0.9527, 0.9407, 0.9373, 0.77,...       NaN       NaN  \n",
       "42  [0.8957, 0.9417, 0.7332, 0.826, 0.9587, 0.7926...       NaN       NaN  \n",
       "43  [0.4507, 0.6985, 0.3944, 0.3687, 0.8233, 0.211...       NaN       NaN  \n",
       "44  [0.4401, 0.7334, 0.2022, 0.4017, 0.8157, 0.062...       NaN       NaN  \n",
       "45  [0.7405, 0.8922, 0.5967, 0.7421, 0.9239, 0.670...       NaN       NaN  \n",
       "46  [0.4233, 0.673, 0.4753, 0.5864, 0.5475, 0.5276...       NaN       NaN  \n",
       "47  [0.6158, 0.5533, 0.6506, 0.5211, 0.7491, 0.663...       NaN       NaN  \n",
       "48  [0.907, 0.6815, 0.968, 0.928, 0.6883, 0.9266, ...       NaN       NaN  \n",
       "49  [0.7871, 0.1238, 0.6649, 0.5636, 0.3954, 0.692...       NaN       NaN  \n",
       "50  [0.6193, 0.5496, 0.3585, 0.6669, 0.8075, 0.661...       NaN       NaN  \n",
       "51  [0.5598, 0.4842, 0.4364, 0.5399, 0.4229, 0.674...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881 : Training: loss:  0.13530236\n",
      "882 : Training: loss:  0.13149846\n",
      "883 : Training: loss:  0.14314492\n",
      "884 : Training: loss:  0.13200462\n",
      "885 : Training: loss:  0.12414151\n",
      "886 : Training: loss:  0.13521743\n",
      "887 : Training: loss:  0.13196348\n",
      "888 : Training: loss:  0.12129459\n",
      "889 : Training: loss:  0.13083865\n",
      "890 : Training: loss:  0.12197858\n",
      "891 : Training: loss:  0.12781622\n",
      "892 : Training: loss:  0.106160484\n",
      "893 : Training: loss:  0.12233947\n",
      "894 : Training: loss:  0.1273481\n",
      "895 : Training: loss:  0.15340595\n",
      "896 : Training: loss:  0.12181977\n",
      "897 : Training: loss:  0.11827527\n",
      "898 : Training: loss:  0.12431887\n",
      "899 : Training: loss:  0.12697937\n",
      "900 : Training: loss:  0.14185145\n",
      "Validation: Loss:  0.14289364  Accuracy:  0.48076922\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1086, 0.0466, 0.0233, 0.0538, 0.0297, 0.037...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1646, 0.3072, 0.1149, 0.1792, 0.1248, 0.104...</td>\n",
       "      <td>[0.8354, 0.6928, 0.8851, 0.8208, 0.8752, 0.895...</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.142894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1241, 0.0907, 0.0244, 0.0579, 0.0691, 0.040...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5554, 0.5732, 0.2647, 0.4445, 0.3646, 0.218...</td>\n",
       "      <td>[0.4446, 0.4268, 0.7353, 0.5555, 0.6354, 0.781...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0943, 0.0493, 0.021, 0.0531, 0.0322, 0.0386...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.313, 0.3864, 0.169, 0.2184, 0.1304, 0.1266,...</td>\n",
       "      <td>[0.687, 0.6136, 0.831, 0.7816, 0.8696, 0.8734,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0455, 0.0386, 0.0132, 0.0294, 0.0341, 0.020...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4076, 0.3806, 0.2029, 0.4192, 0.6169, 0.352...</td>\n",
       "      <td>[0.5924, 0.6194, 0.7971, 0.5808, 0.3831, 0.648...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0709, 0.0292, 0.0132, 0.031, 0.0159, 0.0239...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0828, 0.1139, 0.061, 0.0934, 0.1027, 0.0393...</td>\n",
       "      <td>[0.9172, 0.8861, 0.939, 0.9066, 0.8973, 0.9607...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0955, 0.0427, 0.0299, 0.0535, 0.0256, 0.044...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0787, 0.0586, 0.0888, 0.0973, 0.1134, 0.197...</td>\n",
       "      <td>[0.9213, 0.9414, 0.9112, 0.9027, 0.8866, 0.802...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0506, 0.0452, 0.0104, 0.1733, 0.0317, 0.015...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8425, 0.6392, 0.8633, 0.7428, 0.3698, 0.778...</td>\n",
       "      <td>[0.1575, 0.3608, 0.1367, 0.2572, 0.6302, 0.221...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0742, 0.0462, 0.023, 0.1143, 0.0293, 0.034,...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6701, 0.3011, 0.6641, 0.4116, 0.2587, 0.673...</td>\n",
       "      <td>[0.3299, 0.6989, 0.3359, 0.5884, 0.7413, 0.326...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1155, 0.0731, 0.04, 0.1398, 0.051, 0.0646, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5034, 0.3029, 0.6682, 0.3615, 0.232, 0.5086...</td>\n",
       "      <td>[0.4966, 0.6971, 0.3318, 0.6385, 0.768, 0.4914...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0475, 0.1151, 0.048, 0.0801, 0.266, 0.0683,...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8152, 0.913, 0.8394, 0.7925, 0.9401, 0.698,...</td>\n",
       "      <td>[0.1848, 0.087, 0.1606, 0.2075, 0.0599, 0.302,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0487, 0.078, 0.0145, 0.0362, 0.0779, 0.0429...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5453, 0.8336, 0.3974, 0.4875, 0.6696, 0.469...</td>\n",
       "      <td>[0.4547, 0.1664, 0.6026, 0.5125, 0.3304, 0.530...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0399, 0.0413, 0.0163, 0.0409, 0.0451, 0.041...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3532, 0.6542, 0.3353, 0.3701, 0.6883, 0.422...</td>\n",
       "      <td>[0.6468, 0.3458, 0.6647, 0.6299, 0.3117, 0.577...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0391, 0.0633, 0.0262, 0.073, 0.0589, 0.0303...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6964, 0.7624, 0.8618, 0.8825, 0.7484, 0.761...</td>\n",
       "      <td>[0.3036, 0.2376, 0.1382, 0.1175, 0.2516, 0.238...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.2042, 0.16, 0.1457, 0.2162, 0.0851, 0.1839,...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.7036, 0.4561, 0.5271, 0.5181, 0.7247, 0.912...</td>\n",
       "      <td>[0.2964, 0.5439, 0.4729, 0.4819, 0.2753, 0.087...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0313, 0.0266, 0.0098, 0.0532, 0.0266, 0.014...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5425, 0.4731, 0.5668, 0.6947, 0.6028, 0.466...</td>\n",
       "      <td>[0.4575, 0.5269, 0.4332, 0.3053, 0.3972, 0.533...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0646, 0.043, 0.018, 0.0379, 0.0395, 0.0265,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2167, 0.5278, 0.4139, 0.3355, 0.4273, 0.243...</td>\n",
       "      <td>[0.7833, 0.4722, 0.5861, 0.6645, 0.5727, 0.756...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0849, 0.0338, 0.0232, 0.0462, 0.0188, 0.036...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0907, 0.0729, 0.0713, 0.1412, 0.1576, 0.183...</td>\n",
       "      <td>[0.9093, 0.9271, 0.9287, 0.8588, 0.8424, 0.816...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0679, 0.0445, 0.0227, 0.0563, 0.0318, 0.034...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3059, 0.3678, 0.3219, 0.4848, 0.4806, 0.439...</td>\n",
       "      <td>[0.6941, 0.6322, 0.6781, 0.5152, 0.5194, 0.560...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0904, 0.0294, 0.0197, 0.0392, 0.0162, 0.030...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0991, 0.3393, 0.0824, 0.098, 0.3518, 0.1276...</td>\n",
       "      <td>[0.9009, 0.6607, 0.9176, 0.902, 0.6482, 0.8724...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0817, 0.0299, 0.0209, 0.0359, 0.0172, 0.030...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1298, 0.5238, 0.0918, 0.2134, 0.431, 0.1313...</td>\n",
       "      <td>[0.8702, 0.4762, 0.9082, 0.7866, 0.569, 0.8687...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0896, 0.0461, 0.0232, 0.0627, 0.0279, 0.042...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2352, 0.3291, 0.3622, 0.5349, 0.143, 0.2259...</td>\n",
       "      <td>[0.7648, 0.6709, 0.6378, 0.4651, 0.857, 0.7741...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0521, 0.0191, 0.0096, 0.0324, 0.0087, 0.015...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1487, 0.1483, 0.1579, 0.4201, 0.0587, 0.149...</td>\n",
       "      <td>[0.8513, 0.8517, 0.8421, 0.5799, 0.9413, 0.850...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0644, 0.0269, 0.014, 0.0379, 0.0131, 0.0218...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1232, 0.1696, 0.1508, 0.2987, 0.1049, 0.251...</td>\n",
       "      <td>[0.8768, 0.8304, 0.8492, 0.7013, 0.8951, 0.748...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0767, 0.0421, 0.0202, 0.0537, 0.0279, 0.040...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5148, 0.4916, 0.4627, 0.3372, 0.4642, 0.516...</td>\n",
       "      <td>[0.4852, 0.5084, 0.5373, 0.6628, 0.5358, 0.483...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0864, 0.0272, 0.018, 0.0419, 0.015, 0.0276,...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0776, 0.0767, 0.1172, 0.0815, 0.0787, 0.119...</td>\n",
       "      <td>[0.9224, 0.9233, 0.8828, 0.9185, 0.9213, 0.880...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.065, 0.0406, 0.0213, 0.0552, 0.0333, 0.032,...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1783, 0.1877, 0.619, 0.3742, 0.2862, 0.3403...</td>\n",
       "      <td>[0.8217, 0.8123, 0.381, 0.6258, 0.7138, 0.6597...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0652, 0.0231, 0.013, 0.0323, 0.0117, 0.0192...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0611, 0.0357, 0.2336, 0.1381, 0.066, 0.1575...</td>\n",
       "      <td>[0.9389, 0.9643, 0.7664, 0.8619, 0.934, 0.8425...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0275, 0.0252, 0.0104, 0.0316, 0.0257, 0.015...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.4468, 0.4304, 0.4471, 0.3557, 0.8097, 0.583...</td>\n",
       "      <td>[0.5532, 0.5696, 0.5529, 0.6443, 0.1903, 0.416...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0427, 0.0502, 0.0297, 0.0874, 0.0385, 0.035...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7445, 0.5283, 0.6433, 0.5925, 0.9355, 0.878...</td>\n",
       "      <td>[0.2555, 0.4717, 0.3567, 0.4075, 0.0645, 0.121...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0181, 0.0315, 0.0108, 0.0502, 0.0455, 0.015...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7862, 0.5176, 0.8151, 0.7152, 0.9552, 0.865...</td>\n",
       "      <td>[0.2138, 0.4824, 0.1849, 0.2848, 0.0448, 0.134...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3299, 0.5138, 0.4749, 0.3722, 0.6499, 0.454...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9999, 0.9976, 0.9997, 1.0, 0.9984, ...</td>\n",
       "      <td>[0.0002, 1e-04, 0.0024, 0.0003, 0.0, 0.0016, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0619, 0.0491, 0.0223, 0.0697, 0.0317, 0.027...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6855, 0.3942, 0.6055, 0.4536, 0.3516, 0.361...</td>\n",
       "      <td>[0.3145, 0.6058, 0.3945, 0.5464, 0.6484, 0.638...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0459, 0.0314, 0.0124, 0.0557, 0.019, 0.0154...</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4454, 0.5344, 0.6627, 0.6414, 0.2714, 0.523...</td>\n",
       "      <td>[0.5546, 0.4656, 0.3373, 0.3586, 0.7286, 0.476...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0556, 0.0167, 0.0088, 0.0227, 0.0084, 0.015...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.037, 0.0781, 0.0498, 0.0374, 0.0624, 0.0646...</td>\n",
       "      <td>[0.963, 0.9219, 0.9502, 0.9626, 0.9376, 0.9354...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0659, 0.0396, 0.0236, 0.0443, 0.0409, 0.036...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3582, 0.6835, 0.0974, 0.2021, 0.4996, 0.127...</td>\n",
       "      <td>[0.6418, 0.3165, 0.9026, 0.7979, 0.5004, 0.872...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0477, 0.0156, 0.0071, 0.0223, 0.0069, 0.012...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0837, 0.0789, 0.238, 0.1305, 0.0582, 0.2175...</td>\n",
       "      <td>[0.9163, 0.9211, 0.762, 0.8695, 0.9418, 0.7825...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0663, 0.0233, 0.013, 0.0341, 0.0104, 0.021,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0864, 0.0584, 0.1671, 0.1609, 0.0588, 0.161...</td>\n",
       "      <td>[0.9136, 0.9416, 0.8329, 0.8391, 0.9412, 0.838...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0452, 0.0355, 0.0152, 0.065, 0.0117, 0.0163...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3373, 0.3394, 0.6358, 0.6556, 0.156, 0.8542...</td>\n",
       "      <td>[0.6627, 0.6606, 0.3642, 0.3444, 0.844, 0.1458...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0656, 0.0245, 0.0201, 0.0607, 0.0176, 0.022...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3779, 0.1243, 0.1513, 0.2064, 0.1725, 0.619...</td>\n",
       "      <td>[0.6221, 0.8757, 0.8487, 0.7936, 0.8275, 0.380...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0649, 0.0217, 0.0145, 0.0401, 0.0124, 0.021...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1906, 0.0322, 0.0472, 0.1136, 0.0495, 0.151...</td>\n",
       "      <td>[0.8094, 0.9678, 0.9528, 0.8864, 0.9505, 0.848...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0604, 0.0533, 0.0159, 0.0882, 0.0446, 0.025...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.638, 0.5046, 0.3623, 0.6978, 0.4569, 0.5287...</td>\n",
       "      <td>[0.362, 0.4954, 0.6377, 0.3022, 0.5431, 0.4713...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0757, 0.022, 0.0159, 0.0383, 0.0119, 0.0234...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0649, 0.0343, 0.0467, 0.0593, 0.0626, 0.237...</td>\n",
       "      <td>[0.9351, 0.9657, 0.9533, 0.9407, 0.9374, 0.762...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0456, 0.0151, 0.0071, 0.0244, 0.0059, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1046, 0.0573, 0.2702, 0.1749, 0.0404, 0.210...</td>\n",
       "      <td>[0.8954, 0.9427, 0.7298, 0.8251, 0.9596, 0.789...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0403, 0.0271, 0.0102, 0.0507, 0.0088, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5507, 0.2896, 0.6054, 0.6314, 0.1692, 0.793...</td>\n",
       "      <td>[0.4493, 0.7104, 0.3946, 0.3686, 0.8308, 0.206...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0455, 0.0305, 0.0111, 0.0576, 0.0099, 0.010...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5576, 0.2533, 0.7984, 0.5968, 0.1768, 0.939...</td>\n",
       "      <td>[0.4424, 0.7467, 0.2016, 0.4032, 0.8232, 0.060...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0346, 0.0132, 0.0054, 0.0261, 0.0045, 0.007...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2605, 0.1044, 0.4051, 0.2581, 0.0736, 0.335...</td>\n",
       "      <td>[0.7395, 0.8956, 0.5949, 0.7419, 0.9264, 0.664...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0986, 0.0721, 0.057, 0.0977, 0.0444, 0.0792...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5817, 0.3243, 0.5324, 0.4149, 0.4533, 0.475...</td>\n",
       "      <td>[0.4183, 0.6757, 0.4676, 0.5851, 0.5467, 0.525...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0716, 0.0612, 0.0226, 0.0523, 0.0441, 0.038...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.3898, 0.4475, 0.3565, 0.4859, 0.2504, 0.336...</td>\n",
       "      <td>[0.6102, 0.5525, 0.6435, 0.5141, 0.7496, 0.664...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0359, 0.0141, 0.0056, 0.0131, 0.0088, 0.010...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0961, 0.3239, 0.0313, 0.0735, 0.3152, 0.074...</td>\n",
       "      <td>[0.9039, 0.6761, 0.9687, 0.9265, 0.6848, 0.925...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0677, 0.0657, 0.0226, 0.0463, 0.0557, 0.045...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2158, 0.8792, 0.3338, 0.4428, 0.6083, 0.307...</td>\n",
       "      <td>[0.7842, 0.1208, 0.6662, 0.5572, 0.3917, 0.692...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0866, 0.0611, 0.0278, 0.0732, 0.0568, 0.047...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3841, 0.4502, 0.6411, 0.3373, 0.1921, 0.336...</td>\n",
       "      <td>[0.6159, 0.5498, 0.3589, 0.6627, 0.8079, 0.663...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0437, 0.0455, 0.0206, 0.0555, 0.0553, 0.020...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4476, 0.5231, 0.5675, 0.4656, 0.5791, 0.325...</td>\n",
       "      <td>[0.5524, 0.4769, 0.4325, 0.5344, 0.4209, 0.674...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1086, 0.0466, 0.0233, 0.0538, 0.0297, 0.037...               0   \n",
       "1   [0.1241, 0.0907, 0.0244, 0.0579, 0.0691, 0.040...               0   \n",
       "2   [0.0943, 0.0493, 0.021, 0.0531, 0.0322, 0.0386...               0   \n",
       "3   [0.0455, 0.0386, 0.0132, 0.0294, 0.0341, 0.020...               1   \n",
       "4   [0.0709, 0.0292, 0.0132, 0.031, 0.0159, 0.0239...               1   \n",
       "5   [0.0955, 0.0427, 0.0299, 0.0535, 0.0256, 0.044...               2   \n",
       "6   [0.0506, 0.0452, 0.0104, 0.1733, 0.0317, 0.015...               3   \n",
       "7   [0.0742, 0.0462, 0.023, 0.1143, 0.0293, 0.034,...               3   \n",
       "8   [0.1155, 0.0731, 0.04, 0.1398, 0.051, 0.0646, ...               3   \n",
       "9   [0.0475, 0.1151, 0.048, 0.0801, 0.266, 0.0683,...               4   \n",
       "10  [0.0487, 0.078, 0.0145, 0.0362, 0.0779, 0.0429...               4   \n",
       "11  [0.0399, 0.0413, 0.0163, 0.0409, 0.0451, 0.041...               5   \n",
       "12  [0.0391, 0.0633, 0.0262, 0.073, 0.0589, 0.0303...               6   \n",
       "13  [0.2042, 0.16, 0.1457, 0.2162, 0.0851, 0.1839,...               7   \n",
       "14  [0.0313, 0.0266, 0.0098, 0.0532, 0.0266, 0.014...               8   \n",
       "15  [0.0646, 0.043, 0.018, 0.0379, 0.0395, 0.0265,...               8   \n",
       "16  [0.0849, 0.0338, 0.0232, 0.0462, 0.0188, 0.036...               9   \n",
       "17  [0.0679, 0.0445, 0.0227, 0.0563, 0.0318, 0.034...               9   \n",
       "18  [0.0904, 0.0294, 0.0197, 0.0392, 0.0162, 0.030...              10   \n",
       "19  [0.0817, 0.0299, 0.0209, 0.0359, 0.0172, 0.030...              10   \n",
       "20  [0.0896, 0.0461, 0.0232, 0.0627, 0.0279, 0.042...              11   \n",
       "21  [0.0521, 0.0191, 0.0096, 0.0324, 0.0087, 0.015...              11   \n",
       "22  [0.0644, 0.0269, 0.014, 0.0379, 0.0131, 0.0218...              12   \n",
       "23  [0.0767, 0.0421, 0.0202, 0.0537, 0.0279, 0.040...              13   \n",
       "24  [0.0864, 0.0272, 0.018, 0.0419, 0.015, 0.0276,...              13   \n",
       "25  [0.065, 0.0406, 0.0213, 0.0552, 0.0333, 0.032,...              14   \n",
       "26  [0.0652, 0.0231, 0.013, 0.0323, 0.0117, 0.0192...              14   \n",
       "27  [0.0275, 0.0252, 0.0104, 0.0316, 0.0257, 0.015...              15   \n",
       "28  [0.0427, 0.0502, 0.0297, 0.0874, 0.0385, 0.035...              15   \n",
       "29  [0.0181, 0.0315, 0.0108, 0.0502, 0.0455, 0.015...              15   \n",
       "30  [0.3299, 0.5138, 0.4749, 0.3722, 0.6499, 0.454...              16   \n",
       "31  [0.0619, 0.0491, 0.0223, 0.0697, 0.0317, 0.027...              17   \n",
       "32  [0.0459, 0.0314, 0.0124, 0.0557, 0.019, 0.0154...              17   \n",
       "33  [0.0556, 0.0167, 0.0088, 0.0227, 0.0084, 0.015...              18   \n",
       "34  [0.0659, 0.0396, 0.0236, 0.0443, 0.0409, 0.036...              19   \n",
       "35  [0.0477, 0.0156, 0.0071, 0.0223, 0.0069, 0.012...              20   \n",
       "36  [0.0663, 0.0233, 0.013, 0.0341, 0.0104, 0.021,...              21   \n",
       "37  [0.0452, 0.0355, 0.0152, 0.065, 0.0117, 0.0163...              21   \n",
       "38  [0.0656, 0.0245, 0.0201, 0.0607, 0.0176, 0.022...              22   \n",
       "39  [0.0649, 0.0217, 0.0145, 0.0401, 0.0124, 0.021...              22   \n",
       "40  [0.0604, 0.0533, 0.0159, 0.0882, 0.0446, 0.025...              22   \n",
       "41  [0.0757, 0.022, 0.0159, 0.0383, 0.0119, 0.0234...              22   \n",
       "42  [0.0456, 0.0151, 0.0071, 0.0244, 0.0059, 0.011...              23   \n",
       "43  [0.0403, 0.0271, 0.0102, 0.0507, 0.0088, 0.011...              23   \n",
       "44  [0.0455, 0.0305, 0.0111, 0.0576, 0.0099, 0.010...              23   \n",
       "45  [0.0346, 0.0132, 0.0054, 0.0261, 0.0045, 0.007...              23   \n",
       "46  [0.0986, 0.0721, 0.057, 0.0977, 0.0444, 0.0792...              24   \n",
       "47  [0.0716, 0.0612, 0.0226, 0.0523, 0.0441, 0.038...              24   \n",
       "48  [0.0359, 0.0141, 0.0056, 0.0131, 0.0088, 0.010...              25   \n",
       "49  [0.0677, 0.0657, 0.0226, 0.0463, 0.0557, 0.045...              25   \n",
       "50  [0.0866, 0.0611, 0.0278, 0.0732, 0.0568, 0.047...              26   \n",
       "51  [0.0437, 0.0455, 0.0206, 0.0555, 0.0553, 0.020...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1646, 0.3072, 0.1149, 0.1792, 0.1248, 0.104...   \n",
       "1                  0  [0.5554, 0.5732, 0.2647, 0.4445, 0.3646, 0.218...   \n",
       "2                  0  [0.313, 0.3864, 0.169, 0.2184, 0.1304, 0.1266,...   \n",
       "3                  0  [0.4076, 0.3806, 0.2029, 0.4192, 0.6169, 0.352...   \n",
       "4                  0  [0.0828, 0.1139, 0.061, 0.0934, 0.1027, 0.0393...   \n",
       "5                  0  [0.0787, 0.0586, 0.0888, 0.0973, 0.1134, 0.197...   \n",
       "6                  3  [0.8425, 0.6392, 0.8633, 0.7428, 0.3698, 0.778...   \n",
       "7                 23  [0.6701, 0.3011, 0.6641, 0.4116, 0.2587, 0.673...   \n",
       "8                  3  [0.5034, 0.3029, 0.6682, 0.3615, 0.232, 0.5086...   \n",
       "9                  4  [0.8152, 0.913, 0.8394, 0.7925, 0.9401, 0.698,...   \n",
       "10                 8  [0.5453, 0.8336, 0.3974, 0.4875, 0.6696, 0.469...   \n",
       "11                 8  [0.3532, 0.6542, 0.3353, 0.3701, 0.6883, 0.422...   \n",
       "12                17  [0.6964, 0.7624, 0.8618, 0.8825, 0.7484, 0.761...   \n",
       "13                23  [0.7036, 0.4561, 0.5271, 0.5181, 0.7247, 0.912...   \n",
       "14                 3  [0.5425, 0.4731, 0.5668, 0.6947, 0.6028, 0.466...   \n",
       "15                 8  [0.2167, 0.5278, 0.4139, 0.3355, 0.4273, 0.243...   \n",
       "16                23  [0.0907, 0.0729, 0.0713, 0.1412, 0.1576, 0.183...   \n",
       "17                23  [0.3059, 0.3678, 0.3219, 0.4848, 0.4806, 0.439...   \n",
       "18                10  [0.0991, 0.3393, 0.0824, 0.098, 0.3518, 0.1276...   \n",
       "19                10  [0.1298, 0.5238, 0.0918, 0.2134, 0.431, 0.1313...   \n",
       "20                 0  [0.2352, 0.3291, 0.3622, 0.5349, 0.143, 0.2259...   \n",
       "21                23  [0.1487, 0.1483, 0.1579, 0.4201, 0.0587, 0.149...   \n",
       "22                22  [0.1232, 0.1696, 0.1508, 0.2987, 0.1049, 0.251...   \n",
       "23                13  [0.5148, 0.4916, 0.4627, 0.3372, 0.4642, 0.516...   \n",
       "24                 0  [0.0776, 0.0767, 0.1172, 0.0815, 0.0787, 0.119...   \n",
       "25                 0  [0.1783, 0.1877, 0.619, 0.3742, 0.2862, 0.3403...   \n",
       "26                23  [0.0611, 0.0357, 0.2336, 0.1381, 0.066, 0.1575...   \n",
       "27                15  [0.4468, 0.4304, 0.4471, 0.3557, 0.8097, 0.583...   \n",
       "28                15  [0.7445, 0.5283, 0.6433, 0.5925, 0.9355, 0.878...   \n",
       "29                15  [0.7862, 0.5176, 0.8151, 0.7152, 0.9552, 0.865...   \n",
       "30                16  [0.9998, 0.9999, 0.9976, 0.9997, 1.0, 0.9984, ...   \n",
       "31                17  [0.6855, 0.3942, 0.6055, 0.4536, 0.3516, 0.361...   \n",
       "32                23  [0.4454, 0.5344, 0.6627, 0.6414, 0.2714, 0.523...   \n",
       "33                 0  [0.037, 0.0781, 0.0498, 0.0374, 0.0624, 0.0646...   \n",
       "34                 0  [0.3582, 0.6835, 0.0974, 0.2021, 0.4996, 0.127...   \n",
       "35                23  [0.0837, 0.0789, 0.238, 0.1305, 0.0582, 0.2175...   \n",
       "36                23  [0.0864, 0.0584, 0.1671, 0.1609, 0.0588, 0.161...   \n",
       "37                23  [0.3373, 0.3394, 0.6358, 0.6556, 0.156, 0.8542...   \n",
       "38                22  [0.3779, 0.1243, 0.1513, 0.2064, 0.1725, 0.619...   \n",
       "39                22  [0.1906, 0.0322, 0.0472, 0.1136, 0.0495, 0.151...   \n",
       "40                23  [0.638, 0.5046, 0.3623, 0.6978, 0.4569, 0.5287...   \n",
       "41                22  [0.0649, 0.0343, 0.0467, 0.0593, 0.0626, 0.237...   \n",
       "42                23  [0.1046, 0.0573, 0.2702, 0.1749, 0.0404, 0.210...   \n",
       "43                23  [0.5507, 0.2896, 0.6054, 0.6314, 0.1692, 0.793...   \n",
       "44                23  [0.5576, 0.2533, 0.7984, 0.5968, 0.1768, 0.939...   \n",
       "45                23  [0.2605, 0.1044, 0.4051, 0.2581, 0.0736, 0.335...   \n",
       "46                13  [0.5817, 0.3243, 0.5324, 0.4149, 0.4533, 0.475...   \n",
       "47                24  [0.3898, 0.4475, 0.3565, 0.4859, 0.2504, 0.336...   \n",
       "48                 0  [0.0961, 0.3239, 0.0313, 0.0735, 0.3152, 0.074...   \n",
       "49                25  [0.2158, 0.8792, 0.3338, 0.4428, 0.6083, 0.307...   \n",
       "50                23  [0.3841, 0.4502, 0.6411, 0.3373, 0.1921, 0.336...   \n",
       "51                26  [0.4476, 0.5231, 0.5675, 0.4656, 0.5791, 0.325...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8354, 0.6928, 0.8851, 0.8208, 0.8752, 0.895...  0.480769  0.142894  \n",
       "1   [0.4446, 0.4268, 0.7353, 0.5555, 0.6354, 0.781...       NaN       NaN  \n",
       "2   [0.687, 0.6136, 0.831, 0.7816, 0.8696, 0.8734,...       NaN       NaN  \n",
       "3   [0.5924, 0.6194, 0.7971, 0.5808, 0.3831, 0.648...       NaN       NaN  \n",
       "4   [0.9172, 0.8861, 0.939, 0.9066, 0.8973, 0.9607...       NaN       NaN  \n",
       "5   [0.9213, 0.9414, 0.9112, 0.9027, 0.8866, 0.802...       NaN       NaN  \n",
       "6   [0.1575, 0.3608, 0.1367, 0.2572, 0.6302, 0.221...       NaN       NaN  \n",
       "7   [0.3299, 0.6989, 0.3359, 0.5884, 0.7413, 0.326...       NaN       NaN  \n",
       "8   [0.4966, 0.6971, 0.3318, 0.6385, 0.768, 0.4914...       NaN       NaN  \n",
       "9   [0.1848, 0.087, 0.1606, 0.2075, 0.0599, 0.302,...       NaN       NaN  \n",
       "10  [0.4547, 0.1664, 0.6026, 0.5125, 0.3304, 0.530...       NaN       NaN  \n",
       "11  [0.6468, 0.3458, 0.6647, 0.6299, 0.3117, 0.577...       NaN       NaN  \n",
       "12  [0.3036, 0.2376, 0.1382, 0.1175, 0.2516, 0.238...       NaN       NaN  \n",
       "13  [0.2964, 0.5439, 0.4729, 0.4819, 0.2753, 0.087...       NaN       NaN  \n",
       "14  [0.4575, 0.5269, 0.4332, 0.3053, 0.3972, 0.533...       NaN       NaN  \n",
       "15  [0.7833, 0.4722, 0.5861, 0.6645, 0.5727, 0.756...       NaN       NaN  \n",
       "16  [0.9093, 0.9271, 0.9287, 0.8588, 0.8424, 0.816...       NaN       NaN  \n",
       "17  [0.6941, 0.6322, 0.6781, 0.5152, 0.5194, 0.560...       NaN       NaN  \n",
       "18  [0.9009, 0.6607, 0.9176, 0.902, 0.6482, 0.8724...       NaN       NaN  \n",
       "19  [0.8702, 0.4762, 0.9082, 0.7866, 0.569, 0.8687...       NaN       NaN  \n",
       "20  [0.7648, 0.6709, 0.6378, 0.4651, 0.857, 0.7741...       NaN       NaN  \n",
       "21  [0.8513, 0.8517, 0.8421, 0.5799, 0.9413, 0.850...       NaN       NaN  \n",
       "22  [0.8768, 0.8304, 0.8492, 0.7013, 0.8951, 0.748...       NaN       NaN  \n",
       "23  [0.4852, 0.5084, 0.5373, 0.6628, 0.5358, 0.483...       NaN       NaN  \n",
       "24  [0.9224, 0.9233, 0.8828, 0.9185, 0.9213, 0.880...       NaN       NaN  \n",
       "25  [0.8217, 0.8123, 0.381, 0.6258, 0.7138, 0.6597...       NaN       NaN  \n",
       "26  [0.9389, 0.9643, 0.7664, 0.8619, 0.934, 0.8425...       NaN       NaN  \n",
       "27  [0.5532, 0.5696, 0.5529, 0.6443, 0.1903, 0.416...       NaN       NaN  \n",
       "28  [0.2555, 0.4717, 0.3567, 0.4075, 0.0645, 0.121...       NaN       NaN  \n",
       "29  [0.2138, 0.4824, 0.1849, 0.2848, 0.0448, 0.134...       NaN       NaN  \n",
       "30  [0.0002, 1e-04, 0.0024, 0.0003, 0.0, 0.0016, 0...       NaN       NaN  \n",
       "31  [0.3145, 0.6058, 0.3945, 0.5464, 0.6484, 0.638...       NaN       NaN  \n",
       "32  [0.5546, 0.4656, 0.3373, 0.3586, 0.7286, 0.476...       NaN       NaN  \n",
       "33  [0.963, 0.9219, 0.9502, 0.9626, 0.9376, 0.9354...       NaN       NaN  \n",
       "34  [0.6418, 0.3165, 0.9026, 0.7979, 0.5004, 0.872...       NaN       NaN  \n",
       "35  [0.9163, 0.9211, 0.762, 0.8695, 0.9418, 0.7825...       NaN       NaN  \n",
       "36  [0.9136, 0.9416, 0.8329, 0.8391, 0.9412, 0.838...       NaN       NaN  \n",
       "37  [0.6627, 0.6606, 0.3642, 0.3444, 0.844, 0.1458...       NaN       NaN  \n",
       "38  [0.6221, 0.8757, 0.8487, 0.7936, 0.8275, 0.380...       NaN       NaN  \n",
       "39  [0.8094, 0.9678, 0.9528, 0.8864, 0.9505, 0.848...       NaN       NaN  \n",
       "40  [0.362, 0.4954, 0.6377, 0.3022, 0.5431, 0.4713...       NaN       NaN  \n",
       "41  [0.9351, 0.9657, 0.9533, 0.9407, 0.9374, 0.762...       NaN       NaN  \n",
       "42  [0.8954, 0.9427, 0.7298, 0.8251, 0.9596, 0.789...       NaN       NaN  \n",
       "43  [0.4493, 0.7104, 0.3946, 0.3686, 0.8308, 0.206...       NaN       NaN  \n",
       "44  [0.4424, 0.7467, 0.2016, 0.4032, 0.8232, 0.060...       NaN       NaN  \n",
       "45  [0.7395, 0.8956, 0.5949, 0.7419, 0.9264, 0.664...       NaN       NaN  \n",
       "46  [0.4183, 0.6757, 0.4676, 0.5851, 0.5467, 0.525...       NaN       NaN  \n",
       "47  [0.6102, 0.5525, 0.6435, 0.5141, 0.7496, 0.664...       NaN       NaN  \n",
       "48  [0.9039, 0.6761, 0.9687, 0.9265, 0.6848, 0.925...       NaN       NaN  \n",
       "49  [0.7842, 0.1208, 0.6662, 0.5572, 0.3917, 0.692...       NaN       NaN  \n",
       "50  [0.6159, 0.5498, 0.3589, 0.6627, 0.8079, 0.663...       NaN       NaN  \n",
       "51  [0.5524, 0.4769, 0.4325, 0.5344, 0.4209, 0.674...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901 : Training: loss:  0.13957725\n",
      "902 : Training: loss:  0.14749636\n",
      "903 : Training: loss:  0.1322978\n",
      "904 : Training: loss:  0.12189503\n",
      "905 : Training: loss:  0.12215144\n",
      "906 : Training: loss:  0.17487024\n",
      "907 : Training: loss:  0.15500842\n",
      "908 : Training: loss:  0.12825263\n",
      "909 : Training: loss:  0.12807214\n",
      "910 : Training: loss:  0.14078338\n",
      "911 : Training: loss:  0.13623363\n",
      "912 : Training: loss:  0.13060348\n",
      "913 : Training: loss:  0.118062064\n",
      "914 : Training: loss:  0.13694274\n",
      "915 : Training: loss:  0.11703873\n",
      "916 : Training: loss:  0.12555736\n",
      "917 : Training: loss:  0.13412187\n",
      "918 : Training: loss:  0.13825671\n",
      "919 : Training: loss:  0.1406263\n",
      "920 : Training: loss:  0.1465213\n",
      "Validation: Loss:  0.14188273  Accuracy:  0.5576923\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1067, 0.048, 0.0244, 0.0526, 0.0296, 0.0391...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1658, 0.3103, 0.1152, 0.1826, 0.1251, 0.104...</td>\n",
       "      <td>[0.8342, 0.6897, 0.8848, 0.8174, 0.8749, 0.895...</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.141883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1235, 0.095, 0.0251, 0.0568, 0.0702, 0.0422...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5589, 0.5783, 0.2649, 0.4507, 0.3674, 0.218...</td>\n",
       "      <td>[0.4411, 0.4217, 0.7351, 0.5493, 0.6326, 0.781...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0927, 0.0509, 0.0218, 0.052, 0.0321, 0.0404...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3151, 0.3907, 0.1693, 0.2234, 0.1309, 0.126...</td>\n",
       "      <td>[0.6849, 0.6093, 0.8307, 0.7766, 0.8691, 0.873...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.044, 0.0405, 0.0137, 0.0283, 0.0345, 0.0219...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4116, 0.3845, 0.2016, 0.4268, 0.6189, 0.352...</td>\n",
       "      <td>[0.5884, 0.6155, 0.7984, 0.5732, 0.3811, 0.647...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0689, 0.0301, 0.014, 0.03, 0.0158, 0.0251, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0834, 0.1154, 0.0606, 0.0954, 0.1031, 0.039...</td>\n",
       "      <td>[0.9166, 0.8846, 0.9394, 0.9046, 0.8969, 0.960...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0929, 0.0434, 0.0315, 0.0521, 0.0252, 0.046...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0786, 0.0583, 0.0893, 0.0985, 0.1138, 0.200...</td>\n",
       "      <td>[0.9214, 0.9417, 0.9107, 0.9015, 0.8862, 0.799...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.049, 0.0455, 0.0104, 0.1721, 0.0304, 0.0151...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8468, 0.6426, 0.8661, 0.7488, 0.3662, 0.783...</td>\n",
       "      <td>[0.1532, 0.3574, 0.1339, 0.2512, 0.6338, 0.216...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0719, 0.0466, 0.0236, 0.1135, 0.0284, 0.034...</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.6758, 0.3012, 0.6664, 0.4163, 0.2575, 0.680...</td>\n",
       "      <td>[0.3242, 0.6988, 0.3336, 0.5837, 0.7425, 0.319...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1123, 0.0732, 0.0407, 0.1382, 0.0496, 0.065...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5083, 0.3051, 0.6731, 0.365, 0.2326, 0.5136...</td>\n",
       "      <td>[0.4917, 0.6949, 0.3269, 0.635, 0.7674, 0.4864...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0459, 0.1176, 0.0474, 0.0769, 0.2667, 0.071...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8166, 0.9148, 0.8377, 0.7945, 0.9408, 0.696...</td>\n",
       "      <td>[0.1834, 0.0852, 0.1623, 0.2055, 0.0592, 0.303...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0473, 0.0816, 0.0149, 0.0344, 0.0795, 0.046...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5441, 0.837, 0.3884, 0.4901, 0.6735, 0.4688...</td>\n",
       "      <td>[0.4559, 0.163, 0.6116, 0.5099, 0.3265, 0.5312...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0382, 0.0423, 0.0168, 0.0391, 0.0453, 0.044...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3526, 0.6582, 0.3311, 0.3732, 0.6908, 0.422...</td>\n",
       "      <td>[0.6474, 0.3418, 0.6689, 0.6268, 0.3092, 0.577...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0368, 0.0627, 0.0258, 0.0687, 0.0559, 0.03,...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6946, 0.7571, 0.8602, 0.8854, 0.7475, 0.760...</td>\n",
       "      <td>[0.3054, 0.2429, 0.1398, 0.1146, 0.2525, 0.239...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1983, 0.1578, 0.147, 0.2116, 0.0818, 0.1843...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.7007, 0.4477, 0.5187, 0.5178, 0.7253, 0.914...</td>\n",
       "      <td>[0.2993, 0.5523, 0.4813, 0.4822, 0.2747, 0.085...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0302, 0.0273, 0.01, 0.0515, 0.0266, 0.015, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5468, 0.4771, 0.5654, 0.7031, 0.6032, 0.466...</td>\n",
       "      <td>[0.4532, 0.5229, 0.4346, 0.2969, 0.3968, 0.533...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.063, 0.0445, 0.0185, 0.0364, 0.0397, 0.0277...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2172, 0.5357, 0.4118, 0.3405, 0.4301, 0.240...</td>\n",
       "      <td>[0.7828, 0.4643, 0.5882, 0.6595, 0.5699, 0.759...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0825, 0.0343, 0.0246, 0.0449, 0.0184, 0.038...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0901, 0.073, 0.0717, 0.1423, 0.1579, 0.1846...</td>\n",
       "      <td>[0.9099, 0.927, 0.9283, 0.8577, 0.8421, 0.8154...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0658, 0.0451, 0.0234, 0.0546, 0.0311, 0.035...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3065, 0.3692, 0.3253, 0.4883, 0.4814, 0.441...</td>\n",
       "      <td>[0.6935, 0.6308, 0.6747, 0.5117, 0.5186, 0.558...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0878, 0.0297, 0.0208, 0.0379, 0.0159, 0.031...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0986, 0.341, 0.0816, 0.0978, 0.3527, 0.1279...</td>\n",
       "      <td>[0.9014, 0.659, 0.9184, 0.9022, 0.6473, 0.8721...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0791, 0.0302, 0.0221, 0.0344, 0.0168, 0.031...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1287, 0.526, 0.0909, 0.2142, 0.4324, 0.1302...</td>\n",
       "      <td>[0.8713, 0.474, 0.9091, 0.7858, 0.5676, 0.8698...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0869, 0.0467, 0.0241, 0.0612, 0.0273, 0.043...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.238, 0.3278, 0.3655, 0.5422, 0.1426, 0.2271...</td>\n",
       "      <td>[0.762, 0.6722, 0.6345, 0.4578, 0.8574, 0.7729...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.05, 0.0194, 0.0103, 0.0316, 0.0085, 0.0162,...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1523, 0.1463, 0.1583, 0.4277, 0.058, 0.1533...</td>\n",
       "      <td>[0.8477, 0.8537, 0.8417, 0.5723, 0.942, 0.8467...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0621, 0.0274, 0.0149, 0.0369, 0.0129, 0.022...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1261, 0.169, 0.1504, 0.3047, 0.1042, 0.254,...</td>\n",
       "      <td>[0.8739, 0.831, 0.8496, 0.6953, 0.8958, 0.746,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0744, 0.0428, 0.0212, 0.0522, 0.0275, 0.042...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5145, 0.4935, 0.462, 0.3399, 0.4669, 0.5168...</td>\n",
       "      <td>[0.4855, 0.5065, 0.538, 0.6601, 0.5331, 0.4832...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0841, 0.0276, 0.0191, 0.0408, 0.0147, 0.028...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.078, 0.0771, 0.119, 0.0822, 0.0782, 0.122, ...</td>\n",
       "      <td>[0.922, 0.9229, 0.881, 0.9178, 0.9218, 0.878, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0623, 0.0411, 0.0219, 0.0535, 0.0328, 0.032...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1807, 0.1891, 0.6226, 0.3796, 0.2877, 0.343...</td>\n",
       "      <td>[0.8193, 0.8109, 0.3774, 0.6204, 0.7123, 0.657...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0625, 0.0234, 0.0137, 0.0311, 0.0114, 0.019...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0622, 0.0351, 0.2345, 0.1412, 0.0658, 0.160...</td>\n",
       "      <td>[0.9378, 0.9649, 0.7655, 0.8588, 0.9342, 0.839...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0261, 0.0258, 0.0107, 0.0303, 0.0255, 0.016...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.453, 0.4335, 0.448, 0.3621, 0.8119, 0.5884,...</td>\n",
       "      <td>[0.547, 0.5665, 0.552, 0.6379, 0.1881, 0.4116,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0407, 0.0502, 0.0302, 0.0844, 0.0372, 0.036...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7468, 0.5262, 0.6417, 0.5955, 0.9363, 0.881...</td>\n",
       "      <td>[0.2532, 0.4738, 0.3583, 0.4045, 0.0637, 0.118...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.017, 0.0319, 0.0108, 0.0478, 0.0445, 0.0163...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7893, 0.5162, 0.815, 0.7216, 0.956, 0.8685,...</td>\n",
       "      <td>[0.2107, 0.4838, 0.185, 0.2784, 0.044, 0.1315,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3273, 0.5205, 0.4693, 0.3669, 0.6545, 0.461...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9999, 0.9972, 0.9997, 1.0, 0.9983, ...</td>\n",
       "      <td>[0.0002, 1e-04, 0.0028, 0.0003, 0.0, 0.0017, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0593, 0.0493, 0.0228, 0.0681, 0.0307, 0.028...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6908, 0.3918, 0.6103, 0.4587, 0.3507, 0.367...</td>\n",
       "      <td>[0.3092, 0.6082, 0.3897, 0.5413, 0.6493, 0.632...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0438, 0.0316, 0.0126, 0.054, 0.0183, 0.0157...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.4501, 0.5335, 0.67, 0.6495, 0.2699, 0.5274,...</td>\n",
       "      <td>[0.5499, 0.4665, 0.33, 0.3505, 0.7301, 0.4726,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0538, 0.0171, 0.0095, 0.022, 0.0083, 0.016,...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.038, 0.0794, 0.0497, 0.0383, 0.0627, 0.0656...</td>\n",
       "      <td>[0.962, 0.9206, 0.9503, 0.9617, 0.9373, 0.9344...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0643, 0.0413, 0.0251, 0.0432, 0.0417, 0.038...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3667, 0.6902, 0.0971, 0.207, 0.5044, 0.1279...</td>\n",
       "      <td>[0.6334, 0.3098, 0.9029, 0.793, 0.4956, 0.8721...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0459, 0.016, 0.0075, 0.0216, 0.0068, 0.0127...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0851, 0.0793, 0.2383, 0.1336, 0.0583, 0.221...</td>\n",
       "      <td>[0.9149, 0.9207, 0.7617, 0.8664, 0.9417, 0.778...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0638, 0.0236, 0.0138, 0.0329, 0.0101, 0.021...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0855, 0.0577, 0.1671, 0.1616, 0.058, 0.1628...</td>\n",
       "      <td>[0.9145, 0.9423, 0.8329, 0.8384, 0.942, 0.8372...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0427, 0.0354, 0.0157, 0.0628, 0.0111, 0.016...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3344, 0.3301, 0.6349, 0.6581, 0.1514, 0.857...</td>\n",
       "      <td>[0.6656, 0.6699, 0.3651, 0.3419, 0.8486, 0.142...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0625, 0.0243, 0.0209, 0.0591, 0.0169, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3865, 0.1219, 0.1486, 0.2086, 0.1701, 0.629...</td>\n",
       "      <td>[0.6135, 0.8781, 0.8514, 0.7914, 0.8299, 0.370...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0623, 0.0218, 0.0153, 0.0389, 0.0121, 0.021...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1947, 0.0316, 0.0468, 0.1159, 0.0487, 0.157...</td>\n",
       "      <td>[0.8053, 0.9684, 0.9532, 0.8841, 0.9513, 0.842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0577, 0.0535, 0.016, 0.0857, 0.0432, 0.0253...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6458, 0.501, 0.3617, 0.7034, 0.4537, 0.5361...</td>\n",
       "      <td>[0.3542, 0.499, 0.6383, 0.2966, 0.5463, 0.4639...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0732, 0.0223, 0.0169, 0.0373, 0.0116, 0.024...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0659, 0.0341, 0.0463, 0.0596, 0.0622, 0.243...</td>\n",
       "      <td>[0.9341, 0.9659, 0.9537, 0.9404, 0.9378, 0.756...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0436, 0.0153, 0.0075, 0.0236, 0.0057, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1045, 0.0562, 0.2723, 0.1772, 0.0394, 0.213...</td>\n",
       "      <td>[0.8955, 0.9438, 0.7277, 0.8228, 0.9606, 0.786...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0377, 0.0269, 0.0105, 0.0482, 0.0082, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5493, 0.2784, 0.6018, 0.6352, 0.1629, 0.798...</td>\n",
       "      <td>[0.4507, 0.7216, 0.3982, 0.3648, 0.8371, 0.201...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0427, 0.03, 0.0112, 0.0548, 0.0091, 0.0102,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5529, 0.2416, 0.7971, 0.5999, 0.1702, 0.941...</td>\n",
       "      <td>[0.4471, 0.7584, 0.2029, 0.4001, 0.8298, 0.058...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0326, 0.0133, 0.0057, 0.0251, 0.0043, 0.008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2605, 0.1012, 0.4055, 0.2604, 0.0712, 0.341...</td>\n",
       "      <td>[0.7395, 0.8988, 0.5945, 0.7396, 0.9288, 0.658...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0946, 0.0713, 0.0584, 0.0947, 0.0428, 0.080...</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.5815, 0.3219, 0.5342, 0.4166, 0.454, 0.4754...</td>\n",
       "      <td>[0.4185, 0.6781, 0.4658, 0.5834, 0.546, 0.5246...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0695, 0.0629, 0.0231, 0.0509, 0.0435, 0.039...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.3945, 0.4487, 0.3619, 0.4931, 0.251, 0.3361...</td>\n",
       "      <td>[0.6055, 0.5513, 0.6381, 0.5069, 0.749, 0.6639...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0344, 0.0146, 0.006, 0.0124, 0.0088, 0.0113...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0978, 0.3274, 0.0305, 0.0753, 0.3187, 0.073...</td>\n",
       "      <td>[0.9022, 0.6726, 0.9695, 0.9247, 0.6813, 0.926...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0659, 0.068, 0.0234, 0.0443, 0.0566, 0.0484...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2154, 0.882, 0.3292, 0.4467, 0.6129, 0.3041...</td>\n",
       "      <td>[0.7846, 0.118, 0.6708, 0.5533, 0.3871, 0.6959...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0838, 0.0615, 0.0281, 0.0708, 0.0555, 0.048...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3883, 0.4515, 0.6429, 0.3425, 0.1923, 0.336...</td>\n",
       "      <td>[0.6117, 0.5485, 0.3571, 0.6575, 0.8077, 0.663...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0419, 0.0462, 0.0208, 0.0534, 0.0546, 0.021...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4523, 0.5295, 0.5701, 0.4724, 0.5804, 0.325...</td>\n",
       "      <td>[0.5477, 0.4705, 0.4299, 0.5276, 0.4196, 0.674...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1067, 0.048, 0.0244, 0.0526, 0.0296, 0.0391...               0   \n",
       "1   [0.1235, 0.095, 0.0251, 0.0568, 0.0702, 0.0422...               0   \n",
       "2   [0.0927, 0.0509, 0.0218, 0.052, 0.0321, 0.0404...               0   \n",
       "3   [0.044, 0.0405, 0.0137, 0.0283, 0.0345, 0.0219...               1   \n",
       "4   [0.0689, 0.0301, 0.014, 0.03, 0.0158, 0.0251, ...               1   \n",
       "5   [0.0929, 0.0434, 0.0315, 0.0521, 0.0252, 0.046...               2   \n",
       "6   [0.049, 0.0455, 0.0104, 0.1721, 0.0304, 0.0151...               3   \n",
       "7   [0.0719, 0.0466, 0.0236, 0.1135, 0.0284, 0.034...               3   \n",
       "8   [0.1123, 0.0732, 0.0407, 0.1382, 0.0496, 0.065...               3   \n",
       "9   [0.0459, 0.1176, 0.0474, 0.0769, 0.2667, 0.071...               4   \n",
       "10  [0.0473, 0.0816, 0.0149, 0.0344, 0.0795, 0.046...               4   \n",
       "11  [0.0382, 0.0423, 0.0168, 0.0391, 0.0453, 0.044...               5   \n",
       "12  [0.0368, 0.0627, 0.0258, 0.0687, 0.0559, 0.03,...               6   \n",
       "13  [0.1983, 0.1578, 0.147, 0.2116, 0.0818, 0.1843...               7   \n",
       "14  [0.0302, 0.0273, 0.01, 0.0515, 0.0266, 0.015, ...               8   \n",
       "15  [0.063, 0.0445, 0.0185, 0.0364, 0.0397, 0.0277...               8   \n",
       "16  [0.0825, 0.0343, 0.0246, 0.0449, 0.0184, 0.038...               9   \n",
       "17  [0.0658, 0.0451, 0.0234, 0.0546, 0.0311, 0.035...               9   \n",
       "18  [0.0878, 0.0297, 0.0208, 0.0379, 0.0159, 0.031...              10   \n",
       "19  [0.0791, 0.0302, 0.0221, 0.0344, 0.0168, 0.031...              10   \n",
       "20  [0.0869, 0.0467, 0.0241, 0.0612, 0.0273, 0.043...              11   \n",
       "21  [0.05, 0.0194, 0.0103, 0.0316, 0.0085, 0.0162,...              11   \n",
       "22  [0.0621, 0.0274, 0.0149, 0.0369, 0.0129, 0.022...              12   \n",
       "23  [0.0744, 0.0428, 0.0212, 0.0522, 0.0275, 0.042...              13   \n",
       "24  [0.0841, 0.0276, 0.0191, 0.0408, 0.0147, 0.028...              13   \n",
       "25  [0.0623, 0.0411, 0.0219, 0.0535, 0.0328, 0.032...              14   \n",
       "26  [0.0625, 0.0234, 0.0137, 0.0311, 0.0114, 0.019...              14   \n",
       "27  [0.0261, 0.0258, 0.0107, 0.0303, 0.0255, 0.016...              15   \n",
       "28  [0.0407, 0.0502, 0.0302, 0.0844, 0.0372, 0.036...              15   \n",
       "29  [0.017, 0.0319, 0.0108, 0.0478, 0.0445, 0.0163...              15   \n",
       "30  [0.3273, 0.5205, 0.4693, 0.3669, 0.6545, 0.461...              16   \n",
       "31  [0.0593, 0.0493, 0.0228, 0.0681, 0.0307, 0.028...              17   \n",
       "32  [0.0438, 0.0316, 0.0126, 0.054, 0.0183, 0.0157...              17   \n",
       "33  [0.0538, 0.0171, 0.0095, 0.022, 0.0083, 0.016,...              18   \n",
       "34  [0.0643, 0.0413, 0.0251, 0.0432, 0.0417, 0.038...              19   \n",
       "35  [0.0459, 0.016, 0.0075, 0.0216, 0.0068, 0.0127...              20   \n",
       "36  [0.0638, 0.0236, 0.0138, 0.0329, 0.0101, 0.021...              21   \n",
       "37  [0.0427, 0.0354, 0.0157, 0.0628, 0.0111, 0.016...              21   \n",
       "38  [0.0625, 0.0243, 0.0209, 0.0591, 0.0169, 0.023...              22   \n",
       "39  [0.0623, 0.0218, 0.0153, 0.0389, 0.0121, 0.021...              22   \n",
       "40  [0.0577, 0.0535, 0.016, 0.0857, 0.0432, 0.0253...              22   \n",
       "41  [0.0732, 0.0223, 0.0169, 0.0373, 0.0116, 0.024...              22   \n",
       "42  [0.0436, 0.0153, 0.0075, 0.0236, 0.0057, 0.011...              23   \n",
       "43  [0.0377, 0.0269, 0.0105, 0.0482, 0.0082, 0.011...              23   \n",
       "44  [0.0427, 0.03, 0.0112, 0.0548, 0.0091, 0.0102,...              23   \n",
       "45  [0.0326, 0.0133, 0.0057, 0.0251, 0.0043, 0.008...              23   \n",
       "46  [0.0946, 0.0713, 0.0584, 0.0947, 0.0428, 0.080...              24   \n",
       "47  [0.0695, 0.0629, 0.0231, 0.0509, 0.0435, 0.039...              24   \n",
       "48  [0.0344, 0.0146, 0.006, 0.0124, 0.0088, 0.0113...              25   \n",
       "49  [0.0659, 0.068, 0.0234, 0.0443, 0.0566, 0.0484...              25   \n",
       "50  [0.0838, 0.0615, 0.0281, 0.0708, 0.0555, 0.048...              26   \n",
       "51  [0.0419, 0.0462, 0.0208, 0.0534, 0.0546, 0.021...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1658, 0.3103, 0.1152, 0.1826, 0.1251, 0.104...   \n",
       "1                  0  [0.5589, 0.5783, 0.2649, 0.4507, 0.3674, 0.218...   \n",
       "2                  0  [0.3151, 0.3907, 0.1693, 0.2234, 0.1309, 0.126...   \n",
       "3                  0  [0.4116, 0.3845, 0.2016, 0.4268, 0.6189, 0.352...   \n",
       "4                  0  [0.0834, 0.1154, 0.0606, 0.0954, 0.1031, 0.039...   \n",
       "5                 23  [0.0786, 0.0583, 0.0893, 0.0985, 0.1138, 0.200...   \n",
       "6                  3  [0.8468, 0.6426, 0.8661, 0.7488, 0.3662, 0.783...   \n",
       "7                 23  [0.6758, 0.3012, 0.6664, 0.4163, 0.2575, 0.680...   \n",
       "8                  3  [0.5083, 0.3051, 0.6731, 0.365, 0.2326, 0.5136...   \n",
       "9                  4  [0.8166, 0.9148, 0.8377, 0.7945, 0.9408, 0.696...   \n",
       "10                 8  [0.5441, 0.837, 0.3884, 0.4901, 0.6735, 0.4688...   \n",
       "11                 8  [0.3526, 0.6582, 0.3311, 0.3732, 0.6908, 0.422...   \n",
       "12                17  [0.6946, 0.7571, 0.8602, 0.8854, 0.7475, 0.760...   \n",
       "13                 7  [0.7007, 0.4477, 0.5187, 0.5178, 0.7253, 0.914...   \n",
       "14                 8  [0.5468, 0.4771, 0.5654, 0.7031, 0.6032, 0.466...   \n",
       "15                 8  [0.2172, 0.5357, 0.4118, 0.3405, 0.4301, 0.240...   \n",
       "16                23  [0.0901, 0.073, 0.0717, 0.1423, 0.1579, 0.1846...   \n",
       "17                23  [0.3065, 0.3692, 0.3253, 0.4883, 0.4814, 0.441...   \n",
       "18                10  [0.0986, 0.341, 0.0816, 0.0978, 0.3527, 0.1279...   \n",
       "19                10  [0.1287, 0.526, 0.0909, 0.2142, 0.4324, 0.1302...   \n",
       "20                 0  [0.238, 0.3278, 0.3655, 0.5422, 0.1426, 0.2271...   \n",
       "21                23  [0.1523, 0.1463, 0.1583, 0.4277, 0.058, 0.1533...   \n",
       "22                22  [0.1261, 0.169, 0.1504, 0.3047, 0.1042, 0.254,...   \n",
       "23                13  [0.5145, 0.4935, 0.462, 0.3399, 0.4669, 0.5168...   \n",
       "24                10  [0.078, 0.0771, 0.119, 0.0822, 0.0782, 0.122, ...   \n",
       "25                 0  [0.1807, 0.1891, 0.6226, 0.3796, 0.2877, 0.343...   \n",
       "26                22  [0.0622, 0.0351, 0.2345, 0.1412, 0.0658, 0.160...   \n",
       "27                15  [0.453, 0.4335, 0.448, 0.3621, 0.8119, 0.5884,...   \n",
       "28                15  [0.7468, 0.5262, 0.6417, 0.5955, 0.9363, 0.881...   \n",
       "29                15  [0.7893, 0.5162, 0.815, 0.7216, 0.956, 0.8685,...   \n",
       "30                16  [0.9998, 0.9999, 0.9972, 0.9997, 1.0, 0.9983, ...   \n",
       "31                17  [0.6908, 0.3918, 0.6103, 0.4587, 0.3507, 0.367...   \n",
       "32                17  [0.4501, 0.5335, 0.67, 0.6495, 0.2699, 0.5274,...   \n",
       "33                 0  [0.038, 0.0794, 0.0497, 0.0383, 0.0627, 0.0656...   \n",
       "34                 0  [0.3667, 0.6902, 0.0971, 0.207, 0.5044, 0.1279...   \n",
       "35                23  [0.0851, 0.0793, 0.2383, 0.1336, 0.0583, 0.221...   \n",
       "36                23  [0.0855, 0.0577, 0.1671, 0.1616, 0.058, 0.1628...   \n",
       "37                23  [0.3344, 0.3301, 0.6349, 0.6581, 0.1514, 0.857...   \n",
       "38                22  [0.3865, 0.1219, 0.1486, 0.2086, 0.1701, 0.629...   \n",
       "39                22  [0.1947, 0.0316, 0.0468, 0.1159, 0.0487, 0.157...   \n",
       "40                22  [0.6458, 0.501, 0.3617, 0.7034, 0.4537, 0.5361...   \n",
       "41                22  [0.0659, 0.0341, 0.0463, 0.0596, 0.0622, 0.243...   \n",
       "42                23  [0.1045, 0.0562, 0.2723, 0.1772, 0.0394, 0.213...   \n",
       "43                23  [0.5493, 0.2784, 0.6018, 0.6352, 0.1629, 0.798...   \n",
       "44                23  [0.5529, 0.2416, 0.7971, 0.5999, 0.1702, 0.941...   \n",
       "45                23  [0.2605, 0.1012, 0.4055, 0.2604, 0.0712, 0.341...   \n",
       "46                17  [0.5815, 0.3219, 0.5342, 0.4166, 0.454, 0.4754...   \n",
       "47                24  [0.3945, 0.4487, 0.3619, 0.4931, 0.251, 0.3361...   \n",
       "48                 0  [0.0978, 0.3274, 0.0305, 0.0753, 0.3187, 0.073...   \n",
       "49                25  [0.2154, 0.882, 0.3292, 0.4467, 0.6129, 0.3041...   \n",
       "50                23  [0.3883, 0.4515, 0.6429, 0.3425, 0.1923, 0.336...   \n",
       "51                26  [0.4523, 0.5295, 0.5701, 0.4724, 0.5804, 0.325...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8342, 0.6897, 0.8848, 0.8174, 0.8749, 0.895...  0.557692  0.141883  \n",
       "1   [0.4411, 0.4217, 0.7351, 0.5493, 0.6326, 0.781...       NaN       NaN  \n",
       "2   [0.6849, 0.6093, 0.8307, 0.7766, 0.8691, 0.873...       NaN       NaN  \n",
       "3   [0.5884, 0.6155, 0.7984, 0.5732, 0.3811, 0.647...       NaN       NaN  \n",
       "4   [0.9166, 0.8846, 0.9394, 0.9046, 0.8969, 0.960...       NaN       NaN  \n",
       "5   [0.9214, 0.9417, 0.9107, 0.9015, 0.8862, 0.799...       NaN       NaN  \n",
       "6   [0.1532, 0.3574, 0.1339, 0.2512, 0.6338, 0.216...       NaN       NaN  \n",
       "7   [0.3242, 0.6988, 0.3336, 0.5837, 0.7425, 0.319...       NaN       NaN  \n",
       "8   [0.4917, 0.6949, 0.3269, 0.635, 0.7674, 0.4864...       NaN       NaN  \n",
       "9   [0.1834, 0.0852, 0.1623, 0.2055, 0.0592, 0.303...       NaN       NaN  \n",
       "10  [0.4559, 0.163, 0.6116, 0.5099, 0.3265, 0.5312...       NaN       NaN  \n",
       "11  [0.6474, 0.3418, 0.6689, 0.6268, 0.3092, 0.577...       NaN       NaN  \n",
       "12  [0.3054, 0.2429, 0.1398, 0.1146, 0.2525, 0.239...       NaN       NaN  \n",
       "13  [0.2993, 0.5523, 0.4813, 0.4822, 0.2747, 0.085...       NaN       NaN  \n",
       "14  [0.4532, 0.5229, 0.4346, 0.2969, 0.3968, 0.533...       NaN       NaN  \n",
       "15  [0.7828, 0.4643, 0.5882, 0.6595, 0.5699, 0.759...       NaN       NaN  \n",
       "16  [0.9099, 0.927, 0.9283, 0.8577, 0.8421, 0.8154...       NaN       NaN  \n",
       "17  [0.6935, 0.6308, 0.6747, 0.5117, 0.5186, 0.558...       NaN       NaN  \n",
       "18  [0.9014, 0.659, 0.9184, 0.9022, 0.6473, 0.8721...       NaN       NaN  \n",
       "19  [0.8713, 0.474, 0.9091, 0.7858, 0.5676, 0.8698...       NaN       NaN  \n",
       "20  [0.762, 0.6722, 0.6345, 0.4578, 0.8574, 0.7729...       NaN       NaN  \n",
       "21  [0.8477, 0.8537, 0.8417, 0.5723, 0.942, 0.8467...       NaN       NaN  \n",
       "22  [0.8739, 0.831, 0.8496, 0.6953, 0.8958, 0.746,...       NaN       NaN  \n",
       "23  [0.4855, 0.5065, 0.538, 0.6601, 0.5331, 0.4832...       NaN       NaN  \n",
       "24  [0.922, 0.9229, 0.881, 0.9178, 0.9218, 0.878, ...       NaN       NaN  \n",
       "25  [0.8193, 0.8109, 0.3774, 0.6204, 0.7123, 0.657...       NaN       NaN  \n",
       "26  [0.9378, 0.9649, 0.7655, 0.8588, 0.9342, 0.839...       NaN       NaN  \n",
       "27  [0.547, 0.5665, 0.552, 0.6379, 0.1881, 0.4116,...       NaN       NaN  \n",
       "28  [0.2532, 0.4738, 0.3583, 0.4045, 0.0637, 0.118...       NaN       NaN  \n",
       "29  [0.2107, 0.4838, 0.185, 0.2784, 0.044, 0.1315,...       NaN       NaN  \n",
       "30  [0.0002, 1e-04, 0.0028, 0.0003, 0.0, 0.0017, 0...       NaN       NaN  \n",
       "31  [0.3092, 0.6082, 0.3897, 0.5413, 0.6493, 0.632...       NaN       NaN  \n",
       "32  [0.5499, 0.4665, 0.33, 0.3505, 0.7301, 0.4726,...       NaN       NaN  \n",
       "33  [0.962, 0.9206, 0.9503, 0.9617, 0.9373, 0.9344...       NaN       NaN  \n",
       "34  [0.6334, 0.3098, 0.9029, 0.793, 0.4956, 0.8721...       NaN       NaN  \n",
       "35  [0.9149, 0.9207, 0.7617, 0.8664, 0.9417, 0.778...       NaN       NaN  \n",
       "36  [0.9145, 0.9423, 0.8329, 0.8384, 0.942, 0.8372...       NaN       NaN  \n",
       "37  [0.6656, 0.6699, 0.3651, 0.3419, 0.8486, 0.142...       NaN       NaN  \n",
       "38  [0.6135, 0.8781, 0.8514, 0.7914, 0.8299, 0.370...       NaN       NaN  \n",
       "39  [0.8053, 0.9684, 0.9532, 0.8841, 0.9513, 0.842...       NaN       NaN  \n",
       "40  [0.3542, 0.499, 0.6383, 0.2966, 0.5463, 0.4639...       NaN       NaN  \n",
       "41  [0.9341, 0.9659, 0.9537, 0.9404, 0.9378, 0.756...       NaN       NaN  \n",
       "42  [0.8955, 0.9438, 0.7277, 0.8228, 0.9606, 0.786...       NaN       NaN  \n",
       "43  [0.4507, 0.7216, 0.3982, 0.3648, 0.8371, 0.201...       NaN       NaN  \n",
       "44  [0.4471, 0.7584, 0.2029, 0.4001, 0.8298, 0.058...       NaN       NaN  \n",
       "45  [0.7395, 0.8988, 0.5945, 0.7396, 0.9288, 0.658...       NaN       NaN  \n",
       "46  [0.4185, 0.6781, 0.4658, 0.5834, 0.546, 0.5246...       NaN       NaN  \n",
       "47  [0.6055, 0.5513, 0.6381, 0.5069, 0.749, 0.6639...       NaN       NaN  \n",
       "48  [0.9022, 0.6726, 0.9695, 0.9247, 0.6813, 0.926...       NaN       NaN  \n",
       "49  [0.7846, 0.118, 0.6708, 0.5533, 0.3871, 0.6959...       NaN       NaN  \n",
       "50  [0.6117, 0.5485, 0.3571, 0.6575, 0.8077, 0.663...       NaN       NaN  \n",
       "51  [0.5477, 0.4705, 0.4299, 0.5276, 0.4196, 0.674...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921 : Training: loss:  0.112084694\n",
      "922 : Training: loss:  0.13427316\n",
      "923 : Training: loss:  0.12427031\n",
      "924 : Training: loss:  0.12779903\n",
      "925 : Training: loss:  0.1136452\n",
      "926 : Training: loss:  0.13052921\n",
      "927 : Training: loss:  0.14215621\n",
      "928 : Training: loss:  0.12900789\n",
      "929 : Training: loss:  0.16354339\n",
      "930 : Training: loss:  0.12813805\n",
      "931 : Training: loss:  0.14066963\n",
      "932 : Training: loss:  0.11903097\n",
      "933 : Training: loss:  0.10783301\n",
      "934 : Training: loss:  0.12830454\n",
      "935 : Training: loss:  0.11686591\n",
      "936 : Training: loss:  0.154737\n",
      "937 : Training: loss:  0.14651857\n",
      "938 : Training: loss:  0.12152914\n",
      "939 : Training: loss:  0.1363334\n",
      "940 : Training: loss:  0.12770496\n",
      "Validation: Loss:  0.14098646  Accuracy:  0.59615386\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1042, 0.0484, 0.0251, 0.052, 0.0289, 0.0419...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1663, 0.3137, 0.1153, 0.1848, 0.1258, 0.103...</td>\n",
       "      <td>[0.8337, 0.6863, 0.8847, 0.8152, 0.8742, 0.896...</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.140986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1233, 0.0992, 0.0258, 0.0568, 0.0703, 0.045...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5611, 0.5839, 0.2654, 0.4551, 0.371, 0.2181...</td>\n",
       "      <td>[0.4389, 0.4161, 0.7346, 0.5449, 0.629, 0.7819...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0904, 0.0518, 0.0224, 0.0518, 0.0316, 0.043...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3176, 0.3954, 0.1702, 0.2282, 0.1315, 0.126...</td>\n",
       "      <td>[0.6824, 0.6046, 0.8298, 0.7718, 0.8685, 0.873...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0426, 0.0425, 0.0141, 0.0281, 0.0347, 0.024...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4164, 0.39, 0.202, 0.4336, 0.6222, 0.3535, ...</td>\n",
       "      <td>[0.5836, 0.61, 0.798, 0.5664, 0.3778, 0.6465, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0666, 0.0305, 0.0146, 0.0294, 0.0155, 0.027...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0839, 0.118, 0.0607, 0.0977, 0.1041, 0.0394...</td>\n",
       "      <td>[0.9161, 0.882, 0.9393, 0.9023, 0.8959, 0.9606...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0896, 0.0432, 0.0326, 0.0513, 0.0244, 0.049...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0786, 0.0581, 0.0907, 0.0997, 0.114, 0.2039...</td>\n",
       "      <td>[0.9214, 0.9419, 0.9093, 0.9003, 0.886, 0.7961...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0469, 0.0458, 0.0102, 0.1815, 0.0287, 0.015...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8513, 0.6476, 0.8703, 0.7572, 0.3603, 0.791...</td>\n",
       "      <td>[0.1487, 0.3524, 0.1297, 0.2428, 0.6397, 0.208...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0688, 0.0463, 0.0238, 0.1161, 0.027, 0.0351...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.6829, 0.3014, 0.6709, 0.4232, 0.2551, 0.691...</td>\n",
       "      <td>[0.3171, 0.6986, 0.3291, 0.5768, 0.7449, 0.309...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1082, 0.0724, 0.0408, 0.1397, 0.0474, 0.066...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5139, 0.3071, 0.6804, 0.3698, 0.2313, 0.519...</td>\n",
       "      <td>[0.4861, 0.6929, 0.3196, 0.6302, 0.7687, 0.480...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0443, 0.1225, 0.047, 0.0772, 0.2651, 0.0765...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8175, 0.9166, 0.838, 0.7962, 0.9413, 0.6959...</td>\n",
       "      <td>[0.1825, 0.0834, 0.162, 0.2038, 0.0587, 0.3041...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0459, 0.0864, 0.0154, 0.0341, 0.081, 0.0518...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5453, 0.8426, 0.3829, 0.4947, 0.6777, 0.472...</td>\n",
       "      <td>[0.4547, 0.1574, 0.6171, 0.5053, 0.3223, 0.527...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0365, 0.0435, 0.0173, 0.039, 0.0452, 0.0496...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3545, 0.6633, 0.3292, 0.3758, 0.6932, 0.425...</td>\n",
       "      <td>[0.6455, 0.3367, 0.6708, 0.6242, 0.3068, 0.574...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.035, 0.0638, 0.026, 0.0684, 0.0539, 0.0305,...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6948, 0.758, 0.8614, 0.8888, 0.7483, 0.7599...</td>\n",
       "      <td>[0.3052, 0.242, 0.1386, 0.1112, 0.2517, 0.2401...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1906, 0.155, 0.1479, 0.2101, 0.0786, 0.1855...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.6997, 0.4379, 0.511, 0.5176, 0.7279, 0.9167...</td>\n",
       "      <td>[0.3003, 0.5621, 0.489, 0.4824, 0.2721, 0.0833...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0289, 0.0281, 0.0101, 0.0522, 0.0263, 0.015...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5514, 0.4852, 0.5665, 0.7105, 0.603, 0.4678...</td>\n",
       "      <td>[0.4486, 0.5148, 0.4335, 0.2895, 0.397, 0.5322...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.061, 0.0457, 0.019, 0.036, 0.0395, 0.0299, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2168, 0.5447, 0.4117, 0.3447, 0.4323, 0.237...</td>\n",
       "      <td>[0.7832, 0.4553, 0.5883, 0.6553, 0.5677, 0.762...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0797, 0.0342, 0.0256, 0.0442, 0.0179, 0.040...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0899, 0.0738, 0.0725, 0.1436, 0.1578, 0.185...</td>\n",
       "      <td>[0.9101, 0.9262, 0.9275, 0.8564, 0.8422, 0.814...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0635, 0.0455, 0.024, 0.0545, 0.0302, 0.0378...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3064, 0.3728, 0.3297, 0.4917, 0.4814, 0.444...</td>\n",
       "      <td>[0.6936, 0.6272, 0.6703, 0.5083, 0.5186, 0.556...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0851, 0.0296, 0.0218, 0.0372, 0.0155, 0.034...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0991, 0.3432, 0.0814, 0.0976, 0.3538, 0.129...</td>\n",
       "      <td>[0.9009, 0.6568, 0.9186, 0.9024, 0.6462, 0.870...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0763, 0.0302, 0.0231, 0.0336, 0.0165, 0.033...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.129, 0.5298, 0.091, 0.2152, 0.4348, 0.13, 0...</td>\n",
       "      <td>[0.871, 0.4702, 0.909, 0.7848, 0.5652, 0.87, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.084, 0.0468, 0.0246, 0.0608, 0.0264, 0.0459...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2395, 0.3268, 0.3701, 0.5489, 0.1418, 0.226...</td>\n",
       "      <td>[0.7605, 0.6732, 0.6299, 0.4511, 0.8582, 0.773...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0478, 0.0195, 0.0108, 0.0314, 0.0083, 0.017...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1569, 0.1456, 0.1599, 0.4351, 0.0576, 0.157...</td>\n",
       "      <td>[0.8431, 0.8544, 0.8401, 0.5649, 0.9424, 0.842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0594, 0.0274, 0.0155, 0.0366, 0.0125, 0.024...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1294, 0.1694, 0.1518, 0.3103, 0.1037, 0.258...</td>\n",
       "      <td>[0.8706, 0.8306, 0.8482, 0.6897, 0.8963, 0.741...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0717, 0.0433, 0.0219, 0.0522, 0.0271, 0.045...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5161, 0.4972, 0.4657, 0.3427, 0.4697, 0.518...</td>\n",
       "      <td>[0.4839, 0.5028, 0.5343, 0.6573, 0.5303, 0.481...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0812, 0.0274, 0.0199, 0.0401, 0.0142, 0.030...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0788, 0.0775, 0.1222, 0.0826, 0.0777, 0.125...</td>\n",
       "      <td>[0.9212, 0.9225, 0.8778, 0.9174, 0.9223, 0.874...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0597, 0.0412, 0.0223, 0.0531, 0.0318, 0.034...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1822, 0.191, 0.6293, 0.3844, 0.288, 0.3457,...</td>\n",
       "      <td>[0.8178, 0.809, 0.3707, 0.6156, 0.712, 0.6543,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0596, 0.0231, 0.0142, 0.0303, 0.011, 0.021,...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0632, 0.035, 0.238, 0.1435, 0.0656, 0.1621,...</td>\n",
       "      <td>[0.9368, 0.965, 0.762, 0.8565, 0.9344, 0.8379,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0247, 0.0264, 0.0109, 0.0303, 0.0249, 0.017...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.4577, 0.4398, 0.4514, 0.3677, 0.813, 0.592,...</td>\n",
       "      <td>[0.5423, 0.5602, 0.5486, 0.6323, 0.187, 0.408,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0384, 0.0508, 0.0306, 0.0852, 0.0358, 0.037...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7499, 0.5265, 0.6421, 0.5997, 0.9372, 0.884...</td>\n",
       "      <td>[0.2501, 0.4735, 0.3579, 0.4003, 0.0628, 0.115...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.016, 0.0331, 0.0108, 0.0484, 0.0433, 0.0171...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7918, 0.5196, 0.8165, 0.7262, 0.9566, 0.870...</td>\n",
       "      <td>[0.2082, 0.4804, 0.1835, 0.2738, 0.0434, 0.129...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.325, 0.5298, 0.468, 0.3639, 0.6592, 0.4744,...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9999, 0.9969, 0.9997, 1.0, 0.9982, ...</td>\n",
       "      <td>[0.0002, 1e-04, 0.0031, 0.0003, 0.0, 0.0018, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0565, 0.0494, 0.0231, 0.0687, 0.0293, 0.029...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6964, 0.3907, 0.6175, 0.4646, 0.3492, 0.373...</td>\n",
       "      <td>[0.3036, 0.6093, 0.3825, 0.5354, 0.6508, 0.626...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0416, 0.0319, 0.0128, 0.0548, 0.0174, 0.016...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.454, 0.5358, 0.6802, 0.6589, 0.267, 0.5304,...</td>\n",
       "      <td>[0.546, 0.4642, 0.3198, 0.3411, 0.733, 0.4696,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0518, 0.0172, 0.01, 0.0217, 0.0081, 0.0174,...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0389, 0.0819, 0.0501, 0.0394, 0.0628, 0.066...</td>\n",
       "      <td>[0.9611, 0.9181, 0.9499, 0.9606, 0.9372, 0.933...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0622, 0.0422, 0.0262, 0.043, 0.042, 0.0422,...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.3745, 0.6988, 0.0972, 0.2113, 0.5098, 0.128...</td>\n",
       "      <td>[0.6255, 0.3012, 0.9028, 0.7887, 0.4902, 0.871...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0439, 0.016, 0.0079, 0.0212, 0.0066, 0.0138...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0868, 0.0802, 0.2413, 0.1362, 0.0584, 0.225...</td>\n",
       "      <td>[0.9132, 0.9198, 0.7587, 0.8638, 0.9416, 0.774...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0613, 0.0235, 0.0144, 0.0324, 0.0097, 0.023...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0855, 0.057, 0.1676, 0.162, 0.057, 0.1657, ...</td>\n",
       "      <td>[0.9145, 0.943, 0.8324, 0.838, 0.943, 0.8343, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0404, 0.0354, 0.0162, 0.0632, 0.0106, 0.016...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3362, 0.3234, 0.6359, 0.6622, 0.1486, 0.864...</td>\n",
       "      <td>[0.6638, 0.6766, 0.3641, 0.3378, 0.8514, 0.136...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.059, 0.0235, 0.0213, 0.0588, 0.0161, 0.0236...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3978, 0.12, 0.1466, 0.2101, 0.1679, 0.6444,...</td>\n",
       "      <td>[0.6022, 0.88, 0.8534, 0.7899, 0.8321, 0.3556,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0594, 0.0214, 0.0159, 0.0383, 0.0116, 0.022...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2003, 0.0313, 0.0466, 0.1177, 0.048, 0.1643...</td>\n",
       "      <td>[0.7997, 0.9687, 0.9534, 0.8823, 0.952, 0.8357...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0552, 0.0539, 0.016, 0.0871, 0.0415, 0.0261...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6529, 0.4981, 0.3616, 0.7079, 0.4507, 0.543...</td>\n",
       "      <td>[0.3471, 0.5019, 0.6384, 0.2921, 0.5493, 0.456...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0702, 0.0219, 0.0176, 0.0367, 0.0111, 0.025...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0678, 0.0338, 0.0461, 0.0598, 0.0619, 0.252...</td>\n",
       "      <td>[0.9322, 0.9662, 0.9539, 0.9402, 0.9381, 0.747...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0416, 0.0154, 0.0079, 0.0233, 0.0055, 0.012...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1049, 0.0558, 0.2765, 0.1798, 0.0386, 0.218...</td>\n",
       "      <td>[0.8951, 0.9442, 0.7235, 0.8202, 0.9614, 0.781...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0358, 0.0271, 0.0109, 0.0486, 0.0079, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5524, 0.2742, 0.6033, 0.6418, 0.1591, 0.806...</td>\n",
       "      <td>[0.4476, 0.7258, 0.3967, 0.3582, 0.8409, 0.193...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0405, 0.0301, 0.0116, 0.0551, 0.0086, 0.010...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5526, 0.2366, 0.799, 0.6053, 0.1663, 0.9439...</td>\n",
       "      <td>[0.4474, 0.7634, 0.201, 0.3947, 0.8337, 0.0561...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0309, 0.0134, 0.006, 0.025, 0.0041, 0.0086,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2619, 0.0995, 0.4079, 0.2634, 0.0692, 0.35,...</td>\n",
       "      <td>[0.7381, 0.9005, 0.5921, 0.7366, 0.9308, 0.65,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0901, 0.0701, 0.0589, 0.0934, 0.041, 0.0819...</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.5832, 0.3207, 0.5398, 0.4192, 0.455, 0.4762...</td>\n",
       "      <td>[0.4168, 0.6793, 0.4602, 0.5808, 0.545, 0.5238...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0667, 0.0639, 0.0233, 0.0508, 0.0422, 0.041...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.3981, 0.4508, 0.3703, 0.5005, 0.2518, 0.334...</td>\n",
       "      <td>[0.6019, 0.5492, 0.6297, 0.4995, 0.7482, 0.665...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0327, 0.0149, 0.0063, 0.012, 0.0088, 0.0126...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1, 0.336, 0.0302, 0.0773, 0.3242, 0.0744, 0...</td>\n",
       "      <td>[0.9, 0.664, 0.9698, 0.9227, 0.6758, 0.9256, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0635, 0.0701, 0.0242, 0.0439, 0.0569, 0.053...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2168, 0.8861, 0.3273, 0.4536, 0.6171, 0.303...</td>\n",
       "      <td>[0.7832, 0.1139, 0.6727, 0.5464, 0.3829, 0.696...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0813, 0.0619, 0.0285, 0.0704, 0.0539, 0.050...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3892, 0.452, 0.6462, 0.3464, 0.1921, 0.3346...</td>\n",
       "      <td>[0.6108, 0.548, 0.3538, 0.6536, 0.8079, 0.6654...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0402, 0.0471, 0.021, 0.0535, 0.0534, 0.022,...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4539, 0.5369, 0.5749, 0.4771, 0.5807, 0.325...</td>\n",
       "      <td>[0.5461, 0.4631, 0.4251, 0.5229, 0.4193, 0.674...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1042, 0.0484, 0.0251, 0.052, 0.0289, 0.0419...               0   \n",
       "1   [0.1233, 0.0992, 0.0258, 0.0568, 0.0703, 0.045...               0   \n",
       "2   [0.0904, 0.0518, 0.0224, 0.0518, 0.0316, 0.043...               0   \n",
       "3   [0.0426, 0.0425, 0.0141, 0.0281, 0.0347, 0.024...               1   \n",
       "4   [0.0666, 0.0305, 0.0146, 0.0294, 0.0155, 0.027...               1   \n",
       "5   [0.0896, 0.0432, 0.0326, 0.0513, 0.0244, 0.049...               2   \n",
       "6   [0.0469, 0.0458, 0.0102, 0.1815, 0.0287, 0.015...               3   \n",
       "7   [0.0688, 0.0463, 0.0238, 0.1161, 0.027, 0.0351...               3   \n",
       "8   [0.1082, 0.0724, 0.0408, 0.1397, 0.0474, 0.066...               3   \n",
       "9   [0.0443, 0.1225, 0.047, 0.0772, 0.2651, 0.0765...               4   \n",
       "10  [0.0459, 0.0864, 0.0154, 0.0341, 0.081, 0.0518...               4   \n",
       "11  [0.0365, 0.0435, 0.0173, 0.039, 0.0452, 0.0496...               5   \n",
       "12  [0.035, 0.0638, 0.026, 0.0684, 0.0539, 0.0305,...               6   \n",
       "13  [0.1906, 0.155, 0.1479, 0.2101, 0.0786, 0.1855...               7   \n",
       "14  [0.0289, 0.0281, 0.0101, 0.0522, 0.0263, 0.015...               8   \n",
       "15  [0.061, 0.0457, 0.019, 0.036, 0.0395, 0.0299, ...               8   \n",
       "16  [0.0797, 0.0342, 0.0256, 0.0442, 0.0179, 0.040...               9   \n",
       "17  [0.0635, 0.0455, 0.024, 0.0545, 0.0302, 0.0378...               9   \n",
       "18  [0.0851, 0.0296, 0.0218, 0.0372, 0.0155, 0.034...              10   \n",
       "19  [0.0763, 0.0302, 0.0231, 0.0336, 0.0165, 0.033...              10   \n",
       "20  [0.084, 0.0468, 0.0246, 0.0608, 0.0264, 0.0459...              11   \n",
       "21  [0.0478, 0.0195, 0.0108, 0.0314, 0.0083, 0.017...              11   \n",
       "22  [0.0594, 0.0274, 0.0155, 0.0366, 0.0125, 0.024...              12   \n",
       "23  [0.0717, 0.0433, 0.0219, 0.0522, 0.0271, 0.045...              13   \n",
       "24  [0.0812, 0.0274, 0.0199, 0.0401, 0.0142, 0.030...              13   \n",
       "25  [0.0597, 0.0412, 0.0223, 0.0531, 0.0318, 0.034...              14   \n",
       "26  [0.0596, 0.0231, 0.0142, 0.0303, 0.011, 0.021,...              14   \n",
       "27  [0.0247, 0.0264, 0.0109, 0.0303, 0.0249, 0.017...              15   \n",
       "28  [0.0384, 0.0508, 0.0306, 0.0852, 0.0358, 0.037...              15   \n",
       "29  [0.016, 0.0331, 0.0108, 0.0484, 0.0433, 0.0171...              15   \n",
       "30  [0.325, 0.5298, 0.468, 0.3639, 0.6592, 0.4744,...              16   \n",
       "31  [0.0565, 0.0494, 0.0231, 0.0687, 0.0293, 0.029...              17   \n",
       "32  [0.0416, 0.0319, 0.0128, 0.0548, 0.0174, 0.016...              17   \n",
       "33  [0.0518, 0.0172, 0.01, 0.0217, 0.0081, 0.0174,...              18   \n",
       "34  [0.0622, 0.0422, 0.0262, 0.043, 0.042, 0.0422,...              19   \n",
       "35  [0.0439, 0.016, 0.0079, 0.0212, 0.0066, 0.0138...              20   \n",
       "36  [0.0613, 0.0235, 0.0144, 0.0324, 0.0097, 0.023...              21   \n",
       "37  [0.0404, 0.0354, 0.0162, 0.0632, 0.0106, 0.016...              21   \n",
       "38  [0.059, 0.0235, 0.0213, 0.0588, 0.0161, 0.0236...              22   \n",
       "39  [0.0594, 0.0214, 0.0159, 0.0383, 0.0116, 0.022...              22   \n",
       "40  [0.0552, 0.0539, 0.016, 0.0871, 0.0415, 0.0261...              22   \n",
       "41  [0.0702, 0.0219, 0.0176, 0.0367, 0.0111, 0.025...              22   \n",
       "42  [0.0416, 0.0154, 0.0079, 0.0233, 0.0055, 0.012...              23   \n",
       "43  [0.0358, 0.0271, 0.0109, 0.0486, 0.0079, 0.011...              23   \n",
       "44  [0.0405, 0.0301, 0.0116, 0.0551, 0.0086, 0.010...              23   \n",
       "45  [0.0309, 0.0134, 0.006, 0.025, 0.0041, 0.0086,...              23   \n",
       "46  [0.0901, 0.0701, 0.0589, 0.0934, 0.041, 0.0819...              24   \n",
       "47  [0.0667, 0.0639, 0.0233, 0.0508, 0.0422, 0.041...              24   \n",
       "48  [0.0327, 0.0149, 0.0063, 0.012, 0.0088, 0.0126...              25   \n",
       "49  [0.0635, 0.0701, 0.0242, 0.0439, 0.0569, 0.053...              25   \n",
       "50  [0.0813, 0.0619, 0.0285, 0.0704, 0.0539, 0.050...              26   \n",
       "51  [0.0402, 0.0471, 0.021, 0.0535, 0.0534, 0.022,...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1663, 0.3137, 0.1153, 0.1848, 0.1258, 0.103...   \n",
       "1                  0  [0.5611, 0.5839, 0.2654, 0.4551, 0.371, 0.2181...   \n",
       "2                  0  [0.3176, 0.3954, 0.1702, 0.2282, 0.1315, 0.126...   \n",
       "3                  0  [0.4164, 0.39, 0.202, 0.4336, 0.6222, 0.3535, ...   \n",
       "4                 23  [0.0839, 0.118, 0.0607, 0.0977, 0.1041, 0.0394...   \n",
       "5                 10  [0.0786, 0.0581, 0.0907, 0.0997, 0.114, 0.2039...   \n",
       "6                  3  [0.8513, 0.6476, 0.8703, 0.7572, 0.3603, 0.791...   \n",
       "7                  3  [0.6829, 0.3014, 0.6709, 0.4232, 0.2551, 0.691...   \n",
       "8                  3  [0.5139, 0.3071, 0.6804, 0.3698, 0.2313, 0.519...   \n",
       "9                  4  [0.8175, 0.9166, 0.838, 0.7962, 0.9413, 0.6959...   \n",
       "10                 8  [0.5453, 0.8426, 0.3829, 0.4947, 0.6777, 0.472...   \n",
       "11                 8  [0.3545, 0.6633, 0.3292, 0.3758, 0.6932, 0.425...   \n",
       "12                17  [0.6948, 0.758, 0.8614, 0.8888, 0.7483, 0.7599...   \n",
       "13                 7  [0.6997, 0.4379, 0.511, 0.5176, 0.7279, 0.9167...   \n",
       "14                 8  [0.5514, 0.4852, 0.5665, 0.7105, 0.603, 0.4678...   \n",
       "15                 8  [0.2168, 0.5447, 0.4117, 0.3447, 0.4323, 0.237...   \n",
       "16                23  [0.0899, 0.0738, 0.0725, 0.1436, 0.1578, 0.185...   \n",
       "17                23  [0.3064, 0.3728, 0.3297, 0.4917, 0.4814, 0.444...   \n",
       "18                10  [0.0991, 0.3432, 0.0814, 0.0976, 0.3538, 0.129...   \n",
       "19                10  [0.129, 0.5298, 0.091, 0.2152, 0.4348, 0.13, 0...   \n",
       "20                 0  [0.2395, 0.3268, 0.3701, 0.5489, 0.1418, 0.226...   \n",
       "21                22  [0.1569, 0.1456, 0.1599, 0.4351, 0.0576, 0.157...   \n",
       "22                22  [0.1294, 0.1694, 0.1518, 0.3103, 0.1037, 0.258...   \n",
       "23                13  [0.5161, 0.4972, 0.4657, 0.3427, 0.4697, 0.518...   \n",
       "24                10  [0.0788, 0.0775, 0.1222, 0.0826, 0.0777, 0.125...   \n",
       "25                22  [0.1822, 0.191, 0.6293, 0.3844, 0.288, 0.3457,...   \n",
       "26                22  [0.0632, 0.035, 0.238, 0.1435, 0.0656, 0.1621,...   \n",
       "27                15  [0.4577, 0.4398, 0.4514, 0.3677, 0.813, 0.592,...   \n",
       "28                15  [0.7499, 0.5265, 0.6421, 0.5997, 0.9372, 0.884...   \n",
       "29                15  [0.7918, 0.5196, 0.8165, 0.7262, 0.9566, 0.870...   \n",
       "30                16  [0.9998, 0.9999, 0.9969, 0.9997, 1.0, 0.9982, ...   \n",
       "31                17  [0.6964, 0.3907, 0.6175, 0.4646, 0.3492, 0.373...   \n",
       "32                17  [0.454, 0.5358, 0.6802, 0.6589, 0.267, 0.5304,...   \n",
       "33                10  [0.0389, 0.0819, 0.0501, 0.0394, 0.0628, 0.066...   \n",
       "34                19  [0.3745, 0.6988, 0.0972, 0.2113, 0.5098, 0.128...   \n",
       "35                23  [0.0868, 0.0802, 0.2413, 0.1362, 0.0584, 0.225...   \n",
       "36                23  [0.0855, 0.057, 0.1676, 0.162, 0.057, 0.1657, ...   \n",
       "37                23  [0.3362, 0.3234, 0.6359, 0.6622, 0.1486, 0.864...   \n",
       "38                22  [0.3978, 0.12, 0.1466, 0.2101, 0.1679, 0.6444,...   \n",
       "39                22  [0.2003, 0.0313, 0.0466, 0.1177, 0.048, 0.1643...   \n",
       "40                22  [0.6529, 0.4981, 0.3616, 0.7079, 0.4507, 0.543...   \n",
       "41                22  [0.0678, 0.0338, 0.0461, 0.0598, 0.0619, 0.252...   \n",
       "42                23  [0.1049, 0.0558, 0.2765, 0.1798, 0.0386, 0.218...   \n",
       "43                23  [0.5524, 0.2742, 0.6033, 0.6418, 0.1591, 0.806...   \n",
       "44                23  [0.5526, 0.2366, 0.799, 0.6053, 0.1663, 0.9439...   \n",
       "45                23  [0.2619, 0.0995, 0.4079, 0.2634, 0.0692, 0.35,...   \n",
       "46                17  [0.5832, 0.3207, 0.5398, 0.4192, 0.455, 0.4762...   \n",
       "47                24  [0.3981, 0.4508, 0.3703, 0.5005, 0.2518, 0.334...   \n",
       "48                10  [0.1, 0.336, 0.0302, 0.0773, 0.3242, 0.0744, 0...   \n",
       "49                25  [0.2168, 0.8861, 0.3273, 0.4536, 0.6171, 0.303...   \n",
       "50                23  [0.3892, 0.452, 0.6462, 0.3464, 0.1921, 0.3346...   \n",
       "51                26  [0.4539, 0.5369, 0.5749, 0.4771, 0.5807, 0.325...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8337, 0.6863, 0.8847, 0.8152, 0.8742, 0.896...  0.596154  0.140986  \n",
       "1   [0.4389, 0.4161, 0.7346, 0.5449, 0.629, 0.7819...       NaN       NaN  \n",
       "2   [0.6824, 0.6046, 0.8298, 0.7718, 0.8685, 0.873...       NaN       NaN  \n",
       "3   [0.5836, 0.61, 0.798, 0.5664, 0.3778, 0.6465, ...       NaN       NaN  \n",
       "4   [0.9161, 0.882, 0.9393, 0.9023, 0.8959, 0.9606...       NaN       NaN  \n",
       "5   [0.9214, 0.9419, 0.9093, 0.9003, 0.886, 0.7961...       NaN       NaN  \n",
       "6   [0.1487, 0.3524, 0.1297, 0.2428, 0.6397, 0.208...       NaN       NaN  \n",
       "7   [0.3171, 0.6986, 0.3291, 0.5768, 0.7449, 0.309...       NaN       NaN  \n",
       "8   [0.4861, 0.6929, 0.3196, 0.6302, 0.7687, 0.480...       NaN       NaN  \n",
       "9   [0.1825, 0.0834, 0.162, 0.2038, 0.0587, 0.3041...       NaN       NaN  \n",
       "10  [0.4547, 0.1574, 0.6171, 0.5053, 0.3223, 0.527...       NaN       NaN  \n",
       "11  [0.6455, 0.3367, 0.6708, 0.6242, 0.3068, 0.574...       NaN       NaN  \n",
       "12  [0.3052, 0.242, 0.1386, 0.1112, 0.2517, 0.2401...       NaN       NaN  \n",
       "13  [0.3003, 0.5621, 0.489, 0.4824, 0.2721, 0.0833...       NaN       NaN  \n",
       "14  [0.4486, 0.5148, 0.4335, 0.2895, 0.397, 0.5322...       NaN       NaN  \n",
       "15  [0.7832, 0.4553, 0.5883, 0.6553, 0.5677, 0.762...       NaN       NaN  \n",
       "16  [0.9101, 0.9262, 0.9275, 0.8564, 0.8422, 0.814...       NaN       NaN  \n",
       "17  [0.6936, 0.6272, 0.6703, 0.5083, 0.5186, 0.556...       NaN       NaN  \n",
       "18  [0.9009, 0.6568, 0.9186, 0.9024, 0.6462, 0.870...       NaN       NaN  \n",
       "19  [0.871, 0.4702, 0.909, 0.7848, 0.5652, 0.87, 0...       NaN       NaN  \n",
       "20  [0.7605, 0.6732, 0.6299, 0.4511, 0.8582, 0.773...       NaN       NaN  \n",
       "21  [0.8431, 0.8544, 0.8401, 0.5649, 0.9424, 0.842...       NaN       NaN  \n",
       "22  [0.8706, 0.8306, 0.8482, 0.6897, 0.8963, 0.741...       NaN       NaN  \n",
       "23  [0.4839, 0.5028, 0.5343, 0.6573, 0.5303, 0.481...       NaN       NaN  \n",
       "24  [0.9212, 0.9225, 0.8778, 0.9174, 0.9223, 0.874...       NaN       NaN  \n",
       "25  [0.8178, 0.809, 0.3707, 0.6156, 0.712, 0.6543,...       NaN       NaN  \n",
       "26  [0.9368, 0.965, 0.762, 0.8565, 0.9344, 0.8379,...       NaN       NaN  \n",
       "27  [0.5423, 0.5602, 0.5486, 0.6323, 0.187, 0.408,...       NaN       NaN  \n",
       "28  [0.2501, 0.4735, 0.3579, 0.4003, 0.0628, 0.115...       NaN       NaN  \n",
       "29  [0.2082, 0.4804, 0.1835, 0.2738, 0.0434, 0.129...       NaN       NaN  \n",
       "30  [0.0002, 1e-04, 0.0031, 0.0003, 0.0, 0.0018, 0...       NaN       NaN  \n",
       "31  [0.3036, 0.6093, 0.3825, 0.5354, 0.6508, 0.626...       NaN       NaN  \n",
       "32  [0.546, 0.4642, 0.3198, 0.3411, 0.733, 0.4696,...       NaN       NaN  \n",
       "33  [0.9611, 0.9181, 0.9499, 0.9606, 0.9372, 0.933...       NaN       NaN  \n",
       "34  [0.6255, 0.3012, 0.9028, 0.7887, 0.4902, 0.871...       NaN       NaN  \n",
       "35  [0.9132, 0.9198, 0.7587, 0.8638, 0.9416, 0.774...       NaN       NaN  \n",
       "36  [0.9145, 0.943, 0.8324, 0.838, 0.943, 0.8343, ...       NaN       NaN  \n",
       "37  [0.6638, 0.6766, 0.3641, 0.3378, 0.8514, 0.136...       NaN       NaN  \n",
       "38  [0.6022, 0.88, 0.8534, 0.7899, 0.8321, 0.3556,...       NaN       NaN  \n",
       "39  [0.7997, 0.9687, 0.9534, 0.8823, 0.952, 0.8357...       NaN       NaN  \n",
       "40  [0.3471, 0.5019, 0.6384, 0.2921, 0.5493, 0.456...       NaN       NaN  \n",
       "41  [0.9322, 0.9662, 0.9539, 0.9402, 0.9381, 0.747...       NaN       NaN  \n",
       "42  [0.8951, 0.9442, 0.7235, 0.8202, 0.9614, 0.781...       NaN       NaN  \n",
       "43  [0.4476, 0.7258, 0.3967, 0.3582, 0.8409, 0.193...       NaN       NaN  \n",
       "44  [0.4474, 0.7634, 0.201, 0.3947, 0.8337, 0.0561...       NaN       NaN  \n",
       "45  [0.7381, 0.9005, 0.5921, 0.7366, 0.9308, 0.65,...       NaN       NaN  \n",
       "46  [0.4168, 0.6793, 0.4602, 0.5808, 0.545, 0.5238...       NaN       NaN  \n",
       "47  [0.6019, 0.5492, 0.6297, 0.4995, 0.7482, 0.665...       NaN       NaN  \n",
       "48  [0.9, 0.664, 0.9698, 0.9227, 0.6758, 0.9256, 0...       NaN       NaN  \n",
       "49  [0.7832, 0.1139, 0.6727, 0.5464, 0.3829, 0.696...       NaN       NaN  \n",
       "50  [0.6108, 0.548, 0.3538, 0.6536, 0.8079, 0.6654...       NaN       NaN  \n",
       "51  [0.5461, 0.4631, 0.4251, 0.5229, 0.4193, 0.674...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941 : Training: loss:  0.1482374\n",
      "942 : Training: loss:  0.12320118\n",
      "943 : Training: loss:  0.12944372\n",
      "944 : Training: loss:  0.14363137\n",
      "945 : Training: loss:  0.11543576\n",
      "946 : Training: loss:  0.1390576\n",
      "947 : Training: loss:  0.14022039\n",
      "948 : Training: loss:  0.09973349\n",
      "949 : Training: loss:  0.10658329\n",
      "950 : Training: loss:  0.12615158\n",
      "951 : Training: loss:  0.14879906\n",
      "952 : Training: loss:  0.15223329\n",
      "953 : Training: loss:  0.13337262\n",
      "954 : Training: loss:  0.14404738\n",
      "955 : Training: loss:  0.16770801\n",
      "956 : Training: loss:  0.12814748\n",
      "957 : Training: loss:  0.13091664\n",
      "958 : Training: loss:  0.1592161\n",
      "959 : Training: loss:  0.116384804\n",
      "960 : Training: loss:  0.13997258\n",
      "Validation: Loss:  0.13987708  Accuracy:  0.5576923\n",
      "961 : Training: loss:  0.13263871\n",
      "962 : Training: loss:  0.13809854\n",
      "963 : Training: loss:  0.11772036\n",
      "964 : Training: loss:  0.11264902\n",
      "965 : Training: loss:  0.13750908\n",
      "966 : Training: loss:  0.109123744\n",
      "967 : Training: loss:  0.1341991\n",
      "968 : Training: loss:  0.119836666\n",
      "969 : Training: loss:  0.13779661\n",
      "970 : Training: loss:  0.13452221\n",
      "971 : Training: loss:  0.15295248\n",
      "972 : Training: loss:  0.12866348\n",
      "973 : Training: loss:  0.11704578\n",
      "974 : Training: loss:  0.13596375\n",
      "975 : Training: loss:  0.13637692\n",
      "976 : Training: loss:  0.1460099\n",
      "977 : Training: loss:  0.13362955\n",
      "978 : Training: loss:  0.12884735\n",
      "979 : Training: loss:  0.11310672\n",
      "980 : Training: loss:  0.12930524\n",
      "Validation: Loss:  0.13883653  Accuracy:  0.5576923\n",
      "981 : Training: loss:  0.117770165\n",
      "982 : Training: loss:  0.103809275\n",
      "983 : Training: loss:  0.14496061\n",
      "984 : Training: loss:  0.122557744\n",
      "985 : Training: loss:  0.09420095\n",
      "986 : Training: loss:  0.13971727\n",
      "987 : Training: loss:  0.13593946\n",
      "988 : Training: loss:  0.11976212\n",
      "989 : Training: loss:  0.103446245\n",
      "990 : Training: loss:  0.13529393\n",
      "991 : Training: loss:  0.1481492\n",
      "992 : Training: loss:  0.12579761\n",
      "993 : Training: loss:  0.13529885\n",
      "994 : Training: loss:  0.12261103\n",
      "995 : Training: loss:  0.108473785\n",
      "996 : Training: loss:  0.12656748\n",
      "997 : Training: loss:  0.13543932\n",
      "998 : Training: loss:  0.12543258\n",
      "999 : Training: loss:  0.09845073\n",
      "1000 : Training: loss:  0.12455929\n",
      "Validation: Loss:  0.13788073  Accuracy:  0.63461536\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1107, 0.0491, 0.0279, 0.0509, 0.031, 0.0414...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1713, 0.3272, 0.1143, 0.1917, 0.1315, 0.102...</td>\n",
       "      <td>[0.8287, 0.6728, 0.8857, 0.8083, 0.8685, 0.898...</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.137881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1344, 0.1051, 0.0273, 0.0555, 0.0778, 0.045...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5692, 0.5991, 0.2666, 0.4631, 0.3865, 0.215...</td>\n",
       "      <td>[0.4308, 0.4009, 0.7334, 0.5369, 0.6135, 0.784...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0965, 0.0529, 0.0243, 0.0517, 0.0342, 0.043...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3268, 0.4141, 0.1698, 0.2409, 0.1369, 0.123...</td>\n",
       "      <td>[0.6732, 0.5859, 0.8302, 0.7591, 0.8631, 0.876...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.044, 0.0449, 0.0153, 0.0269, 0.04, 0.0248, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4269, 0.4084, 0.2012, 0.4473, 0.6403, 0.355...</td>\n",
       "      <td>[0.5731, 0.5916, 0.7988, 0.5527, 0.3597, 0.644...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0704, 0.0315, 0.0169, 0.0282, 0.017, 0.0274...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.086, 0.1256, 0.0593, 0.1037, 0.1101, 0.0394...</td>\n",
       "      <td>[0.914, 0.8744, 0.9407, 0.8963, 0.8899, 0.9606...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0909, 0.0423, 0.0369, 0.0493, 0.0253, 0.048...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0807, 0.058, 0.0953, 0.1027, 0.1183, 0.2132...</td>\n",
       "      <td>[0.9193, 0.942, 0.9047, 0.8973, 0.8817, 0.7868...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0448, 0.0431, 0.0097, 0.1968, 0.0271, 0.013...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.865, 0.6648, 0.8823, 0.7764, 0.3583, 0.8081...</td>\n",
       "      <td>[0.135, 0.3352, 0.1177, 0.2236, 0.6417, 0.1919...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0673, 0.0444, 0.0248, 0.1222, 0.0265, 0.032...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7028, 0.3057, 0.6894, 0.4386, 0.254, 0.7127...</td>\n",
       "      <td>[0.2972, 0.6943, 0.3106, 0.5614, 0.746, 0.2873...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1057, 0.0689, 0.0418, 0.1429, 0.0461, 0.061...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5382, 0.3166, 0.7046, 0.3807, 0.2314, 0.539...</td>\n",
       "      <td>[0.4618, 0.6834, 0.2954, 0.6193, 0.7686, 0.460...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0421, 0.1224, 0.0437, 0.0732, 0.2864, 0.079...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8248, 0.9245, 0.8396, 0.8011, 0.9458, 0.696...</td>\n",
       "      <td>[0.1752, 0.0755, 0.1604, 0.1989, 0.0542, 0.303...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0474, 0.0947, 0.0162, 0.0323, 0.098, 0.0565...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5564, 0.858, 0.3685, 0.5073, 0.6997, 0.4832...</td>\n",
       "      <td>[0.4436, 0.142, 0.6315, 0.4927, 0.3003, 0.5168...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0361, 0.0444, 0.0182, 0.0376, 0.0518, 0.052...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.367, 0.6848, 0.3244, 0.3847, 0.7103, 0.4304...</td>\n",
       "      <td>[0.633, 0.3152, 0.6756, 0.6153, 0.2897, 0.5696...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0308, 0.0597, 0.0255, 0.0616, 0.0512, 0.028...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6958, 0.7566, 0.8596, 0.8946, 0.759, 0.754,...</td>\n",
       "      <td>[0.3042, 0.2434, 0.1404, 0.1054, 0.241, 0.246,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1819, 0.1462, 0.1484, 0.2048, 0.0731, 0.177...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.6917, 0.4097, 0.5002, 0.5188, 0.7333, 0.921...</td>\n",
       "      <td>[0.3083, 0.5903, 0.4998, 0.4812, 0.2667, 0.078...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0289, 0.0284, 0.0104, 0.0523, 0.0294, 0.015...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5701, 0.515, 0.5676, 0.7289, 0.6162, 0.4681...</td>\n",
       "      <td>[0.4299, 0.485, 0.4324, 0.2711, 0.3838, 0.5319...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0633, 0.0466, 0.0201, 0.0342, 0.0443, 0.029...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2229, 0.5784, 0.4049, 0.3549, 0.4487, 0.227...</td>\n",
       "      <td>[0.7771, 0.4216, 0.5951, 0.6451, 0.5513, 0.772...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0806, 0.0332, 0.0291, 0.0421, 0.0182, 0.039...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0919, 0.0753, 0.0755, 0.1473, 0.162, 0.1911...</td>\n",
       "      <td>[0.9081, 0.9247, 0.9245, 0.8527, 0.838, 0.8089...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0627, 0.0443, 0.0257, 0.0522, 0.0309, 0.036...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3138, 0.3829, 0.3418, 0.5001, 0.491, 0.4552...</td>\n",
       "      <td>[0.6862, 0.6171, 0.6582, 0.4999, 0.509, 0.5448...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0893, 0.0291, 0.0252, 0.0356, 0.0165, 0.033...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.103, 0.3551, 0.0826, 0.0972, 0.3644, 0.1327...</td>\n",
       "      <td>[0.897, 0.6449, 0.9174, 0.9028, 0.6356, 0.8673...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0791, 0.0295, 0.0265, 0.0317, 0.0177, 0.033...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1342, 0.5471, 0.0915, 0.2166, 0.4491, 0.129...</td>\n",
       "      <td>[0.8658, 0.4529, 0.9085, 0.7834, 0.5509, 0.870...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0858, 0.0462, 0.0265, 0.0603, 0.027, 0.044,...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2501, 0.3316, 0.384, 0.5681, 0.1433, 0.2291...</td>\n",
       "      <td>[0.7499, 0.6684, 0.616, 0.4319, 0.8567, 0.7709...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0496, 0.0193, 0.0127, 0.0315, 0.0088, 0.016...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1703, 0.1469, 0.1686, 0.4567, 0.0583, 0.167...</td>\n",
       "      <td>[0.8297, 0.8531, 0.8314, 0.5433, 0.9417, 0.832...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0613, 0.0271, 0.0176, 0.036, 0.0132, 0.0236...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1386, 0.1743, 0.1557, 0.325, 0.1048, 0.2659...</td>\n",
       "      <td>[0.8614, 0.8257, 0.8443, 0.675, 0.8952, 0.7341...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0718, 0.0425, 0.024, 0.051, 0.029, 0.0458, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5314, 0.5128, 0.4837, 0.3432, 0.4869, 0.526...</td>\n",
       "      <td>[0.4686, 0.4872, 0.5163, 0.6568, 0.5131, 0.473...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.084, 0.0265, 0.0227, 0.0389, 0.0148, 0.0297...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0836, 0.0808, 0.1336, 0.084, 0.079, 0.1318,...</td>\n",
       "      <td>[0.9164, 0.9192, 0.8664, 0.916, 0.921, 0.8682,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0582, 0.0401, 0.0238, 0.0508, 0.0334, 0.033...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.19, 0.2026, 0.647, 0.3978, 0.2991, 0.3519, ...</td>\n",
       "      <td>[0.81, 0.7974, 0.353, 0.6022, 0.7009, 0.6481, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0607, 0.0224, 0.0161, 0.0287, 0.0114, 0.019...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0663, 0.0352, 0.25, 0.1496, 0.067, 0.1673, ...</td>\n",
       "      <td>[0.9337, 0.9648, 0.75, 0.8504, 0.933, 0.8327, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0231, 0.0254, 0.0115, 0.0286, 0.0262, 0.017...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.4756, 0.4553, 0.468, 0.3806, 0.8256, 0.6088...</td>\n",
       "      <td>[0.5244, 0.5447, 0.532, 0.6194, 0.1744, 0.3912...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0339, 0.0471, 0.0302, 0.0819, 0.0348, 0.034...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7593, 0.5196, 0.656, 0.6127, 0.9412, 0.8976...</td>\n",
       "      <td>[0.2407, 0.4804, 0.344, 0.3873, 0.0588, 0.1024...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0142, 0.0316, 0.0105, 0.0453, 0.0447, 0.016...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8005, 0.5285, 0.8251, 0.7362, 0.9606, 0.879...</td>\n",
       "      <td>[0.1995, 0.4715, 0.1749, 0.2638, 0.0394, 0.120...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3239, 0.5378, 0.4575, 0.3554, 0.6757, 0.490...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9999, 0.9959, 0.9997, 1.0, 0.998, 1...</td>\n",
       "      <td>[0.0002, 1e-04, 0.0041, 0.0003, 0.0, 0.002, 0....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0522, 0.0462, 0.0236, 0.0672, 0.0284, 0.026...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.714, 0.3877, 0.6476, 0.4796, 0.3527, 0.3902...</td>\n",
       "      <td>[0.286, 0.6123, 0.3524, 0.5204, 0.6473, 0.6098...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0393, 0.0299, 0.0131, 0.0546, 0.017, 0.0147...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.4749, 0.5436, 0.7096, 0.6819, 0.2697, 0.543...</td>\n",
       "      <td>[0.5251, 0.4564, 0.2904, 0.3181, 0.7303, 0.456...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0549, 0.0171, 0.0119, 0.021, 0.0087, 0.0172...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0423, 0.0893, 0.0509, 0.0424, 0.0659, 0.070...</td>\n",
       "      <td>[0.9577, 0.9107, 0.9491, 0.9576, 0.9341, 0.929...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0647, 0.0432, 0.0298, 0.0424, 0.0487, 0.043...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.4064, 0.7281, 0.0978, 0.2216, 0.5356, 0.128...</td>\n",
       "      <td>[0.5936, 0.2719, 0.9022, 0.7784, 0.4644, 0.871...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0458, 0.0158, 0.0092, 0.0206, 0.007, 0.0132...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0919, 0.083, 0.2523, 0.1442, 0.0598, 0.2361...</td>\n",
       "      <td>[0.9081, 0.917, 0.7477, 0.8558, 0.9402, 0.7639...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0631, 0.023, 0.0166, 0.0312, 0.01, 0.0225, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0848, 0.0567, 0.171, 0.166, 0.0554, 0.1709,...</td>\n",
       "      <td>[0.9152, 0.9433, 0.829, 0.834, 0.9446, 0.8291,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0387, 0.0339, 0.0175, 0.0623, 0.0104, 0.015...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3321, 0.3094, 0.6425, 0.671, 0.1431, 0.8739...</td>\n",
       "      <td>[0.6679, 0.6906, 0.3575, 0.329, 0.8569, 0.1261...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.057, 0.0216, 0.0233, 0.0575, 0.0158, 0.0215...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4155, 0.1183, 0.1463, 0.2146, 0.1685, 0.664...</td>\n",
       "      <td>[0.5845, 0.8817, 0.8537, 0.7854, 0.8315, 0.335...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0604, 0.0205, 0.0181, 0.0373, 0.012, 0.0215...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2132, 0.0316, 0.0472, 0.1232, 0.0479, 0.176...</td>\n",
       "      <td>[0.7868, 0.9684, 0.9528, 0.8768, 0.9521, 0.823...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0536, 0.0526, 0.016, 0.0876, 0.0417, 0.0243...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6695, 0.4984, 0.3667, 0.7217, 0.4557, 0.556...</td>\n",
       "      <td>[0.3305, 0.5016, 0.6333, 0.2783, 0.5443, 0.443...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0725, 0.021, 0.0203, 0.0358, 0.0114, 0.0246...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0705, 0.0335, 0.0468, 0.0605, 0.0619, 0.267...</td>\n",
       "      <td>[0.9295, 0.9665, 0.9532, 0.9395, 0.9381, 0.732...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0424, 0.0151, 0.0092, 0.0227, 0.0056, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1072, 0.0554, 0.2871, 0.1866, 0.0377, 0.228...</td>\n",
       "      <td>[0.8928, 0.9446, 0.7129, 0.8134, 0.9623, 0.771...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0342, 0.0259, 0.0118, 0.0474, 0.0077, 0.010...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5572, 0.2593, 0.6047, 0.651, 0.1536, 0.8227...</td>\n",
       "      <td>[0.4428, 0.7407, 0.3953, 0.349, 0.8464, 0.1773...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0373, 0.0278, 0.0119, 0.0521, 0.008, 0.009,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5488, 0.2156, 0.8031, 0.6117, 0.1589, 0.949...</td>\n",
       "      <td>[0.4512, 0.7844, 0.1969, 0.3883, 0.8411, 0.050...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0307, 0.013, 0.0069, 0.0245, 0.0042, 0.0079...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2661, 0.096, 0.418, 0.2713, 0.0663, 0.3696,...</td>\n",
       "      <td>[0.7339, 0.904, 0.582, 0.7287, 0.9337, 0.6304,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0844, 0.0649, 0.0607, 0.0897, 0.0394, 0.077...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5969, 0.3177, 0.5646, 0.4235, 0.4601, 0.483...</td>\n",
       "      <td>[0.4031, 0.6823, 0.4354, 0.5765, 0.5399, 0.516...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0659, 0.0635, 0.024, 0.0492, 0.0432, 0.0405...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.4154, 0.4578, 0.3934, 0.5173, 0.2564, 0.337...</td>\n",
       "      <td>[0.5846, 0.5422, 0.6066, 0.4827, 0.7436, 0.662...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0341, 0.0152, 0.0074, 0.0111, 0.0103, 0.012...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1071, 0.3634, 0.0288, 0.0814, 0.3515, 0.073...</td>\n",
       "      <td>[0.8929, 0.6366, 0.9712, 0.9186, 0.6485, 0.926...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0657, 0.0724, 0.0262, 0.0426, 0.0653, 0.056...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2263, 0.8977, 0.3226, 0.4708, 0.6386, 0.299...</td>\n",
       "      <td>[0.7737, 0.1023, 0.6774, 0.5292, 0.3614, 0.700...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0813, 0.0606, 0.0291, 0.0681, 0.0548, 0.047...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3992, 0.4614, 0.651, 0.3602, 0.1957, 0.3325...</td>\n",
       "      <td>[0.6008, 0.5386, 0.349, 0.6398, 0.8043, 0.6675...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0384, 0.0459, 0.0211, 0.0514, 0.0574, 0.020...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4734, 0.5663, 0.5889, 0.4923, 0.5974, 0.328...</td>\n",
       "      <td>[0.5266, 0.4337, 0.4111, 0.5077, 0.4026, 0.671...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1107, 0.0491, 0.0279, 0.0509, 0.031, 0.0414...               0   \n",
       "1   [0.1344, 0.1051, 0.0273, 0.0555, 0.0778, 0.045...               0   \n",
       "2   [0.0965, 0.0529, 0.0243, 0.0517, 0.0342, 0.043...               0   \n",
       "3   [0.044, 0.0449, 0.0153, 0.0269, 0.04, 0.0248, ...               1   \n",
       "4   [0.0704, 0.0315, 0.0169, 0.0282, 0.017, 0.0274...               1   \n",
       "5   [0.0909, 0.0423, 0.0369, 0.0493, 0.0253, 0.048...               2   \n",
       "6   [0.0448, 0.0431, 0.0097, 0.1968, 0.0271, 0.013...               3   \n",
       "7   [0.0673, 0.0444, 0.0248, 0.1222, 0.0265, 0.032...               3   \n",
       "8   [0.1057, 0.0689, 0.0418, 0.1429, 0.0461, 0.061...               3   \n",
       "9   [0.0421, 0.1224, 0.0437, 0.0732, 0.2864, 0.079...               4   \n",
       "10  [0.0474, 0.0947, 0.0162, 0.0323, 0.098, 0.0565...               4   \n",
       "11  [0.0361, 0.0444, 0.0182, 0.0376, 0.0518, 0.052...               5   \n",
       "12  [0.0308, 0.0597, 0.0255, 0.0616, 0.0512, 0.028...               6   \n",
       "13  [0.1819, 0.1462, 0.1484, 0.2048, 0.0731, 0.177...               7   \n",
       "14  [0.0289, 0.0284, 0.0104, 0.0523, 0.0294, 0.015...               8   \n",
       "15  [0.0633, 0.0466, 0.0201, 0.0342, 0.0443, 0.029...               8   \n",
       "16  [0.0806, 0.0332, 0.0291, 0.0421, 0.0182, 0.039...               9   \n",
       "17  [0.0627, 0.0443, 0.0257, 0.0522, 0.0309, 0.036...               9   \n",
       "18  [0.0893, 0.0291, 0.0252, 0.0356, 0.0165, 0.033...              10   \n",
       "19  [0.0791, 0.0295, 0.0265, 0.0317, 0.0177, 0.033...              10   \n",
       "20  [0.0858, 0.0462, 0.0265, 0.0603, 0.027, 0.044,...              11   \n",
       "21  [0.0496, 0.0193, 0.0127, 0.0315, 0.0088, 0.016...              11   \n",
       "22  [0.0613, 0.0271, 0.0176, 0.036, 0.0132, 0.0236...              12   \n",
       "23  [0.0718, 0.0425, 0.024, 0.051, 0.029, 0.0458, ...              13   \n",
       "24  [0.084, 0.0265, 0.0227, 0.0389, 0.0148, 0.0297...              13   \n",
       "25  [0.0582, 0.0401, 0.0238, 0.0508, 0.0334, 0.033...              14   \n",
       "26  [0.0607, 0.0224, 0.0161, 0.0287, 0.0114, 0.019...              14   \n",
       "27  [0.0231, 0.0254, 0.0115, 0.0286, 0.0262, 0.017...              15   \n",
       "28  [0.0339, 0.0471, 0.0302, 0.0819, 0.0348, 0.034...              15   \n",
       "29  [0.0142, 0.0316, 0.0105, 0.0453, 0.0447, 0.016...              15   \n",
       "30  [0.3239, 0.5378, 0.4575, 0.3554, 0.6757, 0.490...              16   \n",
       "31  [0.0522, 0.0462, 0.0236, 0.0672, 0.0284, 0.026...              17   \n",
       "32  [0.0393, 0.0299, 0.0131, 0.0546, 0.017, 0.0147...              17   \n",
       "33  [0.0549, 0.0171, 0.0119, 0.021, 0.0087, 0.0172...              18   \n",
       "34  [0.0647, 0.0432, 0.0298, 0.0424, 0.0487, 0.043...              19   \n",
       "35  [0.0458, 0.0158, 0.0092, 0.0206, 0.007, 0.0132...              20   \n",
       "36  [0.0631, 0.023, 0.0166, 0.0312, 0.01, 0.0225, ...              21   \n",
       "37  [0.0387, 0.0339, 0.0175, 0.0623, 0.0104, 0.015...              21   \n",
       "38  [0.057, 0.0216, 0.0233, 0.0575, 0.0158, 0.0215...              22   \n",
       "39  [0.0604, 0.0205, 0.0181, 0.0373, 0.012, 0.0215...              22   \n",
       "40  [0.0536, 0.0526, 0.016, 0.0876, 0.0417, 0.0243...              22   \n",
       "41  [0.0725, 0.021, 0.0203, 0.0358, 0.0114, 0.0246...              22   \n",
       "42  [0.0424, 0.0151, 0.0092, 0.0227, 0.0056, 0.011...              23   \n",
       "43  [0.0342, 0.0259, 0.0118, 0.0474, 0.0077, 0.010...              23   \n",
       "44  [0.0373, 0.0278, 0.0119, 0.0521, 0.008, 0.009,...              23   \n",
       "45  [0.0307, 0.013, 0.0069, 0.0245, 0.0042, 0.0079...              23   \n",
       "46  [0.0844, 0.0649, 0.0607, 0.0897, 0.0394, 0.077...              24   \n",
       "47  [0.0659, 0.0635, 0.024, 0.0492, 0.0432, 0.0405...              24   \n",
       "48  [0.0341, 0.0152, 0.0074, 0.0111, 0.0103, 0.012...              25   \n",
       "49  [0.0657, 0.0724, 0.0262, 0.0426, 0.0653, 0.056...              25   \n",
       "50  [0.0813, 0.0606, 0.0291, 0.0681, 0.0548, 0.047...              26   \n",
       "51  [0.0384, 0.0459, 0.0211, 0.0514, 0.0574, 0.020...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1713, 0.3272, 0.1143, 0.1917, 0.1315, 0.102...   \n",
       "1                  0  [0.5692, 0.5991, 0.2666, 0.4631, 0.3865, 0.215...   \n",
       "2                  0  [0.3268, 0.4141, 0.1698, 0.2409, 0.1369, 0.123...   \n",
       "3                  1  [0.4269, 0.4084, 0.2012, 0.4473, 0.6403, 0.355...   \n",
       "4                  0  [0.086, 0.1256, 0.0593, 0.1037, 0.1101, 0.0394...   \n",
       "5                 10  [0.0807, 0.058, 0.0953, 0.1027, 0.1183, 0.2132...   \n",
       "6                  3  [0.865, 0.6648, 0.8823, 0.7764, 0.3583, 0.8081...   \n",
       "7                  3  [0.7028, 0.3057, 0.6894, 0.4386, 0.254, 0.7127...   \n",
       "8                  3  [0.5382, 0.3166, 0.7046, 0.3807, 0.2314, 0.539...   \n",
       "9                  4  [0.8248, 0.9245, 0.8396, 0.8011, 0.9458, 0.696...   \n",
       "10                 8  [0.5564, 0.858, 0.3685, 0.5073, 0.6997, 0.4832...   \n",
       "11                 8  [0.367, 0.6848, 0.3244, 0.3847, 0.7103, 0.4304...   \n",
       "12                17  [0.6958, 0.7566, 0.8596, 0.8946, 0.759, 0.754,...   \n",
       "13                 7  [0.6917, 0.4097, 0.5002, 0.5188, 0.7333, 0.921...   \n",
       "14                 8  [0.5701, 0.515, 0.5676, 0.7289, 0.6162, 0.4681...   \n",
       "15                 8  [0.2229, 0.5784, 0.4049, 0.3549, 0.4487, 0.227...   \n",
       "16                23  [0.0919, 0.0753, 0.0755, 0.1473, 0.162, 0.1911...   \n",
       "17                23  [0.3138, 0.3829, 0.3418, 0.5001, 0.491, 0.4552...   \n",
       "18                10  [0.103, 0.3551, 0.0826, 0.0972, 0.3644, 0.1327...   \n",
       "19                10  [0.1342, 0.5471, 0.0915, 0.2166, 0.4491, 0.129...   \n",
       "20                 0  [0.2501, 0.3316, 0.384, 0.5681, 0.1433, 0.2291...   \n",
       "21                22  [0.1703, 0.1469, 0.1686, 0.4567, 0.0583, 0.167...   \n",
       "22                22  [0.1386, 0.1743, 0.1557, 0.325, 0.1048, 0.2659...   \n",
       "23                13  [0.5314, 0.5128, 0.4837, 0.3432, 0.4869, 0.526...   \n",
       "24                10  [0.0836, 0.0808, 0.1336, 0.084, 0.079, 0.1318,...   \n",
       "25                14  [0.19, 0.2026, 0.647, 0.3978, 0.2991, 0.3519, ...   \n",
       "26                22  [0.0663, 0.0352, 0.25, 0.1496, 0.067, 0.1673, ...   \n",
       "27                15  [0.4756, 0.4553, 0.468, 0.3806, 0.8256, 0.6088...   \n",
       "28                15  [0.7593, 0.5196, 0.656, 0.6127, 0.9412, 0.8976...   \n",
       "29                15  [0.8005, 0.5285, 0.8251, 0.7362, 0.9606, 0.879...   \n",
       "30                16  [0.9998, 0.9999, 0.9959, 0.9997, 1.0, 0.998, 1...   \n",
       "31                17  [0.714, 0.3877, 0.6476, 0.4796, 0.3527, 0.3902...   \n",
       "32                17  [0.4749, 0.5436, 0.7096, 0.6819, 0.2697, 0.543...   \n",
       "33                 0  [0.0423, 0.0893, 0.0509, 0.0424, 0.0659, 0.070...   \n",
       "34                19  [0.4064, 0.7281, 0.0978, 0.2216, 0.5356, 0.128...   \n",
       "35                23  [0.0919, 0.083, 0.2523, 0.1442, 0.0598, 0.2361...   \n",
       "36                23  [0.0848, 0.0567, 0.171, 0.166, 0.0554, 0.1709,...   \n",
       "37                23  [0.3321, 0.3094, 0.6425, 0.671, 0.1431, 0.8739...   \n",
       "38                22  [0.4155, 0.1183, 0.1463, 0.2146, 0.1685, 0.664...   \n",
       "39                22  [0.2132, 0.0316, 0.0472, 0.1232, 0.0479, 0.176...   \n",
       "40                22  [0.6695, 0.4984, 0.3667, 0.7217, 0.4557, 0.556...   \n",
       "41                22  [0.0705, 0.0335, 0.0468, 0.0605, 0.0619, 0.267...   \n",
       "42                23  [0.1072, 0.0554, 0.2871, 0.1866, 0.0377, 0.228...   \n",
       "43                23  [0.5572, 0.2593, 0.6047, 0.651, 0.1536, 0.8227...   \n",
       "44                23  [0.5488, 0.2156, 0.8031, 0.6117, 0.1589, 0.949...   \n",
       "45                23  [0.2661, 0.096, 0.418, 0.2713, 0.0663, 0.3696,...   \n",
       "46                13  [0.5969, 0.3177, 0.5646, 0.4235, 0.4601, 0.483...   \n",
       "47                24  [0.4154, 0.4578, 0.3934, 0.5173, 0.2564, 0.337...   \n",
       "48                 0  [0.1071, 0.3634, 0.0288, 0.0814, 0.3515, 0.073...   \n",
       "49                25  [0.2263, 0.8977, 0.3226, 0.4708, 0.6386, 0.299...   \n",
       "50                23  [0.3992, 0.4614, 0.651, 0.3602, 0.1957, 0.3325...   \n",
       "51                26  [0.4734, 0.5663, 0.5889, 0.4923, 0.5974, 0.328...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8287, 0.6728, 0.8857, 0.8083, 0.8685, 0.898...  0.634615  0.137881  \n",
       "1   [0.4308, 0.4009, 0.7334, 0.5369, 0.6135, 0.784...       NaN       NaN  \n",
       "2   [0.6732, 0.5859, 0.8302, 0.7591, 0.8631, 0.876...       NaN       NaN  \n",
       "3   [0.5731, 0.5916, 0.7988, 0.5527, 0.3597, 0.644...       NaN       NaN  \n",
       "4   [0.914, 0.8744, 0.9407, 0.8963, 0.8899, 0.9606...       NaN       NaN  \n",
       "5   [0.9193, 0.942, 0.9047, 0.8973, 0.8817, 0.7868...       NaN       NaN  \n",
       "6   [0.135, 0.3352, 0.1177, 0.2236, 0.6417, 0.1919...       NaN       NaN  \n",
       "7   [0.2972, 0.6943, 0.3106, 0.5614, 0.746, 0.2873...       NaN       NaN  \n",
       "8   [0.4618, 0.6834, 0.2954, 0.6193, 0.7686, 0.460...       NaN       NaN  \n",
       "9   [0.1752, 0.0755, 0.1604, 0.1989, 0.0542, 0.303...       NaN       NaN  \n",
       "10  [0.4436, 0.142, 0.6315, 0.4927, 0.3003, 0.5168...       NaN       NaN  \n",
       "11  [0.633, 0.3152, 0.6756, 0.6153, 0.2897, 0.5696...       NaN       NaN  \n",
       "12  [0.3042, 0.2434, 0.1404, 0.1054, 0.241, 0.246,...       NaN       NaN  \n",
       "13  [0.3083, 0.5903, 0.4998, 0.4812, 0.2667, 0.078...       NaN       NaN  \n",
       "14  [0.4299, 0.485, 0.4324, 0.2711, 0.3838, 0.5319...       NaN       NaN  \n",
       "15  [0.7771, 0.4216, 0.5951, 0.6451, 0.5513, 0.772...       NaN       NaN  \n",
       "16  [0.9081, 0.9247, 0.9245, 0.8527, 0.838, 0.8089...       NaN       NaN  \n",
       "17  [0.6862, 0.6171, 0.6582, 0.4999, 0.509, 0.5448...       NaN       NaN  \n",
       "18  [0.897, 0.6449, 0.9174, 0.9028, 0.6356, 0.8673...       NaN       NaN  \n",
       "19  [0.8658, 0.4529, 0.9085, 0.7834, 0.5509, 0.870...       NaN       NaN  \n",
       "20  [0.7499, 0.6684, 0.616, 0.4319, 0.8567, 0.7709...       NaN       NaN  \n",
       "21  [0.8297, 0.8531, 0.8314, 0.5433, 0.9417, 0.832...       NaN       NaN  \n",
       "22  [0.8614, 0.8257, 0.8443, 0.675, 0.8952, 0.7341...       NaN       NaN  \n",
       "23  [0.4686, 0.4872, 0.5163, 0.6568, 0.5131, 0.473...       NaN       NaN  \n",
       "24  [0.9164, 0.9192, 0.8664, 0.916, 0.921, 0.8682,...       NaN       NaN  \n",
       "25  [0.81, 0.7974, 0.353, 0.6022, 0.7009, 0.6481, ...       NaN       NaN  \n",
       "26  [0.9337, 0.9648, 0.75, 0.8504, 0.933, 0.8327, ...       NaN       NaN  \n",
       "27  [0.5244, 0.5447, 0.532, 0.6194, 0.1744, 0.3912...       NaN       NaN  \n",
       "28  [0.2407, 0.4804, 0.344, 0.3873, 0.0588, 0.1024...       NaN       NaN  \n",
       "29  [0.1995, 0.4715, 0.1749, 0.2638, 0.0394, 0.120...       NaN       NaN  \n",
       "30  [0.0002, 1e-04, 0.0041, 0.0003, 0.0, 0.002, 0....       NaN       NaN  \n",
       "31  [0.286, 0.6123, 0.3524, 0.5204, 0.6473, 0.6098...       NaN       NaN  \n",
       "32  [0.5251, 0.4564, 0.2904, 0.3181, 0.7303, 0.456...       NaN       NaN  \n",
       "33  [0.9577, 0.9107, 0.9491, 0.9576, 0.9341, 0.929...       NaN       NaN  \n",
       "34  [0.5936, 0.2719, 0.9022, 0.7784, 0.4644, 0.871...       NaN       NaN  \n",
       "35  [0.9081, 0.917, 0.7477, 0.8558, 0.9402, 0.7639...       NaN       NaN  \n",
       "36  [0.9152, 0.9433, 0.829, 0.834, 0.9446, 0.8291,...       NaN       NaN  \n",
       "37  [0.6679, 0.6906, 0.3575, 0.329, 0.8569, 0.1261...       NaN       NaN  \n",
       "38  [0.5845, 0.8817, 0.8537, 0.7854, 0.8315, 0.335...       NaN       NaN  \n",
       "39  [0.7868, 0.9684, 0.9528, 0.8768, 0.9521, 0.823...       NaN       NaN  \n",
       "40  [0.3305, 0.5016, 0.6333, 0.2783, 0.5443, 0.443...       NaN       NaN  \n",
       "41  [0.9295, 0.9665, 0.9532, 0.9395, 0.9381, 0.732...       NaN       NaN  \n",
       "42  [0.8928, 0.9446, 0.7129, 0.8134, 0.9623, 0.771...       NaN       NaN  \n",
       "43  [0.4428, 0.7407, 0.3953, 0.349, 0.8464, 0.1773...       NaN       NaN  \n",
       "44  [0.4512, 0.7844, 0.1969, 0.3883, 0.8411, 0.050...       NaN       NaN  \n",
       "45  [0.7339, 0.904, 0.582, 0.7287, 0.9337, 0.6304,...       NaN       NaN  \n",
       "46  [0.4031, 0.6823, 0.4354, 0.5765, 0.5399, 0.516...       NaN       NaN  \n",
       "47  [0.5846, 0.5422, 0.6066, 0.4827, 0.7436, 0.662...       NaN       NaN  \n",
       "48  [0.8929, 0.6366, 0.9712, 0.9186, 0.6485, 0.926...       NaN       NaN  \n",
       "49  [0.7737, 0.1023, 0.6774, 0.5292, 0.3614, 0.700...       NaN       NaN  \n",
       "50  [0.6008, 0.5386, 0.349, 0.6398, 0.8043, 0.6675...       NaN       NaN  \n",
       "51  [0.5266, 0.4337, 0.4111, 0.5077, 0.4026, 0.671...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 : Training: loss:  0.13592574\n",
      "1002 : Training: loss:  0.14001974\n",
      "1003 : Training: loss:  0.109673485\n",
      "1004 : Training: loss:  0.12622811\n",
      "1005 : Training: loss:  0.12527856\n",
      "1006 : Training: loss:  0.1382308\n",
      "1007 : Training: loss:  0.11957183\n",
      "1008 : Training: loss:  0.11385812\n",
      "1009 : Training: loss:  0.1290367\n",
      "1010 : Training: loss:  0.13486688\n",
      "1011 : Training: loss:  0.10587514\n",
      "1012 : Training: loss:  0.11130992\n",
      "1013 : Training: loss:  0.1328426\n",
      "1014 : Training: loss:  0.11980623\n",
      "1015 : Training: loss:  0.11030803\n",
      "1016 : Training: loss:  0.129137\n",
      "1017 : Training: loss:  0.09831821\n",
      "1018 : Training: loss:  0.12194956\n",
      "1019 : Training: loss:  0.15057641\n",
      "1020 : Training: loss:  0.14150353\n",
      "Validation: Loss:  0.13701804  Accuracy:  0.65384614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1077, 0.0479, 0.0271, 0.051, 0.0311, 0.0455...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1721, 0.3283, 0.1144, 0.1938, 0.1326, 0.101...</td>\n",
       "      <td>[0.8279, 0.6717, 0.8856, 0.8062, 0.8674, 0.898...</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.137018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1317, 0.1045, 0.0266, 0.0559, 0.0802, 0.050...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5708, 0.6017, 0.2663, 0.4651, 0.3896, 0.215...</td>\n",
       "      <td>[0.4292, 0.3983, 0.7337, 0.5349, 0.6104, 0.784...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0933, 0.0517, 0.0235, 0.0519, 0.0345, 0.048...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3295, 0.416, 0.1698, 0.2455, 0.138, 0.1238,...</td>\n",
       "      <td>[0.6705, 0.584, 0.8302, 0.7545, 0.862, 0.8762,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.042, 0.0444, 0.0149, 0.0268, 0.0418, 0.0282...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4318, 0.4126, 0.2004, 0.4489, 0.6466, 0.354...</td>\n",
       "      <td>[0.5682, 0.5874, 0.7996, 0.5511, 0.3534, 0.645...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.068, 0.0307, 0.0165, 0.028, 0.0172, 0.0309,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0866, 0.1277, 0.0589, 0.1051, 0.1118, 0.039...</td>\n",
       "      <td>[0.9134, 0.8723, 0.9411, 0.8949, 0.8882, 0.960...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0876, 0.0408, 0.036, 0.0488, 0.0249, 0.0527...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0809, 0.0573, 0.0966, 0.1029, 0.119, 0.2158...</td>\n",
       "      <td>[0.9191, 0.9427, 0.9034, 0.8971, 0.881, 0.7842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0415, 0.041, 0.0091, 0.2057, 0.0267, 0.0135...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8683, 0.6695, 0.8868, 0.7834, 0.3558, 0.814...</td>\n",
       "      <td>[0.1317, 0.3305, 0.1132, 0.2166, 0.6442, 0.186...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0637, 0.0426, 0.0238, 0.1255, 0.0259, 0.033...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7081, 0.306, 0.6953, 0.4458, 0.2531, 0.7194...</td>\n",
       "      <td>[0.2919, 0.694, 0.3047, 0.5542, 0.7469, 0.2806...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.1011, 0.0664, 0.0402, 0.1454, 0.0454, 0.064...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5452, 0.3199, 0.7144, 0.3853, 0.2319, 0.545...</td>\n",
       "      <td>[0.4548, 0.6801, 0.2856, 0.6147, 0.7681, 0.454...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0392, 0.12, 0.0419, 0.072, 0.3004, 0.0871, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8272, 0.926, 0.8373, 0.7996, 0.9474, 0.6962...</td>\n",
       "      <td>[0.1728, 0.074, 0.1627, 0.2004, 0.0526, 0.3038...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0443, 0.0932, 0.0155, 0.0315, 0.1046, 0.065...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.5569, 0.8607, 0.3569, 0.5077, 0.7067, 0.482...</td>\n",
       "      <td>[0.4431, 0.1393, 0.6431, 0.4923, 0.2933, 0.517...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0336, 0.0428, 0.0174, 0.0371, 0.0539, 0.060...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3687, 0.6901, 0.3216, 0.3862, 0.7155, 0.429...</td>\n",
       "      <td>[0.6313, 0.3099, 0.6784, 0.6138, 0.2845, 0.570...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0286, 0.0573, 0.0246, 0.0605, 0.0507, 0.028...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6974, 0.7548, 0.8588, 0.8956, 0.7624, 0.752...</td>\n",
       "      <td>[0.3026, 0.2452, 0.1412, 0.1044, 0.2376, 0.247...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1703, 0.1387, 0.1418, 0.1988, 0.0687, 0.179...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.6855, 0.3915, 0.4826, 0.518, 0.7343, 0.9223...</td>\n",
       "      <td>[0.3145, 0.6085, 0.5174, 0.482, 0.2657, 0.0777...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.027, 0.0274, 0.0098, 0.0526, 0.0304, 0.0167...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.575, 0.5208, 0.568, 0.7349, 0.6198, 0.4686,...</td>\n",
       "      <td>[0.425, 0.4792, 0.432, 0.2651, 0.3802, 0.5314,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0603, 0.0454, 0.0193, 0.0337, 0.0457, 0.032...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2244, 0.5853, 0.4035, 0.357, 0.4561, 0.224,...</td>\n",
       "      <td>[0.7756, 0.4147, 0.5965, 0.643, 0.5439, 0.776,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0773, 0.0319, 0.0284, 0.0415, 0.0179, 0.043...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0918, 0.0756, 0.0763, 0.1473, 0.1627, 0.192...</td>\n",
       "      <td>[0.9082, 0.9244, 0.9237, 0.8527, 0.8373, 0.807...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0594, 0.0426, 0.0247, 0.0517, 0.0306, 0.039...</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3156, 0.3841, 0.3452, 0.5011, 0.4915, 0.459...</td>\n",
       "      <td>[0.6844, 0.6159, 0.6548, 0.4989, 0.5085, 0.540...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0868, 0.028, 0.0247, 0.0353, 0.0165, 0.0378...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1041, 0.3588, 0.0827, 0.0971, 0.3693, 0.132...</td>\n",
       "      <td>[0.8959, 0.6412, 0.9173, 0.9029, 0.6307, 0.867...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0767, 0.0285, 0.026, 0.0314, 0.0178, 0.0377...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1361, 0.5521, 0.0914, 0.2172, 0.4559, 0.128...</td>\n",
       "      <td>[0.8639, 0.4479, 0.9086, 0.7828, 0.5441, 0.871...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0827, 0.0448, 0.0257, 0.0607, 0.0266, 0.047...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.253, 0.3293, 0.3894, 0.5749, 0.1422, 0.2317...</td>\n",
       "      <td>[0.747, 0.6707, 0.6106, 0.4251, 0.8578, 0.7683...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0475, 0.0186, 0.0123, 0.0317, 0.0087, 0.018...</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1738, 0.1465, 0.1716, 0.4638, 0.0579, 0.170...</td>\n",
       "      <td>[0.8262, 0.8535, 0.8284, 0.5362, 0.9421, 0.829...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0587, 0.0261, 0.0172, 0.0361, 0.0131, 0.026...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.141, 0.1747, 0.1575, 0.3292, 0.1047, 0.2671...</td>\n",
       "      <td>[0.859, 0.8253, 0.8425, 0.6708, 0.8953, 0.7329...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0682, 0.0411, 0.0233, 0.0508, 0.0294, 0.050...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.535, 0.5154, 0.4874, 0.344, 0.4933, 0.5266,...</td>\n",
       "      <td>[0.465, 0.4846, 0.5126, 0.656, 0.5067, 0.4734,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0811, 0.0254, 0.0221, 0.0387, 0.0146, 0.032...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0849, 0.0814, 0.1383, 0.0843, 0.0793, 0.134...</td>\n",
       "      <td>[0.9151, 0.9186, 0.8617, 0.9157, 0.9207, 0.866...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0552, 0.0387, 0.0229, 0.0504, 0.0336, 0.035...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1928, 0.2054, 0.6545, 0.3998, 0.3026, 0.355...</td>\n",
       "      <td>[0.8072, 0.7946, 0.3455, 0.6002, 0.6974, 0.644...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0581, 0.0215, 0.0156, 0.0283, 0.0112, 0.021...</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0669, 0.035, 0.2537, 0.1506, 0.0673, 0.1687...</td>\n",
       "      <td>[0.9331, 0.965, 0.7463, 0.8494, 0.9327, 0.8313...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0212, 0.0243, 0.011, 0.0281, 0.0266, 0.0194...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.4811, 0.4574, 0.4702, 0.3805, 0.8295, 0.613...</td>\n",
       "      <td>[0.5189, 0.5426, 0.5298, 0.6195, 0.1705, 0.387...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0308, 0.0446, 0.0286, 0.0803, 0.0342, 0.036...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7607, 0.5125, 0.6511, 0.6124, 0.9424, 0.899...</td>\n",
       "      <td>[0.2393, 0.4875, 0.3489, 0.3876, 0.0576, 0.100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0128, 0.0301, 0.0099, 0.0447, 0.0454, 0.017...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8033, 0.526, 0.8247, 0.7376, 0.9616, 0.8824...</td>\n",
       "      <td>[0.1967, 0.474, 0.1753, 0.2624, 0.0384, 0.1176...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3237, 0.5422, 0.4612, 0.3539, 0.6872, 0.511...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9999, 0.9951, 0.9997, 1.0, 0.9978, ...</td>\n",
       "      <td>[0.0002, 1e-04, 0.0049, 0.0003, 0.0, 0.0022, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0489, 0.0442, 0.0227, 0.0675, 0.028, 0.0279...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7182, 0.3868, 0.6544, 0.483, 0.3519, 0.3958...</td>\n",
       "      <td>[0.2818, 0.6132, 0.3456, 0.517, 0.6481, 0.6042...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0365, 0.0285, 0.0125, 0.0551, 0.0167, 0.015...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.4807, 0.5438, 0.7182, 0.6872, 0.2694, 0.549...</td>\n",
       "      <td>[0.5193, 0.4562, 0.2818, 0.3128, 0.7306, 0.450...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0527, 0.0164, 0.0116, 0.0209, 0.0087, 0.019...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0433, 0.091, 0.0516, 0.043, 0.067, 0.0707, ...</td>\n",
       "      <td>[0.9567, 0.909, 0.9484, 0.957, 0.933, 0.9293, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0622, 0.0423, 0.0294, 0.0424, 0.0504, 0.048...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.4156, 0.7351, 0.0984, 0.2247, 0.544, 0.1279...</td>\n",
       "      <td>[0.5844, 0.2649, 0.9016, 0.7753, 0.456, 0.8721...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0437, 0.0152, 0.0089, 0.0206, 0.0069, 0.014...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.093, 0.083, 0.2565, 0.1457, 0.0605, 0.2387,...</td>\n",
       "      <td>[0.907, 0.917, 0.7435, 0.8543, 0.9395, 0.7613,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0604, 0.0221, 0.0161, 0.0309, 0.0099, 0.024...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0838, 0.0562, 0.1706, 0.1666, 0.0545, 0.172...</td>\n",
       "      <td>[0.9162, 0.9438, 0.8294, 0.8334, 0.9455, 0.827...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.036, 0.0321, 0.0167, 0.062, 0.01, 0.0158, 0...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3284, 0.3004, 0.6369, 0.6743, 0.1398, 0.876...</td>\n",
       "      <td>[0.6716, 0.6996, 0.3631, 0.3257, 0.8602, 0.123...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0537, 0.0203, 0.0225, 0.0571, 0.0151, 0.022...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4195, 0.1161, 0.1446, 0.2149, 0.1675, 0.669...</td>\n",
       "      <td>[0.5805, 0.8839, 0.8554, 0.7851, 0.8325, 0.330...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0578, 0.0195, 0.0175, 0.037, 0.0117, 0.0233...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2157, 0.0315, 0.0474, 0.1251, 0.0474, 0.178...</td>\n",
       "      <td>[0.7843, 0.9685, 0.9526, 0.8749, 0.9526, 0.821...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0504, 0.0504, 0.0153, 0.0886, 0.0411, 0.025...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6737, 0.4955, 0.366, 0.7238, 0.4534, 0.5605...</td>\n",
       "      <td>[0.3263, 0.5045, 0.634, 0.2762, 0.5466, 0.4395...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0697, 0.02, 0.0197, 0.0357, 0.0112, 0.027, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0711, 0.0332, 0.0472, 0.0606, 0.0618, 0.271...</td>\n",
       "      <td>[0.9289, 0.9668, 0.9528, 0.9394, 0.9382, 0.728...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0402, 0.0145, 0.0089, 0.0228, 0.0055, 0.012...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1073, 0.0549, 0.2915, 0.1896, 0.0371, 0.232...</td>\n",
       "      <td>[0.8927, 0.9451, 0.7085, 0.8104, 0.9629, 0.767...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.032, 0.0247, 0.0113, 0.0474, 0.0075, 0.0108...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5572, 0.2523, 0.6017, 0.6553, 0.1514, 0.826...</td>\n",
       "      <td>[0.4428, 0.7477, 0.3983, 0.3447, 0.8486, 0.173...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0346, 0.0263, 0.0114, 0.0518, 0.0077, 0.009...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5444, 0.2071, 0.8016, 0.6146, 0.1567, 0.951...</td>\n",
       "      <td>[0.4556, 0.7929, 0.1984, 0.3854, 0.8433, 0.048...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0289, 0.0125, 0.0067, 0.0245, 0.0041, 0.008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2653, 0.0943, 0.419, 0.2745, 0.065, 0.3755,...</td>\n",
       "      <td>[0.7347, 0.9057, 0.581, 0.7255, 0.935, 0.6245,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0798, 0.0622, 0.0586, 0.0884, 0.0385, 0.080...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5994, 0.317, 0.5706, 0.4232, 0.462, 0.484, ...</td>\n",
       "      <td>[0.4006, 0.683, 0.4294, 0.5768, 0.538, 0.516, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0624, 0.0618, 0.023, 0.0492, 0.0433, 0.044,...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.4201, 0.459, 0.4022, 0.5225, 0.2569, 0.3388...</td>\n",
       "      <td>[0.5799, 0.541, 0.5978, 0.4775, 0.7431, 0.6612...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0322, 0.0147, 0.0072, 0.0108, 0.0106, 0.014...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1088, 0.371, 0.028, 0.0817, 0.3618, 0.0726,...</td>\n",
       "      <td>[0.8912, 0.629, 0.972, 0.9183, 0.6382, 0.9274,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0623, 0.0706, 0.0256, 0.0419, 0.0684, 0.064...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2275, 0.9007, 0.317, 0.474, 0.6468, 0.2957,...</td>\n",
       "      <td>[0.7725, 0.0993, 0.683, 0.526, 0.3532, 0.7043,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0779, 0.0588, 0.028, 0.0676, 0.0547, 0.0505...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3999, 0.4601, 0.6529, 0.3631, 0.196, 0.3324...</td>\n",
       "      <td>[0.6001, 0.5399, 0.3471, 0.6369, 0.804, 0.6676...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.036, 0.0444, 0.0203, 0.0509, 0.0593, 0.0223...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4776, 0.5723, 0.592, 0.4924, 0.6031, 0.328,...</td>\n",
       "      <td>[0.5224, 0.4277, 0.408, 0.5076, 0.3969, 0.672,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1077, 0.0479, 0.0271, 0.051, 0.0311, 0.0455...               0   \n",
       "1   [0.1317, 0.1045, 0.0266, 0.0559, 0.0802, 0.050...               0   \n",
       "2   [0.0933, 0.0517, 0.0235, 0.0519, 0.0345, 0.048...               0   \n",
       "3   [0.042, 0.0444, 0.0149, 0.0268, 0.0418, 0.0282...               1   \n",
       "4   [0.068, 0.0307, 0.0165, 0.028, 0.0172, 0.0309,...               1   \n",
       "5   [0.0876, 0.0408, 0.036, 0.0488, 0.0249, 0.0527...               2   \n",
       "6   [0.0415, 0.041, 0.0091, 0.2057, 0.0267, 0.0135...               3   \n",
       "7   [0.0637, 0.0426, 0.0238, 0.1255, 0.0259, 0.033...               3   \n",
       "8   [0.1011, 0.0664, 0.0402, 0.1454, 0.0454, 0.064...               3   \n",
       "9   [0.0392, 0.12, 0.0419, 0.072, 0.3004, 0.0871, ...               4   \n",
       "10  [0.0443, 0.0932, 0.0155, 0.0315, 0.1046, 0.065...               4   \n",
       "11  [0.0336, 0.0428, 0.0174, 0.0371, 0.0539, 0.060...               5   \n",
       "12  [0.0286, 0.0573, 0.0246, 0.0605, 0.0507, 0.028...               6   \n",
       "13  [0.1703, 0.1387, 0.1418, 0.1988, 0.0687, 0.179...               7   \n",
       "14  [0.027, 0.0274, 0.0098, 0.0526, 0.0304, 0.0167...               8   \n",
       "15  [0.0603, 0.0454, 0.0193, 0.0337, 0.0457, 0.032...               8   \n",
       "16  [0.0773, 0.0319, 0.0284, 0.0415, 0.0179, 0.043...               9   \n",
       "17  [0.0594, 0.0426, 0.0247, 0.0517, 0.0306, 0.039...               9   \n",
       "18  [0.0868, 0.028, 0.0247, 0.0353, 0.0165, 0.0378...              10   \n",
       "19  [0.0767, 0.0285, 0.026, 0.0314, 0.0178, 0.0377...              10   \n",
       "20  [0.0827, 0.0448, 0.0257, 0.0607, 0.0266, 0.047...              11   \n",
       "21  [0.0475, 0.0186, 0.0123, 0.0317, 0.0087, 0.018...              11   \n",
       "22  [0.0587, 0.0261, 0.0172, 0.0361, 0.0131, 0.026...              12   \n",
       "23  [0.0682, 0.0411, 0.0233, 0.0508, 0.0294, 0.050...              13   \n",
       "24  [0.0811, 0.0254, 0.0221, 0.0387, 0.0146, 0.032...              13   \n",
       "25  [0.0552, 0.0387, 0.0229, 0.0504, 0.0336, 0.035...              14   \n",
       "26  [0.0581, 0.0215, 0.0156, 0.0283, 0.0112, 0.021...              14   \n",
       "27  [0.0212, 0.0243, 0.011, 0.0281, 0.0266, 0.0194...              15   \n",
       "28  [0.0308, 0.0446, 0.0286, 0.0803, 0.0342, 0.036...              15   \n",
       "29  [0.0128, 0.0301, 0.0099, 0.0447, 0.0454, 0.017...              15   \n",
       "30  [0.3237, 0.5422, 0.4612, 0.3539, 0.6872, 0.511...              16   \n",
       "31  [0.0489, 0.0442, 0.0227, 0.0675, 0.028, 0.0279...              17   \n",
       "32  [0.0365, 0.0285, 0.0125, 0.0551, 0.0167, 0.015...              17   \n",
       "33  [0.0527, 0.0164, 0.0116, 0.0209, 0.0087, 0.019...              18   \n",
       "34  [0.0622, 0.0423, 0.0294, 0.0424, 0.0504, 0.048...              19   \n",
       "35  [0.0437, 0.0152, 0.0089, 0.0206, 0.0069, 0.014...              20   \n",
       "36  [0.0604, 0.0221, 0.0161, 0.0309, 0.0099, 0.024...              21   \n",
       "37  [0.036, 0.0321, 0.0167, 0.062, 0.01, 0.0158, 0...              21   \n",
       "38  [0.0537, 0.0203, 0.0225, 0.0571, 0.0151, 0.022...              22   \n",
       "39  [0.0578, 0.0195, 0.0175, 0.037, 0.0117, 0.0233...              22   \n",
       "40  [0.0504, 0.0504, 0.0153, 0.0886, 0.0411, 0.025...              22   \n",
       "41  [0.0697, 0.02, 0.0197, 0.0357, 0.0112, 0.027, ...              22   \n",
       "42  [0.0402, 0.0145, 0.0089, 0.0228, 0.0055, 0.012...              23   \n",
       "43  [0.032, 0.0247, 0.0113, 0.0474, 0.0075, 0.0108...              23   \n",
       "44  [0.0346, 0.0263, 0.0114, 0.0518, 0.0077, 0.009...              23   \n",
       "45  [0.0289, 0.0125, 0.0067, 0.0245, 0.0041, 0.008...              23   \n",
       "46  [0.0798, 0.0622, 0.0586, 0.0884, 0.0385, 0.080...              24   \n",
       "47  [0.0624, 0.0618, 0.023, 0.0492, 0.0433, 0.044,...              24   \n",
       "48  [0.0322, 0.0147, 0.0072, 0.0108, 0.0106, 0.014...              25   \n",
       "49  [0.0623, 0.0706, 0.0256, 0.0419, 0.0684, 0.064...              25   \n",
       "50  [0.0779, 0.0588, 0.028, 0.0676, 0.0547, 0.0505...              26   \n",
       "51  [0.036, 0.0444, 0.0203, 0.0509, 0.0593, 0.0223...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1721, 0.3283, 0.1144, 0.1938, 0.1326, 0.101...   \n",
       "1                  0  [0.5708, 0.6017, 0.2663, 0.4651, 0.3896, 0.215...   \n",
       "2                  0  [0.3295, 0.416, 0.1698, 0.2455, 0.138, 0.1238,...   \n",
       "3                  1  [0.4318, 0.4126, 0.2004, 0.4489, 0.6466, 0.354...   \n",
       "4                  0  [0.0866, 0.1277, 0.0589, 0.1051, 0.1118, 0.039...   \n",
       "5                 10  [0.0809, 0.0573, 0.0966, 0.1029, 0.119, 0.2158...   \n",
       "6                  3  [0.8683, 0.6695, 0.8868, 0.7834, 0.3558, 0.814...   \n",
       "7                  3  [0.7081, 0.306, 0.6953, 0.4458, 0.2531, 0.7194...   \n",
       "8                  3  [0.5452, 0.3199, 0.7144, 0.3853, 0.2319, 0.545...   \n",
       "9                  4  [0.8272, 0.926, 0.8373, 0.7996, 0.9474, 0.6962...   \n",
       "10                 4  [0.5569, 0.8607, 0.3569, 0.5077, 0.7067, 0.482...   \n",
       "11                 8  [0.3687, 0.6901, 0.3216, 0.3862, 0.7155, 0.429...   \n",
       "12                17  [0.6974, 0.7548, 0.8588, 0.8956, 0.7624, 0.752...   \n",
       "13                 7  [0.6855, 0.3915, 0.4826, 0.518, 0.7343, 0.9223...   \n",
       "14                 8  [0.575, 0.5208, 0.568, 0.7349, 0.6198, 0.4686,...   \n",
       "15                 8  [0.2244, 0.5853, 0.4035, 0.357, 0.4561, 0.224,...   \n",
       "16                10  [0.0918, 0.0756, 0.0763, 0.1473, 0.1627, 0.192...   \n",
       "17                23  [0.3156, 0.3841, 0.3452, 0.5011, 0.4915, 0.459...   \n",
       "18                10  [0.1041, 0.3588, 0.0827, 0.0971, 0.3693, 0.132...   \n",
       "19                10  [0.1361, 0.5521, 0.0914, 0.2172, 0.4559, 0.128...   \n",
       "20                 0  [0.253, 0.3293, 0.3894, 0.5749, 0.1422, 0.2317...   \n",
       "21                23  [0.1738, 0.1465, 0.1716, 0.4638, 0.0579, 0.170...   \n",
       "22                22  [0.141, 0.1747, 0.1575, 0.3292, 0.1047, 0.2671...   \n",
       "23                13  [0.535, 0.5154, 0.4874, 0.344, 0.4933, 0.5266,...   \n",
       "24                10  [0.0849, 0.0814, 0.1383, 0.0843, 0.0793, 0.134...   \n",
       "25                14  [0.1928, 0.2054, 0.6545, 0.3998, 0.3026, 0.355...   \n",
       "26                23  [0.0669, 0.035, 0.2537, 0.1506, 0.0673, 0.1687...   \n",
       "27                15  [0.4811, 0.4574, 0.4702, 0.3805, 0.8295, 0.613...   \n",
       "28                15  [0.7607, 0.5125, 0.6511, 0.6124, 0.9424, 0.899...   \n",
       "29                15  [0.8033, 0.526, 0.8247, 0.7376, 0.9616, 0.8824...   \n",
       "30                16  [0.9998, 0.9999, 0.9951, 0.9997, 1.0, 0.9978, ...   \n",
       "31                17  [0.7182, 0.3868, 0.6544, 0.483, 0.3519, 0.3958...   \n",
       "32                17  [0.4807, 0.5438, 0.7182, 0.6872, 0.2694, 0.549...   \n",
       "33                10  [0.0433, 0.091, 0.0516, 0.043, 0.067, 0.0707, ...   \n",
       "34                19  [0.4156, 0.7351, 0.0984, 0.2247, 0.544, 0.1279...   \n",
       "35                23  [0.093, 0.083, 0.2565, 0.1457, 0.0605, 0.2387,...   \n",
       "36                23  [0.0838, 0.0562, 0.1706, 0.1666, 0.0545, 0.172...   \n",
       "37                23  [0.3284, 0.3004, 0.6369, 0.6743, 0.1398, 0.876...   \n",
       "38                22  [0.4195, 0.1161, 0.1446, 0.2149, 0.1675, 0.669...   \n",
       "39                22  [0.2157, 0.0315, 0.0474, 0.1251, 0.0474, 0.178...   \n",
       "40                22  [0.6737, 0.4955, 0.366, 0.7238, 0.4534, 0.5605...   \n",
       "41                22  [0.0711, 0.0332, 0.0472, 0.0606, 0.0618, 0.271...   \n",
       "42                23  [0.1073, 0.0549, 0.2915, 0.1896, 0.0371, 0.232...   \n",
       "43                23  [0.5572, 0.2523, 0.6017, 0.6553, 0.1514, 0.826...   \n",
       "44                23  [0.5444, 0.2071, 0.8016, 0.6146, 0.1567, 0.951...   \n",
       "45                23  [0.2653, 0.0943, 0.419, 0.2745, 0.065, 0.3755,...   \n",
       "46                13  [0.5994, 0.317, 0.5706, 0.4232, 0.462, 0.484, ...   \n",
       "47                24  [0.4201, 0.459, 0.4022, 0.5225, 0.2569, 0.3388...   \n",
       "48                10  [0.1088, 0.371, 0.028, 0.0817, 0.3618, 0.0726,...   \n",
       "49                25  [0.2275, 0.9007, 0.317, 0.474, 0.6468, 0.2957,...   \n",
       "50                23  [0.3999, 0.4601, 0.6529, 0.3631, 0.196, 0.3324...   \n",
       "51                26  [0.4776, 0.5723, 0.592, 0.4924, 0.6031, 0.328,...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8279, 0.6717, 0.8856, 0.8062, 0.8674, 0.898...  0.653846  0.137018  \n",
       "1   [0.4292, 0.3983, 0.7337, 0.5349, 0.6104, 0.784...       NaN       NaN  \n",
       "2   [0.6705, 0.584, 0.8302, 0.7545, 0.862, 0.8762,...       NaN       NaN  \n",
       "3   [0.5682, 0.5874, 0.7996, 0.5511, 0.3534, 0.645...       NaN       NaN  \n",
       "4   [0.9134, 0.8723, 0.9411, 0.8949, 0.8882, 0.960...       NaN       NaN  \n",
       "5   [0.9191, 0.9427, 0.9034, 0.8971, 0.881, 0.7842...       NaN       NaN  \n",
       "6   [0.1317, 0.3305, 0.1132, 0.2166, 0.6442, 0.186...       NaN       NaN  \n",
       "7   [0.2919, 0.694, 0.3047, 0.5542, 0.7469, 0.2806...       NaN       NaN  \n",
       "8   [0.4548, 0.6801, 0.2856, 0.6147, 0.7681, 0.454...       NaN       NaN  \n",
       "9   [0.1728, 0.074, 0.1627, 0.2004, 0.0526, 0.3038...       NaN       NaN  \n",
       "10  [0.4431, 0.1393, 0.6431, 0.4923, 0.2933, 0.517...       NaN       NaN  \n",
       "11  [0.6313, 0.3099, 0.6784, 0.6138, 0.2845, 0.570...       NaN       NaN  \n",
       "12  [0.3026, 0.2452, 0.1412, 0.1044, 0.2376, 0.247...       NaN       NaN  \n",
       "13  [0.3145, 0.6085, 0.5174, 0.482, 0.2657, 0.0777...       NaN       NaN  \n",
       "14  [0.425, 0.4792, 0.432, 0.2651, 0.3802, 0.5314,...       NaN       NaN  \n",
       "15  [0.7756, 0.4147, 0.5965, 0.643, 0.5439, 0.776,...       NaN       NaN  \n",
       "16  [0.9082, 0.9244, 0.9237, 0.8527, 0.8373, 0.807...       NaN       NaN  \n",
       "17  [0.6844, 0.6159, 0.6548, 0.4989, 0.5085, 0.540...       NaN       NaN  \n",
       "18  [0.8959, 0.6412, 0.9173, 0.9029, 0.6307, 0.867...       NaN       NaN  \n",
       "19  [0.8639, 0.4479, 0.9086, 0.7828, 0.5441, 0.871...       NaN       NaN  \n",
       "20  [0.747, 0.6707, 0.6106, 0.4251, 0.8578, 0.7683...       NaN       NaN  \n",
       "21  [0.8262, 0.8535, 0.8284, 0.5362, 0.9421, 0.829...       NaN       NaN  \n",
       "22  [0.859, 0.8253, 0.8425, 0.6708, 0.8953, 0.7329...       NaN       NaN  \n",
       "23  [0.465, 0.4846, 0.5126, 0.656, 0.5067, 0.4734,...       NaN       NaN  \n",
       "24  [0.9151, 0.9186, 0.8617, 0.9157, 0.9207, 0.866...       NaN       NaN  \n",
       "25  [0.8072, 0.7946, 0.3455, 0.6002, 0.6974, 0.644...       NaN       NaN  \n",
       "26  [0.9331, 0.965, 0.7463, 0.8494, 0.9327, 0.8313...       NaN       NaN  \n",
       "27  [0.5189, 0.5426, 0.5298, 0.6195, 0.1705, 0.387...       NaN       NaN  \n",
       "28  [0.2393, 0.4875, 0.3489, 0.3876, 0.0576, 0.100...       NaN       NaN  \n",
       "29  [0.1967, 0.474, 0.1753, 0.2624, 0.0384, 0.1176...       NaN       NaN  \n",
       "30  [0.0002, 1e-04, 0.0049, 0.0003, 0.0, 0.0022, 0...       NaN       NaN  \n",
       "31  [0.2818, 0.6132, 0.3456, 0.517, 0.6481, 0.6042...       NaN       NaN  \n",
       "32  [0.5193, 0.4562, 0.2818, 0.3128, 0.7306, 0.450...       NaN       NaN  \n",
       "33  [0.9567, 0.909, 0.9484, 0.957, 0.933, 0.9293, ...       NaN       NaN  \n",
       "34  [0.5844, 0.2649, 0.9016, 0.7753, 0.456, 0.8721...       NaN       NaN  \n",
       "35  [0.907, 0.917, 0.7435, 0.8543, 0.9395, 0.7613,...       NaN       NaN  \n",
       "36  [0.9162, 0.9438, 0.8294, 0.8334, 0.9455, 0.827...       NaN       NaN  \n",
       "37  [0.6716, 0.6996, 0.3631, 0.3257, 0.8602, 0.123...       NaN       NaN  \n",
       "38  [0.5805, 0.8839, 0.8554, 0.7851, 0.8325, 0.330...       NaN       NaN  \n",
       "39  [0.7843, 0.9685, 0.9526, 0.8749, 0.9526, 0.821...       NaN       NaN  \n",
       "40  [0.3263, 0.5045, 0.634, 0.2762, 0.5466, 0.4395...       NaN       NaN  \n",
       "41  [0.9289, 0.9668, 0.9528, 0.9394, 0.9382, 0.728...       NaN       NaN  \n",
       "42  [0.8927, 0.9451, 0.7085, 0.8104, 0.9629, 0.767...       NaN       NaN  \n",
       "43  [0.4428, 0.7477, 0.3983, 0.3447, 0.8486, 0.173...       NaN       NaN  \n",
       "44  [0.4556, 0.7929, 0.1984, 0.3854, 0.8433, 0.048...       NaN       NaN  \n",
       "45  [0.7347, 0.9057, 0.581, 0.7255, 0.935, 0.6245,...       NaN       NaN  \n",
       "46  [0.4006, 0.683, 0.4294, 0.5768, 0.538, 0.516, ...       NaN       NaN  \n",
       "47  [0.5799, 0.541, 0.5978, 0.4775, 0.7431, 0.6612...       NaN       NaN  \n",
       "48  [0.8912, 0.629, 0.972, 0.9183, 0.6382, 0.9274,...       NaN       NaN  \n",
       "49  [0.7725, 0.0993, 0.683, 0.526, 0.3532, 0.7043,...       NaN       NaN  \n",
       "50  [0.6001, 0.5399, 0.3471, 0.6369, 0.804, 0.6676...       NaN       NaN  \n",
       "51  [0.5224, 0.4277, 0.408, 0.5076, 0.3969, 0.672,...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021 : Training: loss:  0.12338384\n",
      "1022 : Training: loss:  0.10887445\n",
      "1023 : Training: loss:  0.112005375\n",
      "1024 : Training: loss:  0.124241546\n",
      "1025 : Training: loss:  0.14895576\n",
      "1026 : Training: loss:  0.094462104\n",
      "1027 : Training: loss:  0.1366854\n",
      "1028 : Training: loss:  0.12151688\n",
      "1029 : Training: loss:  0.1284158\n",
      "1030 : Training: loss:  0.14356528\n",
      "1031 : Training: loss:  0.14952347\n",
      "1032 : Training: loss:  0.1530709\n",
      "1033 : Training: loss:  0.110048264\n",
      "1034 : Training: loss:  0.13590863\n",
      "1035 : Training: loss:  0.12900129\n",
      "1036 : Training: loss:  0.11937477\n",
      "1037 : Training: loss:  0.11776023\n",
      "1038 : Training: loss:  0.11417233\n",
      "1039 : Training: loss:  0.12577736\n",
      "1040 : Training: loss:  0.11337014\n",
      "Validation: Loss:  0.13603836  Accuracy:  0.63461536\n",
      "1041 : Training: loss:  0.13072927\n",
      "1042 : Training: loss:  0.10097459\n",
      "1043 : Training: loss:  0.12598184\n",
      "1044 : Training: loss:  0.15028957\n",
      "1045 : Training: loss:  0.08958979\n",
      "1046 : Training: loss:  0.12035407\n",
      "1047 : Training: loss:  0.13855883\n",
      "1048 : Training: loss:  0.13052931\n",
      "1049 : Training: loss:  0.118636526\n",
      "1050 : Training: loss:  0.1111477\n",
      "1051 : Training: loss:  0.113165125\n",
      "1052 : Training: loss:  0.10551716\n",
      "1053 : Training: loss:  0.13212173\n",
      "1054 : Training: loss:  0.1329181\n",
      "1055 : Training: loss:  0.13732281\n",
      "1056 : Training: loss:  0.11918611\n",
      "1057 : Training: loss:  0.12610912\n",
      "1058 : Training: loss:  0.1348967\n",
      "1059 : Training: loss:  0.11421439\n",
      "1060 : Training: loss:  0.12797281\n",
      "Validation: Loss:  0.13505082  Accuracy:  0.65384614\n",
      "1061 : Training: loss:  0.1509451\n",
      "1062 : Training: loss:  0.13563849\n",
      "1063 : Training: loss:  0.12108694\n",
      "1064 : Training: loss:  0.1395997\n",
      "1065 : Training: loss:  0.13925023\n",
      "1066 : Training: loss:  0.13936065\n",
      "1067 : Training: loss:  0.11235369\n",
      "1068 : Training: loss:  0.10629525\n",
      "1069 : Training: loss:  0.12727465\n",
      "1070 : Training: loss:  0.10822698\n",
      "1071 : Training: loss:  0.12805636\n",
      "1072 : Training: loss:  0.096723564\n",
      "1073 : Training: loss:  0.1282934\n",
      "1074 : Training: loss:  0.14538577\n",
      "1075 : Training: loss:  0.12543814\n",
      "1076 : Training: loss:  0.11247449\n",
      "1077 : Training: loss:  0.12681024\n",
      "1078 : Training: loss:  0.096807376\n",
      "1079 : Training: loss:  0.1308528\n",
      "1080 : Training: loss:  0.13802315\n",
      "Validation: Loss:  0.13438943  Accuracy:  0.6730769\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.108, 0.0493, 0.0292, 0.0537, 0.0279, 0.0438...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1821, 0.3328, 0.1149, 0.204, 0.1362, 0.1028...</td>\n",
       "      <td>[0.8179, 0.6672, 0.8851, 0.796, 0.8638, 0.8972...</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.134389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1351, 0.1132, 0.0275, 0.0585, 0.0761, 0.049...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5852, 0.61, 0.2705, 0.4784, 0.4019, 0.2161,...</td>\n",
       "      <td>[0.4148, 0.39, 0.7295, 0.5216, 0.5981, 0.7839,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0931, 0.0538, 0.0248, 0.0557, 0.0312, 0.046...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3514, 0.423, 0.1722, 0.2646, 0.1416, 0.1261...</td>\n",
       "      <td>[0.6486, 0.577, 0.8278, 0.7354, 0.8584, 0.8739...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0406, 0.0475, 0.0161, 0.0276, 0.039, 0.0281...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4512, 0.4198, 0.2013, 0.4631, 0.6623, 0.359...</td>\n",
       "      <td>[0.5488, 0.5802, 0.7987, 0.5369, 0.3377, 0.640...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0671, 0.032, 0.0181, 0.029, 0.0153, 0.0299,...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0917, 0.1333, 0.0589, 0.1121, 0.1179, 0.04,...</td>\n",
       "      <td>[0.9083, 0.8667, 0.9411, 0.8879, 0.8821, 0.96,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0833, 0.0398, 0.0394, 0.0494, 0.0214, 0.050...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0861, 0.0569, 0.0992, 0.1064, 0.1223, 0.224...</td>\n",
       "      <td>[0.9139, 0.9431, 0.9008, 0.8936, 0.8777, 0.775...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0375, 0.0387, 0.0088, 0.2425, 0.0218, 0.011...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8856, 0.6833, 0.8969, 0.8056, 0.3447, 0.835...</td>\n",
       "      <td>[0.1144, 0.3167, 0.1031, 0.1944, 0.6553, 0.164...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0588, 0.041, 0.0242, 0.1409, 0.0217, 0.0304...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7309, 0.3079, 0.7116, 0.4695, 0.2485, 0.740...</td>\n",
       "      <td>[0.2691, 0.6921, 0.2884, 0.5305, 0.7515, 0.259...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0946, 0.0634, 0.0405, 0.1573, 0.0386, 0.059...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5733, 0.3269, 0.7352, 0.3986, 0.2292, 0.565...</td>\n",
       "      <td>[0.4267, 0.6731, 0.2648, 0.6014, 0.7708, 0.434...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0356, 0.1227, 0.0425, 0.0712, 0.2824, 0.089...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8332, 0.9282, 0.8402, 0.8045, 0.95, 0.6977,...</td>\n",
       "      <td>[0.1668, 0.0718, 0.1598, 0.1955, 0.05, 0.3023,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0429, 0.1021, 0.0164, 0.0316, 0.0977, 0.068...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5665, 0.8654, 0.3485, 0.5199, 0.7188, 0.492...</td>\n",
       "      <td>[0.4335, 0.1346, 0.6515, 0.4801, 0.2812, 0.507...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.031, 0.0436, 0.0184, 0.0379, 0.0482, 0.0614...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.3801, 0.6961, 0.3203, 0.398, 0.7242, 0.4362...</td>\n",
       "      <td>[0.6199, 0.3039, 0.6797, 0.602, 0.2758, 0.5638...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0246, 0.0543, 0.0264, 0.0574, 0.0438, 0.026...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7053, 0.7519, 0.8551, 0.9008, 0.7718, 0.749...</td>\n",
       "      <td>[0.2947, 0.2481, 0.1449, 0.0992, 0.2282, 0.250...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1548, 0.1285, 0.1417, 0.1951, 0.0575, 0.165...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.672, 0.3602, 0.4625, 0.5178, 0.7405, 0.9253...</td>\n",
       "      <td>[0.328, 0.6398, 0.5375, 0.4822, 0.2595, 0.0747...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.025, 0.0277, 0.0101, 0.0571, 0.0268, 0.0154...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.5995, 0.5296, 0.5731, 0.7564, 0.6253, 0.480...</td>\n",
       "      <td>[0.4005, 0.4704, 0.4269, 0.2436, 0.3747, 0.519...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.059, 0.0472, 0.0205, 0.0345, 0.0419, 0.032,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2325, 0.6003, 0.4056, 0.3717, 0.4702, 0.220...</td>\n",
       "      <td>[0.7675, 0.3997, 0.5944, 0.6283, 0.5298, 0.779...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0728, 0.0306, 0.0311, 0.0415, 0.015, 0.0415...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.0966, 0.0764, 0.0776, 0.1488, 0.164, 0.2002...</td>\n",
       "      <td>[0.9034, 0.9236, 0.9224, 0.8512, 0.836, 0.7998...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0547, 0.0413, 0.0261, 0.0521, 0.0259, 0.036...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.3292, 0.3934, 0.3555, 0.5071, 0.4951, 0.468...</td>\n",
       "      <td>[0.6708, 0.6066, 0.6445, 0.4929, 0.5049, 0.531...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0847, 0.0272, 0.0268, 0.0359, 0.014, 0.0355...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1064, 0.3597, 0.0837, 0.0986, 0.3748, 0.137...</td>\n",
       "      <td>[0.8936, 0.6403, 0.9163, 0.9014, 0.6252, 0.862...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0742, 0.0278, 0.0284, 0.0314, 0.0152, 0.035...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1391, 0.5562, 0.092, 0.2236, 0.468, 0.1293,...</td>\n",
       "      <td>[0.8609, 0.4438, 0.908, 0.7764, 0.532, 0.8707,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0786, 0.0446, 0.0269, 0.0642, 0.0231, 0.044...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2743, 0.3283, 0.4032, 0.6006, 0.1431, 0.239...</td>\n",
       "      <td>[0.7257, 0.6717, 0.5968, 0.3994, 0.8569, 0.760...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0449, 0.0184, 0.0136, 0.0344, 0.0073, 0.017...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1941, 0.1421, 0.1776, 0.4928, 0.0571, 0.186...</td>\n",
       "      <td>[0.8059, 0.8579, 0.8224, 0.5072, 0.9429, 0.813...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0563, 0.0261, 0.0188, 0.0384, 0.0112, 0.024...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1544, 0.1744, 0.1599, 0.3476, 0.105, 0.2771...</td>\n",
       "      <td>[0.8456, 0.8256, 0.8401, 0.6524, 0.895, 0.7229...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0634, 0.0401, 0.0244, 0.052, 0.0252, 0.0482...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5509, 0.5166, 0.4998, 0.3498, 0.5051, 0.531...</td>\n",
       "      <td>[0.4491, 0.4834, 0.5002, 0.6502, 0.4949, 0.468...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0782, 0.0246, 0.024, 0.0398, 0.0122, 0.0304...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0903, 0.0816, 0.1475, 0.0867, 0.0783, 0.140...</td>\n",
       "      <td>[0.9097, 0.9184, 0.8525, 0.9133, 0.9217, 0.859...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0509, 0.0379, 0.0243, 0.0513, 0.0295, 0.033...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.2065, 0.2114, 0.6708, 0.4123, 0.3106, 0.366...</td>\n",
       "      <td>[0.7935, 0.7886, 0.3292, 0.5877, 0.6894, 0.633...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0552, 0.0209, 0.0171, 0.0288, 0.0094, 0.02,...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0728, 0.0343, 0.2613, 0.1583, 0.0689, 0.177...</td>\n",
       "      <td>[0.9272, 0.9657, 0.7387, 0.8417, 0.9311, 0.822...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0185, 0.0233, 0.0119, 0.0281, 0.0229, 0.018...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.5095, 0.4629, 0.477, 0.3905, 0.8387, 0.6313...</td>\n",
       "      <td>[0.4905, 0.5371, 0.523, 0.6095, 0.1613, 0.3687...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0262, 0.0418, 0.03, 0.0801, 0.0282, 0.0329,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7654, 0.4987, 0.6521, 0.6204, 0.9455, 0.907...</td>\n",
       "      <td>[0.2346, 0.5013, 0.3479, 0.3796, 0.0545, 0.092...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0106, 0.0286, 0.0103, 0.0438, 0.0382, 0.016...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8138, 0.5194, 0.827, 0.7445, 0.9645, 0.8904...</td>\n",
       "      <td>[0.1862, 0.4806, 0.173, 0.2555, 0.0355, 0.1096...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3322, 0.5625, 0.4734, 0.355, 0.7008, 0.5344...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9999, 0.9932, 0.9997, 1.0, 0.9971, ...</td>\n",
       "      <td>[0.0002, 1e-04, 0.0068, 0.0003, 0.0, 0.0029, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.043, 0.0419, 0.0237, 0.0708, 0.0235, 0.0252...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7413, 0.3828, 0.6727, 0.5029, 0.3513, 0.418...</td>\n",
       "      <td>[0.2587, 0.6172, 0.3273, 0.4971, 0.6487, 0.581...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0324, 0.0271, 0.0131, 0.06, 0.0138, 0.0138,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.5139, 0.5498, 0.7383, 0.7123, 0.2683, 0.569...</td>\n",
       "      <td>[0.4861, 0.4502, 0.2617, 0.2877, 0.7317, 0.430...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0516, 0.0166, 0.0131, 0.0221, 0.0074, 0.018...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0491, 0.0956, 0.0526, 0.0466, 0.0697, 0.075...</td>\n",
       "      <td>[0.9509, 0.9044, 0.9474, 0.9534, 0.9303, 0.924...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0608, 0.0443, 0.0326, 0.0446, 0.0479, 0.049...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.4488, 0.7482, 0.0991, 0.2387, 0.563, 0.1291...</td>\n",
       "      <td>[0.5512, 0.2518, 0.9009, 0.7613, 0.437, 0.8709...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0421, 0.0151, 0.0098, 0.0218, 0.0058, 0.013...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1021, 0.0821, 0.2681, 0.1561, 0.0611, 0.253...</td>\n",
       "      <td>[0.8979, 0.9179, 0.7319, 0.8439, 0.9389, 0.746...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0572, 0.0215, 0.0175, 0.0319, 0.0082, 0.022...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0856, 0.0545, 0.1751, 0.1728, 0.0518, 0.181...</td>\n",
       "      <td>[0.9144, 0.9455, 0.8249, 0.8272, 0.9482, 0.818...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0316, 0.0297, 0.0172, 0.0645, 0.0079, 0.013...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3276, 0.281, 0.6341, 0.6878, 0.1322, 0.8875...</td>\n",
       "      <td>[0.6724, 0.719, 0.3659, 0.3122, 0.8678, 0.1125...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0479, 0.0179, 0.0238, 0.0593, 0.0123, 0.019...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4471, 0.1098, 0.1375, 0.219, 0.1636, 0.6979...</td>\n",
       "      <td>[0.5529, 0.8902, 0.8625, 0.781, 0.8364, 0.3021...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0538, 0.0183, 0.0187, 0.0386, 0.0096, 0.020...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2367, 0.0303, 0.0472, 0.1333, 0.0456, 0.198...</td>\n",
       "      <td>[0.7633, 0.9697, 0.9528, 0.8667, 0.9544, 0.801...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0452, 0.0482, 0.0155, 0.0952, 0.0352, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6993, 0.4888, 0.3641, 0.7382, 0.4512, 0.583...</td>\n",
       "      <td>[0.3007, 0.5112, 0.6359, 0.2618, 0.5488, 0.416...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0668, 0.0189, 0.0215, 0.0374, 0.0093, 0.024...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0762, 0.0321, 0.0471, 0.0615, 0.0608, 0.292...</td>\n",
       "      <td>[0.9238, 0.9679, 0.9529, 0.9385, 0.9392, 0.707...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0376, 0.0143, 0.0098, 0.0242, 0.0045, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1122, 0.0541, 0.3023, 0.1999, 0.0358, 0.245...</td>\n",
       "      <td>[0.8878, 0.9459, 0.6977, 0.8001, 0.9642, 0.754...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0288, 0.0237, 0.0122, 0.0503, 0.0061, 0.009...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5654, 0.2382, 0.5998, 0.6699, 0.145, 0.8395...</td>\n",
       "      <td>[0.4346, 0.7618, 0.4002, 0.3301, 0.855, 0.1605...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0305, 0.0243, 0.012, 0.0534, 0.0061, 0.0077...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.544, 0.1905, 0.8023, 0.624, 0.1491, 0.9559,...</td>\n",
       "      <td>[0.456, 0.8095, 0.1977, 0.376, 0.8509, 0.0441,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0262, 0.012, 0.0074, 0.0261, 0.0033, 0.0075...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2751, 0.0904, 0.4262, 0.2865, 0.0615, 0.398...</td>\n",
       "      <td>[0.7249, 0.9096, 0.5738, 0.7135, 0.9385, 0.601...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0719, 0.0581, 0.0612, 0.0876, 0.0326, 0.075...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.6139, 0.3108, 0.5853, 0.427, 0.4654, 0.4909...</td>\n",
       "      <td>[0.3861, 0.6892, 0.4147, 0.573, 0.5346, 0.5091...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0582, 0.0635, 0.024, 0.0502, 0.0381, 0.0422...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.4436, 0.466, 0.422, 0.5415, 0.2628, 0.3433,...</td>\n",
       "      <td>[0.5564, 0.534, 0.578, 0.4585, 0.7372, 0.6567,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0305, 0.0151, 0.008, 0.0107, 0.0094, 0.0144...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1178, 0.3891, 0.0264, 0.0854, 0.3918, 0.072...</td>\n",
       "      <td>[0.8822, 0.6109, 0.9736, 0.9146, 0.6082, 0.927...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0606, 0.0741, 0.0279, 0.0433, 0.0645, 0.066...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2382, 0.907, 0.3136, 0.4959, 0.6652, 0.2978...</td>\n",
       "      <td>[0.7618, 0.093, 0.6864, 0.5041, 0.3348, 0.7022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0728, 0.0576, 0.0285, 0.0683, 0.0482, 0.046...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4182, 0.4592, 0.6611, 0.3822, 0.1995, 0.335...</td>\n",
       "      <td>[0.5818, 0.5408, 0.3389, 0.6178, 0.8005, 0.664...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0322, 0.0436, 0.021, 0.052, 0.0523, 0.0205,...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.5012, 0.5862, 0.606, 0.5097, 0.6111, 0.3321...</td>\n",
       "      <td>[0.4988, 0.4138, 0.394, 0.4903, 0.3889, 0.6679...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.108, 0.0493, 0.0292, 0.0537, 0.0279, 0.0438...               0   \n",
       "1   [0.1351, 0.1132, 0.0275, 0.0585, 0.0761, 0.049...               0   \n",
       "2   [0.0931, 0.0538, 0.0248, 0.0557, 0.0312, 0.046...               0   \n",
       "3   [0.0406, 0.0475, 0.0161, 0.0276, 0.039, 0.0281...               1   \n",
       "4   [0.0671, 0.032, 0.0181, 0.029, 0.0153, 0.0299,...               1   \n",
       "5   [0.0833, 0.0398, 0.0394, 0.0494, 0.0214, 0.050...               2   \n",
       "6   [0.0375, 0.0387, 0.0088, 0.2425, 0.0218, 0.011...               3   \n",
       "7   [0.0588, 0.041, 0.0242, 0.1409, 0.0217, 0.0304...               3   \n",
       "8   [0.0946, 0.0634, 0.0405, 0.1573, 0.0386, 0.059...               3   \n",
       "9   [0.0356, 0.1227, 0.0425, 0.0712, 0.2824, 0.089...               4   \n",
       "10  [0.0429, 0.1021, 0.0164, 0.0316, 0.0977, 0.068...               4   \n",
       "11  [0.031, 0.0436, 0.0184, 0.0379, 0.0482, 0.0614...               5   \n",
       "12  [0.0246, 0.0543, 0.0264, 0.0574, 0.0438, 0.026...               6   \n",
       "13  [0.1548, 0.1285, 0.1417, 0.1951, 0.0575, 0.165...               7   \n",
       "14  [0.025, 0.0277, 0.0101, 0.0571, 0.0268, 0.0154...               8   \n",
       "15  [0.059, 0.0472, 0.0205, 0.0345, 0.0419, 0.032,...               8   \n",
       "16  [0.0728, 0.0306, 0.0311, 0.0415, 0.015, 0.0415...               9   \n",
       "17  [0.0547, 0.0413, 0.0261, 0.0521, 0.0259, 0.036...               9   \n",
       "18  [0.0847, 0.0272, 0.0268, 0.0359, 0.014, 0.0355...              10   \n",
       "19  [0.0742, 0.0278, 0.0284, 0.0314, 0.0152, 0.035...              10   \n",
       "20  [0.0786, 0.0446, 0.0269, 0.0642, 0.0231, 0.044...              11   \n",
       "21  [0.0449, 0.0184, 0.0136, 0.0344, 0.0073, 0.017...              11   \n",
       "22  [0.0563, 0.0261, 0.0188, 0.0384, 0.0112, 0.024...              12   \n",
       "23  [0.0634, 0.0401, 0.0244, 0.052, 0.0252, 0.0482...              13   \n",
       "24  [0.0782, 0.0246, 0.024, 0.0398, 0.0122, 0.0304...              13   \n",
       "25  [0.0509, 0.0379, 0.0243, 0.0513, 0.0295, 0.033...              14   \n",
       "26  [0.0552, 0.0209, 0.0171, 0.0288, 0.0094, 0.02,...              14   \n",
       "27  [0.0185, 0.0233, 0.0119, 0.0281, 0.0229, 0.018...              15   \n",
       "28  [0.0262, 0.0418, 0.03, 0.0801, 0.0282, 0.0329,...              15   \n",
       "29  [0.0106, 0.0286, 0.0103, 0.0438, 0.0382, 0.016...              15   \n",
       "30  [0.3322, 0.5625, 0.4734, 0.355, 0.7008, 0.5344...              16   \n",
       "31  [0.043, 0.0419, 0.0237, 0.0708, 0.0235, 0.0252...              17   \n",
       "32  [0.0324, 0.0271, 0.0131, 0.06, 0.0138, 0.0138,...              17   \n",
       "33  [0.0516, 0.0166, 0.0131, 0.0221, 0.0074, 0.018...              18   \n",
       "34  [0.0608, 0.0443, 0.0326, 0.0446, 0.0479, 0.049...              19   \n",
       "35  [0.0421, 0.0151, 0.0098, 0.0218, 0.0058, 0.013...              20   \n",
       "36  [0.0572, 0.0215, 0.0175, 0.0319, 0.0082, 0.022...              21   \n",
       "37  [0.0316, 0.0297, 0.0172, 0.0645, 0.0079, 0.013...              21   \n",
       "38  [0.0479, 0.0179, 0.0238, 0.0593, 0.0123, 0.019...              22   \n",
       "39  [0.0538, 0.0183, 0.0187, 0.0386, 0.0096, 0.020...              22   \n",
       "40  [0.0452, 0.0482, 0.0155, 0.0952, 0.0352, 0.023...              22   \n",
       "41  [0.0668, 0.0189, 0.0215, 0.0374, 0.0093, 0.024...              22   \n",
       "42  [0.0376, 0.0143, 0.0098, 0.0242, 0.0045, 0.011...              23   \n",
       "43  [0.0288, 0.0237, 0.0122, 0.0503, 0.0061, 0.009...              23   \n",
       "44  [0.0305, 0.0243, 0.012, 0.0534, 0.0061, 0.0077...              23   \n",
       "45  [0.0262, 0.012, 0.0074, 0.0261, 0.0033, 0.0075...              23   \n",
       "46  [0.0719, 0.0581, 0.0612, 0.0876, 0.0326, 0.075...              24   \n",
       "47  [0.0582, 0.0635, 0.024, 0.0502, 0.0381, 0.0422...              24   \n",
       "48  [0.0305, 0.0151, 0.008, 0.0107, 0.0094, 0.0144...              25   \n",
       "49  [0.0606, 0.0741, 0.0279, 0.0433, 0.0645, 0.066...              25   \n",
       "50  [0.0728, 0.0576, 0.0285, 0.0683, 0.0482, 0.046...              26   \n",
       "51  [0.0322, 0.0436, 0.021, 0.052, 0.0523, 0.0205,...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1821, 0.3328, 0.1149, 0.204, 0.1362, 0.1028...   \n",
       "1                  0  [0.5852, 0.61, 0.2705, 0.4784, 0.4019, 0.2161,...   \n",
       "2                  0  [0.3514, 0.423, 0.1722, 0.2646, 0.1416, 0.1261...   \n",
       "3                  1  [0.4512, 0.4198, 0.2013, 0.4631, 0.6623, 0.359...   \n",
       "4                 10  [0.0917, 0.1333, 0.0589, 0.1121, 0.1179, 0.04,...   \n",
       "5                 10  [0.0861, 0.0569, 0.0992, 0.1064, 0.1223, 0.224...   \n",
       "6                  3  [0.8856, 0.6833, 0.8969, 0.8056, 0.3447, 0.835...   \n",
       "7                  3  [0.7309, 0.3079, 0.7116, 0.4695, 0.2485, 0.740...   \n",
       "8                  3  [0.5733, 0.3269, 0.7352, 0.3986, 0.2292, 0.565...   \n",
       "9                  4  [0.8332, 0.9282, 0.8402, 0.8045, 0.95, 0.6977,...   \n",
       "10                 1  [0.5665, 0.8654, 0.3485, 0.5199, 0.7188, 0.492...   \n",
       "11                 8  [0.3801, 0.6961, 0.3203, 0.398, 0.7242, 0.4362...   \n",
       "12                17  [0.7053, 0.7519, 0.8551, 0.9008, 0.7718, 0.749...   \n",
       "13                 7  [0.672, 0.3602, 0.4625, 0.5178, 0.7405, 0.9253...   \n",
       "14                 8  [0.5995, 0.5296, 0.5731, 0.7564, 0.6253, 0.480...   \n",
       "15                 8  [0.2325, 0.6003, 0.4056, 0.3717, 0.4702, 0.220...   \n",
       "16                 9  [0.0966, 0.0764, 0.0776, 0.1488, 0.164, 0.2002...   \n",
       "17                 9  [0.3292, 0.3934, 0.3555, 0.5071, 0.4951, 0.468...   \n",
       "18                10  [0.1064, 0.3597, 0.0837, 0.0986, 0.3748, 0.137...   \n",
       "19                10  [0.1391, 0.5562, 0.092, 0.2236, 0.468, 0.1293,...   \n",
       "20                 0  [0.2743, 0.3283, 0.4032, 0.6006, 0.1431, 0.239...   \n",
       "21                22  [0.1941, 0.1421, 0.1776, 0.4928, 0.0571, 0.186...   \n",
       "22                22  [0.1544, 0.1744, 0.1599, 0.3476, 0.105, 0.2771...   \n",
       "23                13  [0.5509, 0.5166, 0.4998, 0.3498, 0.5051, 0.531...   \n",
       "24                10  [0.0903, 0.0816, 0.1475, 0.0867, 0.0783, 0.140...   \n",
       "25                14  [0.2065, 0.2114, 0.6708, 0.4123, 0.3106, 0.366...   \n",
       "26                22  [0.0728, 0.0343, 0.2613, 0.1583, 0.0689, 0.177...   \n",
       "27                15  [0.5095, 0.4629, 0.477, 0.3905, 0.8387, 0.6313...   \n",
       "28                15  [0.7654, 0.4987, 0.6521, 0.6204, 0.9455, 0.907...   \n",
       "29                15  [0.8138, 0.5194, 0.827, 0.7445, 0.9645, 0.8904...   \n",
       "30                16  [0.9998, 0.9999, 0.9932, 0.9997, 1.0, 0.9971, ...   \n",
       "31                17  [0.7413, 0.3828, 0.6727, 0.5029, 0.3513, 0.418...   \n",
       "32                17  [0.5139, 0.5498, 0.7383, 0.7123, 0.2683, 0.569...   \n",
       "33                10  [0.0491, 0.0956, 0.0526, 0.0466, 0.0697, 0.075...   \n",
       "34                19  [0.4488, 0.7482, 0.0991, 0.2387, 0.563, 0.1291...   \n",
       "35                10  [0.1021, 0.0821, 0.2681, 0.1561, 0.0611, 0.253...   \n",
       "36                23  [0.0856, 0.0545, 0.1751, 0.1728, 0.0518, 0.181...   \n",
       "37                23  [0.3276, 0.281, 0.6341, 0.6878, 0.1322, 0.8875...   \n",
       "38                22  [0.4471, 0.1098, 0.1375, 0.219, 0.1636, 0.6979...   \n",
       "39                22  [0.2367, 0.0303, 0.0472, 0.1333, 0.0456, 0.198...   \n",
       "40                22  [0.6993, 0.4888, 0.3641, 0.7382, 0.4512, 0.583...   \n",
       "41                22  [0.0762, 0.0321, 0.0471, 0.0615, 0.0608, 0.292...   \n",
       "42                23  [0.1122, 0.0541, 0.3023, 0.1999, 0.0358, 0.245...   \n",
       "43                23  [0.5654, 0.2382, 0.5998, 0.6699, 0.145, 0.8395...   \n",
       "44                23  [0.544, 0.1905, 0.8023, 0.624, 0.1491, 0.9559,...   \n",
       "45                23  [0.2751, 0.0904, 0.4262, 0.2865, 0.0615, 0.398...   \n",
       "46                13  [0.6139, 0.3108, 0.5853, 0.427, 0.4654, 0.4909...   \n",
       "47                24  [0.4436, 0.466, 0.422, 0.5415, 0.2628, 0.3433,...   \n",
       "48                10  [0.1178, 0.3891, 0.0264, 0.0854, 0.3918, 0.072...   \n",
       "49                25  [0.2382, 0.907, 0.3136, 0.4959, 0.6652, 0.2978...   \n",
       "50                23  [0.4182, 0.4592, 0.6611, 0.3822, 0.1995, 0.335...   \n",
       "51                26  [0.5012, 0.5862, 0.606, 0.5097, 0.6111, 0.3321...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8179, 0.6672, 0.8851, 0.796, 0.8638, 0.8972...  0.673077  0.134389  \n",
       "1   [0.4148, 0.39, 0.7295, 0.5216, 0.5981, 0.7839,...       NaN       NaN  \n",
       "2   [0.6486, 0.577, 0.8278, 0.7354, 0.8584, 0.8739...       NaN       NaN  \n",
       "3   [0.5488, 0.5802, 0.7987, 0.5369, 0.3377, 0.640...       NaN       NaN  \n",
       "4   [0.9083, 0.8667, 0.9411, 0.8879, 0.8821, 0.96,...       NaN       NaN  \n",
       "5   [0.9139, 0.9431, 0.9008, 0.8936, 0.8777, 0.775...       NaN       NaN  \n",
       "6   [0.1144, 0.3167, 0.1031, 0.1944, 0.6553, 0.164...       NaN       NaN  \n",
       "7   [0.2691, 0.6921, 0.2884, 0.5305, 0.7515, 0.259...       NaN       NaN  \n",
       "8   [0.4267, 0.6731, 0.2648, 0.6014, 0.7708, 0.434...       NaN       NaN  \n",
       "9   [0.1668, 0.0718, 0.1598, 0.1955, 0.05, 0.3023,...       NaN       NaN  \n",
       "10  [0.4335, 0.1346, 0.6515, 0.4801, 0.2812, 0.507...       NaN       NaN  \n",
       "11  [0.6199, 0.3039, 0.6797, 0.602, 0.2758, 0.5638...       NaN       NaN  \n",
       "12  [0.2947, 0.2481, 0.1449, 0.0992, 0.2282, 0.250...       NaN       NaN  \n",
       "13  [0.328, 0.6398, 0.5375, 0.4822, 0.2595, 0.0747...       NaN       NaN  \n",
       "14  [0.4005, 0.4704, 0.4269, 0.2436, 0.3747, 0.519...       NaN       NaN  \n",
       "15  [0.7675, 0.3997, 0.5944, 0.6283, 0.5298, 0.779...       NaN       NaN  \n",
       "16  [0.9034, 0.9236, 0.9224, 0.8512, 0.836, 0.7998...       NaN       NaN  \n",
       "17  [0.6708, 0.6066, 0.6445, 0.4929, 0.5049, 0.531...       NaN       NaN  \n",
       "18  [0.8936, 0.6403, 0.9163, 0.9014, 0.6252, 0.862...       NaN       NaN  \n",
       "19  [0.8609, 0.4438, 0.908, 0.7764, 0.532, 0.8707,...       NaN       NaN  \n",
       "20  [0.7257, 0.6717, 0.5968, 0.3994, 0.8569, 0.760...       NaN       NaN  \n",
       "21  [0.8059, 0.8579, 0.8224, 0.5072, 0.9429, 0.813...       NaN       NaN  \n",
       "22  [0.8456, 0.8256, 0.8401, 0.6524, 0.895, 0.7229...       NaN       NaN  \n",
       "23  [0.4491, 0.4834, 0.5002, 0.6502, 0.4949, 0.468...       NaN       NaN  \n",
       "24  [0.9097, 0.9184, 0.8525, 0.9133, 0.9217, 0.859...       NaN       NaN  \n",
       "25  [0.7935, 0.7886, 0.3292, 0.5877, 0.6894, 0.633...       NaN       NaN  \n",
       "26  [0.9272, 0.9657, 0.7387, 0.8417, 0.9311, 0.822...       NaN       NaN  \n",
       "27  [0.4905, 0.5371, 0.523, 0.6095, 0.1613, 0.3687...       NaN       NaN  \n",
       "28  [0.2346, 0.5013, 0.3479, 0.3796, 0.0545, 0.092...       NaN       NaN  \n",
       "29  [0.1862, 0.4806, 0.173, 0.2555, 0.0355, 0.1096...       NaN       NaN  \n",
       "30  [0.0002, 1e-04, 0.0068, 0.0003, 0.0, 0.0029, 0...       NaN       NaN  \n",
       "31  [0.2587, 0.6172, 0.3273, 0.4971, 0.6487, 0.581...       NaN       NaN  \n",
       "32  [0.4861, 0.4502, 0.2617, 0.2877, 0.7317, 0.430...       NaN       NaN  \n",
       "33  [0.9509, 0.9044, 0.9474, 0.9534, 0.9303, 0.924...       NaN       NaN  \n",
       "34  [0.5512, 0.2518, 0.9009, 0.7613, 0.437, 0.8709...       NaN       NaN  \n",
       "35  [0.8979, 0.9179, 0.7319, 0.8439, 0.9389, 0.746...       NaN       NaN  \n",
       "36  [0.9144, 0.9455, 0.8249, 0.8272, 0.9482, 0.818...       NaN       NaN  \n",
       "37  [0.6724, 0.719, 0.3659, 0.3122, 0.8678, 0.1125...       NaN       NaN  \n",
       "38  [0.5529, 0.8902, 0.8625, 0.781, 0.8364, 0.3021...       NaN       NaN  \n",
       "39  [0.7633, 0.9697, 0.9528, 0.8667, 0.9544, 0.801...       NaN       NaN  \n",
       "40  [0.3007, 0.5112, 0.6359, 0.2618, 0.5488, 0.416...       NaN       NaN  \n",
       "41  [0.9238, 0.9679, 0.9529, 0.9385, 0.9392, 0.707...       NaN       NaN  \n",
       "42  [0.8878, 0.9459, 0.6977, 0.8001, 0.9642, 0.754...       NaN       NaN  \n",
       "43  [0.4346, 0.7618, 0.4002, 0.3301, 0.855, 0.1605...       NaN       NaN  \n",
       "44  [0.456, 0.8095, 0.1977, 0.376, 0.8509, 0.0441,...       NaN       NaN  \n",
       "45  [0.7249, 0.9096, 0.5738, 0.7135, 0.9385, 0.601...       NaN       NaN  \n",
       "46  [0.3861, 0.6892, 0.4147, 0.573, 0.5346, 0.5091...       NaN       NaN  \n",
       "47  [0.5564, 0.534, 0.578, 0.4585, 0.7372, 0.6567,...       NaN       NaN  \n",
       "48  [0.8822, 0.6109, 0.9736, 0.9146, 0.6082, 0.927...       NaN       NaN  \n",
       "49  [0.7618, 0.093, 0.6864, 0.5041, 0.3348, 0.7022...       NaN       NaN  \n",
       "50  [0.5818, 0.5408, 0.3389, 0.6178, 0.8005, 0.664...       NaN       NaN  \n",
       "51  [0.4988, 0.4138, 0.394, 0.4903, 0.3889, 0.6679...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081 : Training: loss:  0.15234035\n",
      "1082 : Training: loss:  0.13614497\n",
      "1083 : Training: loss:  0.12419551\n",
      "1084 : Training: loss:  0.10769514\n",
      "1085 : Training: loss:  0.099125415\n",
      "1086 : Training: loss:  0.11937407\n",
      "1087 : Training: loss:  0.13034126\n",
      "1088 : Training: loss:  0.13050334\n",
      "1089 : Training: loss:  0.12149024\n",
      "1090 : Training: loss:  0.12111114\n",
      "1091 : Training: loss:  0.107372716\n",
      "1092 : Training: loss:  0.109522715\n",
      "1093 : Training: loss:  0.1307779\n",
      "1094 : Training: loss:  0.11420169\n",
      "1095 : Training: loss:  0.11905117\n",
      "1096 : Training: loss:  0.14400908\n",
      "1097 : Training: loss:  0.1386146\n",
      "1098 : Training: loss:  0.12615061\n",
      "1099 : Training: loss:  0.10682684\n",
      "1100 : Training: loss:  0.10238114\n",
      "Validation: Loss:  0.13359475  Accuracy:  0.6730769\n",
      "1101 : Training: loss:  0.1258578\n",
      "1102 : Training: loss:  0.11976258\n",
      "1103 : Training: loss:  0.12412499\n",
      "1104 : Training: loss:  0.13642108\n",
      "1105 : Training: loss:  0.13181561\n",
      "1106 : Training: loss:  0.12343002\n",
      "1107 : Training: loss:  0.10835206\n",
      "1108 : Training: loss:  0.14989427\n",
      "1109 : Training: loss:  0.10510731\n",
      "1110 : Training: loss:  0.09447128\n",
      "1111 : Training: loss:  0.1211007\n",
      "1112 : Training: loss:  0.10138491\n",
      "1113 : Training: loss:  0.12777889\n",
      "1114 : Training: loss:  0.13862675\n",
      "1115 : Training: loss:  0.12577885\n",
      "1116 : Training: loss:  0.12367209\n",
      "1117 : Training: loss:  0.117815085\n",
      "1118 : Training: loss:  0.1059025\n",
      "1119 : Training: loss:  0.10734125\n",
      "1120 : Training: loss:  0.1291005\n",
      "Validation: Loss:  0.13251825  Accuracy:  0.6923077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1136, 0.0489, 0.0293, 0.0537, 0.0267, 0.044...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1856, 0.3354, 0.114, 0.2074, 0.1377, 0.1024...</td>\n",
       "      <td>[0.8144, 0.6646, 0.886, 0.7926, 0.8623, 0.8976...</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.132518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1441, 0.1178, 0.0271, 0.0574, 0.0774, 0.051...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5898, 0.6154, 0.2686, 0.4835, 0.4094, 0.214...</td>\n",
       "      <td>[0.4102, 0.3846, 0.7314, 0.5165, 0.5906, 0.785...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0984, 0.0542, 0.0245, 0.0562, 0.0301, 0.048...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3602, 0.4286, 0.172, 0.2744, 0.1424, 0.1256...</td>\n",
       "      <td>[0.6398, 0.5714, 0.828, 0.7256, 0.8576, 0.8744...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0419, 0.0491, 0.0159, 0.0266, 0.0397, 0.029...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4591, 0.4249, 0.2005, 0.4714, 0.6685, 0.358...</td>\n",
       "      <td>[0.5409, 0.5751, 0.7995, 0.5286, 0.3315, 0.641...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.07, 0.032, 0.0184, 0.0286, 0.0146, 0.0311, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0935, 0.1373, 0.0581, 0.1155, 0.1207, 0.039...</td>\n",
       "      <td>[0.9065, 0.8627, 0.9419, 0.8845, 0.8793, 0.960...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0843, 0.0383, 0.0401, 0.0486, 0.0199, 0.051...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0889, 0.0571, 0.1006, 0.1086, 0.1231, 0.229...</td>\n",
       "      <td>[0.9111, 0.9429, 0.8994, 0.8914, 0.8769, 0.770...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0372, 0.036, 0.0079, 0.2591, 0.0196, 0.0105...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8927, 0.6914, 0.9019, 0.8199, 0.3317, 0.846...</td>\n",
       "      <td>[0.1073, 0.3086, 0.0981, 0.1801, 0.6683, 0.153...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0583, 0.0394, 0.0232, 0.1489, 0.0199, 0.028...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7443, 0.3088, 0.7214, 0.4868, 0.2436, 0.754...</td>\n",
       "      <td>[0.2557, 0.6912, 0.2786, 0.5132, 0.7564, 0.245...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0933, 0.0601, 0.0386, 0.1628, 0.0352, 0.056...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5884, 0.3302, 0.7478, 0.41, 0.2256, 0.5781,...</td>\n",
       "      <td>[0.4116, 0.6698, 0.2522, 0.59, 0.7744, 0.4219,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0347, 0.1229, 0.0392, 0.0664, 0.2885, 0.092...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8347, 0.9296, 0.8382, 0.807, 0.9509, 0.6937...</td>\n",
       "      <td>[0.1653, 0.0704, 0.1618, 0.193, 0.0491, 0.3063...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0446, 0.1068, 0.0158, 0.0299, 0.0997, 0.073...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5687, 0.8707, 0.3327, 0.5254, 0.723, 0.4872...</td>\n",
       "      <td>[0.4313, 0.1293, 0.6673, 0.4746, 0.277, 0.5128...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0312, 0.0437, 0.0179, 0.0366, 0.0476, 0.066...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.3848, 0.7018, 0.3136, 0.4036, 0.7271, 0.433...</td>\n",
       "      <td>[0.6152, 0.2982, 0.6864, 0.5964, 0.2729, 0.566...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0228, 0.0507, 0.0246, 0.052, 0.0403, 0.0244...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7034, 0.7456, 0.8535, 0.9042, 0.7695, 0.741...</td>\n",
       "      <td>[0.2966, 0.2544, 0.1465, 0.0958, 0.2305, 0.258...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1505, 0.1225, 0.1385, 0.1919, 0.0521, 0.160...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.6663, 0.3456, 0.4513, 0.5156, 0.7416, 0.926...</td>\n",
       "      <td>[0.3337, 0.6544, 0.5487, 0.4844, 0.2584, 0.073...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0256, 0.0274, 0.0095, 0.0577, 0.0259, 0.015...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.6101, 0.5343, 0.5712, 0.7676, 0.6232, 0.478...</td>\n",
       "      <td>[0.3899, 0.4657, 0.4288, 0.2324, 0.3768, 0.521...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0613, 0.047, 0.0197, 0.0334, 0.0405, 0.0324...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2335, 0.6069, 0.4046, 0.379, 0.475, 0.2123,...</td>\n",
       "      <td>[0.7665, 0.3931, 0.5954, 0.621, 0.525, 0.7877,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0734, 0.0291, 0.0316, 0.0405, 0.0137, 0.042...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.0985, 0.077, 0.0786, 0.1499, 0.163, 0.2054,...</td>\n",
       "      <td>[0.9015, 0.923, 0.9214, 0.8501, 0.837, 0.7946,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0545, 0.0396, 0.0254, 0.0506, 0.024, 0.036,...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.3313, 0.3975, 0.3619, 0.511, 0.4908, 0.4729...</td>\n",
       "      <td>[0.6687, 0.6025, 0.6381, 0.489, 0.5092, 0.5271...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0877, 0.0264, 0.0275, 0.0356, 0.0131, 0.036...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1075, 0.3619, 0.083, 0.0997, 0.3775, 0.14, ...</td>\n",
       "      <td>[0.8925, 0.6381, 0.917, 0.9003, 0.6225, 0.86, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0767, 0.027, 0.0293, 0.031, 0.0143, 0.0373,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1409, 0.5605, 0.091, 0.228, 0.4749, 0.1288,...</td>\n",
       "      <td>[0.8591, 0.4395, 0.909, 0.772, 0.5251, 0.8712,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0791, 0.0433, 0.0263, 0.0646, 0.0215, 0.043...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2851, 0.3265, 0.4128, 0.6158, 0.1413, 0.243...</td>\n",
       "      <td>[0.7149, 0.6735, 0.5872, 0.3842, 0.8587, 0.756...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0454, 0.0177, 0.0138, 0.0352, 0.0068, 0.017...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2062, 0.1397, 0.1795, 0.51, 0.0559, 0.1977,...</td>\n",
       "      <td>[0.7938, 0.8603, 0.8205, 0.49, 0.9441, 0.8023,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0577, 0.0257, 0.0192, 0.039, 0.0106, 0.0253...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1629, 0.1739, 0.1614, 0.3586, 0.1034, 0.283...</td>\n",
       "      <td>[0.8371, 0.8261, 0.8386, 0.6414, 0.8966, 0.716...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0649, 0.0398, 0.0246, 0.0521, 0.0244, 0.049...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.5583, 0.5179, 0.4987, 0.3571, 0.5097, 0.531...</td>\n",
       "      <td>[0.4417, 0.4821, 0.5013, 0.6429, 0.4903, 0.468...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0806, 0.0235, 0.0245, 0.0397, 0.0113, 0.030...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0925, 0.082, 0.1517, 0.0877, 0.077, 0.1443,...</td>\n",
       "      <td>[0.9075, 0.918, 0.8483, 0.9123, 0.923, 0.8557,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0495, 0.0359, 0.0233, 0.0497, 0.0276, 0.032...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.2116, 0.214, 0.6811, 0.4202, 0.3109, 0.3717...</td>\n",
       "      <td>[0.7884, 0.786, 0.3189, 0.5798, 0.6891, 0.6283...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.056, 0.0201, 0.0172, 0.0282, 0.0087, 0.0199...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0754, 0.0335, 0.2676, 0.1625, 0.0683, 0.183...</td>\n",
       "      <td>[0.9246, 0.9665, 0.7324, 0.8375, 0.9317, 0.816...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0181, 0.0226, 0.0116, 0.0271, 0.0219, 0.018...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.5197, 0.4677, 0.4786, 0.3974, 0.8395, 0.637...</td>\n",
       "      <td>[0.4803, 0.5323, 0.5214, 0.6026, 0.1605, 0.362...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0248, 0.0402, 0.0287, 0.0774, 0.0263, 0.032...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7666, 0.4957, 0.6498, 0.6238, 0.9461, 0.909...</td>\n",
       "      <td>[0.2334, 0.5043, 0.3502, 0.3762, 0.0539, 0.090...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.01, 0.0276, 0.0095, 0.0412, 0.0367, 0.0157,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8152, 0.5168, 0.8248, 0.7483, 0.9648, 0.891...</td>\n",
       "      <td>[0.1848, 0.4832, 0.1752, 0.2517, 0.0352, 0.109...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.334, 0.5747, 0.4687, 0.3512, 0.7133, 0.5485...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9999, 0.9912, 0.9997, 1.0, 0.9965, ...</td>\n",
       "      <td>[0.0002, 1e-04, 0.0088, 0.0003, 0.0, 0.0035, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0411, 0.0398, 0.023, 0.0697, 0.022, 0.0242,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7495, 0.3791, 0.6815, 0.5144, 0.3476, 0.431...</td>\n",
       "      <td>[0.2505, 0.6209, 0.3185, 0.4856, 0.6524, 0.568...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0314, 0.0254, 0.0124, 0.0599, 0.0125, 0.013...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.5237, 0.5502, 0.7503, 0.7273, 0.2606, 0.578...</td>\n",
       "      <td>[0.4763, 0.4498, 0.2497, 0.2727, 0.7394, 0.421...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0541, 0.0161, 0.0135, 0.0221, 0.0069, 0.019...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.052, 0.0987, 0.0532, 0.049, 0.0699, 0.0772,...</td>\n",
       "      <td>[0.948, 0.9013, 0.9468, 0.951, 0.9301, 0.9228,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.063, 0.0449, 0.0332, 0.0449, 0.0484, 0.0518...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.4707, 0.7566, 0.0981, 0.248, 0.5719, 0.1284...</td>\n",
       "      <td>[0.5293, 0.2434, 0.9019, 0.752, 0.4281, 0.8716...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0433, 0.0146, 0.0099, 0.0217, 0.0053, 0.013...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1068, 0.0822, 0.2734, 0.1609, 0.0607, 0.260...</td>\n",
       "      <td>[0.8932, 0.9178, 0.7266, 0.8391, 0.9393, 0.739...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0578, 0.0206, 0.0177, 0.0313, 0.0075, 0.022...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0848, 0.0533, 0.1797, 0.1747, 0.0496, 0.186...</td>\n",
       "      <td>[0.9152, 0.9467, 0.8203, 0.8253, 0.9504, 0.813...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0299, 0.0272, 0.0165, 0.0624, 0.007, 0.0122...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3196, 0.2673, 0.6305, 0.6939, 0.1233, 0.892...</td>\n",
       "      <td>[0.6804, 0.7327, 0.3695, 0.3061, 0.8767, 0.107...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0463, 0.016, 0.0235, 0.0593, 0.0109, 0.0185...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4653, 0.1063, 0.1338, 0.221, 0.159, 0.7171,...</td>\n",
       "      <td>[0.5347, 0.8937, 0.8662, 0.779, 0.841, 0.2829,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0539, 0.017, 0.0189, 0.0387, 0.0087, 0.0204...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2497, 0.0298, 0.0472, 0.1386, 0.0441, 0.212...</td>\n",
       "      <td>[0.7503, 0.9702, 0.9528, 0.8614, 0.9559, 0.787...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0436, 0.0453, 0.0146, 0.0941, 0.0326, 0.021...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7102, 0.4836, 0.3639, 0.7455, 0.4442, 0.596...</td>\n",
       "      <td>[0.2898, 0.5164, 0.6361, 0.2545, 0.5558, 0.403...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0682, 0.0177, 0.0219, 0.0377, 0.0084, 0.024...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0799, 0.0317, 0.0472, 0.0617, 0.0603, 0.308...</td>\n",
       "      <td>[0.9201, 0.9683, 0.9528, 0.9383, 0.9397, 0.691...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0378, 0.0136, 0.0098, 0.0238, 0.0041, 0.011...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1121, 0.0525, 0.3082, 0.2051, 0.0341, 0.252...</td>\n",
       "      <td>[0.8879, 0.9475, 0.6918, 0.7949, 0.9659, 0.747...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0278, 0.022, 0.0117, 0.0488, 0.0054, 0.0084...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5642, 0.2239, 0.5958, 0.6789, 0.1352, 0.845...</td>\n",
       "      <td>[0.4358, 0.7761, 0.4042, 0.3211, 0.8648, 0.154...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0287, 0.0219, 0.0113, 0.0503, 0.0052, 0.006...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5338, 0.1752, 0.8017, 0.6279, 0.1383, 0.957...</td>\n",
       "      <td>[0.4662, 0.8248, 0.1983, 0.3721, 0.8617, 0.042...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0257, 0.0113, 0.0073, 0.0256, 0.0029, 0.007...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2742, 0.0859, 0.4302, 0.2929, 0.0575, 0.411...</td>\n",
       "      <td>[0.7258, 0.9141, 0.5698, 0.7071, 0.9425, 0.588...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0697, 0.0554, 0.0609, 0.0863, 0.0304, 0.074...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.6224, 0.3063, 0.5954, 0.433, 0.4657, 0.4965...</td>\n",
       "      <td>[0.3776, 0.6937, 0.4046, 0.567, 0.5343, 0.5035...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0584, 0.064, 0.0233, 0.0485, 0.037, 0.043, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.4551, 0.4668, 0.4318, 0.5511, 0.2651, 0.345...</td>\n",
       "      <td>[0.5449, 0.5332, 0.5682, 0.4489, 0.7349, 0.654...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0317, 0.0151, 0.0081, 0.0103, 0.0092, 0.015...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1221, 0.399, 0.0252, 0.0876, 0.4045, 0.0711...</td>\n",
       "      <td>[0.8779, 0.601, 0.9748, 0.9124, 0.5955, 0.9289...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0631, 0.0761, 0.028, 0.0423, 0.0653, 0.0721...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2414, 0.9109, 0.3072, 0.5084, 0.673, 0.2916...</td>\n",
       "      <td>[0.7586, 0.0891, 0.6928, 0.4916, 0.327, 0.7084...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0709, 0.0547, 0.0265, 0.0652, 0.0444, 0.044...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4254, 0.4571, 0.6665, 0.3904, 0.1985, 0.333...</td>\n",
       "      <td>[0.5746, 0.5429, 0.3335, 0.6096, 0.8015, 0.666...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0316, 0.0421, 0.0198, 0.0502, 0.0501, 0.019...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.5082, 0.5933, 0.6127, 0.521, 0.6099, 0.3288...</td>\n",
       "      <td>[0.4918, 0.4067, 0.3873, 0.479, 0.3901, 0.6712...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.1136, 0.0489, 0.0293, 0.0537, 0.0267, 0.044...               0   \n",
       "1   [0.1441, 0.1178, 0.0271, 0.0574, 0.0774, 0.051...               0   \n",
       "2   [0.0984, 0.0542, 0.0245, 0.0562, 0.0301, 0.048...               0   \n",
       "3   [0.0419, 0.0491, 0.0159, 0.0266, 0.0397, 0.029...               1   \n",
       "4   [0.07, 0.032, 0.0184, 0.0286, 0.0146, 0.0311, ...               1   \n",
       "5   [0.0843, 0.0383, 0.0401, 0.0486, 0.0199, 0.051...               2   \n",
       "6   [0.0372, 0.036, 0.0079, 0.2591, 0.0196, 0.0105...               3   \n",
       "7   [0.0583, 0.0394, 0.0232, 0.1489, 0.0199, 0.028...               3   \n",
       "8   [0.0933, 0.0601, 0.0386, 0.1628, 0.0352, 0.056...               3   \n",
       "9   [0.0347, 0.1229, 0.0392, 0.0664, 0.2885, 0.092...               4   \n",
       "10  [0.0446, 0.1068, 0.0158, 0.0299, 0.0997, 0.073...               4   \n",
       "11  [0.0312, 0.0437, 0.0179, 0.0366, 0.0476, 0.066...               5   \n",
       "12  [0.0228, 0.0507, 0.0246, 0.052, 0.0403, 0.0244...               6   \n",
       "13  [0.1505, 0.1225, 0.1385, 0.1919, 0.0521, 0.160...               7   \n",
       "14  [0.0256, 0.0274, 0.0095, 0.0577, 0.0259, 0.015...               8   \n",
       "15  [0.0613, 0.047, 0.0197, 0.0334, 0.0405, 0.0324...               8   \n",
       "16  [0.0734, 0.0291, 0.0316, 0.0405, 0.0137, 0.042...               9   \n",
       "17  [0.0545, 0.0396, 0.0254, 0.0506, 0.024, 0.036,...               9   \n",
       "18  [0.0877, 0.0264, 0.0275, 0.0356, 0.0131, 0.036...              10   \n",
       "19  [0.0767, 0.027, 0.0293, 0.031, 0.0143, 0.0373,...              10   \n",
       "20  [0.0791, 0.0433, 0.0263, 0.0646, 0.0215, 0.043...              11   \n",
       "21  [0.0454, 0.0177, 0.0138, 0.0352, 0.0068, 0.017...              11   \n",
       "22  [0.0577, 0.0257, 0.0192, 0.039, 0.0106, 0.0253...              12   \n",
       "23  [0.0649, 0.0398, 0.0246, 0.0521, 0.0244, 0.049...              13   \n",
       "24  [0.0806, 0.0235, 0.0245, 0.0397, 0.0113, 0.030...              13   \n",
       "25  [0.0495, 0.0359, 0.0233, 0.0497, 0.0276, 0.032...              14   \n",
       "26  [0.056, 0.0201, 0.0172, 0.0282, 0.0087, 0.0199...              14   \n",
       "27  [0.0181, 0.0226, 0.0116, 0.0271, 0.0219, 0.018...              15   \n",
       "28  [0.0248, 0.0402, 0.0287, 0.0774, 0.0263, 0.032...              15   \n",
       "29  [0.01, 0.0276, 0.0095, 0.0412, 0.0367, 0.0157,...              15   \n",
       "30  [0.334, 0.5747, 0.4687, 0.3512, 0.7133, 0.5485...              16   \n",
       "31  [0.0411, 0.0398, 0.023, 0.0697, 0.022, 0.0242,...              17   \n",
       "32  [0.0314, 0.0254, 0.0124, 0.0599, 0.0125, 0.013...              17   \n",
       "33  [0.0541, 0.0161, 0.0135, 0.0221, 0.0069, 0.019...              18   \n",
       "34  [0.063, 0.0449, 0.0332, 0.0449, 0.0484, 0.0518...              19   \n",
       "35  [0.0433, 0.0146, 0.0099, 0.0217, 0.0053, 0.013...              20   \n",
       "36  [0.0578, 0.0206, 0.0177, 0.0313, 0.0075, 0.022...              21   \n",
       "37  [0.0299, 0.0272, 0.0165, 0.0624, 0.007, 0.0122...              21   \n",
       "38  [0.0463, 0.016, 0.0235, 0.0593, 0.0109, 0.0185...              22   \n",
       "39  [0.0539, 0.017, 0.0189, 0.0387, 0.0087, 0.0204...              22   \n",
       "40  [0.0436, 0.0453, 0.0146, 0.0941, 0.0326, 0.021...              22   \n",
       "41  [0.0682, 0.0177, 0.0219, 0.0377, 0.0084, 0.024...              22   \n",
       "42  [0.0378, 0.0136, 0.0098, 0.0238, 0.0041, 0.011...              23   \n",
       "43  [0.0278, 0.022, 0.0117, 0.0488, 0.0054, 0.0084...              23   \n",
       "44  [0.0287, 0.0219, 0.0113, 0.0503, 0.0052, 0.006...              23   \n",
       "45  [0.0257, 0.0113, 0.0073, 0.0256, 0.0029, 0.007...              23   \n",
       "46  [0.0697, 0.0554, 0.0609, 0.0863, 0.0304, 0.074...              24   \n",
       "47  [0.0584, 0.064, 0.0233, 0.0485, 0.037, 0.043, ...              24   \n",
       "48  [0.0317, 0.0151, 0.0081, 0.0103, 0.0092, 0.015...              25   \n",
       "49  [0.0631, 0.0761, 0.028, 0.0423, 0.0653, 0.0721...              25   \n",
       "50  [0.0709, 0.0547, 0.0265, 0.0652, 0.0444, 0.044...              26   \n",
       "51  [0.0316, 0.0421, 0.0198, 0.0502, 0.0501, 0.019...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1856, 0.3354, 0.114, 0.2074, 0.1377, 0.1024...   \n",
       "1                  0  [0.5898, 0.6154, 0.2686, 0.4835, 0.4094, 0.214...   \n",
       "2                  0  [0.3602, 0.4286, 0.172, 0.2744, 0.1424, 0.1256...   \n",
       "3                  1  [0.4591, 0.4249, 0.2005, 0.4714, 0.6685, 0.358...   \n",
       "4                 10  [0.0935, 0.1373, 0.0581, 0.1155, 0.1207, 0.039...   \n",
       "5                 10  [0.0889, 0.0571, 0.1006, 0.1086, 0.1231, 0.229...   \n",
       "6                  3  [0.8927, 0.6914, 0.9019, 0.8199, 0.3317, 0.846...   \n",
       "7                  3  [0.7443, 0.3088, 0.7214, 0.4868, 0.2436, 0.754...   \n",
       "8                  3  [0.5884, 0.3302, 0.7478, 0.41, 0.2256, 0.5781,...   \n",
       "9                  4  [0.8347, 0.9296, 0.8382, 0.807, 0.9509, 0.6937...   \n",
       "10                 1  [0.5687, 0.8707, 0.3327, 0.5254, 0.723, 0.4872...   \n",
       "11                 5  [0.3848, 0.7018, 0.3136, 0.4036, 0.7271, 0.433...   \n",
       "12                17  [0.7034, 0.7456, 0.8535, 0.9042, 0.7695, 0.741...   \n",
       "13                 7  [0.6663, 0.3456, 0.4513, 0.5156, 0.7416, 0.926...   \n",
       "14                 3  [0.6101, 0.5343, 0.5712, 0.7676, 0.6232, 0.478...   \n",
       "15                 8  [0.2335, 0.6069, 0.4046, 0.379, 0.475, 0.2123,...   \n",
       "16                 9  [0.0985, 0.077, 0.0786, 0.1499, 0.163, 0.2054,...   \n",
       "17                 9  [0.3313, 0.3975, 0.3619, 0.511, 0.4908, 0.4729...   \n",
       "18                10  [0.1075, 0.3619, 0.083, 0.0997, 0.3775, 0.14, ...   \n",
       "19                10  [0.1409, 0.5605, 0.091, 0.228, 0.4749, 0.1288,...   \n",
       "20                 0  [0.2851, 0.3265, 0.4128, 0.6158, 0.1413, 0.243...   \n",
       "21                22  [0.2062, 0.1397, 0.1795, 0.51, 0.0559, 0.1977,...   \n",
       "22                22  [0.1629, 0.1739, 0.1614, 0.3586, 0.1034, 0.283...   \n",
       "23                13  [0.5583, 0.5179, 0.4987, 0.3571, 0.5097, 0.531...   \n",
       "24                10  [0.0925, 0.082, 0.1517, 0.0877, 0.077, 0.1443,...   \n",
       "25                14  [0.2116, 0.214, 0.6811, 0.4202, 0.3109, 0.3717...   \n",
       "26                22  [0.0754, 0.0335, 0.2676, 0.1625, 0.0683, 0.183...   \n",
       "27                15  [0.5197, 0.4677, 0.4786, 0.3974, 0.8395, 0.637...   \n",
       "28                15  [0.7666, 0.4957, 0.6498, 0.6238, 0.9461, 0.909...   \n",
       "29                15  [0.8152, 0.5168, 0.8248, 0.7483, 0.9648, 0.891...   \n",
       "30                16  [0.9998, 0.9999, 0.9912, 0.9997, 1.0, 0.9965, ...   \n",
       "31                17  [0.7495, 0.3791, 0.6815, 0.5144, 0.3476, 0.431...   \n",
       "32                17  [0.5237, 0.5502, 0.7503, 0.7273, 0.2606, 0.578...   \n",
       "33                10  [0.052, 0.0987, 0.0532, 0.049, 0.0699, 0.0772,...   \n",
       "34                19  [0.4707, 0.7566, 0.0981, 0.248, 0.5719, 0.1284...   \n",
       "35                23  [0.1068, 0.0822, 0.2734, 0.1609, 0.0607, 0.260...   \n",
       "36                23  [0.0848, 0.0533, 0.1797, 0.1747, 0.0496, 0.186...   \n",
       "37                23  [0.3196, 0.2673, 0.6305, 0.6939, 0.1233, 0.892...   \n",
       "38                22  [0.4653, 0.1063, 0.1338, 0.221, 0.159, 0.7171,...   \n",
       "39                22  [0.2497, 0.0298, 0.0472, 0.1386, 0.0441, 0.212...   \n",
       "40                22  [0.7102, 0.4836, 0.3639, 0.7455, 0.4442, 0.596...   \n",
       "41                22  [0.0799, 0.0317, 0.0472, 0.0617, 0.0603, 0.308...   \n",
       "42                23  [0.1121, 0.0525, 0.3082, 0.2051, 0.0341, 0.252...   \n",
       "43                23  [0.5642, 0.2239, 0.5958, 0.6789, 0.1352, 0.845...   \n",
       "44                23  [0.5338, 0.1752, 0.8017, 0.6279, 0.1383, 0.957...   \n",
       "45                23  [0.2742, 0.0859, 0.4302, 0.2929, 0.0575, 0.411...   \n",
       "46                13  [0.6224, 0.3063, 0.5954, 0.433, 0.4657, 0.4965...   \n",
       "47                24  [0.4551, 0.4668, 0.4318, 0.5511, 0.2651, 0.345...   \n",
       "48                25  [0.1221, 0.399, 0.0252, 0.0876, 0.4045, 0.0711...   \n",
       "49                25  [0.2414, 0.9109, 0.3072, 0.5084, 0.673, 0.2916...   \n",
       "50                23  [0.4254, 0.4571, 0.6665, 0.3904, 0.1985, 0.333...   \n",
       "51                26  [0.5082, 0.5933, 0.6127, 0.521, 0.6099, 0.3288...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8144, 0.6646, 0.886, 0.7926, 0.8623, 0.8976...  0.692308  0.132518  \n",
       "1   [0.4102, 0.3846, 0.7314, 0.5165, 0.5906, 0.785...       NaN       NaN  \n",
       "2   [0.6398, 0.5714, 0.828, 0.7256, 0.8576, 0.8744...       NaN       NaN  \n",
       "3   [0.5409, 0.5751, 0.7995, 0.5286, 0.3315, 0.641...       NaN       NaN  \n",
       "4   [0.9065, 0.8627, 0.9419, 0.8845, 0.8793, 0.960...       NaN       NaN  \n",
       "5   [0.9111, 0.9429, 0.8994, 0.8914, 0.8769, 0.770...       NaN       NaN  \n",
       "6   [0.1073, 0.3086, 0.0981, 0.1801, 0.6683, 0.153...       NaN       NaN  \n",
       "7   [0.2557, 0.6912, 0.2786, 0.5132, 0.7564, 0.245...       NaN       NaN  \n",
       "8   [0.4116, 0.6698, 0.2522, 0.59, 0.7744, 0.4219,...       NaN       NaN  \n",
       "9   [0.1653, 0.0704, 0.1618, 0.193, 0.0491, 0.3063...       NaN       NaN  \n",
       "10  [0.4313, 0.1293, 0.6673, 0.4746, 0.277, 0.5128...       NaN       NaN  \n",
       "11  [0.6152, 0.2982, 0.6864, 0.5964, 0.2729, 0.566...       NaN       NaN  \n",
       "12  [0.2966, 0.2544, 0.1465, 0.0958, 0.2305, 0.258...       NaN       NaN  \n",
       "13  [0.3337, 0.6544, 0.5487, 0.4844, 0.2584, 0.073...       NaN       NaN  \n",
       "14  [0.3899, 0.4657, 0.4288, 0.2324, 0.3768, 0.521...       NaN       NaN  \n",
       "15  [0.7665, 0.3931, 0.5954, 0.621, 0.525, 0.7877,...       NaN       NaN  \n",
       "16  [0.9015, 0.923, 0.9214, 0.8501, 0.837, 0.7946,...       NaN       NaN  \n",
       "17  [0.6687, 0.6025, 0.6381, 0.489, 0.5092, 0.5271...       NaN       NaN  \n",
       "18  [0.8925, 0.6381, 0.917, 0.9003, 0.6225, 0.86, ...       NaN       NaN  \n",
       "19  [0.8591, 0.4395, 0.909, 0.772, 0.5251, 0.8712,...       NaN       NaN  \n",
       "20  [0.7149, 0.6735, 0.5872, 0.3842, 0.8587, 0.756...       NaN       NaN  \n",
       "21  [0.7938, 0.8603, 0.8205, 0.49, 0.9441, 0.8023,...       NaN       NaN  \n",
       "22  [0.8371, 0.8261, 0.8386, 0.6414, 0.8966, 0.716...       NaN       NaN  \n",
       "23  [0.4417, 0.4821, 0.5013, 0.6429, 0.4903, 0.468...       NaN       NaN  \n",
       "24  [0.9075, 0.918, 0.8483, 0.9123, 0.923, 0.8557,...       NaN       NaN  \n",
       "25  [0.7884, 0.786, 0.3189, 0.5798, 0.6891, 0.6283...       NaN       NaN  \n",
       "26  [0.9246, 0.9665, 0.7324, 0.8375, 0.9317, 0.816...       NaN       NaN  \n",
       "27  [0.4803, 0.5323, 0.5214, 0.6026, 0.1605, 0.362...       NaN       NaN  \n",
       "28  [0.2334, 0.5043, 0.3502, 0.3762, 0.0539, 0.090...       NaN       NaN  \n",
       "29  [0.1848, 0.4832, 0.1752, 0.2517, 0.0352, 0.109...       NaN       NaN  \n",
       "30  [0.0002, 1e-04, 0.0088, 0.0003, 0.0, 0.0035, 0...       NaN       NaN  \n",
       "31  [0.2505, 0.6209, 0.3185, 0.4856, 0.6524, 0.568...       NaN       NaN  \n",
       "32  [0.4763, 0.4498, 0.2497, 0.2727, 0.7394, 0.421...       NaN       NaN  \n",
       "33  [0.948, 0.9013, 0.9468, 0.951, 0.9301, 0.9228,...       NaN       NaN  \n",
       "34  [0.5293, 0.2434, 0.9019, 0.752, 0.4281, 0.8716...       NaN       NaN  \n",
       "35  [0.8932, 0.9178, 0.7266, 0.8391, 0.9393, 0.739...       NaN       NaN  \n",
       "36  [0.9152, 0.9467, 0.8203, 0.8253, 0.9504, 0.813...       NaN       NaN  \n",
       "37  [0.6804, 0.7327, 0.3695, 0.3061, 0.8767, 0.107...       NaN       NaN  \n",
       "38  [0.5347, 0.8937, 0.8662, 0.779, 0.841, 0.2829,...       NaN       NaN  \n",
       "39  [0.7503, 0.9702, 0.9528, 0.8614, 0.9559, 0.787...       NaN       NaN  \n",
       "40  [0.2898, 0.5164, 0.6361, 0.2545, 0.5558, 0.403...       NaN       NaN  \n",
       "41  [0.9201, 0.9683, 0.9528, 0.9383, 0.9397, 0.691...       NaN       NaN  \n",
       "42  [0.8879, 0.9475, 0.6918, 0.7949, 0.9659, 0.747...       NaN       NaN  \n",
       "43  [0.4358, 0.7761, 0.4042, 0.3211, 0.8648, 0.154...       NaN       NaN  \n",
       "44  [0.4662, 0.8248, 0.1983, 0.3721, 0.8617, 0.042...       NaN       NaN  \n",
       "45  [0.7258, 0.9141, 0.5698, 0.7071, 0.9425, 0.588...       NaN       NaN  \n",
       "46  [0.3776, 0.6937, 0.4046, 0.567, 0.5343, 0.5035...       NaN       NaN  \n",
       "47  [0.5449, 0.5332, 0.5682, 0.4489, 0.7349, 0.654...       NaN       NaN  \n",
       "48  [0.8779, 0.601, 0.9748, 0.9124, 0.5955, 0.9289...       NaN       NaN  \n",
       "49  [0.7586, 0.0891, 0.6928, 0.4916, 0.327, 0.7084...       NaN       NaN  \n",
       "50  [0.5746, 0.5429, 0.3335, 0.6096, 0.8015, 0.666...       NaN       NaN  \n",
       "51  [0.4918, 0.4067, 0.3873, 0.479, 0.3901, 0.6712...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1121 : Training: loss:  0.121828064\n",
      "1122 : Training: loss:  0.12346806\n",
      "1123 : Training: loss:  0.12712012\n",
      "1124 : Training: loss:  0.11684407\n",
      "1125 : Training: loss:  0.116557725\n",
      "1126 : Training: loss:  0.11917079\n",
      "1127 : Training: loss:  0.12978445\n",
      "1128 : Training: loss:  0.13123907\n",
      "1129 : Training: loss:  0.14769486\n",
      "1130 : Training: loss:  0.11715816\n",
      "1131 : Training: loss:  0.1274831\n",
      "1132 : Training: loss:  0.122554146\n",
      "1133 : Training: loss:  0.10944093\n",
      "1134 : Training: loss:  0.12846707\n",
      "1135 : Training: loss:  0.11150945\n",
      "1136 : Training: loss:  0.1408055\n",
      "1137 : Training: loss:  0.09279617\n",
      "1138 : Training: loss:  0.12320002\n",
      "1139 : Training: loss:  0.12441663\n",
      "1140 : Training: loss:  0.122542396\n",
      "Validation: Loss:  0.13143437  Accuracy:  0.6730769\n",
      "1141 : Training: loss:  0.09216022\n",
      "1142 : Training: loss:  0.12477659\n",
      "1143 : Training: loss:  0.11236194\n",
      "1144 : Training: loss:  0.12461737\n",
      "1145 : Training: loss:  0.1329134\n",
      "1146 : Training: loss:  0.111731716\n",
      "1147 : Training: loss:  0.14091314\n",
      "1148 : Training: loss:  0.10494967\n",
      "1149 : Training: loss:  0.10438013\n",
      "1150 : Training: loss:  0.11155637\n",
      "1151 : Training: loss:  0.112852864\n",
      "1152 : Training: loss:  0.09563571\n",
      "1153 : Training: loss:  0.12199658\n",
      "1154 : Training: loss:  0.13519548\n",
      "1155 : Training: loss:  0.12752125\n",
      "1156 : Training: loss:  0.10927098\n",
      "1157 : Training: loss:  0.09916843\n",
      "1158 : Training: loss:  0.11010391\n",
      "1159 : Training: loss:  0.12406444\n",
      "1160 : Training: loss:  0.10852766\n",
      "Validation: Loss:  0.13029937  Accuracy:  0.6923077\n",
      "1161 : Training: loss:  0.10053668\n",
      "1162 : Training: loss:  0.12501706\n",
      "1163 : Training: loss:  0.10772317\n",
      "1164 : Training: loss:  0.10068609\n",
      "1165 : Training: loss:  0.09986033\n",
      "1166 : Training: loss:  0.10310933\n",
      "1167 : Training: loss:  0.12758344\n",
      "1168 : Training: loss:  0.09951029\n",
      "1169 : Training: loss:  0.12220448\n",
      "1170 : Training: loss:  0.12343381\n",
      "1171 : Training: loss:  0.10132025\n",
      "1172 : Training: loss:  0.10297977\n",
      "1173 : Training: loss:  0.14611371\n",
      "1174 : Training: loss:  0.11722377\n",
      "1175 : Training: loss:  0.13309564\n",
      "1176 : Training: loss:  0.10279023\n",
      "1177 : Training: loss:  0.112594634\n",
      "1178 : Training: loss:  0.11657408\n",
      "1179 : Training: loss:  0.13080862\n",
      "1180 : Training: loss:  0.096607685\n",
      "Validation: Loss:  0.12900105  Accuracy:  0.7307692\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.11, 0.0501, 0.0311, 0.0514, 0.0277, 0.0438,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1912, 0.3419, 0.1116, 0.2145, 0.1412, 0.100...</td>\n",
       "      <td>[0.8088, 0.6581, 0.8884, 0.7855, 0.8588, 0.899...</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.129001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1471, 0.1274, 0.0274, 0.0546, 0.0848, 0.050...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6028, 0.6241, 0.2667, 0.4937, 0.4216, 0.211...</td>\n",
       "      <td>[0.3972, 0.3759, 0.7333, 0.5063, 0.5784, 0.788...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0945, 0.0556, 0.0252, 0.0545, 0.0316, 0.047...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3745, 0.4386, 0.17, 0.2899, 0.1449, 0.1243,...</td>\n",
       "      <td>[0.6255, 0.5614, 0.83, 0.7101, 0.8551, 0.8757,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0382, 0.0528, 0.0163, 0.0242, 0.0448, 0.029...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4789, 0.4328, 0.1974, 0.4858, 0.6806, 0.362...</td>\n",
       "      <td>[0.5211, 0.5672, 0.8026, 0.5142, 0.3194, 0.637...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0662, 0.0335, 0.0201, 0.0267, 0.0156, 0.030...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0979, 0.1437, 0.0563, 0.1213, 0.1268, 0.04,...</td>\n",
       "      <td>[0.9021, 0.8563, 0.9437, 0.8787, 0.8732, 0.96,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0781, 0.0377, 0.0445, 0.0457, 0.02, 0.0506,...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0913, 0.0567, 0.1031, 0.1137, 0.1277, 0.237...</td>\n",
       "      <td>[0.9087, 0.9433, 0.8969, 0.8863, 0.8723, 0.762...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0326, 0.0333, 0.0074, 0.2768, 0.0182, 0.008...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9065, 0.7046, 0.9093, 0.839, 0.3243, 0.8628...</td>\n",
       "      <td>[0.0935, 0.2954, 0.0907, 0.161, 0.6757, 0.1372...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0521, 0.0375, 0.0234, 0.1556, 0.0188, 0.025...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.7658, 0.3105, 0.7345, 0.5098, 0.2397, 0.776...</td>\n",
       "      <td>[0.2342, 0.6895, 0.2655, 0.4902, 0.7603, 0.223...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0848, 0.0565, 0.0386, 0.1672, 0.0332, 0.050...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.6131, 0.3379, 0.7678, 0.4274, 0.2243, 0.600...</td>\n",
       "      <td>[0.3869, 0.6621, 0.2322, 0.5726, 0.7757, 0.399...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0302, 0.1227, 0.0369, 0.0598, 0.3037, 0.090...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8408, 0.9328, 0.8371, 0.8119, 0.9535, 0.693...</td>\n",
       "      <td>[0.1592, 0.0672, 0.1629, 0.1881, 0.0465, 0.306...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0419, 0.1174, 0.0163, 0.0266, 0.1179, 0.078...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.5829, 0.8781, 0.3126, 0.5349, 0.7409, 0.499...</td>\n",
       "      <td>[0.4171, 0.1219, 0.6874, 0.4651, 0.2591, 0.500...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0273, 0.0441, 0.0181, 0.0333, 0.0526, 0.068...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.3995, 0.7132, 0.3039, 0.4118, 0.7371, 0.441...</td>\n",
       "      <td>[0.6005, 0.2868, 0.6961, 0.5882, 0.2629, 0.558...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0187, 0.046, 0.024, 0.0438, 0.0361, 0.021, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7071, 0.733, 0.8516, 0.9108, 0.7749, 0.7382...</td>\n",
       "      <td>[0.2929, 0.267, 0.1484, 0.0892, 0.2251, 0.2618...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1325, 0.1131, 0.1382, 0.1755, 0.0461, 0.151...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.6502, 0.3119, 0.4214, 0.5154, 0.7452, 0.928...</td>\n",
       "      <td>[0.3498, 0.6881, 0.5786, 0.4846, 0.2548, 0.071...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0224, 0.0275, 0.0093, 0.0559, 0.0279, 0.014...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.6356, 0.5465, 0.5655, 0.784, 0.6292, 0.4891...</td>\n",
       "      <td>[0.3644, 0.4535, 0.4345, 0.216, 0.3708, 0.5109...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0553, 0.0474, 0.0196, 0.03, 0.0426, 0.0307,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2385, 0.6218, 0.3976, 0.3882, 0.4849, 0.205...</td>\n",
       "      <td>[0.7615, 0.3782, 0.6024, 0.6118, 0.5151, 0.794...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0677, 0.0285, 0.0355, 0.0379, 0.0136, 0.041...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.1009, 0.0786, 0.0806, 0.1551, 0.1669, 0.210...</td>\n",
       "      <td>[0.8991, 0.9214, 0.9194, 0.8449, 0.8331, 0.789...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0491, 0.0385, 0.0267, 0.0473, 0.0235, 0.034...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.3398, 0.4032, 0.3738, 0.5249, 0.4947, 0.483...</td>\n",
       "      <td>[0.6602, 0.5968, 0.6262, 0.4751, 0.5053, 0.516...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0832, 0.0261, 0.0307, 0.0337, 0.0135, 0.035...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1122, 0.368, 0.0831, 0.1005, 0.3828, 0.1455...</td>\n",
       "      <td>[0.8878, 0.632, 0.9169, 0.8995, 0.6172, 0.8545...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0714, 0.0268, 0.0327, 0.0289, 0.0149, 0.036...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1467, 0.5687, 0.0914, 0.2335, 0.4841, 0.130...</td>\n",
       "      <td>[0.8533, 0.4313, 0.9086, 0.7665, 0.5159, 0.869...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0723, 0.0421, 0.0268, 0.0621, 0.0206, 0.039...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.296, 0.3193, 0.4247, 0.6375, 0.1405, 0.2447...</td>\n",
       "      <td>[0.704, 0.6807, 0.5753, 0.3625, 0.8595, 0.7553...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0413, 0.0175, 0.0153, 0.0346, 0.0069, 0.015...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2255, 0.1342, 0.1831, 0.5326, 0.0549, 0.212...</td>\n",
       "      <td>[0.7745, 0.8658, 0.8169, 0.4674, 0.9451, 0.787...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0525, 0.0255, 0.021, 0.0379, 0.0109, 0.0241...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1819, 0.1714, 0.1623, 0.3779, 0.1015, 0.297...</td>\n",
       "      <td>[0.8181, 0.8286, 0.8377, 0.6221, 0.8985, 0.702...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.06, 0.0393, 0.0264, 0.0498, 0.0251, 0.0497,...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.573, 0.5232, 0.5063, 0.3608, 0.5204, 0.5403...</td>\n",
       "      <td>[0.427, 0.4768, 0.4937, 0.6392, 0.4796, 0.4597...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0757, 0.023, 0.0273, 0.0381, 0.0114, 0.0299...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0992, 0.0845, 0.163, 0.0912, 0.0767, 0.1543...</td>\n",
       "      <td>[0.9008, 0.9155, 0.837, 0.9088, 0.9233, 0.8457...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0433, 0.0345, 0.0239, 0.0465, 0.0275, 0.030...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.2201, 0.2182, 0.6957, 0.4344, 0.3186, 0.381...</td>\n",
       "      <td>[0.7799, 0.7818, 0.3043, 0.5656, 0.6814, 0.618...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0511, 0.0195, 0.0189, 0.0262, 0.0086, 0.018...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0796, 0.0322, 0.2742, 0.1691, 0.0684, 0.191...</td>\n",
       "      <td>[0.9204, 0.9678, 0.7258, 0.8309, 0.9316, 0.808...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.015, 0.0218, 0.0121, 0.0243, 0.0223, 0.0178...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.5384, 0.4748, 0.4812, 0.4096, 0.8468, 0.650...</td>\n",
       "      <td>[0.4616, 0.5252, 0.5188, 0.5904, 0.1532, 0.349...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0202, 0.0374, 0.0289, 0.0703, 0.0243, 0.029...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7699, 0.4834, 0.6482, 0.6335, 0.9489, 0.915...</td>\n",
       "      <td>[0.2301, 0.5166, 0.3518, 0.3665, 0.0511, 0.084...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.008, 0.026, 0.0092, 0.0359, 0.0357, 0.0142,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8208, 0.5111, 0.8236, 0.7568, 0.9673, 0.897...</td>\n",
       "      <td>[0.1792, 0.4889, 0.1764, 0.2432, 0.0327, 0.102...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.318, 0.587, 0.4529, 0.3354, 0.7286, 0.5564,...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9998, 0.9871, 0.9996, 1.0, 0.9955, ...</td>\n",
       "      <td>[0.0002, 0.0002, 0.0129, 0.0004, 0.0, 0.0045, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0358, 0.0376, 0.0238, 0.0673, 0.0206, 0.021...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7639, 0.3758, 0.7004, 0.5327, 0.3473, 0.446...</td>\n",
       "      <td>[0.2361, 0.6242, 0.2996, 0.4673, 0.6527, 0.553...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0268, 0.0238, 0.0126, 0.0586, 0.0116, 0.011...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.5437, 0.5496, 0.772, 0.7526, 0.2575, 0.5915...</td>\n",
       "      <td>[0.4563, 0.4504, 0.228, 0.2474, 0.7425, 0.4085...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0508, 0.0163, 0.0153, 0.021, 0.0072, 0.0185...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0571, 0.1038, 0.0533, 0.0526, 0.072, 0.0815...</td>\n",
       "      <td>[0.9429, 0.8962, 0.9467, 0.9474, 0.928, 0.9185...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0589, 0.0473, 0.036, 0.0434, 0.0557, 0.053,...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.5087, 0.774, 0.0968, 0.2619, 0.587, 0.1314,...</td>\n",
       "      <td>[0.4913, 0.226, 0.9032, 0.7381, 0.413, 0.8686,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0396, 0.0145, 0.011, 0.0206, 0.0054, 0.0131...</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1149, 0.082, 0.2811, 0.1691, 0.0616, 0.2724...</td>\n",
       "      <td>[0.8851, 0.918, 0.7189, 0.8309, 0.9384, 0.7276...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0531, 0.0202, 0.0196, 0.0294, 0.0073, 0.021...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.086, 0.0512, 0.1832, 0.1801, 0.0473, 0.1952...</td>\n",
       "      <td>[0.914, 0.9488, 0.8168, 0.8199, 0.9527, 0.8048...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0258, 0.0251, 0.0171, 0.0582, 0.0063, 0.010...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3159, 0.2427, 0.6264, 0.7027, 0.1145, 0.901...</td>\n",
       "      <td>[0.6841, 0.7573, 0.3736, 0.2973, 0.8855, 0.098...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0401, 0.0142, 0.0252, 0.057, 0.0101, 0.0162...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4964, 0.1006, 0.127, 0.2269, 0.155, 0.7446,...</td>\n",
       "      <td>[0.5036, 0.8994, 0.873, 0.7731, 0.845, 0.2554,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0486, 0.016, 0.0207, 0.0374, 0.0086, 0.0187...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2768, 0.0289, 0.0465, 0.1474, 0.0431, 0.237...</td>\n",
       "      <td>[0.7232, 0.9711, 0.9535, 0.8526, 0.9569, 0.762...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0376, 0.042, 0.014, 0.0894, 0.0306, 0.0187,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7315, 0.4702, 0.3589, 0.7588, 0.4389, 0.614...</td>\n",
       "      <td>[0.2685, 0.5298, 0.6411, 0.2412, 0.5611, 0.385...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0633, 0.0169, 0.0245, 0.0362, 0.0083, 0.023...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.086, 0.031, 0.0467, 0.063, 0.0599, 0.331, 0...</td>\n",
       "      <td>[0.914, 0.969, 0.9533, 0.937, 0.9401, 0.669, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0339, 0.0133, 0.0109, 0.0225, 0.0039, 0.010...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1144, 0.0506, 0.3174, 0.2152, 0.0328, 0.265...</td>\n",
       "      <td>[0.8856, 0.9494, 0.6826, 0.7848, 0.9672, 0.734...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.024, 0.0204, 0.0122, 0.045, 0.0049, 0.007, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.572, 0.2018, 0.5889, 0.6933, 0.1268, 0.8583...</td>\n",
       "      <td>[0.428, 0.7982, 0.4111, 0.3067, 0.8732, 0.1417...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0243, 0.0198, 0.0115, 0.0453, 0.0046, 0.005...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.5278, 0.1533, 0.8002, 0.6396, 0.1301, 0.961...</td>\n",
       "      <td>[0.4722, 0.8467, 0.1998, 0.3604, 0.8699, 0.038...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0226, 0.0108, 0.0081, 0.024, 0.0027, 0.0061...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2787, 0.0797, 0.4337, 0.304, 0.0538, 0.4331...</td>\n",
       "      <td>[0.7213, 0.9203, 0.5663, 0.696, 0.9462, 0.5669...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0617, 0.0518, 0.0632, 0.0812, 0.0285, 0.070...</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.6318, 0.3023, 0.6129, 0.4393, 0.4687, 0.500...</td>\n",
       "      <td>[0.3682, 0.6977, 0.3871, 0.5607, 0.5313, 0.499...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0524, 0.0639, 0.0235, 0.0455, 0.0367, 0.040...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.4696, 0.4738, 0.4506, 0.5688, 0.2685, 0.342...</td>\n",
       "      <td>[0.5304, 0.5262, 0.5494, 0.4312, 0.7315, 0.657...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0289, 0.0157, 0.009, 0.0092, 0.0104, 0.0154...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1338, 0.4143, 0.0232, 0.0918, 0.4252, 0.072...</td>\n",
       "      <td>[0.8662, 0.5857, 0.9768, 0.9082, 0.5748, 0.927...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0586, 0.0785, 0.0291, 0.0393, 0.0722, 0.074...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2495, 0.9153, 0.2971, 0.5224, 0.6853, 0.289...</td>\n",
       "      <td>[0.7505, 0.0847, 0.7029, 0.4776, 0.3147, 0.710...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0628, 0.0518, 0.0256, 0.059, 0.0422, 0.0399...</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.431, 0.4531, 0.6703, 0.4035, 0.1999, 0.3268...</td>\n",
       "      <td>[0.569, 0.5469, 0.3297, 0.5965, 0.8001, 0.6732...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0267, 0.0403, 0.0193, 0.0464, 0.0505, 0.017...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.5284, 0.6089, 0.6243, 0.5393, 0.6166, 0.332...</td>\n",
       "      <td>[0.4716, 0.3911, 0.3757, 0.4607, 0.3834, 0.667...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.11, 0.0501, 0.0311, 0.0514, 0.0277, 0.0438,...               0   \n",
       "1   [0.1471, 0.1274, 0.0274, 0.0546, 0.0848, 0.050...               0   \n",
       "2   [0.0945, 0.0556, 0.0252, 0.0545, 0.0316, 0.047...               0   \n",
       "3   [0.0382, 0.0528, 0.0163, 0.0242, 0.0448, 0.029...               1   \n",
       "4   [0.0662, 0.0335, 0.0201, 0.0267, 0.0156, 0.030...               1   \n",
       "5   [0.0781, 0.0377, 0.0445, 0.0457, 0.02, 0.0506,...               2   \n",
       "6   [0.0326, 0.0333, 0.0074, 0.2768, 0.0182, 0.008...               3   \n",
       "7   [0.0521, 0.0375, 0.0234, 0.1556, 0.0188, 0.025...               3   \n",
       "8   [0.0848, 0.0565, 0.0386, 0.1672, 0.0332, 0.050...               3   \n",
       "9   [0.0302, 0.1227, 0.0369, 0.0598, 0.3037, 0.090...               4   \n",
       "10  [0.0419, 0.1174, 0.0163, 0.0266, 0.1179, 0.078...               4   \n",
       "11  [0.0273, 0.0441, 0.0181, 0.0333, 0.0526, 0.068...               5   \n",
       "12  [0.0187, 0.046, 0.024, 0.0438, 0.0361, 0.021, ...               6   \n",
       "13  [0.1325, 0.1131, 0.1382, 0.1755, 0.0461, 0.151...               7   \n",
       "14  [0.0224, 0.0275, 0.0093, 0.0559, 0.0279, 0.014...               8   \n",
       "15  [0.0553, 0.0474, 0.0196, 0.03, 0.0426, 0.0307,...               8   \n",
       "16  [0.0677, 0.0285, 0.0355, 0.0379, 0.0136, 0.041...               9   \n",
       "17  [0.0491, 0.0385, 0.0267, 0.0473, 0.0235, 0.034...               9   \n",
       "18  [0.0832, 0.0261, 0.0307, 0.0337, 0.0135, 0.035...              10   \n",
       "19  [0.0714, 0.0268, 0.0327, 0.0289, 0.0149, 0.036...              10   \n",
       "20  [0.0723, 0.0421, 0.0268, 0.0621, 0.0206, 0.039...              11   \n",
       "21  [0.0413, 0.0175, 0.0153, 0.0346, 0.0069, 0.015...              11   \n",
       "22  [0.0525, 0.0255, 0.021, 0.0379, 0.0109, 0.0241...              12   \n",
       "23  [0.06, 0.0393, 0.0264, 0.0498, 0.0251, 0.0497,...              13   \n",
       "24  [0.0757, 0.023, 0.0273, 0.0381, 0.0114, 0.0299...              13   \n",
       "25  [0.0433, 0.0345, 0.0239, 0.0465, 0.0275, 0.030...              14   \n",
       "26  [0.0511, 0.0195, 0.0189, 0.0262, 0.0086, 0.018...              14   \n",
       "27  [0.015, 0.0218, 0.0121, 0.0243, 0.0223, 0.0178...              15   \n",
       "28  [0.0202, 0.0374, 0.0289, 0.0703, 0.0243, 0.029...              15   \n",
       "29  [0.008, 0.026, 0.0092, 0.0359, 0.0357, 0.0142,...              15   \n",
       "30  [0.318, 0.587, 0.4529, 0.3354, 0.7286, 0.5564,...              16   \n",
       "31  [0.0358, 0.0376, 0.0238, 0.0673, 0.0206, 0.021...              17   \n",
       "32  [0.0268, 0.0238, 0.0126, 0.0586, 0.0116, 0.011...              17   \n",
       "33  [0.0508, 0.0163, 0.0153, 0.021, 0.0072, 0.0185...              18   \n",
       "34  [0.0589, 0.0473, 0.036, 0.0434, 0.0557, 0.053,...              19   \n",
       "35  [0.0396, 0.0145, 0.011, 0.0206, 0.0054, 0.0131...              20   \n",
       "36  [0.0531, 0.0202, 0.0196, 0.0294, 0.0073, 0.021...              21   \n",
       "37  [0.0258, 0.0251, 0.0171, 0.0582, 0.0063, 0.010...              21   \n",
       "38  [0.0401, 0.0142, 0.0252, 0.057, 0.0101, 0.0162...              22   \n",
       "39  [0.0486, 0.016, 0.0207, 0.0374, 0.0086, 0.0187...              22   \n",
       "40  [0.0376, 0.042, 0.014, 0.0894, 0.0306, 0.0187,...              22   \n",
       "41  [0.0633, 0.0169, 0.0245, 0.0362, 0.0083, 0.023...              22   \n",
       "42  [0.0339, 0.0133, 0.0109, 0.0225, 0.0039, 0.010...              23   \n",
       "43  [0.024, 0.0204, 0.0122, 0.045, 0.0049, 0.007, ...              23   \n",
       "44  [0.0243, 0.0198, 0.0115, 0.0453, 0.0046, 0.005...              23   \n",
       "45  [0.0226, 0.0108, 0.0081, 0.024, 0.0027, 0.0061...              23   \n",
       "46  [0.0617, 0.0518, 0.0632, 0.0812, 0.0285, 0.070...              24   \n",
       "47  [0.0524, 0.0639, 0.0235, 0.0455, 0.0367, 0.040...              24   \n",
       "48  [0.0289, 0.0157, 0.009, 0.0092, 0.0104, 0.0154...              25   \n",
       "49  [0.0586, 0.0785, 0.0291, 0.0393, 0.0722, 0.074...              25   \n",
       "50  [0.0628, 0.0518, 0.0256, 0.059, 0.0422, 0.0399...              26   \n",
       "51  [0.0267, 0.0403, 0.0193, 0.0464, 0.0505, 0.017...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.1912, 0.3419, 0.1116, 0.2145, 0.1412, 0.100...   \n",
       "1                  0  [0.6028, 0.6241, 0.2667, 0.4937, 0.4216, 0.211...   \n",
       "2                  0  [0.3745, 0.4386, 0.17, 0.2899, 0.1449, 0.1243,...   \n",
       "3                  1  [0.4789, 0.4328, 0.1974, 0.4858, 0.6806, 0.362...   \n",
       "4                 10  [0.0979, 0.1437, 0.0563, 0.1213, 0.1268, 0.04,...   \n",
       "5                 10  [0.0913, 0.0567, 0.1031, 0.1137, 0.1277, 0.237...   \n",
       "6                  3  [0.9065, 0.7046, 0.9093, 0.839, 0.3243, 0.8628...   \n",
       "7                  3  [0.7658, 0.3105, 0.7345, 0.5098, 0.2397, 0.776...   \n",
       "8                  3  [0.6131, 0.3379, 0.7678, 0.4274, 0.2243, 0.600...   \n",
       "9                  4  [0.8408, 0.9328, 0.8371, 0.8119, 0.9535, 0.693...   \n",
       "10                 4  [0.5829, 0.8781, 0.3126, 0.5349, 0.7409, 0.499...   \n",
       "11                 5  [0.3995, 0.7132, 0.3039, 0.4118, 0.7371, 0.441...   \n",
       "12                17  [0.7071, 0.733, 0.8516, 0.9108, 0.7749, 0.7382...   \n",
       "13                 7  [0.6502, 0.3119, 0.4214, 0.5154, 0.7452, 0.928...   \n",
       "14                 8  [0.6356, 0.5465, 0.5655, 0.784, 0.6292, 0.4891...   \n",
       "15                 8  [0.2385, 0.6218, 0.3976, 0.3882, 0.4849, 0.205...   \n",
       "16                 9  [0.1009, 0.0786, 0.0806, 0.1551, 0.1669, 0.210...   \n",
       "17                 9  [0.3398, 0.4032, 0.3738, 0.5249, 0.4947, 0.483...   \n",
       "18                10  [0.1122, 0.368, 0.0831, 0.1005, 0.3828, 0.1455...   \n",
       "19                10  [0.1467, 0.5687, 0.0914, 0.2335, 0.4841, 0.130...   \n",
       "20                 0  [0.296, 0.3193, 0.4247, 0.6375, 0.1405, 0.2447...   \n",
       "21                22  [0.2255, 0.1342, 0.1831, 0.5326, 0.0549, 0.212...   \n",
       "22                22  [0.1819, 0.1714, 0.1623, 0.3779, 0.1015, 0.297...   \n",
       "23                13  [0.573, 0.5232, 0.5063, 0.3608, 0.5204, 0.5403...   \n",
       "24                10  [0.0992, 0.0845, 0.163, 0.0912, 0.0767, 0.1543...   \n",
       "25                14  [0.2201, 0.2182, 0.6957, 0.4344, 0.3186, 0.381...   \n",
       "26                22  [0.0796, 0.0322, 0.2742, 0.1691, 0.0684, 0.191...   \n",
       "27                15  [0.5384, 0.4748, 0.4812, 0.4096, 0.8468, 0.650...   \n",
       "28                15  [0.7699, 0.4834, 0.6482, 0.6335, 0.9489, 0.915...   \n",
       "29                15  [0.8208, 0.5111, 0.8236, 0.7568, 0.9673, 0.897...   \n",
       "30                16  [0.9998, 0.9998, 0.9871, 0.9996, 1.0, 0.9955, ...   \n",
       "31                17  [0.7639, 0.3758, 0.7004, 0.5327, 0.3473, 0.446...   \n",
       "32                17  [0.5437, 0.5496, 0.772, 0.7526, 0.2575, 0.5915...   \n",
       "33                10  [0.0571, 0.1038, 0.0533, 0.0526, 0.072, 0.0815...   \n",
       "34                19  [0.5087, 0.774, 0.0968, 0.2619, 0.587, 0.1314,...   \n",
       "35                23  [0.1149, 0.082, 0.2811, 0.1691, 0.0616, 0.2724...   \n",
       "36                23  [0.086, 0.0512, 0.1832, 0.1801, 0.0473, 0.1952...   \n",
       "37                23  [0.3159, 0.2427, 0.6264, 0.7027, 0.1145, 0.901...   \n",
       "38                22  [0.4964, 0.1006, 0.127, 0.2269, 0.155, 0.7446,...   \n",
       "39                22  [0.2768, 0.0289, 0.0465, 0.1474, 0.0431, 0.237...   \n",
       "40                22  [0.7315, 0.4702, 0.3589, 0.7588, 0.4389, 0.614...   \n",
       "41                22  [0.086, 0.031, 0.0467, 0.063, 0.0599, 0.331, 0...   \n",
       "42                23  [0.1144, 0.0506, 0.3174, 0.2152, 0.0328, 0.265...   \n",
       "43                23  [0.572, 0.2018, 0.5889, 0.6933, 0.1268, 0.8583...   \n",
       "44                23  [0.5278, 0.1533, 0.8002, 0.6396, 0.1301, 0.961...   \n",
       "45                23  [0.2787, 0.0797, 0.4337, 0.304, 0.0538, 0.4331...   \n",
       "46                17  [0.6318, 0.3023, 0.6129, 0.4393, 0.4687, 0.500...   \n",
       "47                24  [0.4696, 0.4738, 0.4506, 0.5688, 0.2685, 0.342...   \n",
       "48                25  [0.1338, 0.4143, 0.0232, 0.0918, 0.4252, 0.072...   \n",
       "49                25  [0.2495, 0.9153, 0.2971, 0.5224, 0.6853, 0.289...   \n",
       "50                23  [0.431, 0.4531, 0.6703, 0.4035, 0.1999, 0.3268...   \n",
       "51                26  [0.5284, 0.6089, 0.6243, 0.5393, 0.6166, 0.332...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.8088, 0.6581, 0.8884, 0.7855, 0.8588, 0.899...  0.730769  0.129001  \n",
       "1   [0.3972, 0.3759, 0.7333, 0.5063, 0.5784, 0.788...       NaN       NaN  \n",
       "2   [0.6255, 0.5614, 0.83, 0.7101, 0.8551, 0.8757,...       NaN       NaN  \n",
       "3   [0.5211, 0.5672, 0.8026, 0.5142, 0.3194, 0.637...       NaN       NaN  \n",
       "4   [0.9021, 0.8563, 0.9437, 0.8787, 0.8732, 0.96,...       NaN       NaN  \n",
       "5   [0.9087, 0.9433, 0.8969, 0.8863, 0.8723, 0.762...       NaN       NaN  \n",
       "6   [0.0935, 0.2954, 0.0907, 0.161, 0.6757, 0.1372...       NaN       NaN  \n",
       "7   [0.2342, 0.6895, 0.2655, 0.4902, 0.7603, 0.223...       NaN       NaN  \n",
       "8   [0.3869, 0.6621, 0.2322, 0.5726, 0.7757, 0.399...       NaN       NaN  \n",
       "9   [0.1592, 0.0672, 0.1629, 0.1881, 0.0465, 0.306...       NaN       NaN  \n",
       "10  [0.4171, 0.1219, 0.6874, 0.4651, 0.2591, 0.500...       NaN       NaN  \n",
       "11  [0.6005, 0.2868, 0.6961, 0.5882, 0.2629, 0.558...       NaN       NaN  \n",
       "12  [0.2929, 0.267, 0.1484, 0.0892, 0.2251, 0.2618...       NaN       NaN  \n",
       "13  [0.3498, 0.6881, 0.5786, 0.4846, 0.2548, 0.071...       NaN       NaN  \n",
       "14  [0.3644, 0.4535, 0.4345, 0.216, 0.3708, 0.5109...       NaN       NaN  \n",
       "15  [0.7615, 0.3782, 0.6024, 0.6118, 0.5151, 0.794...       NaN       NaN  \n",
       "16  [0.8991, 0.9214, 0.9194, 0.8449, 0.8331, 0.789...       NaN       NaN  \n",
       "17  [0.6602, 0.5968, 0.6262, 0.4751, 0.5053, 0.516...       NaN       NaN  \n",
       "18  [0.8878, 0.632, 0.9169, 0.8995, 0.6172, 0.8545...       NaN       NaN  \n",
       "19  [0.8533, 0.4313, 0.9086, 0.7665, 0.5159, 0.869...       NaN       NaN  \n",
       "20  [0.704, 0.6807, 0.5753, 0.3625, 0.8595, 0.7553...       NaN       NaN  \n",
       "21  [0.7745, 0.8658, 0.8169, 0.4674, 0.9451, 0.787...       NaN       NaN  \n",
       "22  [0.8181, 0.8286, 0.8377, 0.6221, 0.8985, 0.702...       NaN       NaN  \n",
       "23  [0.427, 0.4768, 0.4937, 0.6392, 0.4796, 0.4597...       NaN       NaN  \n",
       "24  [0.9008, 0.9155, 0.837, 0.9088, 0.9233, 0.8457...       NaN       NaN  \n",
       "25  [0.7799, 0.7818, 0.3043, 0.5656, 0.6814, 0.618...       NaN       NaN  \n",
       "26  [0.9204, 0.9678, 0.7258, 0.8309, 0.9316, 0.808...       NaN       NaN  \n",
       "27  [0.4616, 0.5252, 0.5188, 0.5904, 0.1532, 0.349...       NaN       NaN  \n",
       "28  [0.2301, 0.5166, 0.3518, 0.3665, 0.0511, 0.084...       NaN       NaN  \n",
       "29  [0.1792, 0.4889, 0.1764, 0.2432, 0.0327, 0.102...       NaN       NaN  \n",
       "30  [0.0002, 0.0002, 0.0129, 0.0004, 0.0, 0.0045, ...       NaN       NaN  \n",
       "31  [0.2361, 0.6242, 0.2996, 0.4673, 0.6527, 0.553...       NaN       NaN  \n",
       "32  [0.4563, 0.4504, 0.228, 0.2474, 0.7425, 0.4085...       NaN       NaN  \n",
       "33  [0.9429, 0.8962, 0.9467, 0.9474, 0.928, 0.9185...       NaN       NaN  \n",
       "34  [0.4913, 0.226, 0.9032, 0.7381, 0.413, 0.8686,...       NaN       NaN  \n",
       "35  [0.8851, 0.918, 0.7189, 0.8309, 0.9384, 0.7276...       NaN       NaN  \n",
       "36  [0.914, 0.9488, 0.8168, 0.8199, 0.9527, 0.8048...       NaN       NaN  \n",
       "37  [0.6841, 0.7573, 0.3736, 0.2973, 0.8855, 0.098...       NaN       NaN  \n",
       "38  [0.5036, 0.8994, 0.873, 0.7731, 0.845, 0.2554,...       NaN       NaN  \n",
       "39  [0.7232, 0.9711, 0.9535, 0.8526, 0.9569, 0.762...       NaN       NaN  \n",
       "40  [0.2685, 0.5298, 0.6411, 0.2412, 0.5611, 0.385...       NaN       NaN  \n",
       "41  [0.914, 0.969, 0.9533, 0.937, 0.9401, 0.669, 0...       NaN       NaN  \n",
       "42  [0.8856, 0.9494, 0.6826, 0.7848, 0.9672, 0.734...       NaN       NaN  \n",
       "43  [0.428, 0.7982, 0.4111, 0.3067, 0.8732, 0.1417...       NaN       NaN  \n",
       "44  [0.4722, 0.8467, 0.1998, 0.3604, 0.8699, 0.038...       NaN       NaN  \n",
       "45  [0.7213, 0.9203, 0.5663, 0.696, 0.9462, 0.5669...       NaN       NaN  \n",
       "46  [0.3682, 0.6977, 0.3871, 0.5607, 0.5313, 0.499...       NaN       NaN  \n",
       "47  [0.5304, 0.5262, 0.5494, 0.4312, 0.7315, 0.657...       NaN       NaN  \n",
       "48  [0.8662, 0.5857, 0.9768, 0.9082, 0.5748, 0.927...       NaN       NaN  \n",
       "49  [0.7505, 0.0847, 0.7029, 0.4776, 0.3147, 0.710...       NaN       NaN  \n",
       "50  [0.569, 0.5469, 0.3297, 0.5965, 0.8001, 0.6732...       NaN       NaN  \n",
       "51  [0.4716, 0.3911, 0.3757, 0.4607, 0.3834, 0.667...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181 : Training: loss:  0.11103478\n",
      "1182 : Training: loss:  0.10932189\n",
      "1183 : Training: loss:  0.11235077\n",
      "1184 : Training: loss:  0.09955758\n",
      "1185 : Training: loss:  0.09901729\n",
      "1186 : Training: loss:  0.12631875\n",
      "1187 : Training: loss:  0.098428816\n",
      "1188 : Training: loss:  0.09484059\n",
      "1189 : Training: loss:  0.1185689\n",
      "1190 : Training: loss:  0.11283613\n",
      "1191 : Training: loss:  0.11714722\n",
      "1192 : Training: loss:  0.122276194\n",
      "1193 : Training: loss:  0.09560377\n",
      "1194 : Training: loss:  0.104673944\n",
      "1195 : Training: loss:  0.109535076\n",
      "1196 : Training: loss:  0.10586793\n",
      "1197 : Training: loss:  0.13824604\n",
      "1198 : Training: loss:  0.09643146\n",
      "1199 : Training: loss:  0.11491489\n",
      "1200 : Training: loss:  0.11802034\n",
      "Validation: Loss:  0.12794565  Accuracy:  0.6923077\n",
      "1201 : Training: loss:  0.12344893\n",
      "1202 : Training: loss:  0.12025925\n",
      "1203 : Training: loss:  0.12397745\n",
      "1204 : Training: loss:  0.10930837\n",
      "1205 : Training: loss:  0.123121805\n",
      "1206 : Training: loss:  0.13250841\n",
      "1207 : Training: loss:  0.1251857\n",
      "1208 : Training: loss:  0.11573256\n",
      "1209 : Training: loss:  0.09448598\n",
      "1210 : Training: loss:  0.1037907\n",
      "1211 : Training: loss:  0.12856351\n",
      "1212 : Training: loss:  0.099487305\n",
      "1213 : Training: loss:  0.09980966\n",
      "1214 : Training: loss:  0.11775891\n",
      "1215 : Training: loss:  0.09952184\n",
      "1216 : Training: loss:  0.09569972\n",
      "1217 : Training: loss:  0.09026306\n",
      "1218 : Training: loss:  0.11771293\n",
      "1219 : Training: loss:  0.0797902\n",
      "1220 : Training: loss:  0.10076841\n",
      "Validation: Loss:  0.12672003  Accuracy:  0.7307692\n",
      "1221 : Training: loss:  0.11331436\n",
      "1222 : Training: loss:  0.092189446\n",
      "1223 : Training: loss:  0.09391494\n",
      "1224 : Training: loss:  0.10309685\n",
      "1225 : Training: loss:  0.09339619\n",
      "1226 : Training: loss:  0.10536461\n",
      "1227 : Training: loss:  0.10825985\n",
      "1228 : Training: loss:  0.1442784\n",
      "1229 : Training: loss:  0.12040489\n",
      "1230 : Training: loss:  0.102625884\n",
      "1231 : Training: loss:  0.11365552\n",
      "1232 : Training: loss:  0.13387913\n",
      "1233 : Training: loss:  0.11860035\n",
      "1234 : Training: loss:  0.09404393\n",
      "1235 : Training: loss:  0.112037055\n",
      "1236 : Training: loss:  0.11558355\n",
      "1237 : Training: loss:  0.11984993\n",
      "1238 : Training: loss:  0.12081364\n",
      "1239 : Training: loss:  0.13176253\n",
      "1240 : Training: loss:  0.12137216\n",
      "Validation: Loss:  0.12582463  Accuracy:  0.7307692\n",
      "1241 : Training: loss:  0.12003618\n",
      "1242 : Training: loss:  0.09836117\n",
      "1243 : Training: loss:  0.10093694\n",
      "1244 : Training: loss:  0.10240355\n",
      "1245 : Training: loss:  0.105404384\n",
      "1246 : Training: loss:  0.108927265\n",
      "1247 : Training: loss:  0.106285855\n",
      "1248 : Training: loss:  0.10621442\n",
      "1249 : Training: loss:  0.10858016\n",
      "1250 : Training: loss:  0.108763374\n",
      "1251 : Training: loss:  0.11305314\n",
      "1252 : Training: loss:  0.11750928\n",
      "1253 : Training: loss:  0.13279931\n",
      "1254 : Training: loss:  0.106473334\n",
      "1255 : Training: loss:  0.14858164\n",
      "1256 : Training: loss:  0.12885244\n",
      "1257 : Training: loss:  0.091051675\n",
      "1258 : Training: loss:  0.110160545\n",
      "1259 : Training: loss:  0.10763269\n",
      "1260 : Training: loss:  0.08901523\n",
      "Validation: Loss:  0.12476205  Accuracy:  0.71153843\n",
      "1261 : Training: loss:  0.1110298\n",
      "1262 : Training: loss:  0.08312565\n",
      "1263 : Training: loss:  0.11545526\n",
      "1264 : Training: loss:  0.1250746\n",
      "1265 : Training: loss:  0.10585617\n",
      "1266 : Training: loss:  0.107897274\n",
      "1267 : Training: loss:  0.11567799\n",
      "1268 : Training: loss:  0.10898439\n",
      "1269 : Training: loss:  0.117838636\n",
      "1270 : Training: loss:  0.09757854\n",
      "1271 : Training: loss:  0.104444236\n",
      "1272 : Training: loss:  0.09556878\n",
      "1273 : Training: loss:  0.08810397\n",
      "1274 : Training: loss:  0.10195694\n",
      "1275 : Training: loss:  0.11003912\n",
      "1276 : Training: loss:  0.10941288\n",
      "1277 : Training: loss:  0.096347764\n",
      "1278 : Training: loss:  0.105533205\n",
      "1279 : Training: loss:  0.08611941\n",
      "1280 : Training: loss:  0.08989366\n",
      "Validation: Loss:  0.12357037  Accuracy:  0.7307692\n",
      "1281 : Training: loss:  0.10634756\n",
      "1282 : Training: loss:  0.1078005\n",
      "1283 : Training: loss:  0.10505863\n",
      "1284 : Training: loss:  0.08746661\n",
      "1285 : Training: loss:  0.12667969\n",
      "1286 : Training: loss:  0.12143649\n",
      "1287 : Training: loss:  0.11817008\n",
      "1288 : Training: loss:  0.1035518\n",
      "1289 : Training: loss:  0.11041364\n",
      "1290 : Training: loss:  0.11121167\n",
      "1291 : Training: loss:  0.0956316\n",
      "1292 : Training: loss:  0.09701889\n",
      "1293 : Training: loss:  0.1166984\n",
      "1294 : Training: loss:  0.07726239\n",
      "1295 : Training: loss:  0.11196556\n",
      "1296 : Training: loss:  0.08097008\n",
      "1297 : Training: loss:  0.11361528\n",
      "1298 : Training: loss:  0.101024434\n",
      "1299 : Training: loss:  0.11101542\n",
      "1300 : Training: loss:  0.097950235\n",
      "Validation: Loss:  0.12246595  Accuracy:  0.7307692\n",
      "1301 : Training: loss:  0.1053808\n",
      "1302 : Training: loss:  0.11065808\n",
      "1303 : Training: loss:  0.08997663\n",
      "1304 : Training: loss:  0.12368947\n",
      "1305 : Training: loss:  0.111255884\n",
      "1306 : Training: loss:  0.10304621\n",
      "1307 : Training: loss:  0.1251735\n",
      "1308 : Training: loss:  0.10842224\n",
      "1309 : Training: loss:  0.109799474\n",
      "1310 : Training: loss:  0.09142223\n",
      "1311 : Training: loss:  0.07792546\n",
      "1312 : Training: loss:  0.11104132\n",
      "1313 : Training: loss:  0.09205622\n",
      "1314 : Training: loss:  0.0884836\n",
      "1315 : Training: loss:  0.09081516\n",
      "1316 : Training: loss:  0.08277338\n",
      "1317 : Training: loss:  0.09333314\n",
      "1318 : Training: loss:  0.106930055\n",
      "1319 : Training: loss:  0.11715687\n",
      "1320 : Training: loss:  0.08616107\n",
      "Validation: Loss:  0.12105283  Accuracy:  0.7307692\n",
      "1321 : Training: loss:  0.10952743\n",
      "1322 : Training: loss:  0.10795039\n",
      "1323 : Training: loss:  0.09900282\n",
      "1324 : Training: loss:  0.11163892\n",
      "1325 : Training: loss:  0.11448968\n",
      "1326 : Training: loss:  0.09726936\n",
      "1327 : Training: loss:  0.12448451\n",
      "1328 : Training: loss:  0.1083185\n",
      "1329 : Training: loss:  0.10999532\n",
      "1330 : Training: loss:  0.11306879\n",
      "1331 : Training: loss:  0.111402184\n",
      "1332 : Training: loss:  0.11862177\n",
      "1333 : Training: loss:  0.1139123\n",
      "1334 : Training: loss:  0.09221348\n",
      "1335 : Training: loss:  0.1044422\n",
      "1336 : Training: loss:  0.09788298\n",
      "1337 : Training: loss:  0.0925091\n",
      "1338 : Training: loss:  0.13042518\n",
      "1339 : Training: loss:  0.0948471\n",
      "1340 : Training: loss:  0.121238776\n",
      "Validation: Loss:  0.11991126  Accuracy:  0.75\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.153, 0.0559, 0.0304, 0.0487, 0.0286, 0.0495...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2104, 0.3637, 0.1078, 0.2401, 0.1569, 0.096...</td>\n",
       "      <td>[0.7896, 0.6363, 0.8922, 0.7599, 0.8431, 0.904...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.119911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.2183, 0.1623, 0.0243, 0.0496, 0.1068, 0.058...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.641, 0.6547, 0.2615, 0.5191, 0.4618, 0.2079...</td>\n",
       "      <td>[0.359, 0.3453, 0.7385, 0.4809, 0.5382, 0.7921...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.1407, 0.0664, 0.0228, 0.0553, 0.0346, 0.057...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4151, 0.4734, 0.1677, 0.3367, 0.1578, 0.121...</td>\n",
       "      <td>[0.5849, 0.5266, 0.8323, 0.6633, 0.8422, 0.878...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0489, 0.0663, 0.0141, 0.0201, 0.0589, 0.038...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.52, 0.4495, 0.1912, 0.5243, 0.7136, 0.3701,...</td>\n",
       "      <td>[0.48, 0.5505, 0.8088, 0.4757, 0.2864, 0.6299,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0872, 0.0387, 0.0197, 0.0226, 0.0159, 0.035...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1068, 0.1572, 0.0546, 0.1459, 0.1456, 0.038...</td>\n",
       "      <td>[0.8932, 0.8428, 0.9454, 0.8541, 0.8544, 0.961...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0875, 0.0354, 0.0479, 0.0394, 0.0177, 0.056...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1005, 0.0566, 0.1159, 0.1249, 0.137, 0.2641...</td>\n",
       "      <td>[0.8995, 0.9434, 0.8841, 0.8751, 0.863, 0.7359...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.04, 0.0288, 0.0053, 0.351, 0.0144, 0.0073, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9276, 0.7354, 0.9259, 0.8802, 0.2984, 0.891...</td>\n",
       "      <td>[0.0724, 0.2646, 0.0741, 0.1198, 0.7016, 0.108...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0578, 0.0333, 0.0199, 0.1874, 0.0149, 0.021...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8075, 0.3179, 0.7658, 0.5569, 0.2294, 0.819...</td>\n",
       "      <td>[0.1925, 0.6821, 0.2342, 0.4431, 0.7706, 0.180...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.091, 0.0485, 0.0333, 0.1893, 0.0269, 0.0453...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.6717, 0.3637, 0.8131, 0.4555, 0.222, 0.6488...</td>\n",
       "      <td>[0.3283, 0.6363, 0.1869, 0.5445, 0.778, 0.3512...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0303, 0.1252, 0.0274, 0.0463, 0.3766, 0.121...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8577, 0.9407, 0.8379, 0.821, 0.9592, 0.6991...</td>\n",
       "      <td>[0.1423, 0.0593, 0.1621, 0.179, 0.0408, 0.3009...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0534, 0.1534, 0.0137, 0.0203, 0.1662, 0.120...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.6205, 0.8961, 0.2792, 0.5694, 0.7725, 0.519...</td>\n",
       "      <td>[0.3795, 0.1039, 0.7208, 0.4306, 0.2275, 0.480...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0285, 0.0454, 0.0149, 0.0261, 0.0625, 0.103...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4364, 0.7402, 0.2979, 0.4458, 0.7651, 0.454...</td>\n",
       "      <td>[0.5636, 0.2598, 0.7021, 0.5542, 0.2349, 0.545...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0153, 0.0378, 0.0201, 0.0302, 0.0283, 0.018...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7061, 0.7108, 0.8469, 0.9272, 0.7926, 0.725...</td>\n",
       "      <td>[0.2939, 0.2892, 0.1531, 0.0728, 0.2074, 0.274...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1319, 0.1015, 0.1314, 0.1634, 0.0337, 0.141...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.6306, 0.2573, 0.3924, 0.5353, 0.7532, 0.937...</td>\n",
       "      <td>[0.3694, 0.7427, 0.6076, 0.4647, 0.2468, 0.062...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0269, 0.0281, 0.007, 0.0541, 0.0317, 0.015,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.6858, 0.5769, 0.5684, 0.8309, 0.6484, 0.502...</td>\n",
       "      <td>[0.3142, 0.4231, 0.4316, 0.1691, 0.3516, 0.497...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0684, 0.0491, 0.016, 0.0243, 0.0485, 0.0337...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2552, 0.6654, 0.4023, 0.4287, 0.536, 0.1813...</td>\n",
       "      <td>[0.7448, 0.3346, 0.5977, 0.5713, 0.464, 0.8187...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0749, 0.026, 0.0387, 0.0318, 0.0113, 0.0456...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.111, 0.083, 0.0871, 0.1703, 0.1748, 0.2255,...</td>\n",
       "      <td>[0.889, 0.917, 0.9129, 0.8297, 0.8252, 0.7745,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0534, 0.0359, 0.0243, 0.0406, 0.0208, 0.033...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.3658, 0.4296, 0.4051, 0.5566, 0.5021, 0.500...</td>\n",
       "      <td>[0.6342, 0.5704, 0.5949, 0.4434, 0.4979, 0.499...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.1023, 0.0238, 0.0319, 0.0287, 0.0115, 0.038...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1236, 0.38, 0.0861, 0.105, 0.4021, 0.1612, ...</td>\n",
       "      <td>[0.8764, 0.62, 0.9139, 0.895, 0.5979, 0.8388, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0867, 0.025, 0.0345, 0.0241, 0.0133, 0.0418...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1604, 0.5868, 0.0959, 0.2546, 0.517, 0.1338...</td>\n",
       "      <td>[0.8396, 0.4132, 0.9041, 0.7454, 0.483, 0.8662...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0864, 0.0412, 0.0245, 0.06, 0.0176, 0.038, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3277, 0.3163, 0.4568, 0.6886, 0.1423, 0.252...</td>\n",
       "      <td>[0.6723, 0.6837, 0.5432, 0.3114, 0.8577, 0.747...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0481, 0.0157, 0.0152, 0.0339, 0.0055, 0.014...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2757, 0.1236, 0.1987, 0.5966, 0.0534, 0.258...</td>\n",
       "      <td>[0.7243, 0.8764, 0.8013, 0.4034, 0.9466, 0.741...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0623, 0.0245, 0.0209, 0.0355, 0.0094, 0.025...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.2207, 0.1726, 0.1687, 0.4246, 0.101, 0.3212...</td>\n",
       "      <td>[0.7793, 0.8274, 0.8313, 0.5754, 0.899, 0.6788...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.066, 0.0366, 0.0254, 0.046, 0.0247, 0.0588,...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.6243, 0.5477, 0.5541, 0.3671, 0.5551, 0.569...</td>\n",
       "      <td>[0.3757, 0.4523, 0.4459, 0.6329, 0.4449, 0.430...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.093, 0.0208, 0.0287, 0.0345, 0.0097, 0.0315...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1186, 0.0929, 0.2025, 0.0978, 0.0768, 0.178...</td>\n",
       "      <td>[0.8814, 0.9071, 0.7975, 0.9022, 0.9232, 0.821...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0438, 0.0313, 0.0215, 0.0402, 0.0265, 0.030...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.2468, 0.2339, 0.7417, 0.477, 0.3436, 0.4056...</td>\n",
       "      <td>[0.7532, 0.7661, 0.2583, 0.523, 0.6564, 0.5944...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0586, 0.0175, 0.0188, 0.0217, 0.0071, 0.017...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.0921, 0.0298, 0.3056, 0.1911, 0.0708, 0.220...</td>\n",
       "      <td>[0.9079, 0.9702, 0.6944, 0.8089, 0.9292, 0.779...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0135, 0.0198, 0.0107, 0.02, 0.0234, 0.0203,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.599, 0.4932, 0.5062, 0.4596, 0.8615, 0.6945...</td>\n",
       "      <td>[0.401, 0.5068, 0.4938, 0.5404, 0.1385, 0.3055...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0172, 0.033, 0.0252, 0.0618, 0.0208, 0.0283...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7898, 0.4643, 0.669, 0.6804, 0.9531, 0.9314...</td>\n",
       "      <td>[0.2102, 0.5357, 0.331, 0.3196, 0.0469, 0.0686...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0067, 0.0233, 0.0073, 0.028, 0.036, 0.0147,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8477, 0.5048, 0.8391, 0.7973, 0.9718, 0.916...</td>\n",
       "      <td>[0.1523, 0.4952, 0.1609, 0.2027, 0.0282, 0.083...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.3232, 0.6219, 0.4249, 0.3128, 0.7874, 0.613...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9998, 0.9997, 0.9752, 0.9995, 1.0, 0.992, 1...</td>\n",
       "      <td>[0.0002, 0.0003, 0.0248, 0.0005, 0.0, 0.008, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0333, 0.0325, 0.0215, 0.0645, 0.017, 0.0194...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.8015, 0.3713, 0.7426, 0.5684, 0.3422, 0.506...</td>\n",
       "      <td>[0.1985, 0.6287, 0.2574, 0.4316, 0.6578, 0.493...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0278, 0.0209, 0.011, 0.0599, 0.0095, 0.0103...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.5852, 0.5628, 0.8177, 0.8027, 0.2492, 0.627...</td>\n",
       "      <td>[0.4148, 0.4372, 0.1823, 0.1973, 0.7508, 0.372...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0667, 0.0159, 0.0161, 0.0188, 0.0065, 0.020...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0732, 0.1178, 0.0575, 0.0659, 0.0806, 0.089...</td>\n",
       "      <td>[0.9268, 0.8822, 0.9425, 0.9341, 0.9194, 0.910...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0704, 0.0503, 0.0353, 0.0404, 0.0705, 0.066...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.6038, 0.8131, 0.0981, 0.3016, 0.6401, 0.133...</td>\n",
       "      <td>[0.3962, 0.1869, 0.9019, 0.6984, 0.3599, 0.866...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.052, 0.0143, 0.0112, 0.0189, 0.0047, 0.0137...</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1363, 0.0846, 0.309, 0.1977, 0.0659, 0.3026...</td>\n",
       "      <td>[0.8637, 0.9154, 0.691, 0.8023, 0.9341, 0.6974...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0626, 0.0186, 0.0197, 0.0245, 0.0056, 0.020...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0843, 0.0485, 0.1872, 0.1898, 0.0421, 0.215...</td>\n",
       "      <td>[0.9157, 0.9515, 0.8128, 0.8102, 0.9579, 0.784...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0253, 0.0203, 0.0146, 0.0495, 0.0041, 0.007...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2939, 0.1971, 0.626, 0.7241, 0.0931, 0.9218...</td>\n",
       "      <td>[0.7061, 0.8029, 0.374, 0.2759, 0.9069, 0.0782...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0383, 0.0099, 0.0236, 0.052, 0.0069, 0.0125...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5699, 0.0868, 0.1096, 0.2413, 0.1417, 0.807...</td>\n",
       "      <td>[0.4301, 0.9132, 0.8904, 0.7587, 0.8583, 0.192...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0531, 0.0126, 0.02, 0.0338, 0.0063, 0.0159,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3353, 0.0263, 0.0445, 0.1732, 0.0396, 0.304...</td>\n",
       "      <td>[0.6647, 0.9737, 0.9555, 0.8268, 0.9604, 0.695...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.039, 0.0368, 0.011, 0.0831, 0.0249, 0.0164,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7729, 0.4441, 0.3429, 0.7924, 0.425, 0.6632...</td>\n",
       "      <td>[0.2271, 0.5559, 0.6571, 0.2076, 0.575, 0.3368...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0764, 0.0142, 0.0254, 0.0333, 0.0064, 0.021...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1024, 0.0292, 0.0455, 0.0648, 0.059, 0.3906...</td>\n",
       "      <td>[0.8976, 0.9708, 0.9545, 0.9352, 0.941, 0.6094...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0397, 0.012, 0.0104, 0.0193, 0.0029, 0.0084...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1161, 0.0466, 0.3498, 0.2425, 0.0302, 0.287...</td>\n",
       "      <td>[0.8839, 0.9534, 0.6502, 0.7575, 0.9698, 0.712...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0246, 0.0166, 0.0101, 0.0373, 0.0032, 0.004...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.559, 0.1531, 0.5905, 0.727, 0.1069, 0.8808,...</td>\n",
       "      <td>[0.441, 0.8469, 0.4095, 0.273, 0.8931, 0.1192,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.023, 0.0155, 0.0094, 0.0357, 0.0029, 0.0036...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4888, 0.1078, 0.8106, 0.6737, 0.1099, 0.969...</td>\n",
       "      <td>[0.5112, 0.8922, 0.1894, 0.3263, 0.8901, 0.030...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0251, 0.0093, 0.0074, 0.0203, 0.0019, 0.004...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2753, 0.0658, 0.4496, 0.3329, 0.0453, 0.480...</td>\n",
       "      <td>[0.7247, 0.9342, 0.5504, 0.6671, 0.9547, 0.519...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.057, 0.0428, 0.0614, 0.0716, 0.0227, 0.0687...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.6701, 0.3025, 0.6663, 0.4416, 0.4836, 0.524...</td>\n",
       "      <td>[0.3299, 0.6975, 0.3337, 0.5584, 0.5164, 0.475...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0579, 0.0688, 0.0199, 0.0393, 0.0378, 0.044...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.5216, 0.4928, 0.5121, 0.6142, 0.2868, 0.343...</td>\n",
       "      <td>[0.4784, 0.5072, 0.4879, 0.3858, 0.7132, 0.656...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0353, 0.0167, 0.0085, 0.0066, 0.0121, 0.019...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1669, 0.4505, 0.0208, 0.1077, 0.4887, 0.074...</td>\n",
       "      <td>[0.8331, 0.5495, 0.9792, 0.8923, 0.5113, 0.926...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0666, 0.0821, 0.0248, 0.0327, 0.0887, 0.099...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2731, 0.9287, 0.2881, 0.5662, 0.726, 0.2839...</td>\n",
       "      <td>[0.7269, 0.0713, 0.7119, 0.4338, 0.274, 0.7161...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0709, 0.0482, 0.021, 0.0497, 0.0378, 0.0381...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.453, 0.4563, 0.6848, 0.4534, 0.2119, 0.3122...</td>\n",
       "      <td>[0.547, 0.5437, 0.3152, 0.5466, 0.7881, 0.6878...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0265, 0.0354, 0.0146, 0.0389, 0.0521, 0.017...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.5709, 0.652, 0.6593, 0.5861, 0.6427, 0.3342...</td>\n",
       "      <td>[0.4291, 0.348, 0.3407, 0.4139, 0.3573, 0.6658...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.153, 0.0559, 0.0304, 0.0487, 0.0286, 0.0495...               0   \n",
       "1   [0.2183, 0.1623, 0.0243, 0.0496, 0.1068, 0.058...               0   \n",
       "2   [0.1407, 0.0664, 0.0228, 0.0553, 0.0346, 0.057...               0   \n",
       "3   [0.0489, 0.0663, 0.0141, 0.0201, 0.0589, 0.038...               1   \n",
       "4   [0.0872, 0.0387, 0.0197, 0.0226, 0.0159, 0.035...               1   \n",
       "5   [0.0875, 0.0354, 0.0479, 0.0394, 0.0177, 0.056...               2   \n",
       "6   [0.04, 0.0288, 0.0053, 0.351, 0.0144, 0.0073, ...               3   \n",
       "7   [0.0578, 0.0333, 0.0199, 0.1874, 0.0149, 0.021...               3   \n",
       "8   [0.091, 0.0485, 0.0333, 0.1893, 0.0269, 0.0453...               3   \n",
       "9   [0.0303, 0.1252, 0.0274, 0.0463, 0.3766, 0.121...               4   \n",
       "10  [0.0534, 0.1534, 0.0137, 0.0203, 0.1662, 0.120...               4   \n",
       "11  [0.0285, 0.0454, 0.0149, 0.0261, 0.0625, 0.103...               5   \n",
       "12  [0.0153, 0.0378, 0.0201, 0.0302, 0.0283, 0.018...               6   \n",
       "13  [0.1319, 0.1015, 0.1314, 0.1634, 0.0337, 0.141...               7   \n",
       "14  [0.0269, 0.0281, 0.007, 0.0541, 0.0317, 0.015,...               8   \n",
       "15  [0.0684, 0.0491, 0.016, 0.0243, 0.0485, 0.0337...               8   \n",
       "16  [0.0749, 0.026, 0.0387, 0.0318, 0.0113, 0.0456...               9   \n",
       "17  [0.0534, 0.0359, 0.0243, 0.0406, 0.0208, 0.033...               9   \n",
       "18  [0.1023, 0.0238, 0.0319, 0.0287, 0.0115, 0.038...              10   \n",
       "19  [0.0867, 0.025, 0.0345, 0.0241, 0.0133, 0.0418...              10   \n",
       "20  [0.0864, 0.0412, 0.0245, 0.06, 0.0176, 0.038, ...              11   \n",
       "21  [0.0481, 0.0157, 0.0152, 0.0339, 0.0055, 0.014...              11   \n",
       "22  [0.0623, 0.0245, 0.0209, 0.0355, 0.0094, 0.025...              12   \n",
       "23  [0.066, 0.0366, 0.0254, 0.046, 0.0247, 0.0588,...              13   \n",
       "24  [0.093, 0.0208, 0.0287, 0.0345, 0.0097, 0.0315...              13   \n",
       "25  [0.0438, 0.0313, 0.0215, 0.0402, 0.0265, 0.030...              14   \n",
       "26  [0.0586, 0.0175, 0.0188, 0.0217, 0.0071, 0.017...              14   \n",
       "27  [0.0135, 0.0198, 0.0107, 0.02, 0.0234, 0.0203,...              15   \n",
       "28  [0.0172, 0.033, 0.0252, 0.0618, 0.0208, 0.0283...              15   \n",
       "29  [0.0067, 0.0233, 0.0073, 0.028, 0.036, 0.0147,...              15   \n",
       "30  [0.3232, 0.6219, 0.4249, 0.3128, 0.7874, 0.613...              16   \n",
       "31  [0.0333, 0.0325, 0.0215, 0.0645, 0.017, 0.0194...              17   \n",
       "32  [0.0278, 0.0209, 0.011, 0.0599, 0.0095, 0.0103...              17   \n",
       "33  [0.0667, 0.0159, 0.0161, 0.0188, 0.0065, 0.020...              18   \n",
       "34  [0.0704, 0.0503, 0.0353, 0.0404, 0.0705, 0.066...              19   \n",
       "35  [0.052, 0.0143, 0.0112, 0.0189, 0.0047, 0.0137...              20   \n",
       "36  [0.0626, 0.0186, 0.0197, 0.0245, 0.0056, 0.020...              21   \n",
       "37  [0.0253, 0.0203, 0.0146, 0.0495, 0.0041, 0.007...              21   \n",
       "38  [0.0383, 0.0099, 0.0236, 0.052, 0.0069, 0.0125...              22   \n",
       "39  [0.0531, 0.0126, 0.02, 0.0338, 0.0063, 0.0159,...              22   \n",
       "40  [0.039, 0.0368, 0.011, 0.0831, 0.0249, 0.0164,...              22   \n",
       "41  [0.0764, 0.0142, 0.0254, 0.0333, 0.0064, 0.021...              22   \n",
       "42  [0.0397, 0.012, 0.0104, 0.0193, 0.0029, 0.0084...              23   \n",
       "43  [0.0246, 0.0166, 0.0101, 0.0373, 0.0032, 0.004...              23   \n",
       "44  [0.023, 0.0155, 0.0094, 0.0357, 0.0029, 0.0036...              23   \n",
       "45  [0.0251, 0.0093, 0.0074, 0.0203, 0.0019, 0.004...              23   \n",
       "46  [0.057, 0.0428, 0.0614, 0.0716, 0.0227, 0.0687...              24   \n",
       "47  [0.0579, 0.0688, 0.0199, 0.0393, 0.0378, 0.044...              24   \n",
       "48  [0.0353, 0.0167, 0.0085, 0.0066, 0.0121, 0.019...              25   \n",
       "49  [0.0666, 0.0821, 0.0248, 0.0327, 0.0887, 0.099...              25   \n",
       "50  [0.0709, 0.0482, 0.021, 0.0497, 0.0378, 0.0381...              26   \n",
       "51  [0.0265, 0.0354, 0.0146, 0.0389, 0.0521, 0.017...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.2104, 0.3637, 0.1078, 0.2401, 0.1569, 0.096...   \n",
       "1                  0  [0.641, 0.6547, 0.2615, 0.5191, 0.4618, 0.2079...   \n",
       "2                  0  [0.4151, 0.4734, 0.1677, 0.3367, 0.1578, 0.121...   \n",
       "3                  1  [0.52, 0.4495, 0.1912, 0.5243, 0.7136, 0.3701,...   \n",
       "4                  0  [0.1068, 0.1572, 0.0546, 0.1459, 0.1456, 0.038...   \n",
       "5                 10  [0.1005, 0.0566, 0.1159, 0.1249, 0.137, 0.2641...   \n",
       "6                  3  [0.9276, 0.7354, 0.9259, 0.8802, 0.2984, 0.891...   \n",
       "7                  3  [0.8075, 0.3179, 0.7658, 0.5569, 0.2294, 0.819...   \n",
       "8                  3  [0.6717, 0.3637, 0.8131, 0.4555, 0.222, 0.6488...   \n",
       "9                  4  [0.8577, 0.9407, 0.8379, 0.821, 0.9592, 0.6991...   \n",
       "10                 4  [0.6205, 0.8961, 0.2792, 0.5694, 0.7725, 0.519...   \n",
       "11                 5  [0.4364, 0.7402, 0.2979, 0.4458, 0.7651, 0.454...   \n",
       "12                17  [0.7061, 0.7108, 0.8469, 0.9272, 0.7926, 0.725...   \n",
       "13                 7  [0.6306, 0.2573, 0.3924, 0.5353, 0.7532, 0.937...   \n",
       "14                 8  [0.6858, 0.5769, 0.5684, 0.8309, 0.6484, 0.502...   \n",
       "15                 8  [0.2552, 0.6654, 0.4023, 0.4287, 0.536, 0.1813...   \n",
       "16                 9  [0.111, 0.083, 0.0871, 0.1703, 0.1748, 0.2255,...   \n",
       "17                 9  [0.3658, 0.4296, 0.4051, 0.5566, 0.5021, 0.500...   \n",
       "18                10  [0.1236, 0.38, 0.0861, 0.105, 0.4021, 0.1612, ...   \n",
       "19                10  [0.1604, 0.5868, 0.0959, 0.2546, 0.517, 0.1338...   \n",
       "20                 0  [0.3277, 0.3163, 0.4568, 0.6886, 0.1423, 0.252...   \n",
       "21                22  [0.2757, 0.1236, 0.1987, 0.5966, 0.0534, 0.258...   \n",
       "22                22  [0.2207, 0.1726, 0.1687, 0.4246, 0.101, 0.3212...   \n",
       "23                13  [0.6243, 0.5477, 0.5541, 0.3671, 0.5551, 0.569...   \n",
       "24                10  [0.1186, 0.0929, 0.2025, 0.0978, 0.0768, 0.178...   \n",
       "25                14  [0.2468, 0.2339, 0.7417, 0.477, 0.3436, 0.4056...   \n",
       "26                22  [0.0921, 0.0298, 0.3056, 0.1911, 0.0708, 0.220...   \n",
       "27                15  [0.599, 0.4932, 0.5062, 0.4596, 0.8615, 0.6945...   \n",
       "28                15  [0.7898, 0.4643, 0.669, 0.6804, 0.9531, 0.9314...   \n",
       "29                15  [0.8477, 0.5048, 0.8391, 0.7973, 0.9718, 0.916...   \n",
       "30                16  [0.9998, 0.9997, 0.9752, 0.9995, 1.0, 0.992, 1...   \n",
       "31                17  [0.8015, 0.3713, 0.7426, 0.5684, 0.3422, 0.506...   \n",
       "32                17  [0.5852, 0.5628, 0.8177, 0.8027, 0.2492, 0.627...   \n",
       "33                10  [0.0732, 0.1178, 0.0575, 0.0659, 0.0806, 0.089...   \n",
       "34                19  [0.6038, 0.8131, 0.0981, 0.3016, 0.6401, 0.133...   \n",
       "35                22  [0.1363, 0.0846, 0.309, 0.1977, 0.0659, 0.3026...   \n",
       "36                23  [0.0843, 0.0485, 0.1872, 0.1898, 0.0421, 0.215...   \n",
       "37                23  [0.2939, 0.1971, 0.626, 0.7241, 0.0931, 0.9218...   \n",
       "38                22  [0.5699, 0.0868, 0.1096, 0.2413, 0.1417, 0.807...   \n",
       "39                22  [0.3353, 0.0263, 0.0445, 0.1732, 0.0396, 0.304...   \n",
       "40                22  [0.7729, 0.4441, 0.3429, 0.7924, 0.425, 0.6632...   \n",
       "41                22  [0.1024, 0.0292, 0.0455, 0.0648, 0.059, 0.3906...   \n",
       "42                23  [0.1161, 0.0466, 0.3498, 0.2425, 0.0302, 0.287...   \n",
       "43                23  [0.559, 0.1531, 0.5905, 0.727, 0.1069, 0.8808,...   \n",
       "44                23  [0.4888, 0.1078, 0.8106, 0.6737, 0.1099, 0.969...   \n",
       "45                23  [0.2753, 0.0658, 0.4496, 0.3329, 0.0453, 0.480...   \n",
       "46                13  [0.6701, 0.3025, 0.6663, 0.4416, 0.4836, 0.524...   \n",
       "47                24  [0.5216, 0.4928, 0.5121, 0.6142, 0.2868, 0.343...   \n",
       "48                25  [0.1669, 0.4505, 0.0208, 0.1077, 0.4887, 0.074...   \n",
       "49                25  [0.2731, 0.9287, 0.2881, 0.5662, 0.726, 0.2839...   \n",
       "50                26  [0.453, 0.4563, 0.6848, 0.4534, 0.2119, 0.3122...   \n",
       "51                26  [0.5709, 0.652, 0.6593, 0.5861, 0.6427, 0.3342...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.7896, 0.6363, 0.8922, 0.7599, 0.8431, 0.904...      0.75  0.119911  \n",
       "1   [0.359, 0.3453, 0.7385, 0.4809, 0.5382, 0.7921...       NaN       NaN  \n",
       "2   [0.5849, 0.5266, 0.8323, 0.6633, 0.8422, 0.878...       NaN       NaN  \n",
       "3   [0.48, 0.5505, 0.8088, 0.4757, 0.2864, 0.6299,...       NaN       NaN  \n",
       "4   [0.8932, 0.8428, 0.9454, 0.8541, 0.8544, 0.961...       NaN       NaN  \n",
       "5   [0.8995, 0.9434, 0.8841, 0.8751, 0.863, 0.7359...       NaN       NaN  \n",
       "6   [0.0724, 0.2646, 0.0741, 0.1198, 0.7016, 0.108...       NaN       NaN  \n",
       "7   [0.1925, 0.6821, 0.2342, 0.4431, 0.7706, 0.180...       NaN       NaN  \n",
       "8   [0.3283, 0.6363, 0.1869, 0.5445, 0.778, 0.3512...       NaN       NaN  \n",
       "9   [0.1423, 0.0593, 0.1621, 0.179, 0.0408, 0.3009...       NaN       NaN  \n",
       "10  [0.3795, 0.1039, 0.7208, 0.4306, 0.2275, 0.480...       NaN       NaN  \n",
       "11  [0.5636, 0.2598, 0.7021, 0.5542, 0.2349, 0.545...       NaN       NaN  \n",
       "12  [0.2939, 0.2892, 0.1531, 0.0728, 0.2074, 0.274...       NaN       NaN  \n",
       "13  [0.3694, 0.7427, 0.6076, 0.4647, 0.2468, 0.062...       NaN       NaN  \n",
       "14  [0.3142, 0.4231, 0.4316, 0.1691, 0.3516, 0.497...       NaN       NaN  \n",
       "15  [0.7448, 0.3346, 0.5977, 0.5713, 0.464, 0.8187...       NaN       NaN  \n",
       "16  [0.889, 0.917, 0.9129, 0.8297, 0.8252, 0.7745,...       NaN       NaN  \n",
       "17  [0.6342, 0.5704, 0.5949, 0.4434, 0.4979, 0.499...       NaN       NaN  \n",
       "18  [0.8764, 0.62, 0.9139, 0.895, 0.5979, 0.8388, ...       NaN       NaN  \n",
       "19  [0.8396, 0.4132, 0.9041, 0.7454, 0.483, 0.8662...       NaN       NaN  \n",
       "20  [0.6723, 0.6837, 0.5432, 0.3114, 0.8577, 0.747...       NaN       NaN  \n",
       "21  [0.7243, 0.8764, 0.8013, 0.4034, 0.9466, 0.741...       NaN       NaN  \n",
       "22  [0.7793, 0.8274, 0.8313, 0.5754, 0.899, 0.6788...       NaN       NaN  \n",
       "23  [0.3757, 0.4523, 0.4459, 0.6329, 0.4449, 0.430...       NaN       NaN  \n",
       "24  [0.8814, 0.9071, 0.7975, 0.9022, 0.9232, 0.821...       NaN       NaN  \n",
       "25  [0.7532, 0.7661, 0.2583, 0.523, 0.6564, 0.5944...       NaN       NaN  \n",
       "26  [0.9079, 0.9702, 0.6944, 0.8089, 0.9292, 0.779...       NaN       NaN  \n",
       "27  [0.401, 0.5068, 0.4938, 0.5404, 0.1385, 0.3055...       NaN       NaN  \n",
       "28  [0.2102, 0.5357, 0.331, 0.3196, 0.0469, 0.0686...       NaN       NaN  \n",
       "29  [0.1523, 0.4952, 0.1609, 0.2027, 0.0282, 0.083...       NaN       NaN  \n",
       "30  [0.0002, 0.0003, 0.0248, 0.0005, 0.0, 0.008, 0...       NaN       NaN  \n",
       "31  [0.1985, 0.6287, 0.2574, 0.4316, 0.6578, 0.493...       NaN       NaN  \n",
       "32  [0.4148, 0.4372, 0.1823, 0.1973, 0.7508, 0.372...       NaN       NaN  \n",
       "33  [0.9268, 0.8822, 0.9425, 0.9341, 0.9194, 0.910...       NaN       NaN  \n",
       "34  [0.3962, 0.1869, 0.9019, 0.6984, 0.3599, 0.866...       NaN       NaN  \n",
       "35  [0.8637, 0.9154, 0.691, 0.8023, 0.9341, 0.6974...       NaN       NaN  \n",
       "36  [0.9157, 0.9515, 0.8128, 0.8102, 0.9579, 0.784...       NaN       NaN  \n",
       "37  [0.7061, 0.8029, 0.374, 0.2759, 0.9069, 0.0782...       NaN       NaN  \n",
       "38  [0.4301, 0.9132, 0.8904, 0.7587, 0.8583, 0.192...       NaN       NaN  \n",
       "39  [0.6647, 0.9737, 0.9555, 0.8268, 0.9604, 0.695...       NaN       NaN  \n",
       "40  [0.2271, 0.5559, 0.6571, 0.2076, 0.575, 0.3368...       NaN       NaN  \n",
       "41  [0.8976, 0.9708, 0.9545, 0.9352, 0.941, 0.6094...       NaN       NaN  \n",
       "42  [0.8839, 0.9534, 0.6502, 0.7575, 0.9698, 0.712...       NaN       NaN  \n",
       "43  [0.441, 0.8469, 0.4095, 0.273, 0.8931, 0.1192,...       NaN       NaN  \n",
       "44  [0.5112, 0.8922, 0.1894, 0.3263, 0.8901, 0.030...       NaN       NaN  \n",
       "45  [0.7247, 0.9342, 0.5504, 0.6671, 0.9547, 0.519...       NaN       NaN  \n",
       "46  [0.3299, 0.6975, 0.3337, 0.5584, 0.5164, 0.475...       NaN       NaN  \n",
       "47  [0.4784, 0.5072, 0.4879, 0.3858, 0.7132, 0.656...       NaN       NaN  \n",
       "48  [0.8331, 0.5495, 0.9792, 0.8923, 0.5113, 0.926...       NaN       NaN  \n",
       "49  [0.7269, 0.0713, 0.7119, 0.4338, 0.274, 0.7161...       NaN       NaN  \n",
       "50  [0.547, 0.5437, 0.3152, 0.5466, 0.7881, 0.6878...       NaN       NaN  \n",
       "51  [0.4291, 0.348, 0.3407, 0.4139, 0.3573, 0.6658...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1341 : Training: loss:  0.09782968\n",
      "1342 : Training: loss:  0.11490001\n",
      "1343 : Training: loss:  0.10257053\n",
      "1344 : Training: loss:  0.10857276\n",
      "1345 : Training: loss:  0.110736884\n",
      "1346 : Training: loss:  0.10818215\n",
      "1347 : Training: loss:  0.09092851\n",
      "1348 : Training: loss:  0.10539109\n",
      "1349 : Training: loss:  0.10796652\n",
      "1350 : Training: loss:  0.1327417\n",
      "1351 : Training: loss:  0.09588292\n",
      "1352 : Training: loss:  0.12437693\n",
      "1353 : Training: loss:  0.119996786\n",
      "1354 : Training: loss:  0.11831055\n",
      "1355 : Training: loss:  0.12563813\n",
      "1356 : Training: loss:  0.13564724\n",
      "1357 : Training: loss:  0.077275045\n",
      "1358 : Training: loss:  0.10560936\n",
      "1359 : Training: loss:  0.10441197\n",
      "1360 : Training: loss:  0.12629741\n",
      "Validation: Loss:  0.11894461  Accuracy:  0.71153843\n",
      "1361 : Training: loss:  0.10011646\n",
      "1362 : Training: loss:  0.1181887\n",
      "1363 : Training: loss:  0.058636043\n",
      "1364 : Training: loss:  0.11328777\n",
      "1365 : Training: loss:  0.11752005\n",
      "1366 : Training: loss:  0.09674915\n",
      "1367 : Training: loss:  0.114114515\n",
      "1368 : Training: loss:  0.10944441\n",
      "1369 : Training: loss:  0.08858151\n",
      "1370 : Training: loss:  0.094784826\n",
      "1371 : Training: loss:  0.10848026\n",
      "1372 : Training: loss:  0.06418156\n",
      "1373 : Training: loss:  0.0701505\n",
      "1374 : Training: loss:  0.11598019\n",
      "1375 : Training: loss:  0.09445524\n",
      "1376 : Training: loss:  0.07591692\n",
      "1377 : Training: loss:  0.076456346\n",
      "1378 : Training: loss:  0.14047547\n",
      "1379 : Training: loss:  0.08903565\n",
      "1380 : Training: loss:  0.08683677\n",
      "Validation: Loss:  0.1177898  Accuracy:  0.71153843\n",
      "1381 : Training: loss:  0.08763174\n",
      "1382 : Training: loss:  0.082719624\n",
      "1383 : Training: loss:  0.11254886\n",
      "1384 : Training: loss:  0.106840864\n",
      "1385 : Training: loss:  0.08481735\n",
      "1386 : Training: loss:  0.0912037\n",
      "1387 : Training: loss:  0.0906363\n",
      "1388 : Training: loss:  0.09995529\n",
      "1389 : Training: loss:  0.09300428\n",
      "1390 : Training: loss:  0.09584652\n",
      "1391 : Training: loss:  0.07892224\n",
      "1392 : Training: loss:  0.07169271\n",
      "1393 : Training: loss:  0.13708325\n",
      "1394 : Training: loss:  0.10626095\n",
      "1395 : Training: loss:  0.09701949\n",
      "1396 : Training: loss:  0.0746956\n",
      "1397 : Training: loss:  0.0928461\n",
      "1398 : Training: loss:  0.12045216\n",
      "1399 : Training: loss:  0.1083573\n",
      "1400 : Training: loss:  0.08225227\n",
      "Validation: Loss:  0.11651118  Accuracy:  0.71153843\n",
      "1401 : Training: loss:  0.062312324\n",
      "1402 : Training: loss:  0.099757135\n",
      "1403 : Training: loss:  0.095736705\n",
      "1404 : Training: loss:  0.10824581\n",
      "1405 : Training: loss:  0.116159186\n",
      "1406 : Training: loss:  0.10219108\n",
      "1407 : Training: loss:  0.103994064\n",
      "1408 : Training: loss:  0.10083423\n",
      "1409 : Training: loss:  0.09978118\n",
      "1410 : Training: loss:  0.08619622\n",
      "1411 : Training: loss:  0.08788139\n",
      "1412 : Training: loss:  0.12209706\n",
      "1413 : Training: loss:  0.089188166\n",
      "1414 : Training: loss:  0.08937382\n",
      "1415 : Training: loss:  0.14570023\n",
      "1416 : Training: loss:  0.093070894\n",
      "1417 : Training: loss:  0.11494709\n",
      "1418 : Training: loss:  0.11122108\n",
      "1419 : Training: loss:  0.09006576\n",
      "1420 : Training: loss:  0.08683658\n",
      "Validation: Loss:  0.11530508  Accuracy:  0.71153843\n",
      "1421 : Training: loss:  0.105065234\n",
      "1422 : Training: loss:  0.098504\n",
      "1423 : Training: loss:  0.07665085\n",
      "1424 : Training: loss:  0.09420982\n",
      "1425 : Training: loss:  0.088902205\n",
      "1426 : Training: loss:  0.12290403\n",
      "1427 : Training: loss:  0.09172285\n",
      "1428 : Training: loss:  0.08050035\n",
      "1429 : Training: loss:  0.0913411\n",
      "1430 : Training: loss:  0.098206885\n",
      "1431 : Training: loss:  0.07722937\n",
      "1432 : Training: loss:  0.10112566\n",
      "1433 : Training: loss:  0.07485015\n",
      "1434 : Training: loss:  0.11840157\n",
      "1435 : Training: loss:  0.13915633\n",
      "1436 : Training: loss:  0.096046485\n",
      "1437 : Training: loss:  0.09428259\n",
      "1438 : Training: loss:  0.10622011\n",
      "1439 : Training: loss:  0.13004313\n",
      "1440 : Training: loss:  0.1080732\n",
      "Validation: Loss:  0.11416524  Accuracy:  0.71153843\n",
      "1441 : Training: loss:  0.09982412\n",
      "1442 : Training: loss:  0.07353134\n",
      "1443 : Training: loss:  0.10054684\n",
      "1444 : Training: loss:  0.10945344\n",
      "1445 : Training: loss:  0.08584813\n",
      "1446 : Training: loss:  0.11254233\n",
      "1447 : Training: loss:  0.11520505\n",
      "1448 : Training: loss:  0.12201186\n",
      "1449 : Training: loss:  0.08199977\n",
      "1450 : Training: loss:  0.086182564\n",
      "1451 : Training: loss:  0.09045185\n",
      "1452 : Training: loss:  0.08394451\n",
      "1453 : Training: loss:  0.10423673\n",
      "1454 : Training: loss:  0.09921979\n",
      "1455 : Training: loss:  0.11313086\n",
      "1456 : Training: loss:  0.10861915\n",
      "1457 : Training: loss:  0.08631671\n",
      "1458 : Training: loss:  0.08086763\n",
      "1459 : Training: loss:  0.09519319\n",
      "1460 : Training: loss:  0.094367996\n",
      "Validation: Loss:  0.11296574  Accuracy:  0.71153843\n",
      "1461 : Training: loss:  0.096137285\n",
      "1462 : Training: loss:  0.07944912\n",
      "1463 : Training: loss:  0.099039316\n",
      "1464 : Training: loss:  0.114262424\n",
      "1465 : Training: loss:  0.08596796\n",
      "1466 : Training: loss:  0.09027224\n",
      "1467 : Training: loss:  0.122081086\n",
      "1468 : Training: loss:  0.082509704\n",
      "1469 : Training: loss:  0.08585917\n",
      "1470 : Training: loss:  0.115393095\n",
      "1471 : Training: loss:  0.076155774\n",
      "1472 : Training: loss:  0.08015539\n",
      "1473 : Training: loss:  0.097744524\n",
      "1474 : Training: loss:  0.09099582\n",
      "1475 : Training: loss:  0.10717999\n",
      "1476 : Training: loss:  0.09606366\n",
      "1477 : Training: loss:  0.0922677\n",
      "1478 : Training: loss:  0.09134398\n",
      "1479 : Training: loss:  0.09447111\n",
      "1480 : Training: loss:  0.09671255\n",
      "Validation: Loss:  0.11161114  Accuracy:  0.7307692\n",
      "1481 : Training: loss:  0.10088655\n",
      "1482 : Training: loss:  0.10054025\n",
      "1483 : Training: loss:  0.085194774\n",
      "1484 : Training: loss:  0.08864027\n",
      "1485 : Training: loss:  0.10470361\n",
      "1486 : Training: loss:  0.06385359\n",
      "1487 : Training: loss:  0.10796295\n",
      "1488 : Training: loss:  0.09360396\n",
      "1489 : Training: loss:  0.10109965\n",
      "1490 : Training: loss:  0.06778487\n",
      "1491 : Training: loss:  0.082045905\n",
      "1492 : Training: loss:  0.097867\n",
      "1493 : Training: loss:  0.1177807\n",
      "1494 : Training: loss:  0.060696978\n",
      "1495 : Training: loss:  0.08444646\n",
      "1496 : Training: loss:  0.10897894\n",
      "1497 : Training: loss:  0.07827519\n",
      "1498 : Training: loss:  0.08798477\n",
      "1499 : Training: loss:  0.101106934\n",
      "1500 : Training: loss:  0.07852163\n",
      "Validation: Loss:  0.11048176  Accuracy:  0.7307692\n",
      "1501 : Training: loss:  0.07608689\n",
      "1502 : Training: loss:  0.091155164\n",
      "1503 : Training: loss:  0.13035868\n",
      "1504 : Training: loss:  0.08429513\n",
      "1505 : Training: loss:  0.10709073\n",
      "1506 : Training: loss:  0.11610976\n",
      "1507 : Training: loss:  0.089342736\n",
      "1508 : Training: loss:  0.102723874\n",
      "1509 : Training: loss:  0.11333632\n",
      "1510 : Training: loss:  0.09126281\n",
      "1511 : Training: loss:  0.093190275\n",
      "1512 : Training: loss:  0.09697109\n",
      "1513 : Training: loss:  0.11511267\n",
      "1514 : Training: loss:  0.08937122\n",
      "1515 : Training: loss:  0.10168742\n",
      "1516 : Training: loss:  0.077352\n",
      "1517 : Training: loss:  0.08009046\n",
      "1518 : Training: loss:  0.094252355\n",
      "1519 : Training: loss:  0.110531\n",
      "1520 : Training: loss:  0.11053858\n",
      "Validation: Loss:  0.109246366  Accuracy:  0.7307692\n",
      "1521 : Training: loss:  0.10542923\n",
      "1522 : Training: loss:  0.07517304\n",
      "1523 : Training: loss:  0.12145987\n",
      "1524 : Training: loss:  0.07898479\n",
      "1525 : Training: loss:  0.09694785\n",
      "1526 : Training: loss:  0.10149828\n",
      "1527 : Training: loss:  0.08763184\n",
      "1528 : Training: loss:  0.09582663\n",
      "1529 : Training: loss:  0.09240109\n",
      "1530 : Training: loss:  0.08661451\n",
      "1531 : Training: loss:  0.09217535\n",
      "1532 : Training: loss:  0.094333015\n",
      "1533 : Training: loss:  0.109375045\n",
      "1534 : Training: loss:  0.07511381\n",
      "1535 : Training: loss:  0.07672887\n",
      "1536 : Training: loss:  0.1089119\n",
      "1537 : Training: loss:  0.096701846\n",
      "1538 : Training: loss:  0.086161986\n",
      "1539 : Training: loss:  0.10595553\n",
      "1540 : Training: loss:  0.060419545\n",
      "Validation: Loss:  0.108173564  Accuracy:  0.7307692\n",
      "1541 : Training: loss:  0.08162624\n",
      "1542 : Training: loss:  0.086683646\n",
      "1543 : Training: loss:  0.082807414\n",
      "1544 : Training: loss:  0.099998124\n",
      "1545 : Training: loss:  0.10688119\n",
      "1546 : Training: loss:  0.10110044\n",
      "1547 : Training: loss:  0.08397189\n",
      "1548 : Training: loss:  0.08662518\n",
      "1549 : Training: loss:  0.0787201\n",
      "1550 : Training: loss:  0.111553654\n",
      "1551 : Training: loss:  0.098885626\n",
      "1552 : Training: loss:  0.10381255\n",
      "1553 : Training: loss:  0.101309694\n",
      "1554 : Training: loss:  0.069234975\n",
      "1555 : Training: loss:  0.10346275\n",
      "1556 : Training: loss:  0.10280417\n",
      "1557 : Training: loss:  0.09905175\n",
      "1558 : Training: loss:  0.08526576\n",
      "1559 : Training: loss:  0.13083859\n",
      "1560 : Training: loss:  0.06084731\n",
      "Validation: Loss:  0.10714094  Accuracy:  0.75\n",
      "1561 : Training: loss:  0.06893927\n",
      "1562 : Training: loss:  0.10723093\n",
      "1563 : Training: loss:  0.08686126\n",
      "1564 : Training: loss:  0.08485442\n",
      "1565 : Training: loss:  0.12893844\n",
      "1566 : Training: loss:  0.08007168\n",
      "1567 : Training: loss:  0.06536494\n",
      "1568 : Training: loss:  0.098634854\n",
      "1569 : Training: loss:  0.07688738\n",
      "1570 : Training: loss:  0.08736078\n",
      "1571 : Training: loss:  0.08973007\n",
      "1572 : Training: loss:  0.07541735\n",
      "1573 : Training: loss:  0.09544465\n",
      "1574 : Training: loss:  0.081967354\n",
      "1575 : Training: loss:  0.06917194\n",
      "1576 : Training: loss:  0.09768432\n",
      "1577 : Training: loss:  0.09773267\n",
      "1578 : Training: loss:  0.09956768\n",
      "1579 : Training: loss:  0.08537246\n",
      "1580 : Training: loss:  0.072314724\n",
      "Validation: Loss:  0.1057781  Accuracy:  0.75\n",
      "1581 : Training: loss:  0.081927784\n",
      "1582 : Training: loss:  0.0813982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583 : Training: loss:  0.064593256\n",
      "1584 : Training: loss:  0.07945925\n",
      "1585 : Training: loss:  0.08702507\n",
      "1586 : Training: loss:  0.07509004\n",
      "1587 : Training: loss:  0.06456512\n",
      "1588 : Training: loss:  0.08414673\n",
      "1589 : Training: loss:  0.11949873\n",
      "1590 : Training: loss:  0.07622358\n",
      "1591 : Training: loss:  0.060181405\n",
      "1592 : Training: loss:  0.0631557\n",
      "1593 : Training: loss:  0.05506889\n",
      "1594 : Training: loss:  0.10493773\n",
      "1595 : Training: loss:  0.08521103\n",
      "1596 : Training: loss:  0.08814529\n",
      "1597 : Training: loss:  0.078017466\n",
      "1598 : Training: loss:  0.08201341\n",
      "1599 : Training: loss:  0.07497929\n",
      "1600 : Training: loss:  0.07631039\n",
      "Validation: Loss:  0.104595736  Accuracy:  0.75\n",
      "1601 : Training: loss:  0.08904143\n",
      "1602 : Training: loss:  0.07009049\n",
      "1603 : Training: loss:  0.074614316\n",
      "1604 : Training: loss:  0.07306561\n",
      "1605 : Training: loss:  0.07179399\n",
      "1606 : Training: loss:  0.044390246\n",
      "1607 : Training: loss:  0.07907539\n",
      "1608 : Training: loss:  0.09799085\n",
      "1609 : Training: loss:  0.09795154\n",
      "1610 : Training: loss:  0.09595751\n",
      "1611 : Training: loss:  0.0877004\n",
      "1612 : Training: loss:  0.079656266\n",
      "1613 : Training: loss:  0.059073176\n",
      "1614 : Training: loss:  0.095949635\n",
      "1615 : Training: loss:  0.10139241\n",
      "1616 : Training: loss:  0.094160356\n",
      "1617 : Training: loss:  0.056191728\n",
      "1618 : Training: loss:  0.100859635\n",
      "1619 : Training: loss:  0.062485185\n",
      "1620 : Training: loss:  0.07530323\n",
      "Validation: Loss:  0.10340939  Accuracy:  0.75\n",
      "1621 : Training: loss:  0.07093912\n",
      "1622 : Training: loss:  0.06281116\n",
      "1623 : Training: loss:  0.08058077\n",
      "1624 : Training: loss:  0.09256324\n",
      "1625 : Training: loss:  0.08680114\n",
      "1626 : Training: loss:  0.10157071\n",
      "1627 : Training: loss:  0.08936572\n",
      "1628 : Training: loss:  0.11296225\n",
      "1629 : Training: loss:  0.07030522\n",
      "1630 : Training: loss:  0.097507894\n",
      "1631 : Training: loss:  0.06233746\n",
      "1632 : Training: loss:  0.110686265\n",
      "1633 : Training: loss:  0.06181898\n",
      "1634 : Training: loss:  0.119369425\n",
      "1635 : Training: loss:  0.07543241\n",
      "1636 : Training: loss:  0.07542249\n",
      "1637 : Training: loss:  0.09870358\n",
      "1638 : Training: loss:  0.08988561\n",
      "1639 : Training: loss:  0.109679736\n",
      "1640 : Training: loss:  0.07372143\n",
      "Validation: Loss:  0.102146246  Accuracy:  0.75\n",
      "1641 : Training: loss:  0.06921726\n",
      "1642 : Training: loss:  0.079338185\n",
      "1643 : Training: loss:  0.088791564\n",
      "1644 : Training: loss:  0.106819384\n",
      "1645 : Training: loss:  0.08915491\n",
      "1646 : Training: loss:  0.09848986\n",
      "1647 : Training: loss:  0.07445263\n",
      "1648 : Training: loss:  0.09652473\n",
      "1649 : Training: loss:  0.10535436\n",
      "1650 : Training: loss:  0.092834406\n",
      "1651 : Training: loss:  0.063664764\n",
      "1652 : Training: loss:  0.10207116\n",
      "1653 : Training: loss:  0.107758515\n",
      "1654 : Training: loss:  0.08738944\n",
      "1655 : Training: loss:  0.057025097\n",
      "1656 : Training: loss:  0.11491145\n",
      "1657 : Training: loss:  0.084811\n",
      "1658 : Training: loss:  0.07013553\n",
      "1659 : Training: loss:  0.07095254\n",
      "1660 : Training: loss:  0.075841054\n",
      "Validation: Loss:  0.100915395  Accuracy:  0.75\n",
      "1661 : Training: loss:  0.07238302\n",
      "1662 : Training: loss:  0.09183732\n",
      "1663 : Training: loss:  0.07580757\n",
      "1664 : Training: loss:  0.08843494\n",
      "1665 : Training: loss:  0.09158295\n",
      "1666 : Training: loss:  0.07452923\n",
      "1667 : Training: loss:  0.09053365\n",
      "1668 : Training: loss:  0.08066917\n",
      "1669 : Training: loss:  0.08085007\n",
      "1670 : Training: loss:  0.084308006\n",
      "1671 : Training: loss:  0.07855953\n",
      "1672 : Training: loss:  0.089526884\n",
      "1673 : Training: loss:  0.06674704\n",
      "1674 : Training: loss:  0.08732465\n",
      "1675 : Training: loss:  0.047208667\n",
      "1676 : Training: loss:  0.08300177\n",
      "1677 : Training: loss:  0.07755926\n",
      "1678 : Training: loss:  0.084987536\n",
      "1679 : Training: loss:  0.08089112\n",
      "1680 : Training: loss:  0.06682876\n",
      "Validation: Loss:  0.099894665  Accuracy:  0.75\n",
      "1681 : Training: loss:  0.09575036\n",
      "1682 : Training: loss:  0.06545719\n",
      "1683 : Training: loss:  0.09949531\n",
      "1684 : Training: loss:  0.098996766\n",
      "1685 : Training: loss:  0.09368974\n",
      "1686 : Training: loss:  0.09758268\n",
      "1687 : Training: loss:  0.10880556\n",
      "1688 : Training: loss:  0.08200342\n",
      "1689 : Training: loss:  0.09349491\n",
      "1690 : Training: loss:  0.051413942\n",
      "1691 : Training: loss:  0.084981814\n",
      "1692 : Training: loss:  0.072474055\n",
      "1693 : Training: loss:  0.057652634\n",
      "1694 : Training: loss:  0.12773854\n",
      "1695 : Training: loss:  0.0806012\n",
      "1696 : Training: loss:  0.05824104\n",
      "1697 : Training: loss:  0.07032995\n",
      "1698 : Training: loss:  0.09205893\n",
      "1699 : Training: loss:  0.089608625\n",
      "1700 : Training: loss:  0.07087896\n",
      "Validation: Loss:  0.098767534  Accuracy:  0.75\n",
      "1701 : Training: loss:  0.08025485\n",
      "1702 : Training: loss:  0.07707996\n",
      "1703 : Training: loss:  0.054755904\n",
      "1704 : Training: loss:  0.11006804\n",
      "1705 : Training: loss:  0.113952704\n",
      "1706 : Training: loss:  0.08091642\n",
      "1707 : Training: loss:  0.09429212\n",
      "1708 : Training: loss:  0.08106798\n",
      "1709 : Training: loss:  0.06577851\n",
      "1710 : Training: loss:  0.08370956\n",
      "1711 : Training: loss:  0.07683628\n",
      "1712 : Training: loss:  0.07179121\n",
      "1713 : Training: loss:  0.08916497\n",
      "1714 : Training: loss:  0.059631016\n",
      "1715 : Training: loss:  0.051041752\n",
      "1716 : Training: loss:  0.07833966\n",
      "1717 : Training: loss:  0.06245769\n",
      "1718 : Training: loss:  0.08102296\n",
      "1719 : Training: loss:  0.09868316\n",
      "1720 : Training: loss:  0.07976839\n",
      "Validation: Loss:  0.09748707  Accuracy:  0.75\n",
      "1721 : Training: loss:  0.07818862\n",
      "1722 : Training: loss:  0.07972891\n",
      "1723 : Training: loss:  0.068193726\n",
      "1724 : Training: loss:  0.05848532\n",
      "1725 : Training: loss:  0.07222474\n",
      "1726 : Training: loss:  0.0935152\n",
      "1727 : Training: loss:  0.096545085\n",
      "1728 : Training: loss:  0.0764495\n",
      "1729 : Training: loss:  0.063904434\n",
      "1730 : Training: loss:  0.09145846\n",
      "1731 : Training: loss:  0.06626058\n",
      "1732 : Training: loss:  0.077324964\n",
      "1733 : Training: loss:  0.094137125\n",
      "1734 : Training: loss:  0.073798284\n",
      "1735 : Training: loss:  0.08972768\n",
      "1736 : Training: loss:  0.08903858\n",
      "1737 : Training: loss:  0.056709535\n",
      "1738 : Training: loss:  0.07463078\n",
      "1739 : Training: loss:  0.07615434\n",
      "1740 : Training: loss:  0.064542525\n",
      "Validation: Loss:  0.09651203  Accuracy:  0.75\n",
      "1741 : Training: loss:  0.079330795\n",
      "1742 : Training: loss:  0.080885395\n",
      "1743 : Training: loss:  0.06810551\n",
      "1744 : Training: loss:  0.066265285\n",
      "1745 : Training: loss:  0.06644462\n",
      "1746 : Training: loss:  0.085253604\n",
      "1747 : Training: loss:  0.07213629\n",
      "1748 : Training: loss:  0.06295162\n",
      "1749 : Training: loss:  0.07196972\n",
      "1750 : Training: loss:  0.06659997\n",
      "1751 : Training: loss:  0.09768076\n",
      "1752 : Training: loss:  0.06795937\n",
      "1753 : Training: loss:  0.07004777\n",
      "1754 : Training: loss:  0.08647901\n",
      "1755 : Training: loss:  0.04196115\n",
      "1756 : Training: loss:  0.0629366\n",
      "1757 : Training: loss:  0.06426309\n",
      "1758 : Training: loss:  0.084314294\n",
      "1759 : Training: loss:  0.087386176\n",
      "1760 : Training: loss:  0.088486046\n",
      "Validation: Loss:  0.09546818  Accuracy:  0.7307692\n",
      "1761 : Training: loss:  0.06600366\n",
      "1762 : Training: loss:  0.059447642\n",
      "1763 : Training: loss:  0.061631445\n",
      "1764 : Training: loss:  0.08382602\n",
      "1765 : Training: loss:  0.07923549\n",
      "1766 : Training: loss:  0.08949128\n",
      "1767 : Training: loss:  0.08515971\n",
      "1768 : Training: loss:  0.08501051\n",
      "1769 : Training: loss:  0.07028722\n",
      "1770 : Training: loss:  0.064703934\n",
      "1771 : Training: loss:  0.073663525\n",
      "1772 : Training: loss:  0.08064663\n",
      "1773 : Training: loss:  0.111375265\n",
      "1774 : Training: loss:  0.08544488\n",
      "1775 : Training: loss:  0.05387742\n",
      "1776 : Training: loss:  0.07682155\n",
      "1777 : Training: loss:  0.09900933\n",
      "1778 : Training: loss:  0.06774777\n",
      "1779 : Training: loss:  0.066895396\n",
      "1780 : Training: loss:  0.08500729\n",
      "Validation: Loss:  0.09437161  Accuracy:  0.7307692\n",
      "1781 : Training: loss:  0.08748082\n",
      "1782 : Training: loss:  0.08397594\n",
      "1783 : Training: loss:  0.07668663\n",
      "1784 : Training: loss:  0.043032996\n",
      "1785 : Training: loss:  0.09130002\n",
      "1786 : Training: loss:  0.10002718\n",
      "1787 : Training: loss:  0.09219788\n",
      "1788 : Training: loss:  0.04778616\n",
      "1789 : Training: loss:  0.06797512\n",
      "1790 : Training: loss:  0.048101272\n",
      "1791 : Training: loss:  0.08741992\n",
      "1792 : Training: loss:  0.075756215\n",
      "1793 : Training: loss:  0.07050298\n",
      "1794 : Training: loss:  0.0867969\n",
      "1795 : Training: loss:  0.08052438\n",
      "1796 : Training: loss:  0.078986354\n",
      "1797 : Training: loss:  0.055149198\n",
      "1798 : Training: loss:  0.061625905\n",
      "1799 : Training: loss:  0.06766521\n",
      "1800 : Training: loss:  0.06068693\n",
      "Validation: Loss:  0.09319524  Accuracy:  0.7307692\n",
      "1801 : Training: loss:  0.08346141\n",
      "1802 : Training: loss:  0.08823827\n",
      "1803 : Training: loss:  0.067228235\n",
      "1804 : Training: loss:  0.034050886\n",
      "1805 : Training: loss:  0.077368565\n",
      "1806 : Training: loss:  0.07080633\n",
      "1807 : Training: loss:  0.07111486\n",
      "1808 : Training: loss:  0.067046486\n",
      "1809 : Training: loss:  0.06255795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810 : Training: loss:  0.08614059\n",
      "1811 : Training: loss:  0.075984575\n",
      "1812 : Training: loss:  0.06534573\n",
      "1813 : Training: loss:  0.09283868\n",
      "1814 : Training: loss:  0.06443816\n",
      "1815 : Training: loss:  0.0614314\n",
      "1816 : Training: loss:  0.06949372\n",
      "1817 : Training: loss:  0.084483474\n",
      "1818 : Training: loss:  0.054391757\n",
      "1819 : Training: loss:  0.04743943\n",
      "1820 : Training: loss:  0.05858397\n",
      "Validation: Loss:  0.09228502  Accuracy:  0.7307692\n",
      "1821 : Training: loss:  0.07954721\n",
      "1822 : Training: loss:  0.05165817\n",
      "1823 : Training: loss:  0.06056187\n",
      "1824 : Training: loss:  0.073774815\n",
      "1825 : Training: loss:  0.07610312\n",
      "1826 : Training: loss:  0.068344526\n",
      "1827 : Training: loss:  0.046567794\n",
      "1828 : Training: loss:  0.086352475\n",
      "1829 : Training: loss:  0.075844854\n",
      "1830 : Training: loss:  0.07535195\n",
      "1831 : Training: loss:  0.078896664\n",
      "1832 : Training: loss:  0.084445275\n",
      "1833 : Training: loss:  0.079567336\n",
      "1834 : Training: loss:  0.077318765\n",
      "1835 : Training: loss:  0.07313951\n",
      "1836 : Training: loss:  0.08160795\n",
      "1837 : Training: loss:  0.06150388\n",
      "1838 : Training: loss:  0.095278755\n",
      "1839 : Training: loss:  0.068757705\n",
      "1840 : Training: loss:  0.06246399\n",
      "Validation: Loss:  0.09113133  Accuracy:  0.75\n",
      "1841 : Training: loss:  0.06420843\n",
      "1842 : Training: loss:  0.07737101\n",
      "1843 : Training: loss:  0.062023357\n",
      "1844 : Training: loss:  0.056374267\n",
      "1845 : Training: loss:  0.055825625\n",
      "1846 : Training: loss:  0.0673173\n",
      "1847 : Training: loss:  0.07756907\n",
      "1848 : Training: loss:  0.088134184\n",
      "1849 : Training: loss:  0.064698555\n",
      "1850 : Training: loss:  0.067893796\n",
      "1851 : Training: loss:  0.086185105\n",
      "1852 : Training: loss:  0.06730064\n",
      "1853 : Training: loss:  0.10513302\n",
      "1854 : Training: loss:  0.09050152\n",
      "1855 : Training: loss:  0.07746502\n",
      "1856 : Training: loss:  0.065414496\n",
      "1857 : Training: loss:  0.05952048\n",
      "1858 : Training: loss:  0.038758032\n",
      "1859 : Training: loss:  0.06799548\n",
      "1860 : Training: loss:  0.07034103\n",
      "Validation: Loss:  0.090130456  Accuracy:  0.75\n",
      "1861 : Training: loss:  0.06753572\n",
      "1862 : Training: loss:  0.082902\n",
      "1863 : Training: loss:  0.08108487\n",
      "1864 : Training: loss:  0.06904916\n",
      "1865 : Training: loss:  0.064773634\n",
      "1866 : Training: loss:  0.03932781\n",
      "1867 : Training: loss:  0.06773046\n",
      "1868 : Training: loss:  0.07001363\n",
      "1869 : Training: loss:  0.054624956\n",
      "1870 : Training: loss:  0.07329951\n",
      "1871 : Training: loss:  0.054718874\n",
      "1872 : Training: loss:  0.07235386\n",
      "1873 : Training: loss:  0.04488435\n",
      "1874 : Training: loss:  0.051056463\n",
      "1875 : Training: loss:  0.0755627\n",
      "1876 : Training: loss:  0.0881745\n",
      "1877 : Training: loss:  0.08685725\n",
      "1878 : Training: loss:  0.053297553\n",
      "1879 : Training: loss:  0.038955346\n",
      "1880 : Training: loss:  0.0799467\n",
      "Validation: Loss:  0.08917023  Accuracy:  0.7692308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.2469, 0.0955, 0.0256, 0.0394, 0.0229, 0.037...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2968, 0.4246, 0.0961, 0.3518, 0.2, 0.0929, ...</td>\n",
       "      <td>[0.7032, 0.5754, 0.9039, 0.6482, 0.8, 0.9071, ...</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.08917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.4264, 0.3668, 0.0136, 0.0369, 0.1312, 0.040...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.747, 0.7093, 0.2401, 0.6055, 0.5527, 0.2253...</td>\n",
       "      <td>[0.253, 0.2907, 0.7599, 0.3945, 0.4473, 0.7747...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.3363, 0.1571, 0.0154, 0.0727, 0.0334, 0.053...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5404, 0.5789, 0.1728, 0.5211, 0.1902, 0.128...</td>\n",
       "      <td>[0.4596, 0.4211, 0.8272, 0.4789, 0.8098, 0.872...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0408, 0.1568, 0.007, 0.0096, 0.0849, 0.0326...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6074, 0.4952, 0.167, 0.6335, 0.7882, 0.4243...</td>\n",
       "      <td>[0.3926, 0.5048, 0.833, 0.3665, 0.2118, 0.5757...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1004, 0.0917, 0.0197, 0.0108, 0.0115, 0.032...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1455, 0.2036, 0.0433, 0.2416, 0.2009, 0.039...</td>\n",
       "      <td>[0.8545, 0.7964, 0.9567, 0.7584, 0.7991, 0.960...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0483, 0.0268, 0.1048, 0.0199, 0.0094, 0.052...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1667, 0.0622, 0.154, 0.1836, 0.1453, 0.4157...</td>\n",
       "      <td>[0.8333, 0.9378, 0.846, 0.8164, 0.8547, 0.5843...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.036, 0.015, 0.0024, 0.6891, 0.0062, 0.0017,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9693, 0.8535, 0.9583, 0.9537, 0.2072, 0.942...</td>\n",
       "      <td>[0.0307, 0.1465, 0.0417, 0.0463, 0.7928, 0.057...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.039, 0.0249, 0.0161, 0.3905, 0.0069, 0.0085...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8903, 0.3842, 0.8435, 0.6955, 0.1912, 0.912...</td>\n",
       "      <td>[0.1097, 0.6158, 0.1565, 0.3045, 0.8088, 0.087...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0581, 0.0252, 0.0281, 0.3443, 0.0136, 0.020...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8253, 0.5011, 0.9133, 0.5721, 0.1933, 0.803...</td>\n",
       "      <td>[0.1747, 0.4989, 0.0867, 0.4279, 0.8067, 0.196...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0131, 0.1039, 0.0098, 0.0248, 0.5432, 0.089...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8831, 0.9592, 0.8196, 0.8253, 0.9691, 0.737...</td>\n",
       "      <td>[0.1169, 0.0408, 0.1804, 0.1747, 0.0309, 0.262...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0508, 0.3705, 0.0071, 0.009, 0.289, 0.1946,...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6846, 0.9388, 0.1679, 0.5928, 0.8299, 0.615...</td>\n",
       "      <td>[0.3154, 0.0612, 0.8321, 0.4072, 0.1701, 0.384...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0121, 0.0464, 0.0082, 0.0106, 0.081, 0.165,...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4944, 0.8288, 0.2611, 0.5065, 0.8287, 0.526...</td>\n",
       "      <td>[0.5056, 0.1712, 0.7389, 0.4935, 0.1713, 0.473...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0033, 0.0154, 0.0203, 0.0071, 0.0091, 0.006...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6986, 0.6746, 0.8512, 0.9561, 0.8174, 0.701...</td>\n",
       "      <td>[0.3014, 0.3254, 0.1488, 0.0439, 0.1826, 0.298...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0898, 0.0825, 0.1774, 0.1206, 0.0096, 0.109...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.4637, 0.1378, 0.2963, 0.5644, 0.7441, 0.952...</td>\n",
       "      <td>[0.5363, 0.8622, 0.7037, 0.4356, 0.2559, 0.047...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0211, 0.0292, 0.0028, 0.0492, 0.0318, 0.006...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7712, 0.7056, 0.5821, 0.9189, 0.6751, 0.560...</td>\n",
       "      <td>[0.2288, 0.2944, 0.4179, 0.0811, 0.3249, 0.439...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0595, 0.0452, 0.0078, 0.0111, 0.0382, 0.017...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2704, 0.7893, 0.4083, 0.5138, 0.6191, 0.143...</td>\n",
       "      <td>[0.7296, 0.2107, 0.5917, 0.4862, 0.3809, 0.856...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0328, 0.0158, 0.0811, 0.0125, 0.0036, 0.034...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.1875, 0.1096, 0.1057, 0.2381, 0.1741, 0.309...</td>\n",
       "      <td>[0.8125, 0.8904, 0.8943, 0.7619, 0.8259, 0.690...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0291, 0.0209, 0.0252, 0.0223, 0.0088, 0.014...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.505, 0.5374, 0.4874, 0.6455, 0.4651, 0.596,...</td>\n",
       "      <td>[0.495, 0.4626, 0.5126, 0.3545, 0.5349, 0.404,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0809, 0.0173, 0.0498, 0.0118, 0.0051, 0.027...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1555, 0.4436, 0.089, 0.1047, 0.472, 0.2345,...</td>\n",
       "      <td>[0.8445, 0.5564, 0.911, 0.8953, 0.528, 0.7655,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0604, 0.0187, 0.0541, 0.0091, 0.0065, 0.032...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1947, 0.6686, 0.1041, 0.2661, 0.6127, 0.163...</td>\n",
       "      <td>[0.8053, 0.3314, 0.8959, 0.7339, 0.3873, 0.836...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0775, 0.0416, 0.0227, 0.0577, 0.0075, 0.015...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4426, 0.3261, 0.5696, 0.837, 0.1321, 0.3143...</td>\n",
       "      <td>[0.5574, 0.6739, 0.4304, 0.163, 0.8679, 0.6857...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0297, 0.0122, 0.0234, 0.0322, 0.0021, 0.006...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.407, 0.1193, 0.2624, 0.7869, 0.0485, 0.428,...</td>\n",
       "      <td>[0.593, 0.8807, 0.7376, 0.2131, 0.9515, 0.572,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0411, 0.021, 0.0288, 0.0278, 0.0045, 0.0179...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3303, 0.2149, 0.2054, 0.58, 0.0919, 0.4369,...</td>\n",
       "      <td>[0.6697, 0.7851, 0.7946, 0.42, 0.9081, 0.5631,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0407, 0.0245, 0.0299, 0.0406, 0.0222, 0.053...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7381, 0.6578, 0.6775, 0.3662, 0.6228, 0.715...</td>\n",
       "      <td>[0.2619, 0.3422, 0.3225, 0.6338, 0.3772, 0.285...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0665, 0.0119, 0.0522, 0.0222, 0.0047, 0.021...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2353, 0.1571, 0.3584, 0.1335, 0.0734, 0.328...</td>\n",
       "      <td>[0.7647, 0.8429, 0.6416, 0.8665, 0.9266, 0.671...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0191, 0.0194, 0.0202, 0.0264, 0.0169, 0.013...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3474, 0.318, 0.8398, 0.6264, 0.3585, 0.5158...</td>\n",
       "      <td>[0.6526, 0.682, 0.1602, 0.3736, 0.6415, 0.4842...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0299, 0.0115, 0.0259, 0.0084, 0.0024, 0.006...</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1276, 0.0274, 0.4121, 0.2954, 0.0688, 0.350...</td>\n",
       "      <td>[0.8724, 0.9726, 0.5879, 0.7046, 0.9312, 0.649...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0027, 0.0118, 0.0091, 0.0086, 0.0166, 0.010...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.733, 0.556, 0.5435, 0.605, 0.8742, 0.8015, ...</td>\n",
       "      <td>[0.267, 0.444, 0.4565, 0.395, 0.1258, 0.1985, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0043, 0.022, 0.0242, 0.0357, 0.0082, 0.0112...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7949, 0.4066, 0.6704, 0.7685, 0.9521, 0.959...</td>\n",
       "      <td>[0.2051, 0.5934, 0.3296, 0.2315, 0.0479, 0.040...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0015, 0.0174, 0.0043, 0.0114, 0.0259, 0.005...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8795, 0.5069, 0.8506, 0.8798, 0.9769, 0.950...</td>\n",
       "      <td>[0.1205, 0.4931, 0.1494, 0.1202, 0.0231, 0.049...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.2487, 0.7318, 0.2342, 0.1802, 0.8277, 0.637...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9975, 0.9972, 0.7287, 0.9969, 1.0, 0.9433, ...</td>\n",
       "      <td>[0.0025, 0.0028, 0.2713, 0.0031, 0.0, 0.0567, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0099, 0.0168, 0.0223, 0.0603, 0.0066, 0.005...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.8883, 0.3793, 0.8481, 0.7215, 0.3166, 0.680...</td>\n",
       "      <td>[0.1117, 0.6207, 0.1519, 0.2785, 0.6834, 0.319...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0094, 0.0095, 0.0112, 0.0639, 0.0036, 0.002...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.738, 0.6455, 0.9213, 0.9192, 0.2021, 0.7476...</td>\n",
       "      <td>[0.262, 0.3545, 0.0787, 0.0808, 0.7979, 0.2524...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0597, 0.0158, 0.0296, 0.012, 0.0033, 0.0151...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1431, 0.197, 0.0687, 0.127, 0.0861, 0.1402,...</td>\n",
       "      <td>[0.8569, 0.803, 0.9313, 0.873, 0.9139, 0.8598,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0649, 0.0615, 0.0322, 0.034, 0.1132, 0.0616...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.7973, 0.9085, 0.1041, 0.4189, 0.7418, 0.159...</td>\n",
       "      <td>[0.2027, 0.0915, 0.8959, 0.5811, 0.2582, 0.840...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0472, 0.0153, 0.0174, 0.0144, 0.0021, 0.008...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2121, 0.1021, 0.4078, 0.3302, 0.0652, 0.445...</td>\n",
       "      <td>[0.7879, 0.8979, 0.5922, 0.6698, 0.9348, 0.554...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0431, 0.0149, 0.0292, 0.0113, 0.0018, 0.01,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0817, 0.0484, 0.2264, 0.2632, 0.0322, 0.322...</td>\n",
       "      <td>[0.9183, 0.9516, 0.7736, 0.7368, 0.9678, 0.677...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0105, 0.012, 0.0149, 0.0293, 0.0011, 0.0023...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.183, 0.1324, 0.6574, 0.7769, 0.0565, 0.9555...</td>\n",
       "      <td>[0.817, 0.8676, 0.3426, 0.2231, 0.9435, 0.0445...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0124, 0.0031, 0.0318, 0.028, 0.0017, 0.0034...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7114, 0.0667, 0.0677, 0.2958, 0.105, 0.9063...</td>\n",
       "      <td>[0.2886, 0.9333, 0.9323, 0.7042, 0.895, 0.0937...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0246, 0.0053, 0.0271, 0.0203, 0.0017, 0.005...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5083, 0.0264, 0.0404, 0.2975, 0.0329, 0.531...</td>\n",
       "      <td>[0.4917, 0.9736, 0.9596, 0.7025, 0.9671, 0.468...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0185, 0.0253, 0.0065, 0.0548, 0.0103, 0.005...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.8503, 0.3934, 0.2936, 0.883, 0.3862, 0.7573...</td>\n",
       "      <td>[0.1497, 0.6066, 0.7064, 0.117, 0.6138, 0.2427...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0513, 0.0076, 0.0432, 0.0205, 0.0018, 0.010...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1589, 0.0287, 0.0427, 0.0801, 0.0523, 0.585...</td>\n",
       "      <td>[0.8411, 0.9713, 0.9573, 0.9199, 0.9477, 0.414...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0261, 0.0097, 0.0144, 0.0115, 0.0008, 0.002...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1398, 0.0489, 0.4548, 0.3591, 0.0245, 0.387...</td>\n",
       "      <td>[0.8602, 0.9511, 0.5452, 0.6409, 0.9755, 0.612...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0126, 0.0102, 0.0101, 0.0198, 0.0008, 0.001...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4722, 0.0991, 0.6126, 0.7929, 0.0728, 0.924...</td>\n",
       "      <td>[0.5278, 0.9009, 0.3874, 0.2071, 0.9272, 0.075...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.008, 0.0078, 0.0097, 0.0157, 0.0007, 0.0008...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3576, 0.0543, 0.845, 0.7401, 0.0732, 0.9814...</td>\n",
       "      <td>[0.6424, 0.9457, 0.155, 0.2599, 0.9268, 0.0186...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0144, 0.0066, 0.0093, 0.0112, 0.0005, 0.001...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2586, 0.0509, 0.5036, 0.4514, 0.031, 0.6061...</td>\n",
       "      <td>[0.7414, 0.9491, 0.4964, 0.5486, 0.969, 0.3939...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0206, 0.0183, 0.0926, 0.0489, 0.0102, 0.038...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7664, 0.3391, 0.8137, 0.4853, 0.5036, 0.663...</td>\n",
       "      <td>[0.2336, 0.6609, 0.1863, 0.5147, 0.4964, 0.336...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0312, 0.0837, 0.0144, 0.0277, 0.0263, 0.022...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.6959, 0.5616, 0.676, 0.7469, 0.3223, 0.3951...</td>\n",
       "      <td>[0.3041, 0.4384, 0.324, 0.2531, 0.6777, 0.6049...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0196, 0.0186, 0.0087, 0.0017, 0.0116, 0.018...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2369, 0.5839, 0.0137, 0.133, 0.6237, 0.0843...</td>\n",
       "      <td>[0.7631, 0.4161, 0.9863, 0.867, 0.3763, 0.9157...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0369, 0.0646, 0.0136, 0.0157, 0.079, 0.1049...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2637, 0.9563, 0.2553, 0.6264, 0.7892, 0.289...</td>\n",
       "      <td>[0.7363, 0.0437, 0.7447, 0.3736, 0.2108, 0.711...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0526, 0.0347, 0.014, 0.0283, 0.0179, 0.0145...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.5049, 0.4635, 0.7259, 0.618, 0.2195, 0.2885...</td>\n",
       "      <td>[0.4951, 0.5365, 0.2741, 0.382, 0.7805, 0.7115...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0105, 0.017, 0.0073, 0.0254, 0.0415, 0.0043...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7006, 0.7791, 0.7413, 0.7145, 0.6706, 0.380...</td>\n",
       "      <td>[0.2994, 0.2209, 0.2587, 0.2855, 0.3294, 0.619...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.2469, 0.0955, 0.0256, 0.0394, 0.0229, 0.037...               0   \n",
       "1   [0.4264, 0.3668, 0.0136, 0.0369, 0.1312, 0.040...               0   \n",
       "2   [0.3363, 0.1571, 0.0154, 0.0727, 0.0334, 0.053...               0   \n",
       "3   [0.0408, 0.1568, 0.007, 0.0096, 0.0849, 0.0326...               1   \n",
       "4   [0.1004, 0.0917, 0.0197, 0.0108, 0.0115, 0.032...               1   \n",
       "5   [0.0483, 0.0268, 0.1048, 0.0199, 0.0094, 0.052...               2   \n",
       "6   [0.036, 0.015, 0.0024, 0.6891, 0.0062, 0.0017,...               3   \n",
       "7   [0.039, 0.0249, 0.0161, 0.3905, 0.0069, 0.0085...               3   \n",
       "8   [0.0581, 0.0252, 0.0281, 0.3443, 0.0136, 0.020...               3   \n",
       "9   [0.0131, 0.1039, 0.0098, 0.0248, 0.5432, 0.089...               4   \n",
       "10  [0.0508, 0.3705, 0.0071, 0.009, 0.289, 0.1946,...               4   \n",
       "11  [0.0121, 0.0464, 0.0082, 0.0106, 0.081, 0.165,...               5   \n",
       "12  [0.0033, 0.0154, 0.0203, 0.0071, 0.0091, 0.006...               6   \n",
       "13  [0.0898, 0.0825, 0.1774, 0.1206, 0.0096, 0.109...               7   \n",
       "14  [0.0211, 0.0292, 0.0028, 0.0492, 0.0318, 0.006...               8   \n",
       "15  [0.0595, 0.0452, 0.0078, 0.0111, 0.0382, 0.017...               8   \n",
       "16  [0.0328, 0.0158, 0.0811, 0.0125, 0.0036, 0.034...               9   \n",
       "17  [0.0291, 0.0209, 0.0252, 0.0223, 0.0088, 0.014...               9   \n",
       "18  [0.0809, 0.0173, 0.0498, 0.0118, 0.0051, 0.027...              10   \n",
       "19  [0.0604, 0.0187, 0.0541, 0.0091, 0.0065, 0.032...              10   \n",
       "20  [0.0775, 0.0416, 0.0227, 0.0577, 0.0075, 0.015...              11   \n",
       "21  [0.0297, 0.0122, 0.0234, 0.0322, 0.0021, 0.006...              11   \n",
       "22  [0.0411, 0.021, 0.0288, 0.0278, 0.0045, 0.0179...              12   \n",
       "23  [0.0407, 0.0245, 0.0299, 0.0406, 0.0222, 0.053...              13   \n",
       "24  [0.0665, 0.0119, 0.0522, 0.0222, 0.0047, 0.021...              13   \n",
       "25  [0.0191, 0.0194, 0.0202, 0.0264, 0.0169, 0.013...              14   \n",
       "26  [0.0299, 0.0115, 0.0259, 0.0084, 0.0024, 0.006...              14   \n",
       "27  [0.0027, 0.0118, 0.0091, 0.0086, 0.0166, 0.010...              15   \n",
       "28  [0.0043, 0.022, 0.0242, 0.0357, 0.0082, 0.0112...              15   \n",
       "29  [0.0015, 0.0174, 0.0043, 0.0114, 0.0259, 0.005...              15   \n",
       "30  [0.2487, 0.7318, 0.2342, 0.1802, 0.8277, 0.637...              16   \n",
       "31  [0.0099, 0.0168, 0.0223, 0.0603, 0.0066, 0.005...              17   \n",
       "32  [0.0094, 0.0095, 0.0112, 0.0639, 0.0036, 0.002...              17   \n",
       "33  [0.0597, 0.0158, 0.0296, 0.012, 0.0033, 0.0151...              18   \n",
       "34  [0.0649, 0.0615, 0.0322, 0.034, 0.1132, 0.0616...              19   \n",
       "35  [0.0472, 0.0153, 0.0174, 0.0144, 0.0021, 0.008...              20   \n",
       "36  [0.0431, 0.0149, 0.0292, 0.0113, 0.0018, 0.01,...              21   \n",
       "37  [0.0105, 0.012, 0.0149, 0.0293, 0.0011, 0.0023...              21   \n",
       "38  [0.0124, 0.0031, 0.0318, 0.028, 0.0017, 0.0034...              22   \n",
       "39  [0.0246, 0.0053, 0.0271, 0.0203, 0.0017, 0.005...              22   \n",
       "40  [0.0185, 0.0253, 0.0065, 0.0548, 0.0103, 0.005...              22   \n",
       "41  [0.0513, 0.0076, 0.0432, 0.0205, 0.0018, 0.010...              22   \n",
       "42  [0.0261, 0.0097, 0.0144, 0.0115, 0.0008, 0.002...              23   \n",
       "43  [0.0126, 0.0102, 0.0101, 0.0198, 0.0008, 0.001...              23   \n",
       "44  [0.008, 0.0078, 0.0097, 0.0157, 0.0007, 0.0008...              23   \n",
       "45  [0.0144, 0.0066, 0.0093, 0.0112, 0.0005, 0.001...              23   \n",
       "46  [0.0206, 0.0183, 0.0926, 0.0489, 0.0102, 0.038...              24   \n",
       "47  [0.0312, 0.0837, 0.0144, 0.0277, 0.0263, 0.022...              24   \n",
       "48  [0.0196, 0.0186, 0.0087, 0.0017, 0.0116, 0.018...              25   \n",
       "49  [0.0369, 0.0646, 0.0136, 0.0157, 0.079, 0.1049...              25   \n",
       "50  [0.0526, 0.0347, 0.014, 0.0283, 0.0179, 0.0145...              26   \n",
       "51  [0.0105, 0.017, 0.0073, 0.0254, 0.0415, 0.0043...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.2968, 0.4246, 0.0961, 0.3518, 0.2, 0.0929, ...   \n",
       "1                  0  [0.747, 0.7093, 0.2401, 0.6055, 0.5527, 0.2253...   \n",
       "2                  0  [0.5404, 0.5789, 0.1728, 0.5211, 0.1902, 0.128...   \n",
       "3                  1  [0.6074, 0.4952, 0.167, 0.6335, 0.7882, 0.4243...   \n",
       "4                  0  [0.1455, 0.2036, 0.0433, 0.2416, 0.2009, 0.039...   \n",
       "5                 10  [0.1667, 0.0622, 0.154, 0.1836, 0.1453, 0.4157...   \n",
       "6                  3  [0.9693, 0.8535, 0.9583, 0.9537, 0.2072, 0.942...   \n",
       "7                  3  [0.8903, 0.3842, 0.8435, 0.6955, 0.1912, 0.912...   \n",
       "8                  3  [0.8253, 0.5011, 0.9133, 0.5721, 0.1933, 0.803...   \n",
       "9                  4  [0.8831, 0.9592, 0.8196, 0.8253, 0.9691, 0.737...   \n",
       "10                 1  [0.6846, 0.9388, 0.1679, 0.5928, 0.8299, 0.615...   \n",
       "11                 5  [0.4944, 0.8288, 0.2611, 0.5065, 0.8287, 0.526...   \n",
       "12                 6  [0.6986, 0.6746, 0.8512, 0.9561, 0.8174, 0.701...   \n",
       "13                 7  [0.4637, 0.1378, 0.2963, 0.5644, 0.7441, 0.952...   \n",
       "14                 8  [0.7712, 0.7056, 0.5821, 0.9189, 0.6751, 0.560...   \n",
       "15                 8  [0.2704, 0.7893, 0.4083, 0.5138, 0.6191, 0.143...   \n",
       "16                 9  [0.1875, 0.1096, 0.1057, 0.2381, 0.1741, 0.309...   \n",
       "17                 9  [0.505, 0.5374, 0.4874, 0.6455, 0.4651, 0.596,...   \n",
       "18                10  [0.1555, 0.4436, 0.089, 0.1047, 0.472, 0.2345,...   \n",
       "19                10  [0.1947, 0.6686, 0.1041, 0.2661, 0.6127, 0.163...   \n",
       "20                11  [0.4426, 0.3261, 0.5696, 0.837, 0.1321, 0.3143...   \n",
       "21                22  [0.407, 0.1193, 0.2624, 0.7869, 0.0485, 0.428,...   \n",
       "22                22  [0.3303, 0.2149, 0.2054, 0.58, 0.0919, 0.4369,...   \n",
       "23                13  [0.7381, 0.6578, 0.6775, 0.3662, 0.6228, 0.715...   \n",
       "24                10  [0.2353, 0.1571, 0.3584, 0.1335, 0.0734, 0.328...   \n",
       "25                14  [0.3474, 0.318, 0.8398, 0.6264, 0.3585, 0.5158...   \n",
       "26                22  [0.1276, 0.0274, 0.4121, 0.2954, 0.0688, 0.350...   \n",
       "27                15  [0.733, 0.556, 0.5435, 0.605, 0.8742, 0.8015, ...   \n",
       "28                15  [0.7949, 0.4066, 0.6704, 0.7685, 0.9521, 0.959...   \n",
       "29                15  [0.8795, 0.5069, 0.8506, 0.8798, 0.9769, 0.950...   \n",
       "30                16  [0.9975, 0.9972, 0.7287, 0.9969, 1.0, 0.9433, ...   \n",
       "31                17  [0.8883, 0.3793, 0.8481, 0.7215, 0.3166, 0.680...   \n",
       "32                17  [0.738, 0.6455, 0.9213, 0.9192, 0.2021, 0.7476...   \n",
       "33                10  [0.1431, 0.197, 0.0687, 0.127, 0.0861, 0.1402,...   \n",
       "34                19  [0.7973, 0.9085, 0.1041, 0.4189, 0.7418, 0.159...   \n",
       "35                10  [0.2121, 0.1021, 0.4078, 0.3302, 0.0652, 0.445...   \n",
       "36                23  [0.0817, 0.0484, 0.2264, 0.2632, 0.0322, 0.322...   \n",
       "37                23  [0.183, 0.1324, 0.6574, 0.7769, 0.0565, 0.9555...   \n",
       "38                22  [0.7114, 0.0667, 0.0677, 0.2958, 0.105, 0.9063...   \n",
       "39                22  [0.5083, 0.0264, 0.0404, 0.2975, 0.0329, 0.531...   \n",
       "40                22  [0.8503, 0.3934, 0.2936, 0.883, 0.3862, 0.7573...   \n",
       "41                22  [0.1589, 0.0287, 0.0427, 0.0801, 0.0523, 0.585...   \n",
       "42                23  [0.1398, 0.0489, 0.4548, 0.3591, 0.0245, 0.387...   \n",
       "43                23  [0.4722, 0.0991, 0.6126, 0.7929, 0.0728, 0.924...   \n",
       "44                23  [0.3576, 0.0543, 0.845, 0.7401, 0.0732, 0.9814...   \n",
       "45                23  [0.2586, 0.0509, 0.5036, 0.4514, 0.031, 0.6061...   \n",
       "46                13  [0.7664, 0.3391, 0.8137, 0.4853, 0.5036, 0.663...   \n",
       "47                24  [0.6959, 0.5616, 0.676, 0.7469, 0.3223, 0.3951...   \n",
       "48                25  [0.2369, 0.5839, 0.0137, 0.133, 0.6237, 0.0843...   \n",
       "49                25  [0.2637, 0.9563, 0.2553, 0.6264, 0.7892, 0.289...   \n",
       "50                26  [0.5049, 0.4635, 0.7259, 0.618, 0.2195, 0.2885...   \n",
       "51                26  [0.7006, 0.7791, 0.7413, 0.7145, 0.6706, 0.380...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy     Loss  \n",
       "0   [0.7032, 0.5754, 0.9039, 0.6482, 0.8, 0.9071, ...  0.769231  0.08917  \n",
       "1   [0.253, 0.2907, 0.7599, 0.3945, 0.4473, 0.7747...       NaN      NaN  \n",
       "2   [0.4596, 0.4211, 0.8272, 0.4789, 0.8098, 0.872...       NaN      NaN  \n",
       "3   [0.3926, 0.5048, 0.833, 0.3665, 0.2118, 0.5757...       NaN      NaN  \n",
       "4   [0.8545, 0.7964, 0.9567, 0.7584, 0.7991, 0.960...       NaN      NaN  \n",
       "5   [0.8333, 0.9378, 0.846, 0.8164, 0.8547, 0.5843...       NaN      NaN  \n",
       "6   [0.0307, 0.1465, 0.0417, 0.0463, 0.7928, 0.057...       NaN      NaN  \n",
       "7   [0.1097, 0.6158, 0.1565, 0.3045, 0.8088, 0.087...       NaN      NaN  \n",
       "8   [0.1747, 0.4989, 0.0867, 0.4279, 0.8067, 0.196...       NaN      NaN  \n",
       "9   [0.1169, 0.0408, 0.1804, 0.1747, 0.0309, 0.262...       NaN      NaN  \n",
       "10  [0.3154, 0.0612, 0.8321, 0.4072, 0.1701, 0.384...       NaN      NaN  \n",
       "11  [0.5056, 0.1712, 0.7389, 0.4935, 0.1713, 0.473...       NaN      NaN  \n",
       "12  [0.3014, 0.3254, 0.1488, 0.0439, 0.1826, 0.298...       NaN      NaN  \n",
       "13  [0.5363, 0.8622, 0.7037, 0.4356, 0.2559, 0.047...       NaN      NaN  \n",
       "14  [0.2288, 0.2944, 0.4179, 0.0811, 0.3249, 0.439...       NaN      NaN  \n",
       "15  [0.7296, 0.2107, 0.5917, 0.4862, 0.3809, 0.856...       NaN      NaN  \n",
       "16  [0.8125, 0.8904, 0.8943, 0.7619, 0.8259, 0.690...       NaN      NaN  \n",
       "17  [0.495, 0.4626, 0.5126, 0.3545, 0.5349, 0.404,...       NaN      NaN  \n",
       "18  [0.8445, 0.5564, 0.911, 0.8953, 0.528, 0.7655,...       NaN      NaN  \n",
       "19  [0.8053, 0.3314, 0.8959, 0.7339, 0.3873, 0.836...       NaN      NaN  \n",
       "20  [0.5574, 0.6739, 0.4304, 0.163, 0.8679, 0.6857...       NaN      NaN  \n",
       "21  [0.593, 0.8807, 0.7376, 0.2131, 0.9515, 0.572,...       NaN      NaN  \n",
       "22  [0.6697, 0.7851, 0.7946, 0.42, 0.9081, 0.5631,...       NaN      NaN  \n",
       "23  [0.2619, 0.3422, 0.3225, 0.6338, 0.3772, 0.285...       NaN      NaN  \n",
       "24  [0.7647, 0.8429, 0.6416, 0.8665, 0.9266, 0.671...       NaN      NaN  \n",
       "25  [0.6526, 0.682, 0.1602, 0.3736, 0.6415, 0.4842...       NaN      NaN  \n",
       "26  [0.8724, 0.9726, 0.5879, 0.7046, 0.9312, 0.649...       NaN      NaN  \n",
       "27  [0.267, 0.444, 0.4565, 0.395, 0.1258, 0.1985, ...       NaN      NaN  \n",
       "28  [0.2051, 0.5934, 0.3296, 0.2315, 0.0479, 0.040...       NaN      NaN  \n",
       "29  [0.1205, 0.4931, 0.1494, 0.1202, 0.0231, 0.049...       NaN      NaN  \n",
       "30  [0.0025, 0.0028, 0.2713, 0.0031, 0.0, 0.0567, ...       NaN      NaN  \n",
       "31  [0.1117, 0.6207, 0.1519, 0.2785, 0.6834, 0.319...       NaN      NaN  \n",
       "32  [0.262, 0.3545, 0.0787, 0.0808, 0.7979, 0.2524...       NaN      NaN  \n",
       "33  [0.8569, 0.803, 0.9313, 0.873, 0.9139, 0.8598,...       NaN      NaN  \n",
       "34  [0.2027, 0.0915, 0.8959, 0.5811, 0.2582, 0.840...       NaN      NaN  \n",
       "35  [0.7879, 0.8979, 0.5922, 0.6698, 0.9348, 0.554...       NaN      NaN  \n",
       "36  [0.9183, 0.9516, 0.7736, 0.7368, 0.9678, 0.677...       NaN      NaN  \n",
       "37  [0.817, 0.8676, 0.3426, 0.2231, 0.9435, 0.0445...       NaN      NaN  \n",
       "38  [0.2886, 0.9333, 0.9323, 0.7042, 0.895, 0.0937...       NaN      NaN  \n",
       "39  [0.4917, 0.9736, 0.9596, 0.7025, 0.9671, 0.468...       NaN      NaN  \n",
       "40  [0.1497, 0.6066, 0.7064, 0.117, 0.6138, 0.2427...       NaN      NaN  \n",
       "41  [0.8411, 0.9713, 0.9573, 0.9199, 0.9477, 0.414...       NaN      NaN  \n",
       "42  [0.8602, 0.9511, 0.5452, 0.6409, 0.9755, 0.612...       NaN      NaN  \n",
       "43  [0.5278, 0.9009, 0.3874, 0.2071, 0.9272, 0.075...       NaN      NaN  \n",
       "44  [0.6424, 0.9457, 0.155, 0.2599, 0.9268, 0.0186...       NaN      NaN  \n",
       "45  [0.7414, 0.9491, 0.4964, 0.5486, 0.969, 0.3939...       NaN      NaN  \n",
       "46  [0.2336, 0.6609, 0.1863, 0.5147, 0.4964, 0.336...       NaN      NaN  \n",
       "47  [0.3041, 0.4384, 0.324, 0.2531, 0.6777, 0.6049...       NaN      NaN  \n",
       "48  [0.7631, 0.4161, 0.9863, 0.867, 0.3763, 0.9157...       NaN      NaN  \n",
       "49  [0.7363, 0.0437, 0.7447, 0.3736, 0.2108, 0.711...       NaN      NaN  \n",
       "50  [0.4951, 0.5365, 0.2741, 0.382, 0.7805, 0.7115...       NaN      NaN  \n",
       "51  [0.2994, 0.2209, 0.2587, 0.2855, 0.3294, 0.619...       NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881 : Training: loss:  0.079741396\n",
      "1882 : Training: loss:  0.060293086\n",
      "1883 : Training: loss:  0.07291372\n",
      "1884 : Training: loss:  0.07133983\n",
      "1885 : Training: loss:  0.07902423\n",
      "1886 : Training: loss:  0.09118608\n",
      "1887 : Training: loss:  0.072515674\n",
      "1888 : Training: loss:  0.10837444\n",
      "1889 : Training: loss:  0.07959081\n",
      "1890 : Training: loss:  0.06838528\n",
      "1891 : Training: loss:  0.05218475\n",
      "1892 : Training: loss:  0.053979874\n",
      "1893 : Training: loss:  0.09700063\n",
      "1894 : Training: loss:  0.07385439\n",
      "1895 : Training: loss:  0.05136483\n",
      "1896 : Training: loss:  0.061363623\n",
      "1897 : Training: loss:  0.079213135\n",
      "1898 : Training: loss:  0.07091756\n",
      "1899 : Training: loss:  0.0899324\n",
      "1900 : Training: loss:  0.054629788\n",
      "Validation: Loss:  0.087996565  Accuracy:  0.8076923\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.2423, 0.1, 0.0253, 0.0382, 0.0229, 0.039, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3016, 0.428, 0.0955, 0.3577, 0.2031, 0.0932...</td>\n",
       "      <td>[0.6984, 0.572, 0.9045, 0.6423, 0.7969, 0.9068...</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.087997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.4235, 0.3898, 0.0131, 0.0358, 0.1304, 0.041...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7491, 0.7109, 0.2417, 0.6114, 0.5565, 0.227...</td>\n",
       "      <td>[0.2509, 0.2891, 0.7583, 0.3886, 0.4435, 0.773...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.3344, 0.1664, 0.0151, 0.0713, 0.0334, 0.056...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5439, 0.583, 0.1729, 0.5295, 0.1928, 0.1285...</td>\n",
       "      <td>[0.4561, 0.417, 0.8271, 0.4705, 0.8072, 0.8715...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0391, 0.1716, 0.0067, 0.0091, 0.085, 0.0342...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6075, 0.497, 0.1679, 0.6402, 0.7919, 0.4289...</td>\n",
       "      <td>[0.3925, 0.503, 0.8321, 0.3598, 0.2081, 0.5711...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0968, 0.0976, 0.0196, 0.0103, 0.0115, 0.033...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1471, 0.2061, 0.0431, 0.2475, 0.2052, 0.039...</td>\n",
       "      <td>[0.8529, 0.7939, 0.9569, 0.7525, 0.7948, 0.960...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0456, 0.0268, 0.1075, 0.0187, 0.0092, 0.054...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1669, 0.0625, 0.1571, 0.1891, 0.1464, 0.421...</td>\n",
       "      <td>[0.8331, 0.9375, 0.8429, 0.8109, 0.8536, 0.578...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0343, 0.0152, 0.0023, 0.6851, 0.006, 0.0017...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9701, 0.856, 0.9585, 0.9554, 0.2063, 0.9434...</td>\n",
       "      <td>[0.0299, 0.144, 0.0415, 0.0446, 0.7937, 0.0566...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0375, 0.0254, 0.016, 0.3846, 0.0068, 0.0085...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8912, 0.3849, 0.8452, 0.6992, 0.1912, 0.914...</td>\n",
       "      <td>[0.1088, 0.6151, 0.1548, 0.3008, 0.8088, 0.085...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0559, 0.025, 0.0281, 0.3386, 0.0135, 0.0207...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8274, 0.5048, 0.9146, 0.5755, 0.1929, 0.806...</td>\n",
       "      <td>[0.1726, 0.4952, 0.0854, 0.4245, 0.8071, 0.193...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0123, 0.1093, 0.0093, 0.0236, 0.545, 0.0945...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8828, 0.9598, 0.8197, 0.8258, 0.9694, 0.739...</td>\n",
       "      <td>[0.1172, 0.0402, 0.1803, 0.1742, 0.0306, 0.260...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0477, 0.3882, 0.0067, 0.0085, 0.2894, 0.208...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6848, 0.9402, 0.1629, 0.5932, 0.8316, 0.616...</td>\n",
       "      <td>[0.3152, 0.0598, 0.8371, 0.4068, 0.1684, 0.383...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0111, 0.0475, 0.0079, 0.0098, 0.0816, 0.178...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4942, 0.8318, 0.2566, 0.5069, 0.8309, 0.527...</td>\n",
       "      <td>[0.5058, 0.1682, 0.7434, 0.4931, 0.1691, 0.472...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0031, 0.0155, 0.0203, 0.0067, 0.0087, 0.006...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6971, 0.675, 0.8545, 0.9582, 0.8172, 0.7018...</td>\n",
       "      <td>[0.3029, 0.325, 0.1455, 0.0418, 0.1828, 0.2982...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0865, 0.0826, 0.1778, 0.1159, 0.0091, 0.109...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.4563, 0.1349, 0.2977, 0.5717, 0.7426, 0.952...</td>\n",
       "      <td>[0.5437, 0.8651, 0.7023, 0.4283, 0.2574, 0.047...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0198, 0.0308, 0.0026, 0.0467, 0.0322, 0.006...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7722, 0.7096, 0.5803, 0.9209, 0.6771, 0.564...</td>\n",
       "      <td>[0.2278, 0.2904, 0.4197, 0.0791, 0.3229, 0.435...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0562, 0.0466, 0.0076, 0.0104, 0.0379, 0.017...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2701, 0.7931, 0.4071, 0.516, 0.6223, 0.1435...</td>\n",
       "      <td>[0.7299, 0.2069, 0.5929, 0.484, 0.3777, 0.8565...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0309, 0.0156, 0.0838, 0.0117, 0.0035, 0.034...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.1889, 0.1106, 0.106, 0.2428, 0.175, 0.3128,...</td>\n",
       "      <td>[0.8111, 0.8894, 0.894, 0.7572, 0.825, 0.6872,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0278, 0.0208, 0.0254, 0.021, 0.0085, 0.0142...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5072, 0.539, 0.4881, 0.65, 0.465, 0.5997, 0...</td>\n",
       "      <td>[0.4928, 0.461, 0.5119, 0.35, 0.535, 0.4003, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0775, 0.0173, 0.0508, 0.0112, 0.0051, 0.028...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1563, 0.4477, 0.0885, 0.1044, 0.4745, 0.235...</td>\n",
       "      <td>[0.8437, 0.5523, 0.9115, 0.8956, 0.5255, 0.764...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0574, 0.0186, 0.0551, 0.0086, 0.0065, 0.033...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1951, 0.6733, 0.1035, 0.2657, 0.6151, 0.162...</td>\n",
       "      <td>[0.8049, 0.3267, 0.8965, 0.7343, 0.3849, 0.837...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0747, 0.0423, 0.0228, 0.0555, 0.0073, 0.015...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4444, 0.3263, 0.5758, 0.8405, 0.1326, 0.317...</td>\n",
       "      <td>[0.5556, 0.6737, 0.4242, 0.1595, 0.8674, 0.682...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.028, 0.0123, 0.0237, 0.0307, 0.0021, 0.0061...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4088, 0.1196, 0.2672, 0.793, 0.0487, 0.4348...</td>\n",
       "      <td>[0.5912, 0.8804, 0.7328, 0.207, 0.9513, 0.5652...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0391, 0.0213, 0.0291, 0.0267, 0.0045, 0.018...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3329, 0.217, 0.2076, 0.5869, 0.0922, 0.4402...</td>\n",
       "      <td>[0.6671, 0.783, 0.7924, 0.4131, 0.9078, 0.5598...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.039, 0.0247, 0.0296, 0.039, 0.0224, 0.0565,...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7374, 0.661, 0.6789, 0.3679, 0.6241, 0.7167...</td>\n",
       "      <td>[0.2626, 0.339, 0.3211, 0.6321, 0.3759, 0.2833...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0638, 0.0117, 0.0535, 0.0212, 0.0046, 0.022...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2385, 0.1605, 0.363, 0.1364, 0.0732, 0.3325...</td>\n",
       "      <td>[0.7615, 0.8395, 0.637, 0.8636, 0.9268, 0.6675...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0178, 0.0194, 0.0202, 0.025, 0.0167, 0.0138...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3464, 0.3209, 0.8448, 0.6355, 0.361, 0.5242...</td>\n",
       "      <td>[0.6536, 0.6791, 0.1552, 0.3645, 0.639, 0.4758...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0279, 0.0115, 0.0262, 0.0079, 0.0024, 0.006...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1267, 0.0273, 0.4237, 0.3037, 0.0692, 0.359...</td>\n",
       "      <td>[0.8733, 0.9727, 0.5763, 0.6963, 0.9308, 0.640...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0025, 0.0121, 0.0089, 0.008, 0.0163, 0.0105...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7338, 0.5576, 0.5471, 0.617, 0.8751, 0.8072...</td>\n",
       "      <td>[0.2662, 0.4424, 0.4529, 0.383, 0.1249, 0.1928...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.004, 0.0225, 0.0236, 0.0335, 0.0078, 0.011,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.794, 0.4046, 0.6734, 0.7769, 0.9516, 0.9607...</td>\n",
       "      <td>[0.206, 0.5954, 0.3266, 0.2231, 0.0484, 0.0393...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0014, 0.0184, 0.0041, 0.0106, 0.0252, 0.005...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8781, 0.506, 0.8522, 0.8857, 0.977, 0.9527,...</td>\n",
       "      <td>[0.1219, 0.494, 0.1478, 0.1143, 0.023, 0.0473,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.2389, 0.744, 0.2212, 0.1734, 0.8226, 0.6393...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9972, 0.9971, 0.7174, 0.9967, 1.0, 0.9397, ...</td>\n",
       "      <td>[0.0028, 0.0029, 0.2826, 0.0033, 0.0, 0.0603, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0093, 0.017, 0.022, 0.0579, 0.0064, 0.0053,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.8896, 0.3804, 0.8538, 0.7302, 0.3155, 0.686...</td>\n",
       "      <td>[0.1104, 0.6196, 0.1462, 0.2698, 0.6845, 0.313...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0087, 0.0095, 0.0113, 0.0612, 0.0035, 0.002...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7421, 0.6485, 0.9244, 0.9234, 0.2008, 0.752...</td>\n",
       "      <td>[0.2579, 0.3515, 0.0756, 0.0766, 0.7992, 0.247...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0567, 0.016, 0.0304, 0.0114, 0.0032, 0.0155...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1452, 0.2011, 0.0689, 0.1314, 0.0873, 0.143...</td>\n",
       "      <td>[0.8548, 0.7989, 0.9311, 0.8686, 0.9127, 0.856...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0611, 0.0637, 0.0315, 0.0323, 0.1164, 0.064...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8005, 0.911, 0.1025, 0.4243, 0.7473, 0.1606...</td>\n",
       "      <td>[0.1995, 0.089, 0.8975, 0.5757, 0.2527, 0.8394...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0451, 0.0158, 0.0178, 0.014, 0.0021, 0.0089...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2147, 0.1037, 0.4164, 0.3408, 0.065, 0.4524...</td>\n",
       "      <td>[0.7853, 0.8963, 0.5836, 0.6592, 0.935, 0.5476...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0413, 0.015, 0.0295, 0.0107, 0.0017, 0.01, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0812, 0.0481, 0.2313, 0.2653, 0.0321, 0.326...</td>\n",
       "      <td>[0.9188, 0.9519, 0.7687, 0.7347, 0.9679, 0.673...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0098, 0.0121, 0.0148, 0.0278, 0.001, 0.0022...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1799, 0.1298, 0.6602, 0.7793, 0.0556, 0.955...</td>\n",
       "      <td>[0.8201, 0.8702, 0.3398, 0.2207, 0.9444, 0.044...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0116, 0.0031, 0.0323, 0.0263, 0.0017, 0.003...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7133, 0.0666, 0.0679, 0.3004, 0.105, 0.9083...</td>\n",
       "      <td>[0.2867, 0.9334, 0.9321, 0.6996, 0.895, 0.0917...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0232, 0.0052, 0.0274, 0.0192, 0.0016, 0.005...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5096, 0.0264, 0.0409, 0.3041, 0.0331, 0.539...</td>\n",
       "      <td>[0.4904, 0.9736, 0.9591, 0.6959, 0.9669, 0.460...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0175, 0.026, 0.0063, 0.0525, 0.01, 0.005, 0...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.8516, 0.392, 0.2975, 0.8867, 0.3868, 0.7617...</td>\n",
       "      <td>[0.1484, 0.608, 0.7025, 0.1133, 0.6132, 0.2383...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0488, 0.0075, 0.0443, 0.0194, 0.0018, 0.010...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1597, 0.0288, 0.0432, 0.0813, 0.0526, 0.591...</td>\n",
       "      <td>[0.8403, 0.9712, 0.9568, 0.9187, 0.9474, 0.408...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0248, 0.0099, 0.0145, 0.0109, 0.0008, 0.002...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.141, 0.0488, 0.4588, 0.3643, 0.0245, 0.3906...</td>\n",
       "      <td>[0.859, 0.9512, 0.5412, 0.6357, 0.9755, 0.6094...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0119, 0.0103, 0.01, 0.0186, 0.0008, 0.0011,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.4695, 0.0971, 0.6151, 0.797, 0.0718, 0.9249...</td>\n",
       "      <td>[0.5305, 0.9029, 0.3849, 0.203, 0.9282, 0.0751...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0075, 0.0079, 0.0095, 0.0148, 0.0006, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3542, 0.0528, 0.8476, 0.7472, 0.0718, 0.981...</td>\n",
       "      <td>[0.6458, 0.9472, 0.1524, 0.2528, 0.9282, 0.018...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0137, 0.0067, 0.0093, 0.0106, 0.0005, 0.001...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2592, 0.0503, 0.5081, 0.4558, 0.0307, 0.610...</td>\n",
       "      <td>[0.7408, 0.9497, 0.4919, 0.5442, 0.9693, 0.389...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0197, 0.0181, 0.0935, 0.0466, 0.0101, 0.039...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7655, 0.3417, 0.8188, 0.4895, 0.5029, 0.666...</td>\n",
       "      <td>[0.2345, 0.6583, 0.1812, 0.5105, 0.4971, 0.333...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0293, 0.0874, 0.0138, 0.0264, 0.0256, 0.022...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.7016, 0.5646, 0.6809, 0.7519, 0.3233, 0.397...</td>\n",
       "      <td>[0.2984, 0.4354, 0.3191, 0.2481, 0.6767, 0.602...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0179, 0.0192, 0.0086, 0.0015, 0.0119, 0.019...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2363, 0.5902, 0.0136, 0.1356, 0.63, 0.085, ...</td>\n",
       "      <td>[0.7637, 0.4098, 0.9864, 0.8644, 0.37, 0.915, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0343, 0.0657, 0.0131, 0.0147, 0.0781, 0.109...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2622, 0.9571, 0.2536, 0.6257, 0.7907, 0.287...</td>\n",
       "      <td>[0.7378, 0.0429, 0.7464, 0.3743, 0.2093, 0.712...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.05, 0.035, 0.0139, 0.0274, 0.0174, 0.0144, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.507, 0.4643, 0.7314, 0.6262, 0.2197, 0.2911...</td>\n",
       "      <td>[0.493, 0.5357, 0.2686, 0.3738, 0.7803, 0.7089...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0097, 0.0171, 0.0071, 0.024, 0.0411, 0.0042...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7045, 0.7823, 0.7438, 0.7211, 0.6717, 0.385...</td>\n",
       "      <td>[0.2955, 0.2177, 0.2562, 0.2789, 0.3283, 0.615...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.2423, 0.1, 0.0253, 0.0382, 0.0229, 0.039, 0...               0   \n",
       "1   [0.4235, 0.3898, 0.0131, 0.0358, 0.1304, 0.041...               0   \n",
       "2   [0.3344, 0.1664, 0.0151, 0.0713, 0.0334, 0.056...               0   \n",
       "3   [0.0391, 0.1716, 0.0067, 0.0091, 0.085, 0.0342...               1   \n",
       "4   [0.0968, 0.0976, 0.0196, 0.0103, 0.0115, 0.033...               1   \n",
       "5   [0.0456, 0.0268, 0.1075, 0.0187, 0.0092, 0.054...               2   \n",
       "6   [0.0343, 0.0152, 0.0023, 0.6851, 0.006, 0.0017...               3   \n",
       "7   [0.0375, 0.0254, 0.016, 0.3846, 0.0068, 0.0085...               3   \n",
       "8   [0.0559, 0.025, 0.0281, 0.3386, 0.0135, 0.0207...               3   \n",
       "9   [0.0123, 0.1093, 0.0093, 0.0236, 0.545, 0.0945...               4   \n",
       "10  [0.0477, 0.3882, 0.0067, 0.0085, 0.2894, 0.208...               4   \n",
       "11  [0.0111, 0.0475, 0.0079, 0.0098, 0.0816, 0.178...               5   \n",
       "12  [0.0031, 0.0155, 0.0203, 0.0067, 0.0087, 0.006...               6   \n",
       "13  [0.0865, 0.0826, 0.1778, 0.1159, 0.0091, 0.109...               7   \n",
       "14  [0.0198, 0.0308, 0.0026, 0.0467, 0.0322, 0.006...               8   \n",
       "15  [0.0562, 0.0466, 0.0076, 0.0104, 0.0379, 0.017...               8   \n",
       "16  [0.0309, 0.0156, 0.0838, 0.0117, 0.0035, 0.034...               9   \n",
       "17  [0.0278, 0.0208, 0.0254, 0.021, 0.0085, 0.0142...               9   \n",
       "18  [0.0775, 0.0173, 0.0508, 0.0112, 0.0051, 0.028...              10   \n",
       "19  [0.0574, 0.0186, 0.0551, 0.0086, 0.0065, 0.033...              10   \n",
       "20  [0.0747, 0.0423, 0.0228, 0.0555, 0.0073, 0.015...              11   \n",
       "21  [0.028, 0.0123, 0.0237, 0.0307, 0.0021, 0.0061...              11   \n",
       "22  [0.0391, 0.0213, 0.0291, 0.0267, 0.0045, 0.018...              12   \n",
       "23  [0.039, 0.0247, 0.0296, 0.039, 0.0224, 0.0565,...              13   \n",
       "24  [0.0638, 0.0117, 0.0535, 0.0212, 0.0046, 0.022...              13   \n",
       "25  [0.0178, 0.0194, 0.0202, 0.025, 0.0167, 0.0138...              14   \n",
       "26  [0.0279, 0.0115, 0.0262, 0.0079, 0.0024, 0.006...              14   \n",
       "27  [0.0025, 0.0121, 0.0089, 0.008, 0.0163, 0.0105...              15   \n",
       "28  [0.004, 0.0225, 0.0236, 0.0335, 0.0078, 0.011,...              15   \n",
       "29  [0.0014, 0.0184, 0.0041, 0.0106, 0.0252, 0.005...              15   \n",
       "30  [0.2389, 0.744, 0.2212, 0.1734, 0.8226, 0.6393...              16   \n",
       "31  [0.0093, 0.017, 0.022, 0.0579, 0.0064, 0.0053,...              17   \n",
       "32  [0.0087, 0.0095, 0.0113, 0.0612, 0.0035, 0.002...              17   \n",
       "33  [0.0567, 0.016, 0.0304, 0.0114, 0.0032, 0.0155...              18   \n",
       "34  [0.0611, 0.0637, 0.0315, 0.0323, 0.1164, 0.064...              19   \n",
       "35  [0.0451, 0.0158, 0.0178, 0.014, 0.0021, 0.0089...              20   \n",
       "36  [0.0413, 0.015, 0.0295, 0.0107, 0.0017, 0.01, ...              21   \n",
       "37  [0.0098, 0.0121, 0.0148, 0.0278, 0.001, 0.0022...              21   \n",
       "38  [0.0116, 0.0031, 0.0323, 0.0263, 0.0017, 0.003...              22   \n",
       "39  [0.0232, 0.0052, 0.0274, 0.0192, 0.0016, 0.005...              22   \n",
       "40  [0.0175, 0.026, 0.0063, 0.0525, 0.01, 0.005, 0...              22   \n",
       "41  [0.0488, 0.0075, 0.0443, 0.0194, 0.0018, 0.010...              22   \n",
       "42  [0.0248, 0.0099, 0.0145, 0.0109, 0.0008, 0.002...              23   \n",
       "43  [0.0119, 0.0103, 0.01, 0.0186, 0.0008, 0.0011,...              23   \n",
       "44  [0.0075, 0.0079, 0.0095, 0.0148, 0.0006, 0.000...              23   \n",
       "45  [0.0137, 0.0067, 0.0093, 0.0106, 0.0005, 0.001...              23   \n",
       "46  [0.0197, 0.0181, 0.0935, 0.0466, 0.0101, 0.039...              24   \n",
       "47  [0.0293, 0.0874, 0.0138, 0.0264, 0.0256, 0.022...              24   \n",
       "48  [0.0179, 0.0192, 0.0086, 0.0015, 0.0119, 0.019...              25   \n",
       "49  [0.0343, 0.0657, 0.0131, 0.0147, 0.0781, 0.109...              25   \n",
       "50  [0.05, 0.035, 0.0139, 0.0274, 0.0174, 0.0144, ...              26   \n",
       "51  [0.0097, 0.0171, 0.0071, 0.024, 0.0411, 0.0042...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.3016, 0.428, 0.0955, 0.3577, 0.2031, 0.0932...   \n",
       "1                  0  [0.7491, 0.7109, 0.2417, 0.6114, 0.5565, 0.227...   \n",
       "2                  0  [0.5439, 0.583, 0.1729, 0.5295, 0.1928, 0.1285...   \n",
       "3                  1  [0.6075, 0.497, 0.1679, 0.6402, 0.7919, 0.4289...   \n",
       "4                  1  [0.1471, 0.2061, 0.0431, 0.2475, 0.2052, 0.039...   \n",
       "5                 10  [0.1669, 0.0625, 0.1571, 0.1891, 0.1464, 0.421...   \n",
       "6                  3  [0.9701, 0.856, 0.9585, 0.9554, 0.2063, 0.9434...   \n",
       "7                  3  [0.8912, 0.3849, 0.8452, 0.6992, 0.1912, 0.914...   \n",
       "8                  3  [0.8274, 0.5048, 0.9146, 0.5755, 0.1929, 0.806...   \n",
       "9                  4  [0.8828, 0.9598, 0.8197, 0.8258, 0.9694, 0.739...   \n",
       "10                 1  [0.6848, 0.9402, 0.1629, 0.5932, 0.8316, 0.616...   \n",
       "11                 5  [0.4942, 0.8318, 0.2566, 0.5069, 0.8309, 0.527...   \n",
       "12                 6  [0.6971, 0.675, 0.8545, 0.9582, 0.8172, 0.7018...   \n",
       "13                 7  [0.4563, 0.1349, 0.2977, 0.5717, 0.7426, 0.952...   \n",
       "14                 8  [0.7722, 0.7096, 0.5803, 0.9209, 0.6771, 0.564...   \n",
       "15                 8  [0.2701, 0.7931, 0.4071, 0.516, 0.6223, 0.1435...   \n",
       "16                 9  [0.1889, 0.1106, 0.106, 0.2428, 0.175, 0.3128,...   \n",
       "17                 9  [0.5072, 0.539, 0.4881, 0.65, 0.465, 0.5997, 0...   \n",
       "18                10  [0.1563, 0.4477, 0.0885, 0.1044, 0.4745, 0.235...   \n",
       "19                10  [0.1951, 0.6733, 0.1035, 0.2657, 0.6151, 0.162...   \n",
       "20                11  [0.4444, 0.3263, 0.5758, 0.8405, 0.1326, 0.317...   \n",
       "21                22  [0.4088, 0.1196, 0.2672, 0.793, 0.0487, 0.4348...   \n",
       "22                22  [0.3329, 0.217, 0.2076, 0.5869, 0.0922, 0.4402...   \n",
       "23                13  [0.7374, 0.661, 0.6789, 0.3679, 0.6241, 0.7167...   \n",
       "24                10  [0.2385, 0.1605, 0.363, 0.1364, 0.0732, 0.3325...   \n",
       "25                14  [0.3464, 0.3209, 0.8448, 0.6355, 0.361, 0.5242...   \n",
       "26                14  [0.1267, 0.0273, 0.4237, 0.3037, 0.0692, 0.359...   \n",
       "27                15  [0.7338, 0.5576, 0.5471, 0.617, 0.8751, 0.8072...   \n",
       "28                15  [0.794, 0.4046, 0.6734, 0.7769, 0.9516, 0.9607...   \n",
       "29                15  [0.8781, 0.506, 0.8522, 0.8857, 0.977, 0.9527,...   \n",
       "30                16  [0.9972, 0.9971, 0.7174, 0.9967, 1.0, 0.9397, ...   \n",
       "31                17  [0.8896, 0.3804, 0.8538, 0.7302, 0.3155, 0.686...   \n",
       "32                17  [0.7421, 0.6485, 0.9244, 0.9234, 0.2008, 0.752...   \n",
       "33                10  [0.1452, 0.2011, 0.0689, 0.1314, 0.0873, 0.143...   \n",
       "34                19  [0.8005, 0.911, 0.1025, 0.4243, 0.7473, 0.1606...   \n",
       "35                10  [0.2147, 0.1037, 0.4164, 0.3408, 0.065, 0.4524...   \n",
       "36                23  [0.0812, 0.0481, 0.2313, 0.2653, 0.0321, 0.326...   \n",
       "37                23  [0.1799, 0.1298, 0.6602, 0.7793, 0.0556, 0.955...   \n",
       "38                22  [0.7133, 0.0666, 0.0679, 0.3004, 0.105, 0.9083...   \n",
       "39                22  [0.5096, 0.0264, 0.0409, 0.3041, 0.0331, 0.539...   \n",
       "40                22  [0.8516, 0.392, 0.2975, 0.8867, 0.3868, 0.7617...   \n",
       "41                22  [0.1597, 0.0288, 0.0432, 0.0813, 0.0526, 0.591...   \n",
       "42                23  [0.141, 0.0488, 0.4588, 0.3643, 0.0245, 0.3906...   \n",
       "43                23  [0.4695, 0.0971, 0.6151, 0.797, 0.0718, 0.9249...   \n",
       "44                23  [0.3542, 0.0528, 0.8476, 0.7472, 0.0718, 0.981...   \n",
       "45                23  [0.2592, 0.0503, 0.5081, 0.4558, 0.0307, 0.610...   \n",
       "46                13  [0.7655, 0.3417, 0.8188, 0.4895, 0.5029, 0.666...   \n",
       "47                24  [0.7016, 0.5646, 0.6809, 0.7519, 0.3233, 0.397...   \n",
       "48                25  [0.2363, 0.5902, 0.0136, 0.1356, 0.63, 0.085, ...   \n",
       "49                25  [0.2622, 0.9571, 0.2536, 0.6257, 0.7907, 0.287...   \n",
       "50                26  [0.507, 0.4643, 0.7314, 0.6262, 0.2197, 0.2911...   \n",
       "51                26  [0.7045, 0.7823, 0.7438, 0.7211, 0.6717, 0.385...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.6984, 0.572, 0.9045, 0.6423, 0.7969, 0.9068...  0.807692  0.087997  \n",
       "1   [0.2509, 0.2891, 0.7583, 0.3886, 0.4435, 0.773...       NaN       NaN  \n",
       "2   [0.4561, 0.417, 0.8271, 0.4705, 0.8072, 0.8715...       NaN       NaN  \n",
       "3   [0.3925, 0.503, 0.8321, 0.3598, 0.2081, 0.5711...       NaN       NaN  \n",
       "4   [0.8529, 0.7939, 0.9569, 0.7525, 0.7948, 0.960...       NaN       NaN  \n",
       "5   [0.8331, 0.9375, 0.8429, 0.8109, 0.8536, 0.578...       NaN       NaN  \n",
       "6   [0.0299, 0.144, 0.0415, 0.0446, 0.7937, 0.0566...       NaN       NaN  \n",
       "7   [0.1088, 0.6151, 0.1548, 0.3008, 0.8088, 0.085...       NaN       NaN  \n",
       "8   [0.1726, 0.4952, 0.0854, 0.4245, 0.8071, 0.193...       NaN       NaN  \n",
       "9   [0.1172, 0.0402, 0.1803, 0.1742, 0.0306, 0.260...       NaN       NaN  \n",
       "10  [0.3152, 0.0598, 0.8371, 0.4068, 0.1684, 0.383...       NaN       NaN  \n",
       "11  [0.5058, 0.1682, 0.7434, 0.4931, 0.1691, 0.472...       NaN       NaN  \n",
       "12  [0.3029, 0.325, 0.1455, 0.0418, 0.1828, 0.2982...       NaN       NaN  \n",
       "13  [0.5437, 0.8651, 0.7023, 0.4283, 0.2574, 0.047...       NaN       NaN  \n",
       "14  [0.2278, 0.2904, 0.4197, 0.0791, 0.3229, 0.435...       NaN       NaN  \n",
       "15  [0.7299, 0.2069, 0.5929, 0.484, 0.3777, 0.8565...       NaN       NaN  \n",
       "16  [0.8111, 0.8894, 0.894, 0.7572, 0.825, 0.6872,...       NaN       NaN  \n",
       "17  [0.4928, 0.461, 0.5119, 0.35, 0.535, 0.4003, 0...       NaN       NaN  \n",
       "18  [0.8437, 0.5523, 0.9115, 0.8956, 0.5255, 0.764...       NaN       NaN  \n",
       "19  [0.8049, 0.3267, 0.8965, 0.7343, 0.3849, 0.837...       NaN       NaN  \n",
       "20  [0.5556, 0.6737, 0.4242, 0.1595, 0.8674, 0.682...       NaN       NaN  \n",
       "21  [0.5912, 0.8804, 0.7328, 0.207, 0.9513, 0.5652...       NaN       NaN  \n",
       "22  [0.6671, 0.783, 0.7924, 0.4131, 0.9078, 0.5598...       NaN       NaN  \n",
       "23  [0.2626, 0.339, 0.3211, 0.6321, 0.3759, 0.2833...       NaN       NaN  \n",
       "24  [0.7615, 0.8395, 0.637, 0.8636, 0.9268, 0.6675...       NaN       NaN  \n",
       "25  [0.6536, 0.6791, 0.1552, 0.3645, 0.639, 0.4758...       NaN       NaN  \n",
       "26  [0.8733, 0.9727, 0.5763, 0.6963, 0.9308, 0.640...       NaN       NaN  \n",
       "27  [0.2662, 0.4424, 0.4529, 0.383, 0.1249, 0.1928...       NaN       NaN  \n",
       "28  [0.206, 0.5954, 0.3266, 0.2231, 0.0484, 0.0393...       NaN       NaN  \n",
       "29  [0.1219, 0.494, 0.1478, 0.1143, 0.023, 0.0473,...       NaN       NaN  \n",
       "30  [0.0028, 0.0029, 0.2826, 0.0033, 0.0, 0.0603, ...       NaN       NaN  \n",
       "31  [0.1104, 0.6196, 0.1462, 0.2698, 0.6845, 0.313...       NaN       NaN  \n",
       "32  [0.2579, 0.3515, 0.0756, 0.0766, 0.7992, 0.247...       NaN       NaN  \n",
       "33  [0.8548, 0.7989, 0.9311, 0.8686, 0.9127, 0.856...       NaN       NaN  \n",
       "34  [0.1995, 0.089, 0.8975, 0.5757, 0.2527, 0.8394...       NaN       NaN  \n",
       "35  [0.7853, 0.8963, 0.5836, 0.6592, 0.935, 0.5476...       NaN       NaN  \n",
       "36  [0.9188, 0.9519, 0.7687, 0.7347, 0.9679, 0.673...       NaN       NaN  \n",
       "37  [0.8201, 0.8702, 0.3398, 0.2207, 0.9444, 0.044...       NaN       NaN  \n",
       "38  [0.2867, 0.9334, 0.9321, 0.6996, 0.895, 0.0917...       NaN       NaN  \n",
       "39  [0.4904, 0.9736, 0.9591, 0.6959, 0.9669, 0.460...       NaN       NaN  \n",
       "40  [0.1484, 0.608, 0.7025, 0.1133, 0.6132, 0.2383...       NaN       NaN  \n",
       "41  [0.8403, 0.9712, 0.9568, 0.9187, 0.9474, 0.408...       NaN       NaN  \n",
       "42  [0.859, 0.9512, 0.5412, 0.6357, 0.9755, 0.6094...       NaN       NaN  \n",
       "43  [0.5305, 0.9029, 0.3849, 0.203, 0.9282, 0.0751...       NaN       NaN  \n",
       "44  [0.6458, 0.9472, 0.1524, 0.2528, 0.9282, 0.018...       NaN       NaN  \n",
       "45  [0.7408, 0.9497, 0.4919, 0.5442, 0.9693, 0.389...       NaN       NaN  \n",
       "46  [0.2345, 0.6583, 0.1812, 0.5105, 0.4971, 0.333...       NaN       NaN  \n",
       "47  [0.2984, 0.4354, 0.3191, 0.2481, 0.6767, 0.602...       NaN       NaN  \n",
       "48  [0.7637, 0.4098, 0.9864, 0.8644, 0.37, 0.915, ...       NaN       NaN  \n",
       "49  [0.7378, 0.0429, 0.7464, 0.3743, 0.2093, 0.712...       NaN       NaN  \n",
       "50  [0.493, 0.5357, 0.2686, 0.3738, 0.7803, 0.7089...       NaN       NaN  \n",
       "51  [0.2955, 0.2177, 0.2562, 0.2789, 0.3283, 0.615...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901 : Training: loss:  0.049698945\n",
      "1902 : Training: loss:  0.10213911\n",
      "1903 : Training: loss:  0.07811187\n",
      "1904 : Training: loss:  0.07994918\n",
      "1905 : Training: loss:  0.06368841\n",
      "1906 : Training: loss:  0.064837605\n",
      "1907 : Training: loss:  0.077120304\n",
      "1908 : Training: loss:  0.085234515\n",
      "1909 : Training: loss:  0.053641185\n",
      "1910 : Training: loss:  0.085688695\n",
      "1911 : Training: loss:  0.06856493\n",
      "1912 : Training: loss:  0.040456314\n",
      "1913 : Training: loss:  0.06644042\n",
      "1914 : Training: loss:  0.0717801\n",
      "1915 : Training: loss:  0.06643977\n",
      "1916 : Training: loss:  0.06452275\n",
      "1917 : Training: loss:  0.0949646\n",
      "1918 : Training: loss:  0.061912186\n",
      "1919 : Training: loss:  0.07515057\n",
      "1920 : Training: loss:  0.03504407\n",
      "Validation: Loss:  0.08677757  Accuracy:  0.78846157\n",
      "1921 : Training: loss:  0.05682471\n",
      "1922 : Training: loss:  0.07328686\n",
      "1923 : Training: loss:  0.07232181\n",
      "1924 : Training: loss:  0.06344377\n",
      "1925 : Training: loss:  0.057332095\n",
      "1926 : Training: loss:  0.06094616\n",
      "1927 : Training: loss:  0.07108729\n",
      "1928 : Training: loss:  0.083254345\n",
      "1929 : Training: loss:  0.065046206\n",
      "1930 : Training: loss:  0.0764401\n",
      "1931 : Training: loss:  0.07363967\n",
      "1932 : Training: loss:  0.07411045\n",
      "1933 : Training: loss:  0.06251344\n",
      "1934 : Training: loss:  0.07022022\n",
      "1935 : Training: loss:  0.059731524\n",
      "1936 : Training: loss:  0.030728\n",
      "1937 : Training: loss:  0.052016303\n",
      "1938 : Training: loss:  0.064499006\n",
      "1939 : Training: loss:  0.074788235\n",
      "1940 : Training: loss:  0.044999193\n",
      "Validation: Loss:  0.08531438  Accuracy:  0.78846157\n",
      "1941 : Training: loss:  0.054556802\n",
      "1942 : Training: loss:  0.065018766\n",
      "1943 : Training: loss:  0.05895024\n",
      "1944 : Training: loss:  0.057026334\n",
      "1945 : Training: loss:  0.06578607\n",
      "1946 : Training: loss:  0.05224028\n",
      "1947 : Training: loss:  0.09424857\n",
      "1948 : Training: loss:  0.07514681\n",
      "1949 : Training: loss:  0.10216734\n",
      "1950 : Training: loss:  0.075140014\n",
      "1951 : Training: loss:  0.09287639\n",
      "1952 : Training: loss:  0.042397365\n",
      "1953 : Training: loss:  0.0838858\n",
      "1954 : Training: loss:  0.0711863\n",
      "1955 : Training: loss:  0.04829679\n",
      "1956 : Training: loss:  0.03637537\n",
      "1957 : Training: loss:  0.045621917\n",
      "1958 : Training: loss:  0.061988592\n",
      "1959 : Training: loss:  0.060702063\n",
      "1960 : Training: loss:  0.068663016\n",
      "Validation: Loss:  0.08398938  Accuracy:  0.78846157\n",
      "1961 : Training: loss:  0.10438599\n",
      "1962 : Training: loss:  0.066304624\n",
      "1963 : Training: loss:  0.061951146\n",
      "1964 : Training: loss:  0.06319423\n",
      "1965 : Training: loss:  0.051431872\n",
      "1966 : Training: loss:  0.09210493\n",
      "1967 : Training: loss:  0.09252523\n",
      "1968 : Training: loss:  0.02923459\n",
      "1969 : Training: loss:  0.064725906\n",
      "1970 : Training: loss:  0.04432452\n",
      "1971 : Training: loss:  0.06172416\n",
      "1972 : Training: loss:  0.053238146\n",
      "1973 : Training: loss:  0.097268924\n",
      "1974 : Training: loss:  0.04730769\n",
      "1975 : Training: loss:  0.05164323\n",
      "1976 : Training: loss:  0.052472476\n",
      "1977 : Training: loss:  0.0658154\n",
      "1978 : Training: loss:  0.0554878\n",
      "1979 : Training: loss:  0.045929313\n",
      "1980 : Training: loss:  0.09862351\n",
      "Validation: Loss:  0.082923025  Accuracy:  0.78846157\n",
      "1981 : Training: loss:  0.07042647\n",
      "1982 : Training: loss:  0.06330729\n",
      "1983 : Training: loss:  0.08032693\n",
      "1984 : Training: loss:  0.06313291\n",
      "1985 : Training: loss:  0.04002164\n",
      "1986 : Training: loss:  0.057240468\n",
      "1987 : Training: loss:  0.044352982\n",
      "1988 : Training: loss:  0.060172636\n",
      "1989 : Training: loss:  0.03528076\n",
      "1990 : Training: loss:  0.049185805\n",
      "1991 : Training: loss:  0.09836713\n",
      "1992 : Training: loss:  0.07129633\n",
      "1993 : Training: loss:  0.057603076\n",
      "1994 : Training: loss:  0.05835886\n",
      "1995 : Training: loss:  0.08277356\n",
      "1996 : Training: loss:  0.05663684\n",
      "1997 : Training: loss:  0.022114456\n",
      "1998 : Training: loss:  0.05617834\n",
      "1999 : Training: loss:  0.038861856\n",
      "2000 : Training: loss:  0.082603104\n",
      "Validation: Loss:  0.081896976  Accuracy:  0.78846157\n",
      "2001 : Training: loss:  0.03712077\n",
      "2002 : Training: loss:  0.047719948\n",
      "2003 : Training: loss:  0.071363226\n",
      "2004 : Training: loss:  0.051521488\n",
      "2005 : Training: loss:  0.07146339\n",
      "2006 : Training: loss:  0.031616267\n",
      "2007 : Training: loss:  0.04427616\n",
      "2008 : Training: loss:  0.074722484\n",
      "2009 : Training: loss:  0.035397377\n",
      "2010 : Training: loss:  0.04273195\n",
      "2011 : Training: loss:  0.043854684\n",
      "2012 : Training: loss:  0.05517469\n",
      "2013 : Training: loss:  0.07534255\n",
      "2014 : Training: loss:  0.078979984\n",
      "2015 : Training: loss:  0.054660983\n",
      "2016 : Training: loss:  0.052926812\n",
      "2017 : Training: loss:  0.06611662\n",
      "2018 : Training: loss:  0.07029663\n",
      "2019 : Training: loss:  0.04725222\n",
      "2020 : Training: loss:  0.058612727\n",
      "Validation: Loss:  0.081020966  Accuracy:  0.78846157\n",
      "2021 : Training: loss:  0.066337936\n",
      "2022 : Training: loss:  0.07614464\n",
      "2023 : Training: loss:  0.040672317\n",
      "2024 : Training: loss:  0.059741896\n",
      "2025 : Training: loss:  0.057055775\n",
      "2026 : Training: loss:  0.0505838\n",
      "2027 : Training: loss:  0.07657609\n",
      "2028 : Training: loss:  0.06049213\n",
      "2029 : Training: loss:  0.03375437\n",
      "2030 : Training: loss:  0.037465207\n",
      "2031 : Training: loss:  0.055627063\n",
      "2032 : Training: loss:  0.060110874\n",
      "2033 : Training: loss:  0.064930044\n",
      "2034 : Training: loss:  0.072238\n",
      "2035 : Training: loss:  0.045411076\n",
      "2036 : Training: loss:  0.031907916\n",
      "2037 : Training: loss:  0.06263859\n",
      "2038 : Training: loss:  0.048102777\n",
      "2039 : Training: loss:  0.084733434\n",
      "2040 : Training: loss:  0.0878037\n",
      "Validation: Loss:  0.08009791  Accuracy:  0.7692308\n",
      "2041 : Training: loss:  0.058020405\n",
      "2042 : Training: loss:  0.062167257\n",
      "2043 : Training: loss:  0.05779682\n",
      "2044 : Training: loss:  0.07220389\n",
      "2045 : Training: loss:  0.034689177\n",
      "2046 : Training: loss:  0.06982975\n",
      "2047 : Training: loss:  0.07033409\n",
      "2048 : Training: loss:  0.03676353\n",
      "2049 : Training: loss:  0.05715547\n",
      "2050 : Training: loss:  0.055616375\n",
      "2051 : Training: loss:  0.060621157\n",
      "2052 : Training: loss:  0.06379973\n",
      "2053 : Training: loss:  0.062160723\n",
      "2054 : Training: loss:  0.057459377\n",
      "2055 : Training: loss:  0.06844529\n",
      "2056 : Training: loss:  0.048574787\n",
      "2057 : Training: loss:  0.021126796\n",
      "2058 : Training: loss:  0.058430657\n",
      "2059 : Training: loss:  0.06780953\n",
      "2060 : Training: loss:  0.043435264\n",
      "Validation: Loss:  0.079201296  Accuracy:  0.75\n",
      "2061 : Training: loss:  0.0740698\n",
      "2062 : Training: loss:  0.050330594\n",
      "2063 : Training: loss:  0.051878955\n",
      "2064 : Training: loss:  0.03304608\n",
      "2065 : Training: loss:  0.06670667\n",
      "2066 : Training: loss:  0.06452635\n",
      "2067 : Training: loss:  0.03190349\n",
      "2068 : Training: loss:  0.05960321\n",
      "2069 : Training: loss:  0.059417292\n",
      "2070 : Training: loss:  0.05978956\n",
      "2071 : Training: loss:  0.0866203\n",
      "2072 : Training: loss:  0.08894634\n",
      "2073 : Training: loss:  0.060087178\n",
      "2074 : Training: loss:  0.041787345\n",
      "2075 : Training: loss:  0.07623577\n",
      "2076 : Training: loss:  0.0775557\n",
      "2077 : Training: loss:  0.073381215\n",
      "2078 : Training: loss:  0.04948063\n",
      "2079 : Training: loss:  0.06657885\n",
      "2080 : Training: loss:  0.08040933\n",
      "Validation: Loss:  0.07805805  Accuracy:  0.7692308\n",
      "2081 : Training: loss:  0.04644008\n",
      "2082 : Training: loss:  0.05497692\n",
      "2083 : Training: loss:  0.0650478\n",
      "2084 : Training: loss:  0.05795594\n",
      "2085 : Training: loss:  0.046205577\n",
      "2086 : Training: loss:  0.07208691\n",
      "2087 : Training: loss:  0.07927037\n",
      "2088 : Training: loss:  0.037666325\n",
      "2089 : Training: loss:  0.04657007\n",
      "2090 : Training: loss:  0.08483189\n",
      "2091 : Training: loss:  0.07887877\n",
      "2092 : Training: loss:  0.042143486\n",
      "2093 : Training: loss:  0.059157643\n",
      "2094 : Training: loss:  0.06409259\n",
      "2095 : Training: loss:  0.052018538\n",
      "2096 : Training: loss:  0.051800534\n",
      "2097 : Training: loss:  0.060097322\n",
      "2098 : Training: loss:  0.0677925\n",
      "2099 : Training: loss:  0.055253275\n",
      "2100 : Training: loss:  0.05254742\n",
      "Validation: Loss:  0.07726416  Accuracy:  0.7692308\n",
      "2101 : Training: loss:  0.056219094\n",
      "2102 : Training: loss:  0.059798427\n",
      "2103 : Training: loss:  0.06600684\n",
      "2104 : Training: loss:  0.035594814\n",
      "2105 : Training: loss:  0.046244238\n",
      "2106 : Training: loss:  0.034776427\n",
      "2107 : Training: loss:  0.07621639\n",
      "2108 : Training: loss:  0.049068436\n",
      "2109 : Training: loss:  0.06244095\n",
      "2110 : Training: loss:  0.0658305\n",
      "2111 : Training: loss:  0.061994076\n",
      "2112 : Training: loss:  0.07272633\n",
      "2113 : Training: loss:  0.060013764\n",
      "2114 : Training: loss:  0.062476296\n",
      "2115 : Training: loss:  0.033364575\n",
      "2116 : Training: loss:  0.07695007\n",
      "2117 : Training: loss:  0.049796\n",
      "2118 : Training: loss:  0.05196223\n",
      "2119 : Training: loss:  0.08438998\n",
      "2120 : Training: loss:  0.067270055\n",
      "Validation: Loss:  0.07642165  Accuracy:  0.8076923\n",
      "2121 : Training: loss:  0.06126504\n",
      "2122 : Training: loss:  0.039899264\n",
      "2123 : Training: loss:  0.06887296\n",
      "2124 : Training: loss:  0.06692215\n",
      "2125 : Training: loss:  0.08658404\n",
      "2126 : Training: loss:  0.06315347\n",
      "2127 : Training: loss:  0.05675005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2128 : Training: loss:  0.055336278\n",
      "2129 : Training: loss:  0.030509282\n",
      "2130 : Training: loss:  0.072239906\n",
      "2131 : Training: loss:  0.038593616\n",
      "2132 : Training: loss:  0.06975001\n",
      "2133 : Training: loss:  0.07365199\n",
      "2134 : Training: loss:  0.035639957\n",
      "2135 : Training: loss:  0.038235206\n",
      "2136 : Training: loss:  0.07049266\n",
      "2137 : Training: loss:  0.04822965\n",
      "2138 : Training: loss:  0.031696387\n",
      "2139 : Training: loss:  0.043820046\n",
      "2140 : Training: loss:  0.071272425\n",
      "Validation: Loss:  0.075476184  Accuracy:  0.8269231\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3133, 0.0846, 0.0188, 0.0337, 0.0199, 0.035...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3138, 0.451, 0.0918, 0.4126, 0.2127, 0.0928...</td>\n",
       "      <td>[0.6862, 0.549, 0.9082, 0.5874, 0.7873, 0.9072...</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.075476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5798, 0.3986, 0.0098, 0.0322, 0.1331, 0.036...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7657, 0.7223, 0.2338, 0.6427, 0.5747, 0.240...</td>\n",
       "      <td>[0.2343, 0.2777, 0.7662, 0.3573, 0.4253, 0.759...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.5212, 0.1603, 0.0105, 0.08, 0.0307, 0.0587,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5559, 0.6209, 0.1717, 0.604, 0.2014, 0.1276...</td>\n",
       "      <td>[0.4441, 0.3791, 0.8283, 0.396, 0.7986, 0.8724...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0403, 0.1627, 0.0041, 0.0068, 0.0926, 0.034...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6018, 0.504, 0.1636, 0.691, 0.8058, 0.4631,...</td>\n",
       "      <td>[0.3982, 0.496, 0.8364, 0.309, 0.1942, 0.5369,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0996, 0.0854, 0.0152, 0.0073, 0.0095, 0.030...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1529, 0.224, 0.0413, 0.3001, 0.2223, 0.0423...</td>\n",
       "      <td>[0.8471, 0.776, 0.9587, 0.6999, 0.7777, 0.9577...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0313, 0.0178, 0.1146, 0.0128, 0.0074, 0.052...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1958, 0.0644, 0.175, 0.2238, 0.1451, 0.4964...</td>\n",
       "      <td>[0.8042, 0.9356, 0.825, 0.7762, 0.8549, 0.5036...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0355, 0.0077, 0.0013, 0.7474, 0.0046, 0.001...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9763, 0.8882, 0.9627, 0.9677, 0.1762, 0.953...</td>\n",
       "      <td>[0.0237, 0.1118, 0.0373, 0.0323, 0.8238, 0.046...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0333, 0.0178, 0.0113, 0.4623, 0.0058, 0.005...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9086, 0.4074, 0.8597, 0.7378, 0.1785, 0.934...</td>\n",
       "      <td>[0.0914, 0.5926, 0.1403, 0.2622, 0.8215, 0.065...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0483, 0.0151, 0.0206, 0.3971, 0.0121, 0.015...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8606, 0.5586, 0.9306, 0.6029, 0.1822, 0.844...</td>\n",
       "      <td>[0.1394, 0.4414, 0.0694, 0.3971, 0.8178, 0.155...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0109, 0.0719, 0.0054, 0.0178, 0.6374, 0.100...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8784, 0.9638, 0.8171, 0.8216, 0.97, 0.7683,...</td>\n",
       "      <td>[0.1216, 0.0362, 0.1829, 0.1784, 0.03, 0.2317,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0494, 0.3386, 0.004, 0.006, 0.3498, 0.2609,...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.6715, 0.9488, 0.1353, 0.5932, 0.8417, 0.656...</td>\n",
       "      <td>[0.3285, 0.0512, 0.8647, 0.4068, 0.1583, 0.343...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.008, 0.0282, 0.0046, 0.0059, 0.0917, 0.2325...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.471, 0.8532, 0.2444, 0.5224, 0.8433, 0.5549...</td>\n",
       "      <td>[0.529, 0.1468, 0.7556, 0.4776, 0.1567, 0.4451...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0017, 0.0092, 0.0142, 0.0035, 0.005, 0.0042...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6628, 0.659, 0.8644, 0.9673, 0.8194, 0.6993...</td>\n",
       "      <td>[0.3372, 0.341, 0.1356, 0.0327, 0.1806, 0.3007...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0735, 0.0667, 0.145, 0.0877, 0.0055, 0.0909...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.3864, 0.1014, 0.2762, 0.6103, 0.7349, 0.955...</td>\n",
       "      <td>[0.6136, 0.8986, 0.7238, 0.3897, 0.2651, 0.044...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0207, 0.0196, 0.0014, 0.0384, 0.0332, 0.004...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7659, 0.7493, 0.5852, 0.9402, 0.6735, 0.576...</td>\n",
       "      <td>[0.2341, 0.2507, 0.4148, 0.0598, 0.3265, 0.423...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0552, 0.0251, 0.0049, 0.0072, 0.0325, 0.012...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2429, 0.8266, 0.4053, 0.5376, 0.6292, 0.134...</td>\n",
       "      <td>[0.7571, 0.1734, 0.5947, 0.4624, 0.3708, 0.866...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.02, 0.0089, 0.081, 0.0069, 0.002, 0.0276, 0...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.2185, 0.1208, 0.1107, 0.2829, 0.1661, 0.358...</td>\n",
       "      <td>[0.7815, 0.8792, 0.8893, 0.7171, 0.8339, 0.641...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0233, 0.0114, 0.0178, 0.0147, 0.0058, 0.009...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5426, 0.5746, 0.5089, 0.6844, 0.4306, 0.642...</td>\n",
       "      <td>[0.4574, 0.4254, 0.4911, 0.3156, 0.5694, 0.357...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0662, 0.0102, 0.0462, 0.0071, 0.0041, 0.023...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1577, 0.4749, 0.0886, 0.1025, 0.4905, 0.280...</td>\n",
       "      <td>[0.8423, 0.5251, 0.9114, 0.8975, 0.5095, 0.719...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0465, 0.0107, 0.0494, 0.0052, 0.0053, 0.029...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1925, 0.705, 0.1059, 0.2571, 0.641, 0.1802,...</td>\n",
       "      <td>[0.8075, 0.295, 0.8941, 0.7429, 0.359, 0.8198,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0728, 0.0312, 0.0166, 0.0524, 0.0051, 0.009...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4697, 0.3385, 0.6335, 0.8765, 0.1243, 0.350...</td>\n",
       "      <td>[0.5303, 0.6615, 0.3665, 0.1235, 0.8757, 0.649...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0202, 0.0081, 0.0199, 0.0286, 0.0016, 0.003...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4264, 0.1211, 0.3178, 0.8486, 0.0467, 0.501...</td>\n",
       "      <td>[0.5736, 0.8789, 0.6822, 0.1514, 0.9533, 0.499...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0321, 0.0143, 0.0232, 0.0232, 0.0036, 0.017...</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.3453, 0.2355, 0.2265, 0.6546, 0.0877, 0.488...</td>\n",
       "      <td>[0.6547, 0.7645, 0.7735, 0.3454, 0.9123, 0.511...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0365, 0.0159, 0.0217, 0.0354, 0.0256, 0.06,...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7469, 0.6954, 0.7024, 0.3535, 0.6347, 0.756...</td>\n",
       "      <td>[0.2531, 0.3046, 0.2976, 0.6465, 0.3653, 0.243...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0549, 0.0062, 0.0514, 0.0164, 0.004, 0.0195...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.2752, 0.1922, 0.4109, 0.1538, 0.0707, 0.401...</td>\n",
       "      <td>[0.7248, 0.8078, 0.5891, 0.8462, 0.9293, 0.598...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0118, 0.0109, 0.0151, 0.0196, 0.0139, 0.009...</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.3618, 0.3582, 0.8728, 0.6949, 0.3573, 0.575...</td>\n",
       "      <td>[0.6382, 0.6418, 0.1272, 0.3051, 0.6427, 0.424...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0176, 0.0065, 0.0213, 0.0048, 0.0016, 0.003...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1287, 0.0268, 0.4851, 0.3725, 0.0658, 0.429...</td>\n",
       "      <td>[0.8713, 0.9732, 0.5149, 0.6275, 0.9342, 0.571...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0013, 0.0068, 0.0063, 0.0057, 0.0142, 0.008...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7496, 0.5776, 0.5619, 0.6884, 0.8685, 0.841...</td>\n",
       "      <td>[0.2504, 0.4224, 0.4381, 0.3116, 0.1315, 0.158...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.002, 0.0143, 0.0151, 0.0216, 0.0054, 0.0071...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7835, 0.3764, 0.6848, 0.8221, 0.9469, 0.967...</td>\n",
       "      <td>[0.2165, 0.6236, 0.3152, 0.1779, 0.0531, 0.032...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0009, 0.0124, 0.0023, 0.007, 0.0233, 0.0039...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8686, 0.5007, 0.857, 0.9153, 0.9766, 0.9587...</td>\n",
       "      <td>[0.1314, 0.4993, 0.143, 0.0847, 0.0234, 0.0413...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.1923, 0.7136, 0.155, 0.1109, 0.8081, 0.6103...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9911, 0.9936, 0.5994, 0.9919, 1.0, 0.8875, ...</td>\n",
       "      <td>[0.0089, 0.0064, 0.4006, 0.0081, 0.0, 0.1125, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0058, 0.0113, 0.0161, 0.0528, 0.0044, 0.003...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9065, 0.3838, 0.883, 0.7843, 0.3011, 0.7462...</td>\n",
       "      <td>[0.0935, 0.6162, 0.117, 0.2157, 0.6989, 0.2538...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0057, 0.0054, 0.0085, 0.0543, 0.0023, 0.001...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7679, 0.6811, 0.9451, 0.9493, 0.1781, 0.792...</td>\n",
       "      <td>[0.2321, 0.3189, 0.0549, 0.0507, 0.8219, 0.207...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.049, 0.0098, 0.0308, 0.0094, 0.0025, 0.0132...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.1619, 0.2442, 0.0741, 0.1731, 0.085, 0.1744...</td>\n",
       "      <td>[0.8381, 0.7558, 0.9259, 0.8269, 0.915, 0.8256...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.058, 0.0429, 0.0242, 0.0284, 0.1336, 0.0616...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8281, 0.9316, 0.1019, 0.4606, 0.7745, 0.169...</td>\n",
       "      <td>[0.1719, 0.0684, 0.8981, 0.5394, 0.2255, 0.830...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.045, 0.0113, 0.0155, 0.0128, 0.0017, 0.0076...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2248, 0.1121, 0.4597, 0.4229, 0.0616, 0.514...</td>\n",
       "      <td>[0.7752, 0.8879, 0.5403, 0.5771, 0.9384, 0.485...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0343, 0.0094, 0.0229, 0.0072, 0.0011, 0.006...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0716, 0.0478, 0.2597, 0.3247, 0.0281, 0.395...</td>\n",
       "      <td>[0.9284, 0.9522, 0.7403, 0.6753, 0.9719, 0.604...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0068, 0.0077, 0.0096, 0.0199, 0.0007, 0.001...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1261, 0.1089, 0.6873, 0.8196, 0.045, 0.9636...</td>\n",
       "      <td>[0.8739, 0.8911, 0.3127, 0.1804, 0.955, 0.0364...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0066, 0.0016, 0.0243, 0.0169, 0.0011, 0.002...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7374, 0.061, 0.0582, 0.3483, 0.0923, 0.9311...</td>\n",
       "      <td>[0.2626, 0.939, 0.9418, 0.6517, 0.9077, 0.0689...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0149, 0.0027, 0.0214, 0.0138, 0.001, 0.003,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5459, 0.0273, 0.041, 0.3778, 0.0315, 0.6252...</td>\n",
       "      <td>[0.4541, 0.9727, 0.959, 0.6222, 0.9685, 0.3748...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0127, 0.017, 0.0038, 0.037, 0.0071, 0.0034,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.8593, 0.3766, 0.3017, 0.9153, 0.3669, 0.792...</td>\n",
       "      <td>[0.1407, 0.6234, 0.6983, 0.0847, 0.6331, 0.207...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0366, 0.0041, 0.0393, 0.014, 0.0011, 0.0073...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1766, 0.0287, 0.0418, 0.0962, 0.0494, 0.670...</td>\n",
       "      <td>[0.8234, 0.9713, 0.9582, 0.9038, 0.9506, 0.329...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0198, 0.0061, 0.0103, 0.0074, 0.0005, 0.001...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1369, 0.0485, 0.5012, 0.422, 0.0216, 0.439,...</td>\n",
       "      <td>[0.8631, 0.9515, 0.4988, 0.578, 0.9784, 0.561,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0085, 0.0058, 0.0059, 0.0111, 0.0005, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3951, 0.0775, 0.6429, 0.8248, 0.0583, 0.936...</td>\n",
       "      <td>[0.6049, 0.9225, 0.3571, 0.1752, 0.9417, 0.064...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0047, 0.0046, 0.0057, 0.0087, 0.0004, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2769, 0.0376, 0.8623, 0.7844, 0.0562, 0.984...</td>\n",
       "      <td>[0.7231, 0.9624, 0.1377, 0.2156, 0.9438, 0.015...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0102, 0.004, 0.006, 0.0069, 0.0003, 0.0004,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2291, 0.0451, 0.5459, 0.5229, 0.0249, 0.666...</td>\n",
       "      <td>[0.7709, 0.9549, 0.4541, 0.4771, 0.9751, 0.334...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0128, 0.0119, 0.0789, 0.0357, 0.0084, 0.033...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7847, 0.3557, 0.8532, 0.5037, 0.5065, 0.716...</td>\n",
       "      <td>[0.2153, 0.6443, 0.1468, 0.4963, 0.4935, 0.283...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0246, 0.0699, 0.0091, 0.0216, 0.0226, 0.016...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.7483, 0.5846, 0.7326, 0.7761, 0.3191, 0.415...</td>\n",
       "      <td>[0.2517, 0.4154, 0.2674, 0.2239, 0.6809, 0.585...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.012, 0.0102, 0.0064, 0.0008, 0.0119, 0.0185...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2324, 0.6272, 0.0121, 0.1519, 0.664, 0.0913...</td>\n",
       "      <td>[0.7676, 0.3728, 0.9879, 0.8481, 0.336, 0.9087...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.027, 0.0353, 0.0087, 0.0113, 0.0702, 0.1138...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2289, 0.9636, 0.2533, 0.6428, 0.797, 0.2968...</td>\n",
       "      <td>[0.7711, 0.0364, 0.7467, 0.3572, 0.203, 0.7032...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0456, 0.0215, 0.0101, 0.0215, 0.0119, 0.010...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.5042, 0.4733, 0.7595, 0.6832, 0.2094, 0.293...</td>\n",
       "      <td>[0.4958, 0.5267, 0.2405, 0.3168, 0.7906, 0.706...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0074, 0.0085, 0.0042, 0.0191, 0.0398, 0.002...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7151, 0.8171, 0.771, 0.7595, 0.6622, 0.4141...</td>\n",
       "      <td>[0.2849, 0.1829, 0.229, 0.2405, 0.3378, 0.5859...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3133, 0.0846, 0.0188, 0.0337, 0.0199, 0.035...               0   \n",
       "1   [0.5798, 0.3986, 0.0098, 0.0322, 0.1331, 0.036...               0   \n",
       "2   [0.5212, 0.1603, 0.0105, 0.08, 0.0307, 0.0587,...               0   \n",
       "3   [0.0403, 0.1627, 0.0041, 0.0068, 0.0926, 0.034...               1   \n",
       "4   [0.0996, 0.0854, 0.0152, 0.0073, 0.0095, 0.030...               1   \n",
       "5   [0.0313, 0.0178, 0.1146, 0.0128, 0.0074, 0.052...               2   \n",
       "6   [0.0355, 0.0077, 0.0013, 0.7474, 0.0046, 0.001...               3   \n",
       "7   [0.0333, 0.0178, 0.0113, 0.4623, 0.0058, 0.005...               3   \n",
       "8   [0.0483, 0.0151, 0.0206, 0.3971, 0.0121, 0.015...               3   \n",
       "9   [0.0109, 0.0719, 0.0054, 0.0178, 0.6374, 0.100...               4   \n",
       "10  [0.0494, 0.3386, 0.004, 0.006, 0.3498, 0.2609,...               4   \n",
       "11  [0.008, 0.0282, 0.0046, 0.0059, 0.0917, 0.2325...               5   \n",
       "12  [0.0017, 0.0092, 0.0142, 0.0035, 0.005, 0.0042...               6   \n",
       "13  [0.0735, 0.0667, 0.145, 0.0877, 0.0055, 0.0909...               7   \n",
       "14  [0.0207, 0.0196, 0.0014, 0.0384, 0.0332, 0.004...               8   \n",
       "15  [0.0552, 0.0251, 0.0049, 0.0072, 0.0325, 0.012...               8   \n",
       "16  [0.02, 0.0089, 0.081, 0.0069, 0.002, 0.0276, 0...               9   \n",
       "17  [0.0233, 0.0114, 0.0178, 0.0147, 0.0058, 0.009...               9   \n",
       "18  [0.0662, 0.0102, 0.0462, 0.0071, 0.0041, 0.023...              10   \n",
       "19  [0.0465, 0.0107, 0.0494, 0.0052, 0.0053, 0.029...              10   \n",
       "20  [0.0728, 0.0312, 0.0166, 0.0524, 0.0051, 0.009...              11   \n",
       "21  [0.0202, 0.0081, 0.0199, 0.0286, 0.0016, 0.003...              11   \n",
       "22  [0.0321, 0.0143, 0.0232, 0.0232, 0.0036, 0.017...              12   \n",
       "23  [0.0365, 0.0159, 0.0217, 0.0354, 0.0256, 0.06,...              13   \n",
       "24  [0.0549, 0.0062, 0.0514, 0.0164, 0.004, 0.0195...              13   \n",
       "25  [0.0118, 0.0109, 0.0151, 0.0196, 0.0139, 0.009...              14   \n",
       "26  [0.0176, 0.0065, 0.0213, 0.0048, 0.0016, 0.003...              14   \n",
       "27  [0.0013, 0.0068, 0.0063, 0.0057, 0.0142, 0.008...              15   \n",
       "28  [0.002, 0.0143, 0.0151, 0.0216, 0.0054, 0.0071...              15   \n",
       "29  [0.0009, 0.0124, 0.0023, 0.007, 0.0233, 0.0039...              15   \n",
       "30  [0.1923, 0.7136, 0.155, 0.1109, 0.8081, 0.6103...              16   \n",
       "31  [0.0058, 0.0113, 0.0161, 0.0528, 0.0044, 0.003...              17   \n",
       "32  [0.0057, 0.0054, 0.0085, 0.0543, 0.0023, 0.001...              17   \n",
       "33  [0.049, 0.0098, 0.0308, 0.0094, 0.0025, 0.0132...              18   \n",
       "34  [0.058, 0.0429, 0.0242, 0.0284, 0.1336, 0.0616...              19   \n",
       "35  [0.045, 0.0113, 0.0155, 0.0128, 0.0017, 0.0076...              20   \n",
       "36  [0.0343, 0.0094, 0.0229, 0.0072, 0.0011, 0.006...              21   \n",
       "37  [0.0068, 0.0077, 0.0096, 0.0199, 0.0007, 0.001...              21   \n",
       "38  [0.0066, 0.0016, 0.0243, 0.0169, 0.0011, 0.002...              22   \n",
       "39  [0.0149, 0.0027, 0.0214, 0.0138, 0.001, 0.003,...              22   \n",
       "40  [0.0127, 0.017, 0.0038, 0.037, 0.0071, 0.0034,...              22   \n",
       "41  [0.0366, 0.0041, 0.0393, 0.014, 0.0011, 0.0073...              22   \n",
       "42  [0.0198, 0.0061, 0.0103, 0.0074, 0.0005, 0.001...              23   \n",
       "43  [0.0085, 0.0058, 0.0059, 0.0111, 0.0005, 0.000...              23   \n",
       "44  [0.0047, 0.0046, 0.0057, 0.0087, 0.0004, 0.000...              23   \n",
       "45  [0.0102, 0.004, 0.006, 0.0069, 0.0003, 0.0004,...              23   \n",
       "46  [0.0128, 0.0119, 0.0789, 0.0357, 0.0084, 0.033...              24   \n",
       "47  [0.0246, 0.0699, 0.0091, 0.0216, 0.0226, 0.016...              24   \n",
       "48  [0.012, 0.0102, 0.0064, 0.0008, 0.0119, 0.0185...              25   \n",
       "49  [0.027, 0.0353, 0.0087, 0.0113, 0.0702, 0.1138...              25   \n",
       "50  [0.0456, 0.0215, 0.0101, 0.0215, 0.0119, 0.010...              26   \n",
       "51  [0.0074, 0.0085, 0.0042, 0.0191, 0.0398, 0.002...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.3138, 0.451, 0.0918, 0.4126, 0.2127, 0.0928...   \n",
       "1                  0  [0.7657, 0.7223, 0.2338, 0.6427, 0.5747, 0.240...   \n",
       "2                  0  [0.5559, 0.6209, 0.1717, 0.604, 0.2014, 0.1276...   \n",
       "3                  1  [0.6018, 0.504, 0.1636, 0.691, 0.8058, 0.4631,...   \n",
       "4                  0  [0.1529, 0.224, 0.0413, 0.3001, 0.2223, 0.0423...   \n",
       "5                 10  [0.1958, 0.0644, 0.175, 0.2238, 0.1451, 0.4964...   \n",
       "6                  3  [0.9763, 0.8882, 0.9627, 0.9677, 0.1762, 0.953...   \n",
       "7                  3  [0.9086, 0.4074, 0.8597, 0.7378, 0.1785, 0.934...   \n",
       "8                  3  [0.8606, 0.5586, 0.9306, 0.6029, 0.1822, 0.844...   \n",
       "9                  4  [0.8784, 0.9638, 0.8171, 0.8216, 0.97, 0.7683,...   \n",
       "10                 4  [0.6715, 0.9488, 0.1353, 0.5932, 0.8417, 0.656...   \n",
       "11                 5  [0.471, 0.8532, 0.2444, 0.5224, 0.8433, 0.5549...   \n",
       "12                 6  [0.6628, 0.659, 0.8644, 0.9673, 0.8194, 0.6993...   \n",
       "13                 7  [0.3864, 0.1014, 0.2762, 0.6103, 0.7349, 0.955...   \n",
       "14                 8  [0.7659, 0.7493, 0.5852, 0.9402, 0.6735, 0.576...   \n",
       "15                 8  [0.2429, 0.8266, 0.4053, 0.5376, 0.6292, 0.134...   \n",
       "16                 9  [0.2185, 0.1208, 0.1107, 0.2829, 0.1661, 0.358...   \n",
       "17                 9  [0.5426, 0.5746, 0.5089, 0.6844, 0.4306, 0.642...   \n",
       "18                10  [0.1577, 0.4749, 0.0886, 0.1025, 0.4905, 0.280...   \n",
       "19                10  [0.1925, 0.705, 0.1059, 0.2571, 0.641, 0.1802,...   \n",
       "20                11  [0.4697, 0.3385, 0.6335, 0.8765, 0.1243, 0.350...   \n",
       "21                22  [0.4264, 0.1211, 0.3178, 0.8486, 0.0467, 0.501...   \n",
       "22                22  [0.3453, 0.2355, 0.2265, 0.6546, 0.0877, 0.488...   \n",
       "23                13  [0.7469, 0.6954, 0.7024, 0.3535, 0.6347, 0.756...   \n",
       "24                13  [0.2752, 0.1922, 0.4109, 0.1538, 0.0707, 0.401...   \n",
       "25                11  [0.3618, 0.3582, 0.8728, 0.6949, 0.3573, 0.575...   \n",
       "26                14  [0.1287, 0.0268, 0.4851, 0.3725, 0.0658, 0.429...   \n",
       "27                15  [0.7496, 0.5776, 0.5619, 0.6884, 0.8685, 0.841...   \n",
       "28                15  [0.7835, 0.3764, 0.6848, 0.8221, 0.9469, 0.967...   \n",
       "29                15  [0.8686, 0.5007, 0.857, 0.9153, 0.9766, 0.9587...   \n",
       "30                16  [0.9911, 0.9936, 0.5994, 0.9919, 1.0, 0.8875, ...   \n",
       "31                17  [0.9065, 0.3838, 0.883, 0.7843, 0.3011, 0.7462...   \n",
       "32                17  [0.7679, 0.6811, 0.9451, 0.9493, 0.1781, 0.792...   \n",
       "33                18  [0.1619, 0.2442, 0.0741, 0.1731, 0.085, 0.1744...   \n",
       "34                19  [0.8281, 0.9316, 0.1019, 0.4606, 0.7745, 0.169...   \n",
       "35                10  [0.2248, 0.1121, 0.4597, 0.4229, 0.0616, 0.514...   \n",
       "36                23  [0.0716, 0.0478, 0.2597, 0.3247, 0.0281, 0.395...   \n",
       "37                23  [0.1261, 0.1089, 0.6873, 0.8196, 0.045, 0.9636...   \n",
       "38                22  [0.7374, 0.061, 0.0582, 0.3483, 0.0923, 0.9311...   \n",
       "39                22  [0.5459, 0.0273, 0.041, 0.3778, 0.0315, 0.6252...   \n",
       "40                22  [0.8593, 0.3766, 0.3017, 0.9153, 0.3669, 0.792...   \n",
       "41                22  [0.1766, 0.0287, 0.0418, 0.0962, 0.0494, 0.670...   \n",
       "42                23  [0.1369, 0.0485, 0.5012, 0.422, 0.0216, 0.439,...   \n",
       "43                23  [0.3951, 0.0775, 0.6429, 0.8248, 0.0583, 0.936...   \n",
       "44                23  [0.2769, 0.0376, 0.8623, 0.7844, 0.0562, 0.984...   \n",
       "45                23  [0.2291, 0.0451, 0.5459, 0.5229, 0.0249, 0.666...   \n",
       "46                13  [0.7847, 0.3557, 0.8532, 0.5037, 0.5065, 0.716...   \n",
       "47                24  [0.7483, 0.5846, 0.7326, 0.7761, 0.3191, 0.415...   \n",
       "48                25  [0.2324, 0.6272, 0.0121, 0.1519, 0.664, 0.0913...   \n",
       "49                25  [0.2289, 0.9636, 0.2533, 0.6428, 0.797, 0.2968...   \n",
       "50                26  [0.5042, 0.4733, 0.7595, 0.6832, 0.2094, 0.293...   \n",
       "51                26  [0.7151, 0.8171, 0.771, 0.7595, 0.6622, 0.4141...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.6862, 0.549, 0.9082, 0.5874, 0.7873, 0.9072...  0.826923  0.075476  \n",
       "1   [0.2343, 0.2777, 0.7662, 0.3573, 0.4253, 0.759...       NaN       NaN  \n",
       "2   [0.4441, 0.3791, 0.8283, 0.396, 0.7986, 0.8724...       NaN       NaN  \n",
       "3   [0.3982, 0.496, 0.8364, 0.309, 0.1942, 0.5369,...       NaN       NaN  \n",
       "4   [0.8471, 0.776, 0.9587, 0.6999, 0.7777, 0.9577...       NaN       NaN  \n",
       "5   [0.8042, 0.9356, 0.825, 0.7762, 0.8549, 0.5036...       NaN       NaN  \n",
       "6   [0.0237, 0.1118, 0.0373, 0.0323, 0.8238, 0.046...       NaN       NaN  \n",
       "7   [0.0914, 0.5926, 0.1403, 0.2622, 0.8215, 0.065...       NaN       NaN  \n",
       "8   [0.1394, 0.4414, 0.0694, 0.3971, 0.8178, 0.155...       NaN       NaN  \n",
       "9   [0.1216, 0.0362, 0.1829, 0.1784, 0.03, 0.2317,...       NaN       NaN  \n",
       "10  [0.3285, 0.0512, 0.8647, 0.4068, 0.1583, 0.343...       NaN       NaN  \n",
       "11  [0.529, 0.1468, 0.7556, 0.4776, 0.1567, 0.4451...       NaN       NaN  \n",
       "12  [0.3372, 0.341, 0.1356, 0.0327, 0.1806, 0.3007...       NaN       NaN  \n",
       "13  [0.6136, 0.8986, 0.7238, 0.3897, 0.2651, 0.044...       NaN       NaN  \n",
       "14  [0.2341, 0.2507, 0.4148, 0.0598, 0.3265, 0.423...       NaN       NaN  \n",
       "15  [0.7571, 0.1734, 0.5947, 0.4624, 0.3708, 0.866...       NaN       NaN  \n",
       "16  [0.7815, 0.8792, 0.8893, 0.7171, 0.8339, 0.641...       NaN       NaN  \n",
       "17  [0.4574, 0.4254, 0.4911, 0.3156, 0.5694, 0.357...       NaN       NaN  \n",
       "18  [0.8423, 0.5251, 0.9114, 0.8975, 0.5095, 0.719...       NaN       NaN  \n",
       "19  [0.8075, 0.295, 0.8941, 0.7429, 0.359, 0.8198,...       NaN       NaN  \n",
       "20  [0.5303, 0.6615, 0.3665, 0.1235, 0.8757, 0.649...       NaN       NaN  \n",
       "21  [0.5736, 0.8789, 0.6822, 0.1514, 0.9533, 0.499...       NaN       NaN  \n",
       "22  [0.6547, 0.7645, 0.7735, 0.3454, 0.9123, 0.511...       NaN       NaN  \n",
       "23  [0.2531, 0.3046, 0.2976, 0.6465, 0.3653, 0.243...       NaN       NaN  \n",
       "24  [0.7248, 0.8078, 0.5891, 0.8462, 0.9293, 0.598...       NaN       NaN  \n",
       "25  [0.6382, 0.6418, 0.1272, 0.3051, 0.6427, 0.424...       NaN       NaN  \n",
       "26  [0.8713, 0.9732, 0.5149, 0.6275, 0.9342, 0.571...       NaN       NaN  \n",
       "27  [0.2504, 0.4224, 0.4381, 0.3116, 0.1315, 0.158...       NaN       NaN  \n",
       "28  [0.2165, 0.6236, 0.3152, 0.1779, 0.0531, 0.032...       NaN       NaN  \n",
       "29  [0.1314, 0.4993, 0.143, 0.0847, 0.0234, 0.0413...       NaN       NaN  \n",
       "30  [0.0089, 0.0064, 0.4006, 0.0081, 0.0, 0.1125, ...       NaN       NaN  \n",
       "31  [0.0935, 0.6162, 0.117, 0.2157, 0.6989, 0.2538...       NaN       NaN  \n",
       "32  [0.2321, 0.3189, 0.0549, 0.0507, 0.8219, 0.207...       NaN       NaN  \n",
       "33  [0.8381, 0.7558, 0.9259, 0.8269, 0.915, 0.8256...       NaN       NaN  \n",
       "34  [0.1719, 0.0684, 0.8981, 0.5394, 0.2255, 0.830...       NaN       NaN  \n",
       "35  [0.7752, 0.8879, 0.5403, 0.5771, 0.9384, 0.485...       NaN       NaN  \n",
       "36  [0.9284, 0.9522, 0.7403, 0.6753, 0.9719, 0.604...       NaN       NaN  \n",
       "37  [0.8739, 0.8911, 0.3127, 0.1804, 0.955, 0.0364...       NaN       NaN  \n",
       "38  [0.2626, 0.939, 0.9418, 0.6517, 0.9077, 0.0689...       NaN       NaN  \n",
       "39  [0.4541, 0.9727, 0.959, 0.6222, 0.9685, 0.3748...       NaN       NaN  \n",
       "40  [0.1407, 0.6234, 0.6983, 0.0847, 0.6331, 0.207...       NaN       NaN  \n",
       "41  [0.8234, 0.9713, 0.9582, 0.9038, 0.9506, 0.329...       NaN       NaN  \n",
       "42  [0.8631, 0.9515, 0.4988, 0.578, 0.9784, 0.561,...       NaN       NaN  \n",
       "43  [0.6049, 0.9225, 0.3571, 0.1752, 0.9417, 0.064...       NaN       NaN  \n",
       "44  [0.7231, 0.9624, 0.1377, 0.2156, 0.9438, 0.015...       NaN       NaN  \n",
       "45  [0.7709, 0.9549, 0.4541, 0.4771, 0.9751, 0.334...       NaN       NaN  \n",
       "46  [0.2153, 0.6443, 0.1468, 0.4963, 0.4935, 0.283...       NaN       NaN  \n",
       "47  [0.2517, 0.4154, 0.2674, 0.2239, 0.6809, 0.585...       NaN       NaN  \n",
       "48  [0.7676, 0.3728, 0.9879, 0.8481, 0.336, 0.9087...       NaN       NaN  \n",
       "49  [0.7711, 0.0364, 0.7467, 0.3572, 0.203, 0.7032...       NaN       NaN  \n",
       "50  [0.4958, 0.5267, 0.2405, 0.3168, 0.7906, 0.706...       NaN       NaN  \n",
       "51  [0.2849, 0.1829, 0.229, 0.2405, 0.3378, 0.5859...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2141 : Training: loss:  0.067590825\n",
      "2142 : Training: loss:  0.06230172\n",
      "2143 : Training: loss:  0.0569321\n",
      "2144 : Training: loss:  0.0466536\n",
      "2145 : Training: loss:  0.064617515\n",
      "2146 : Training: loss:  0.077423\n",
      "2147 : Training: loss:  0.045246813\n",
      "2148 : Training: loss:  0.030701466\n",
      "2149 : Training: loss:  0.036847062\n",
      "2150 : Training: loss:  0.041421965\n",
      "2151 : Training: loss:  0.06883023\n",
      "2152 : Training: loss:  0.042464867\n",
      "2153 : Training: loss:  0.08440815\n",
      "2154 : Training: loss:  0.043595903\n",
      "2155 : Training: loss:  0.076094\n",
      "2156 : Training: loss:  0.04831543\n",
      "2157 : Training: loss:  0.051106233\n",
      "2158 : Training: loss:  0.07103214\n",
      "2159 : Training: loss:  0.04652986\n",
      "2160 : Training: loss:  0.06795886\n",
      "Validation: Loss:  0.07454467  Accuracy:  0.8076923\n",
      "2161 : Training: loss:  0.039029583\n",
      "2162 : Training: loss:  0.066364184\n",
      "2163 : Training: loss:  0.05820749\n",
      "2164 : Training: loss:  0.07117308\n",
      "2165 : Training: loss:  0.038731996\n",
      "2166 : Training: loss:  0.058507904\n",
      "2167 : Training: loss:  0.05201398\n",
      "2168 : Training: loss:  0.09839357\n",
      "2169 : Training: loss:  0.057040073\n",
      "2170 : Training: loss:  0.047424097\n",
      "2171 : Training: loss:  0.07153387\n",
      "2172 : Training: loss:  0.07635346\n",
      "2173 : Training: loss:  0.047943123\n",
      "2174 : Training: loss:  0.039137784\n",
      "2175 : Training: loss:  0.06175632\n",
      "2176 : Training: loss:  0.057169937\n",
      "2177 : Training: loss:  0.044268515\n",
      "2178 : Training: loss:  0.053280234\n",
      "2179 : Training: loss:  0.048806287\n",
      "2180 : Training: loss:  0.04977198\n",
      "Validation: Loss:  0.073585965  Accuracy:  0.78846157\n",
      "2181 : Training: loss:  0.06961802\n",
      "2182 : Training: loss:  0.039551068\n",
      "2183 : Training: loss:  0.073298015\n",
      "2184 : Training: loss:  0.049005542\n",
      "2185 : Training: loss:  0.0727496\n",
      "2186 : Training: loss:  0.06842813\n",
      "2187 : Training: loss:  0.049516264\n",
      "2188 : Training: loss:  0.04760473\n",
      "2189 : Training: loss:  0.052441906\n",
      "2190 : Training: loss:  0.051247112\n",
      "2191 : Training: loss:  0.03376281\n",
      "2192 : Training: loss:  0.06674257\n",
      "2193 : Training: loss:  0.05239763\n",
      "2194 : Training: loss:  0.057922523\n",
      "2195 : Training: loss:  0.059285115\n",
      "2196 : Training: loss:  0.06681887\n",
      "2197 : Training: loss:  0.05176529\n",
      "2198 : Training: loss:  0.036932237\n",
      "2199 : Training: loss:  0.06983027\n",
      "2200 : Training: loss:  0.035372257\n",
      "Validation: Loss:  0.07252983  Accuracy:  0.7692308\n",
      "2201 : Training: loss:  0.06777778\n",
      "2202 : Training: loss:  0.040453743\n",
      "2203 : Training: loss:  0.053224675\n",
      "2204 : Training: loss:  0.03516683\n",
      "2205 : Training: loss:  0.028605673\n",
      "2206 : Training: loss:  0.030264063\n",
      "2207 : Training: loss:  0.06422389\n",
      "2208 : Training: loss:  0.028411048\n",
      "2209 : Training: loss:  0.041914254\n",
      "2210 : Training: loss:  0.067047805\n",
      "2211 : Training: loss:  0.060463317\n",
      "2212 : Training: loss:  0.058192518\n",
      "2213 : Training: loss:  0.0843713\n",
      "2214 : Training: loss:  0.043143168\n",
      "2215 : Training: loss:  0.052266892\n",
      "2216 : Training: loss:  0.0812151\n",
      "2217 : Training: loss:  0.057284683\n",
      "2218 : Training: loss:  0.01737322\n",
      "2219 : Training: loss:  0.039886348\n",
      "2220 : Training: loss:  0.07318214\n",
      "Validation: Loss:  0.07175418  Accuracy:  0.78846157\n",
      "2221 : Training: loss:  0.022069959\n",
      "2222 : Training: loss:  0.041201465\n",
      "2223 : Training: loss:  0.031684253\n",
      "2224 : Training: loss:  0.056153934\n",
      "2225 : Training: loss:  0.029444277\n",
      "2226 : Training: loss:  0.03542984\n",
      "2227 : Training: loss:  0.044055477\n",
      "2228 : Training: loss:  0.036339197\n",
      "2229 : Training: loss:  0.055092327\n",
      "2230 : Training: loss:  0.09708628\n",
      "2231 : Training: loss:  0.056908228\n",
      "2232 : Training: loss:  0.09259177\n",
      "2233 : Training: loss:  0.08192586\n",
      "2234 : Training: loss:  0.038306307\n",
      "2235 : Training: loss:  0.070823945\n",
      "2236 : Training: loss:  0.06553474\n",
      "2237 : Training: loss:  0.047864772\n",
      "2238 : Training: loss:  0.052371424\n",
      "2239 : Training: loss:  0.051492184\n",
      "2240 : Training: loss:  0.059152108\n",
      "Validation: Loss:  0.07087589  Accuracy:  0.8076923\n",
      "2241 : Training: loss:  0.038635533\n",
      "2242 : Training: loss:  0.061322037\n",
      "2243 : Training: loss:  0.051759556\n",
      "2244 : Training: loss:  0.029986802\n",
      "2245 : Training: loss:  0.056198128\n",
      "2246 : Training: loss:  0.034995325\n",
      "2247 : Training: loss:  0.044619475\n",
      "2248 : Training: loss:  0.065674596\n",
      "2249 : Training: loss:  0.042282905\n",
      "2250 : Training: loss:  0.042813256\n",
      "2251 : Training: loss:  0.037233956\n",
      "2252 : Training: loss:  0.06866485\n",
      "2253 : Training: loss:  0.055140514\n",
      "2254 : Training: loss:  0.07593462\n",
      "2255 : Training: loss:  0.055872366\n",
      "2256 : Training: loss:  0.029855452\n",
      "2257 : Training: loss:  0.059200604\n",
      "2258 : Training: loss:  0.04340535\n",
      "2259 : Training: loss:  0.036843184\n",
      "2260 : Training: loss:  0.03740498\n",
      "Validation: Loss:  0.06994654  Accuracy:  0.8076923\n",
      "2261 : Training: loss:  0.044893384\n",
      "2262 : Training: loss:  0.055898223\n",
      "2263 : Training: loss:  0.048757605\n",
      "2264 : Training: loss:  0.05072238\n",
      "2265 : Training: loss:  0.035187587\n",
      "2266 : Training: loss:  0.04556928\n",
      "2267 : Training: loss:  0.04658087\n",
      "2268 : Training: loss:  0.027866988\n",
      "2269 : Training: loss:  0.07085888\n",
      "2270 : Training: loss:  0.049238715\n",
      "2271 : Training: loss:  0.062626325\n",
      "2272 : Training: loss:  0.031887986\n",
      "2273 : Training: loss:  0.032197505\n",
      "2274 : Training: loss:  0.0682043\n",
      "2275 : Training: loss:  0.046530407\n",
      "2276 : Training: loss:  0.052245386\n",
      "2277 : Training: loss:  0.07445609\n",
      "2278 : Training: loss:  0.05799709\n",
      "2279 : Training: loss:  0.043119535\n",
      "2280 : Training: loss:  0.05230422\n",
      "Validation: Loss:  0.0692729  Accuracy:  0.8269231\n",
      "2281 : Training: loss:  0.079386756\n",
      "2282 : Training: loss:  0.06328169\n",
      "2283 : Training: loss:  0.051904563\n",
      "2284 : Training: loss:  0.02500418\n",
      "2285 : Training: loss:  0.050683614\n",
      "2286 : Training: loss:  0.052761126\n",
      "2287 : Training: loss:  0.0667562\n",
      "2288 : Training: loss:  0.0663873\n",
      "2289 : Training: loss:  0.044673655\n",
      "2290 : Training: loss:  0.026553521\n",
      "2291 : Training: loss:  0.06627147\n",
      "2292 : Training: loss:  0.058795378\n",
      "2293 : Training: loss:  0.029247724\n",
      "2294 : Training: loss:  0.04994934\n",
      "2295 : Training: loss:  0.033462796\n",
      "2296 : Training: loss:  0.050557524\n",
      "2297 : Training: loss:  0.07816738\n",
      "2298 : Training: loss:  0.040389273\n",
      "2299 : Training: loss:  0.035237383\n",
      "2300 : Training: loss:  0.04651041\n",
      "Validation: Loss:  0.06852266  Accuracy:  0.8269231\n",
      "2301 : Training: loss:  0.063809045\n",
      "2302 : Training: loss:  0.0686849\n",
      "2303 : Training: loss:  0.036468\n",
      "2304 : Training: loss:  0.029370379\n",
      "2305 : Training: loss:  0.043645844\n",
      "2306 : Training: loss:  0.054104682\n",
      "2307 : Training: loss:  0.049337205\n",
      "2308 : Training: loss:  0.044239845\n",
      "2309 : Training: loss:  0.061787885\n",
      "2310 : Training: loss:  0.0469917\n",
      "2311 : Training: loss:  0.087842315\n",
      "2312 : Training: loss:  0.06118939\n",
      "2313 : Training: loss:  0.044051524\n",
      "2314 : Training: loss:  0.030687252\n",
      "2315 : Training: loss:  0.07643877\n",
      "2316 : Training: loss:  0.02415571\n",
      "2317 : Training: loss:  0.06104048\n",
      "2318 : Training: loss:  0.03042192\n",
      "2319 : Training: loss:  0.067386545\n",
      "2320 : Training: loss:  0.057872906\n",
      "Validation: Loss:  0.06783688  Accuracy:  0.84615386\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3737, 0.0872, 0.0205, 0.0324, 0.0141, 0.032...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3175, 0.4668, 0.0877, 0.442, 0.2222, 0.0949...</td>\n",
       "      <td>[0.6825, 0.5332, 0.9123, 0.558, 0.7778, 0.9051...</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.067837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.7209, 0.4695, 0.0105, 0.033, 0.106, 0.0342,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7726, 0.734, 0.2278, 0.6543, 0.5946, 0.2518...</td>\n",
       "      <td>[0.2274, 0.266, 0.7722, 0.3457, 0.4054, 0.7482...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.6754, 0.1834, 0.0107, 0.0927, 0.0222, 0.056...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5614, 0.6483, 0.168, 0.6403, 0.21, 0.1284, ...</td>\n",
       "      <td>[0.4386, 0.3517, 0.832, 0.3597, 0.79, 0.8716, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.045, 0.2068, 0.0046, 0.006, 0.0713, 0.0359,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5908, 0.5154, 0.1593, 0.7136, 0.8191, 0.489...</td>\n",
       "      <td>[0.4092, 0.4846, 0.8407, 0.2864, 0.1809, 0.511...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1009, 0.101, 0.0187, 0.0058, 0.0066, 0.0278...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1516, 0.2309, 0.0394, 0.3351, 0.2445, 0.045...</td>\n",
       "      <td>[0.8484, 0.7691, 0.9606, 0.6649, 0.7555, 0.954...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.023, 0.0153, 0.21, 0.0103, 0.006, 0.0514, 0...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.2062, 0.0667, 0.1725, 0.2554, 0.154, 0.555,...</td>\n",
       "      <td>[0.7938, 0.9333, 0.8275, 0.7446, 0.846, 0.445,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0373, 0.0054, 0.0015, 0.8231, 0.0035, 0.000...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.979, 0.9047, 0.9663, 0.9743, 0.1636, 0.9598...</td>\n",
       "      <td>[0.021, 0.0953, 0.0337, 0.0257, 0.8364, 0.0402...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0311, 0.0156, 0.0148, 0.5691, 0.0048, 0.004...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9161, 0.433, 0.8695, 0.7575, 0.1727, 0.9467...</td>\n",
       "      <td>[0.0839, 0.567, 0.1305, 0.2425, 0.8273, 0.0533...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0428, 0.0114, 0.0257, 0.495, 0.0106, 0.0119...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8749, 0.6016, 0.9416, 0.6212, 0.1791, 0.869...</td>\n",
       "      <td>[0.1251, 0.3984, 0.0584, 0.3788, 0.8209, 0.130...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.01, 0.0706, 0.0064, 0.0167, 0.6314, 0.1177,...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8733, 0.9652, 0.8178, 0.82, 0.9711, 0.7872,...</td>\n",
       "      <td>[0.1267, 0.0348, 0.1822, 0.18, 0.0289, 0.2128,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0513, 0.3962, 0.0045, 0.0055, 0.3324, 0.336...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6594, 0.9526, 0.1256, 0.6019, 0.8607, 0.686...</td>\n",
       "      <td>[0.3406, 0.0474, 0.8744, 0.3981, 0.1393, 0.313...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0062, 0.0251, 0.0053, 0.0045, 0.0785, 0.300...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4462, 0.8642, 0.2406, 0.5258, 0.8566, 0.576...</td>\n",
       "      <td>[0.5538, 0.1358, 0.7594, 0.4742, 0.1434, 0.423...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.001, 0.0075, 0.0213, 0.0022, 0.0032, 0.0034...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6381, 0.662, 0.8606, 0.9714, 0.8426, 0.6888...</td>\n",
       "      <td>[0.3619, 0.338, 0.1394, 0.0286, 0.1574, 0.3112...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0697, 0.0661, 0.1867, 0.0786, 0.0039, 0.084...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.3511, 0.085, 0.2621, 0.6292, 0.7337, 0.9566...</td>\n",
       "      <td>[0.6489, 0.915, 0.7379, 0.3708, 0.2663, 0.0434...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.022, 0.0178, 0.0013, 0.0395, 0.0251, 0.0041...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7606, 0.7698, 0.5926, 0.9468, 0.6785, 0.587...</td>\n",
       "      <td>[0.2394, 0.2302, 0.4074, 0.0532, 0.3215, 0.413...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0553, 0.0207, 0.0052, 0.0059, 0.0229, 0.010...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2223, 0.8421, 0.412, 0.537, 0.6381, 0.1308,...</td>\n",
       "      <td>[0.7777, 0.1579, 0.588, 0.463, 0.3619, 0.8692,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0145, 0.0067, 0.1347, 0.0055, 0.0012, 0.023...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.2312, 0.1292, 0.1161, 0.3219, 0.1739, 0.403...</td>\n",
       "      <td>[0.7688, 0.8708, 0.8839, 0.6781, 0.8261, 0.596...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.021, 0.0088, 0.0262, 0.0137, 0.0042, 0.007,...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5576, 0.5979, 0.5296, 0.7155, 0.4302, 0.679...</td>\n",
       "      <td>[0.4424, 0.4021, 0.4704, 0.2845, 0.5698, 0.320...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.057, 0.008, 0.0672, 0.0052, 0.0031, 0.0199,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1538, 0.4894, 0.09, 0.099, 0.5107, 0.323, 0...</td>\n",
       "      <td>[0.8462, 0.5106, 0.91, 0.901, 0.4893, 0.677, 0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.039, 0.0085, 0.0726, 0.0038, 0.0039, 0.0245...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.185, 0.7225, 0.1076, 0.247, 0.6665, 0.2006,...</td>\n",
       "      <td>[0.815, 0.2775, 0.8924, 0.753, 0.3335, 0.7994,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0692, 0.0278, 0.0216, 0.0507, 0.0034, 0.006...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4791, 0.3459, 0.6556, 0.8921, 0.1208, 0.373...</td>\n",
       "      <td>[0.5209, 0.6541, 0.3444, 0.1079, 0.8792, 0.626...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0155, 0.0069, 0.029, 0.0285, 0.0012, 0.0026...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.437, 0.1241, 0.343, 0.8684, 0.0463, 0.541, ...</td>\n",
       "      <td>[0.563, 0.8759, 0.657, 0.1316, 0.9537, 0.459, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0276, 0.0118, 0.0301, 0.0208, 0.0027, 0.015...</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.3492, 0.2547, 0.2302, 0.6792, 0.087, 0.5162...</td>\n",
       "      <td>[0.6508, 0.7453, 0.7698, 0.3208, 0.913, 0.4838...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0334, 0.0128, 0.0268, 0.0373, 0.0244, 0.058...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7527, 0.7272, 0.7218, 0.3333, 0.6546, 0.786...</td>\n",
       "      <td>[0.2473, 0.2728, 0.2782, 0.6667, 0.3454, 0.213...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0463, 0.0042, 0.0864, 0.0154, 0.0034, 0.016...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2951, 0.2223, 0.4401, 0.1666, 0.0729, 0.468...</td>\n",
       "      <td>[0.7049, 0.7777, 0.5599, 0.8334, 0.9271, 0.531...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0083, 0.0083, 0.0214, 0.0171, 0.0103, 0.007...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3608, 0.3783, 0.8885, 0.7306, 0.3668, 0.613...</td>\n",
       "      <td>[0.6392, 0.6217, 0.1115, 0.2694, 0.6332, 0.386...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0122, 0.0052, 0.0308, 0.0034, 0.0011, 0.002...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1274, 0.0269, 0.5155, 0.3968, 0.0656, 0.474...</td>\n",
       "      <td>[0.8726, 0.9731, 0.4845, 0.6032, 0.9344, 0.525...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0008, 0.0057, 0.0094, 0.0047, 0.0097, 0.007...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7474, 0.5837, 0.5711, 0.7263, 0.8691, 0.860...</td>\n",
       "      <td>[0.2526, 0.4163, 0.4289, 0.2737, 0.1309, 0.139...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0013, 0.0137, 0.0229, 0.0188, 0.0037, 0.005...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7753, 0.3519, 0.6977, 0.8487, 0.9457, 0.971...</td>\n",
       "      <td>[0.2247, 0.6481, 0.3023, 0.1513, 0.0543, 0.028...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0007, 0.0125, 0.0028, 0.0057, 0.0167, 0.002...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8569, 0.4834, 0.8622, 0.9287, 0.9778, 0.962...</td>\n",
       "      <td>[0.1431, 0.5166, 0.1378, 0.0713, 0.0222, 0.037...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.166, 0.7467, 0.1221, 0.0775, 0.7491, 0.5985...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9828, 0.99, 0.4954, 0.9855, 1.0, 0.826, 0.9...</td>\n",
       "      <td>[0.0172, 0.01, 0.5046, 0.0145, 0.0, 0.174, 1e-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0043, 0.0094, 0.0258, 0.0557, 0.003, 0.0021...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9141, 0.396, 0.8962, 0.811, 0.2979, 0.7822,...</td>\n",
       "      <td>[0.0859, 0.604, 0.1038, 0.189, 0.7021, 0.2178,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.004, 0.0039, 0.0143, 0.0553, 0.0015, 0.001,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7787, 0.7094, 0.9535, 0.9606, 0.1716, 0.815...</td>\n",
       "      <td>[0.2213, 0.2906, 0.0465, 0.0394, 0.8284, 0.185...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0426, 0.008, 0.0473, 0.0084, 0.0018, 0.0105...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.1656, 0.27, 0.0764, 0.1977, 0.0882, 0.1992,...</td>\n",
       "      <td>[0.8344, 0.73, 0.9236, 0.8023, 0.9118, 0.8008,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0622, 0.038, 0.0246, 0.0284, 0.1148, 0.0529...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8382, 0.9415, 0.1028, 0.472, 0.7899, 0.1777...</td>\n",
       "      <td>[0.1618, 0.0585, 0.8972, 0.528, 0.2101, 0.8223...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0442, 0.0104, 0.0235, 0.0127, 0.0012, 0.006...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2272, 0.1218, 0.4828, 0.465, 0.0615, 0.5607...</td>\n",
       "      <td>[0.7728, 0.8782, 0.5172, 0.535, 0.9385, 0.4393...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.03, 0.0076, 0.0328, 0.0059, 0.0007, 0.0054,...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0686, 0.0506, 0.2852, 0.3547, 0.0277, 0.447...</td>\n",
       "      <td>[0.9314, 0.9494, 0.7148, 0.6453, 0.9723, 0.552...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0052, 0.0064, 0.0136, 0.0182, 0.0005, 0.001...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1055, 0.1072, 0.7121, 0.832, 0.0424, 0.9676...</td>\n",
       "      <td>[0.8945, 0.8928, 0.2879, 0.168, 0.9576, 0.0324...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0044, 0.0012, 0.0373, 0.0134, 0.0008, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7417, 0.0592, 0.0516, 0.3804, 0.0861, 0.941...</td>\n",
       "      <td>[0.2583, 0.9408, 0.9484, 0.6196, 0.9139, 0.058...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0108, 0.0019, 0.0309, 0.0116, 0.0007, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5608, 0.0281, 0.0406, 0.4161, 0.0321, 0.680...</td>\n",
       "      <td>[0.4392, 0.9719, 0.9594, 0.5839, 0.9679, 0.319...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0106, 0.0149, 0.005, 0.0308, 0.0047, 0.0026...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.8608, 0.3646, 0.2947, 0.9286, 0.357, 0.8075...</td>\n",
       "      <td>[0.1392, 0.6354, 0.7053, 0.0714, 0.643, 0.1925...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0293, 0.003, 0.0611, 0.0118, 0.0007, 0.0055...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1814, 0.03, 0.0406, 0.1071, 0.049, 0.7196, ...</td>\n",
       "      <td>[0.8186, 0.97, 0.9594, 0.8929, 0.951, 0.2804, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0164, 0.005, 0.0161, 0.0064, 0.0003, 0.0007...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.134, 0.0505, 0.5313, 0.4544, 0.0217, 0.4802...</td>\n",
       "      <td>[0.866, 0.9495, 0.4687, 0.5456, 0.9783, 0.5198...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.007, 0.0048, 0.0087, 0.0089, 0.0003, 0.0003...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3661, 0.0736, 0.6578, 0.834, 0.0576, 0.9412...</td>\n",
       "      <td>[0.6339, 0.9264, 0.3422, 0.166, 0.9424, 0.0588...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0034, 0.0037, 0.0088, 0.0066, 0.0002, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2446, 0.0333, 0.8741, 0.7983, 0.0549, 0.986...</td>\n",
       "      <td>[0.7554, 0.9667, 0.1259, 0.2017, 0.9451, 0.013...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0085, 0.0033, 0.009, 0.006, 0.0002, 0.0003,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.216, 0.0443, 0.576, 0.5545, 0.0235, 0.7015,...</td>\n",
       "      <td>[0.784, 0.9557, 0.424, 0.4455, 0.9765, 0.2985,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0087, 0.009, 0.1128, 0.0326, 0.007, 0.0289,...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7983, 0.3819, 0.8713, 0.5029, 0.5185, 0.752...</td>\n",
       "      <td>[0.2017, 0.6181, 0.1287, 0.4971, 0.4815, 0.247...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0222, 0.0723, 0.0123, 0.022, 0.0166, 0.0123...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.7658, 0.6037, 0.7661, 0.7877, 0.3132, 0.432...</td>\n",
       "      <td>[0.2342, 0.3963, 0.2339, 0.2123, 0.6868, 0.567...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0099, 0.0088, 0.0083, 0.0006, 0.0093, 0.017...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2192, 0.6484, 0.0113, 0.158, 0.6944, 0.0953...</td>\n",
       "      <td>[0.7808, 0.3516, 0.9887, 0.842, 0.3056, 0.9047...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0242, 0.0298, 0.01, 0.0099, 0.0509, 0.1243,...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2042, 0.9671, 0.2515, 0.6443, 0.8025, 0.297...</td>\n",
       "      <td>[0.7958, 0.0329, 0.7485, 0.3557, 0.1975, 0.702...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0421, 0.0173, 0.0123, 0.018, 0.0077, 0.0076...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4929, 0.4714, 0.7755, 0.7051, 0.2054, 0.294...</td>\n",
       "      <td>[0.5071, 0.5286, 0.2245, 0.2949, 0.7946, 0.705...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.006, 0.0062, 0.0054, 0.0183, 0.0312, 0.0018...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7155, 0.8346, 0.7902, 0.7796, 0.6655, 0.440...</td>\n",
       "      <td>[0.2845, 0.1654, 0.2098, 0.2204, 0.3345, 0.559...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3737, 0.0872, 0.0205, 0.0324, 0.0141, 0.032...               0   \n",
       "1   [0.7209, 0.4695, 0.0105, 0.033, 0.106, 0.0342,...               0   \n",
       "2   [0.6754, 0.1834, 0.0107, 0.0927, 0.0222, 0.056...               0   \n",
       "3   [0.045, 0.2068, 0.0046, 0.006, 0.0713, 0.0359,...               1   \n",
       "4   [0.1009, 0.101, 0.0187, 0.0058, 0.0066, 0.0278...               1   \n",
       "5   [0.023, 0.0153, 0.21, 0.0103, 0.006, 0.0514, 0...               2   \n",
       "6   [0.0373, 0.0054, 0.0015, 0.8231, 0.0035, 0.000...               3   \n",
       "7   [0.0311, 0.0156, 0.0148, 0.5691, 0.0048, 0.004...               3   \n",
       "8   [0.0428, 0.0114, 0.0257, 0.495, 0.0106, 0.0119...               3   \n",
       "9   [0.01, 0.0706, 0.0064, 0.0167, 0.6314, 0.1177,...               4   \n",
       "10  [0.0513, 0.3962, 0.0045, 0.0055, 0.3324, 0.336...               4   \n",
       "11  [0.0062, 0.0251, 0.0053, 0.0045, 0.0785, 0.300...               5   \n",
       "12  [0.001, 0.0075, 0.0213, 0.0022, 0.0032, 0.0034...               6   \n",
       "13  [0.0697, 0.0661, 0.1867, 0.0786, 0.0039, 0.084...               7   \n",
       "14  [0.022, 0.0178, 0.0013, 0.0395, 0.0251, 0.0041...               8   \n",
       "15  [0.0553, 0.0207, 0.0052, 0.0059, 0.0229, 0.010...               8   \n",
       "16  [0.0145, 0.0067, 0.1347, 0.0055, 0.0012, 0.023...               9   \n",
       "17  [0.021, 0.0088, 0.0262, 0.0137, 0.0042, 0.007,...               9   \n",
       "18  [0.057, 0.008, 0.0672, 0.0052, 0.0031, 0.0199,...              10   \n",
       "19  [0.039, 0.0085, 0.0726, 0.0038, 0.0039, 0.0245...              10   \n",
       "20  [0.0692, 0.0278, 0.0216, 0.0507, 0.0034, 0.006...              11   \n",
       "21  [0.0155, 0.0069, 0.029, 0.0285, 0.0012, 0.0026...              11   \n",
       "22  [0.0276, 0.0118, 0.0301, 0.0208, 0.0027, 0.015...              12   \n",
       "23  [0.0334, 0.0128, 0.0268, 0.0373, 0.0244, 0.058...              13   \n",
       "24  [0.0463, 0.0042, 0.0864, 0.0154, 0.0034, 0.016...              13   \n",
       "25  [0.0083, 0.0083, 0.0214, 0.0171, 0.0103, 0.007...              14   \n",
       "26  [0.0122, 0.0052, 0.0308, 0.0034, 0.0011, 0.002...              14   \n",
       "27  [0.0008, 0.0057, 0.0094, 0.0047, 0.0097, 0.007...              15   \n",
       "28  [0.0013, 0.0137, 0.0229, 0.0188, 0.0037, 0.005...              15   \n",
       "29  [0.0007, 0.0125, 0.0028, 0.0057, 0.0167, 0.002...              15   \n",
       "30  [0.166, 0.7467, 0.1221, 0.0775, 0.7491, 0.5985...              16   \n",
       "31  [0.0043, 0.0094, 0.0258, 0.0557, 0.003, 0.0021...              17   \n",
       "32  [0.004, 0.0039, 0.0143, 0.0553, 0.0015, 0.001,...              17   \n",
       "33  [0.0426, 0.008, 0.0473, 0.0084, 0.0018, 0.0105...              18   \n",
       "34  [0.0622, 0.038, 0.0246, 0.0284, 0.1148, 0.0529...              19   \n",
       "35  [0.0442, 0.0104, 0.0235, 0.0127, 0.0012, 0.006...              20   \n",
       "36  [0.03, 0.0076, 0.0328, 0.0059, 0.0007, 0.0054,...              21   \n",
       "37  [0.0052, 0.0064, 0.0136, 0.0182, 0.0005, 0.001...              21   \n",
       "38  [0.0044, 0.0012, 0.0373, 0.0134, 0.0008, 0.001...              22   \n",
       "39  [0.0108, 0.0019, 0.0309, 0.0116, 0.0007, 0.001...              22   \n",
       "40  [0.0106, 0.0149, 0.005, 0.0308, 0.0047, 0.0026...              22   \n",
       "41  [0.0293, 0.003, 0.0611, 0.0118, 0.0007, 0.0055...              22   \n",
       "42  [0.0164, 0.005, 0.0161, 0.0064, 0.0003, 0.0007...              23   \n",
       "43  [0.007, 0.0048, 0.0087, 0.0089, 0.0003, 0.0003...              23   \n",
       "44  [0.0034, 0.0037, 0.0088, 0.0066, 0.0002, 0.000...              23   \n",
       "45  [0.0085, 0.0033, 0.009, 0.006, 0.0002, 0.0003,...              23   \n",
       "46  [0.0087, 0.009, 0.1128, 0.0326, 0.007, 0.0289,...              24   \n",
       "47  [0.0222, 0.0723, 0.0123, 0.022, 0.0166, 0.0123...              24   \n",
       "48  [0.0099, 0.0088, 0.0083, 0.0006, 0.0093, 0.017...              25   \n",
       "49  [0.0242, 0.0298, 0.01, 0.0099, 0.0509, 0.1243,...              25   \n",
       "50  [0.0421, 0.0173, 0.0123, 0.018, 0.0077, 0.0076...              26   \n",
       "51  [0.006, 0.0062, 0.0054, 0.0183, 0.0312, 0.0018...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.3175, 0.4668, 0.0877, 0.442, 0.2222, 0.0949...   \n",
       "1                  0  [0.7726, 0.734, 0.2278, 0.6543, 0.5946, 0.2518...   \n",
       "2                  0  [0.5614, 0.6483, 0.168, 0.6403, 0.21, 0.1284, ...   \n",
       "3                  1  [0.5908, 0.5154, 0.1593, 0.7136, 0.8191, 0.489...   \n",
       "4                  1  [0.1516, 0.2309, 0.0394, 0.3351, 0.2445, 0.045...   \n",
       "5                  2  [0.2062, 0.0667, 0.1725, 0.2554, 0.154, 0.555,...   \n",
       "6                  3  [0.979, 0.9047, 0.9663, 0.9743, 0.1636, 0.9598...   \n",
       "7                  3  [0.9161, 0.433, 0.8695, 0.7575, 0.1727, 0.9467...   \n",
       "8                  3  [0.8749, 0.6016, 0.9416, 0.6212, 0.1791, 0.869...   \n",
       "9                  4  [0.8733, 0.9652, 0.8178, 0.82, 0.9711, 0.7872,...   \n",
       "10                 1  [0.6594, 0.9526, 0.1256, 0.6019, 0.8607, 0.686...   \n",
       "11                 5  [0.4462, 0.8642, 0.2406, 0.5258, 0.8566, 0.576...   \n",
       "12                 6  [0.6381, 0.662, 0.8606, 0.9714, 0.8426, 0.6888...   \n",
       "13                 7  [0.3511, 0.085, 0.2621, 0.6292, 0.7337, 0.9566...   \n",
       "14                 8  [0.7606, 0.7698, 0.5926, 0.9468, 0.6785, 0.587...   \n",
       "15                 8  [0.2223, 0.8421, 0.412, 0.537, 0.6381, 0.1308,...   \n",
       "16                 9  [0.2312, 0.1292, 0.1161, 0.3219, 0.1739, 0.403...   \n",
       "17                 9  [0.5576, 0.5979, 0.5296, 0.7155, 0.4302, 0.679...   \n",
       "18                10  [0.1538, 0.4894, 0.09, 0.099, 0.5107, 0.323, 0...   \n",
       "19                10  [0.185, 0.7225, 0.1076, 0.247, 0.6665, 0.2006,...   \n",
       "20                11  [0.4791, 0.3459, 0.6556, 0.8921, 0.1208, 0.373...   \n",
       "21                22  [0.437, 0.1241, 0.343, 0.8684, 0.0463, 0.541, ...   \n",
       "22                21  [0.3492, 0.2547, 0.2302, 0.6792, 0.087, 0.5162...   \n",
       "23                13  [0.7527, 0.7272, 0.7218, 0.3333, 0.6546, 0.786...   \n",
       "24                10  [0.2951, 0.2223, 0.4401, 0.1666, 0.0729, 0.468...   \n",
       "25                14  [0.3608, 0.3783, 0.8885, 0.7306, 0.3668, 0.613...   \n",
       "26                14  [0.1274, 0.0269, 0.5155, 0.3968, 0.0656, 0.474...   \n",
       "27                15  [0.7474, 0.5837, 0.5711, 0.7263, 0.8691, 0.860...   \n",
       "28                15  [0.7753, 0.3519, 0.6977, 0.8487, 0.9457, 0.971...   \n",
       "29                15  [0.8569, 0.4834, 0.8622, 0.9287, 0.9778, 0.962...   \n",
       "30                16  [0.9828, 0.99, 0.4954, 0.9855, 1.0, 0.826, 0.9...   \n",
       "31                17  [0.9141, 0.396, 0.8962, 0.811, 0.2979, 0.7822,...   \n",
       "32                17  [0.7787, 0.7094, 0.9535, 0.9606, 0.1716, 0.815...   \n",
       "33                18  [0.1656, 0.27, 0.0764, 0.1977, 0.0882, 0.1992,...   \n",
       "34                19  [0.8382, 0.9415, 0.1028, 0.472, 0.7899, 0.1777...   \n",
       "35                10  [0.2272, 0.1218, 0.4828, 0.465, 0.0615, 0.5607...   \n",
       "36                23  [0.0686, 0.0506, 0.2852, 0.3547, 0.0277, 0.447...   \n",
       "37                23  [0.1055, 0.1072, 0.7121, 0.832, 0.0424, 0.9676...   \n",
       "38                22  [0.7417, 0.0592, 0.0516, 0.3804, 0.0861, 0.941...   \n",
       "39                22  [0.5608, 0.0281, 0.0406, 0.4161, 0.0321, 0.680...   \n",
       "40                22  [0.8608, 0.3646, 0.2947, 0.9286, 0.357, 0.8075...   \n",
       "41                22  [0.1814, 0.03, 0.0406, 0.1071, 0.049, 0.7196, ...   \n",
       "42                23  [0.134, 0.0505, 0.5313, 0.4544, 0.0217, 0.4802...   \n",
       "43                23  [0.3661, 0.0736, 0.6578, 0.834, 0.0576, 0.9412...   \n",
       "44                23  [0.2446, 0.0333, 0.8741, 0.7983, 0.0549, 0.986...   \n",
       "45                23  [0.216, 0.0443, 0.576, 0.5545, 0.0235, 0.7015,...   \n",
       "46                13  [0.7983, 0.3819, 0.8713, 0.5029, 0.5185, 0.752...   \n",
       "47                24  [0.7658, 0.6037, 0.7661, 0.7877, 0.3132, 0.432...   \n",
       "48                25  [0.2192, 0.6484, 0.0113, 0.158, 0.6944, 0.0953...   \n",
       "49                25  [0.2042, 0.9671, 0.2515, 0.6443, 0.8025, 0.297...   \n",
       "50                26  [0.4929, 0.4714, 0.7755, 0.7051, 0.2054, 0.294...   \n",
       "51                26  [0.7155, 0.8346, 0.7902, 0.7796, 0.6655, 0.440...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.6825, 0.5332, 0.9123, 0.558, 0.7778, 0.9051...  0.846154  0.067837  \n",
       "1   [0.2274, 0.266, 0.7722, 0.3457, 0.4054, 0.7482...       NaN       NaN  \n",
       "2   [0.4386, 0.3517, 0.832, 0.3597, 0.79, 0.8716, ...       NaN       NaN  \n",
       "3   [0.4092, 0.4846, 0.8407, 0.2864, 0.1809, 0.511...       NaN       NaN  \n",
       "4   [0.8484, 0.7691, 0.9606, 0.6649, 0.7555, 0.954...       NaN       NaN  \n",
       "5   [0.7938, 0.9333, 0.8275, 0.7446, 0.846, 0.445,...       NaN       NaN  \n",
       "6   [0.021, 0.0953, 0.0337, 0.0257, 0.8364, 0.0402...       NaN       NaN  \n",
       "7   [0.0839, 0.567, 0.1305, 0.2425, 0.8273, 0.0533...       NaN       NaN  \n",
       "8   [0.1251, 0.3984, 0.0584, 0.3788, 0.8209, 0.130...       NaN       NaN  \n",
       "9   [0.1267, 0.0348, 0.1822, 0.18, 0.0289, 0.2128,...       NaN       NaN  \n",
       "10  [0.3406, 0.0474, 0.8744, 0.3981, 0.1393, 0.313...       NaN       NaN  \n",
       "11  [0.5538, 0.1358, 0.7594, 0.4742, 0.1434, 0.423...       NaN       NaN  \n",
       "12  [0.3619, 0.338, 0.1394, 0.0286, 0.1574, 0.3112...       NaN       NaN  \n",
       "13  [0.6489, 0.915, 0.7379, 0.3708, 0.2663, 0.0434...       NaN       NaN  \n",
       "14  [0.2394, 0.2302, 0.4074, 0.0532, 0.3215, 0.413...       NaN       NaN  \n",
       "15  [0.7777, 0.1579, 0.588, 0.463, 0.3619, 0.8692,...       NaN       NaN  \n",
       "16  [0.7688, 0.8708, 0.8839, 0.6781, 0.8261, 0.596...       NaN       NaN  \n",
       "17  [0.4424, 0.4021, 0.4704, 0.2845, 0.5698, 0.320...       NaN       NaN  \n",
       "18  [0.8462, 0.5106, 0.91, 0.901, 0.4893, 0.677, 0...       NaN       NaN  \n",
       "19  [0.815, 0.2775, 0.8924, 0.753, 0.3335, 0.7994,...       NaN       NaN  \n",
       "20  [0.5209, 0.6541, 0.3444, 0.1079, 0.8792, 0.626...       NaN       NaN  \n",
       "21  [0.563, 0.8759, 0.657, 0.1316, 0.9537, 0.459, ...       NaN       NaN  \n",
       "22  [0.6508, 0.7453, 0.7698, 0.3208, 0.913, 0.4838...       NaN       NaN  \n",
       "23  [0.2473, 0.2728, 0.2782, 0.6667, 0.3454, 0.213...       NaN       NaN  \n",
       "24  [0.7049, 0.7777, 0.5599, 0.8334, 0.9271, 0.531...       NaN       NaN  \n",
       "25  [0.6392, 0.6217, 0.1115, 0.2694, 0.6332, 0.386...       NaN       NaN  \n",
       "26  [0.8726, 0.9731, 0.4845, 0.6032, 0.9344, 0.525...       NaN       NaN  \n",
       "27  [0.2526, 0.4163, 0.4289, 0.2737, 0.1309, 0.139...       NaN       NaN  \n",
       "28  [0.2247, 0.6481, 0.3023, 0.1513, 0.0543, 0.028...       NaN       NaN  \n",
       "29  [0.1431, 0.5166, 0.1378, 0.0713, 0.0222, 0.037...       NaN       NaN  \n",
       "30  [0.0172, 0.01, 0.5046, 0.0145, 0.0, 0.174, 1e-...       NaN       NaN  \n",
       "31  [0.0859, 0.604, 0.1038, 0.189, 0.7021, 0.2178,...       NaN       NaN  \n",
       "32  [0.2213, 0.2906, 0.0465, 0.0394, 0.8284, 0.185...       NaN       NaN  \n",
       "33  [0.8344, 0.73, 0.9236, 0.8023, 0.9118, 0.8008,...       NaN       NaN  \n",
       "34  [0.1618, 0.0585, 0.8972, 0.528, 0.2101, 0.8223...       NaN       NaN  \n",
       "35  [0.7728, 0.8782, 0.5172, 0.535, 0.9385, 0.4393...       NaN       NaN  \n",
       "36  [0.9314, 0.9494, 0.7148, 0.6453, 0.9723, 0.552...       NaN       NaN  \n",
       "37  [0.8945, 0.8928, 0.2879, 0.168, 0.9576, 0.0324...       NaN       NaN  \n",
       "38  [0.2583, 0.9408, 0.9484, 0.6196, 0.9139, 0.058...       NaN       NaN  \n",
       "39  [0.4392, 0.9719, 0.9594, 0.5839, 0.9679, 0.319...       NaN       NaN  \n",
       "40  [0.1392, 0.6354, 0.7053, 0.0714, 0.643, 0.1925...       NaN       NaN  \n",
       "41  [0.8186, 0.97, 0.9594, 0.8929, 0.951, 0.2804, ...       NaN       NaN  \n",
       "42  [0.866, 0.9495, 0.4687, 0.5456, 0.9783, 0.5198...       NaN       NaN  \n",
       "43  [0.6339, 0.9264, 0.3422, 0.166, 0.9424, 0.0588...       NaN       NaN  \n",
       "44  [0.7554, 0.9667, 0.1259, 0.2017, 0.9451, 0.013...       NaN       NaN  \n",
       "45  [0.784, 0.9557, 0.424, 0.4455, 0.9765, 0.2985,...       NaN       NaN  \n",
       "46  [0.2017, 0.6181, 0.1287, 0.4971, 0.4815, 0.247...       NaN       NaN  \n",
       "47  [0.2342, 0.3963, 0.2339, 0.2123, 0.6868, 0.567...       NaN       NaN  \n",
       "48  [0.7808, 0.3516, 0.9887, 0.842, 0.3056, 0.9047...       NaN       NaN  \n",
       "49  [0.7958, 0.0329, 0.7485, 0.3557, 0.1975, 0.702...       NaN       NaN  \n",
       "50  [0.5071, 0.5286, 0.2245, 0.2949, 0.7946, 0.705...       NaN       NaN  \n",
       "51  [0.2845, 0.1654, 0.2098, 0.2204, 0.3345, 0.559...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2321 : Training: loss:  0.052326337\n",
      "2322 : Training: loss:  0.03526999\n",
      "2323 : Training: loss:  0.03913188\n",
      "2324 : Training: loss:  0.052849133\n",
      "2325 : Training: loss:  0.068020806\n",
      "2326 : Training: loss:  0.052827563\n",
      "2327 : Training: loss:  0.0564628\n",
      "2328 : Training: loss:  0.05586521\n",
      "2329 : Training: loss:  0.044174455\n",
      "2330 : Training: loss:  0.06591392\n",
      "2331 : Training: loss:  0.065914206\n",
      "2332 : Training: loss:  0.06520041\n",
      "2333 : Training: loss:  0.055465695\n",
      "2334 : Training: loss:  0.065573126\n",
      "2335 : Training: loss:  0.044355497\n",
      "2336 : Training: loss:  0.04890773\n",
      "2337 : Training: loss:  0.051920477\n",
      "2338 : Training: loss:  0.05667421\n",
      "2339 : Training: loss:  0.032964893\n",
      "2340 : Training: loss:  0.025860844\n",
      "Validation: Loss:  0.0669615  Accuracy:  0.84615386\n",
      "2341 : Training: loss:  0.05773849\n",
      "2342 : Training: loss:  0.03756538\n",
      "2343 : Training: loss:  0.059667446\n",
      "2344 : Training: loss:  0.05179637\n",
      "2345 : Training: loss:  0.067866094\n",
      "2346 : Training: loss:  0.037709728\n",
      "2347 : Training: loss:  0.052845612\n",
      "2348 : Training: loss:  0.045066293\n",
      "2349 : Training: loss:  0.059056632\n",
      "2350 : Training: loss:  0.031862225\n",
      "2351 : Training: loss:  0.029590057\n",
      "2352 : Training: loss:  0.048874445\n",
      "2353 : Training: loss:  0.045298185\n",
      "2354 : Training: loss:  0.07010189\n",
      "2355 : Training: loss:  0.026782144\n",
      "2356 : Training: loss:  0.052625354\n",
      "2357 : Training: loss:  0.06867145\n",
      "2358 : Training: loss:  0.034776486\n",
      "2359 : Training: loss:  0.04795515\n",
      "2360 : Training: loss:  0.034508515\n",
      "Validation: Loss:  0.06609911  Accuracy:  0.84615386\n",
      "2361 : Training: loss:  0.05561683\n",
      "2362 : Training: loss:  0.031353246\n",
      "2363 : Training: loss:  0.052789394\n",
      "2364 : Training: loss:  0.04218569\n",
      "2365 : Training: loss:  0.03742139\n",
      "2366 : Training: loss:  0.03947687\n",
      "2367 : Training: loss:  0.02809445\n",
      "2368 : Training: loss:  0.039002173\n",
      "2369 : Training: loss:  0.036746297\n",
      "2370 : Training: loss:  0.05240943\n",
      "2371 : Training: loss:  0.029097827\n",
      "2372 : Training: loss:  0.054455142\n",
      "2373 : Training: loss:  0.041888595\n",
      "2374 : Training: loss:  0.04406635\n",
      "2375 : Training: loss:  0.032036874\n",
      "2376 : Training: loss:  0.065075345\n",
      "2377 : Training: loss:  0.045167692\n",
      "2378 : Training: loss:  0.036384623\n",
      "2379 : Training: loss:  0.02799386\n",
      "2380 : Training: loss:  0.032246094\n",
      "Validation: Loss:  0.065448664  Accuracy:  0.86538464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3682, 0.0908, 0.0197, 0.0308, 0.0137, 0.029...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.321, 0.4704, 0.0861, 0.4558, 0.2299, 0.0976...</td>\n",
       "      <td>[0.679, 0.5296, 0.9139, 0.5442, 0.7701, 0.9024...</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.065449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.7294, 0.4938, 0.0102, 0.0317, 0.1049, 0.031...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.78, 0.7369, 0.2266, 0.6595, 0.604, 0.2608, ...</td>\n",
       "      <td>[0.22, 0.2631, 0.7734, 0.3405, 0.396, 0.7392, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.6882, 0.1982, 0.0098, 0.0921, 0.0214, 0.053...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5682, 0.6552, 0.1664, 0.6556, 0.2176, 0.130...</td>\n",
       "      <td>[0.4318, 0.3448, 0.8336, 0.3444, 0.7824, 0.869...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0401, 0.22, 0.0044, 0.0055, 0.0702, 0.0322,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5918, 0.5159, 0.1583, 0.7254, 0.8278, 0.501...</td>\n",
       "      <td>[0.4082, 0.4841, 0.8417, 0.2746, 0.1722, 0.498...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0925, 0.1082, 0.0185, 0.0053, 0.0063, 0.025...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1551, 0.2344, 0.0387, 0.3479, 0.2559, 0.047...</td>\n",
       "      <td>[0.8449, 0.7656, 0.9613, 0.6521, 0.7441, 0.952...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0193, 0.0143, 0.2418, 0.0092, 0.0058, 0.048...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.2167, 0.0671, 0.1684, 0.2711, 0.1597, 0.576...</td>\n",
       "      <td>[0.7833, 0.9329, 0.8316, 0.7289, 0.8403, 0.423...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0355, 0.005, 0.0013, 0.8325, 0.0033, 0.0006...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.98, 0.9098, 0.9671, 0.9762, 0.1616, 0.9622,...</td>\n",
       "      <td>[0.02, 0.0902, 0.0329, 0.0238, 0.8384, 0.0378,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0272, 0.0147, 0.0147, 0.5855, 0.0046, 0.003...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9198, 0.439, 0.8706, 0.7643, 0.1734, 0.9502...</td>\n",
       "      <td>[0.0802, 0.561, 0.1294, 0.2357, 0.8266, 0.0498...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0388, 0.0105, 0.0253, 0.5189, 0.0104, 0.010...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8797, 0.6117, 0.9431, 0.6282, 0.1782, 0.875...</td>\n",
       "      <td>[0.1203, 0.3883, 0.0569, 0.3718, 0.8218, 0.124...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0095, 0.0721, 0.0068, 0.0166, 0.6499, 0.106...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8731, 0.9656, 0.8203, 0.8194, 0.9718, 0.796...</td>\n",
       "      <td>[0.1269, 0.0344, 0.1797, 0.1806, 0.0282, 0.203...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0477, 0.4183, 0.0043, 0.0053, 0.3375, 0.326...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6594, 0.9536, 0.1233, 0.6019, 0.8656, 0.698...</td>\n",
       "      <td>[0.3406, 0.0464, 0.8767, 0.3981, 0.1344, 0.301...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0052, 0.0244, 0.0052, 0.004, 0.0785, 0.2859...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4432, 0.8664, 0.2373, 0.5298, 0.863, 0.588,...</td>\n",
       "      <td>[0.5568, 0.1336, 0.7627, 0.4702, 0.137, 0.412,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0009, 0.0069, 0.0227, 0.0018, 0.0029, 0.003...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6305, 0.6598, 0.8565, 0.9722, 0.8517, 0.687...</td>\n",
       "      <td>[0.3695, 0.3402, 0.1435, 0.0278, 0.1483, 0.312...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0616, 0.0631, 0.1804, 0.0709, 0.0034, 0.075...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.3395, 0.0794, 0.259, 0.6346, 0.7357, 0.9567...</td>\n",
       "      <td>[0.6605, 0.9206, 0.741, 0.3654, 0.2643, 0.0433...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0206, 0.0176, 0.0012, 0.0372, 0.0249, 0.003...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7617, 0.776, 0.5928, 0.9499, 0.6864, 0.5904...</td>\n",
       "      <td>[0.2383, 0.224, 0.4072, 0.0501, 0.3136, 0.4096...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.051, 0.0193, 0.005, 0.0053, 0.0215, 0.009, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2168, 0.8457, 0.41, 0.5371, 0.6446, 0.1302,...</td>\n",
       "      <td>[0.7832, 0.1543, 0.59, 0.4629, 0.3554, 0.8698,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0124, 0.0062, 0.1457, 0.0048, 0.0011, 0.021...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.2386, 0.1316, 0.1146, 0.336, 0.1777, 0.4176...</td>\n",
       "      <td>[0.7614, 0.8684, 0.8854, 0.664, 0.8223, 0.5824...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0196, 0.0081, 0.0276, 0.0128, 0.0039, 0.006...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5646, 0.6049, 0.5304, 0.726, 0.43, 0.6895, ...</td>\n",
       "      <td>[0.4354, 0.3951, 0.4696, 0.274, 0.57, 0.3105, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0501, 0.0075, 0.0674, 0.0046, 0.0029, 0.017...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1538, 0.4916, 0.0888, 0.0966, 0.5212, 0.339...</td>\n",
       "      <td>[0.8462, 0.5084, 0.9112, 0.9034, 0.4788, 0.660...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0342, 0.008, 0.0735, 0.0033, 0.0036, 0.0209...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1847, 0.7255, 0.1056, 0.2398, 0.6792, 0.208...</td>\n",
       "      <td>[0.8153, 0.2745, 0.8944, 0.7602, 0.3208, 0.791...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0629, 0.0267, 0.0217, 0.0475, 0.0031, 0.005...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4887, 0.3486, 0.6595, 0.8975, 0.123, 0.384,...</td>\n",
       "      <td>[0.5113, 0.6514, 0.3405, 0.1025, 0.877, 0.616,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0129, 0.0063, 0.0294, 0.0269, 0.0011, 0.002...</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.4457, 0.1248, 0.3484, 0.8758, 0.0475, 0.556...</td>\n",
       "      <td>[0.5543, 0.8752, 0.6516, 0.1242, 0.9525, 0.443...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.024, 0.0113, 0.029, 0.0191, 0.0025, 0.0142,...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.3538, 0.2582, 0.2281, 0.6908, 0.0893, 0.526...</td>\n",
       "      <td>[0.6462, 0.7418, 0.7719, 0.3092, 0.9107, 0.473...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0301, 0.0121, 0.0255, 0.0364, 0.0244, 0.053...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7559, 0.7328, 0.723, 0.328, 0.6637, 0.7921,...</td>\n",
       "      <td>[0.2441, 0.2672, 0.277, 0.672, 0.3363, 0.2079,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0414, 0.0038, 0.0937, 0.0147, 0.0033, 0.015...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.3051, 0.2294, 0.4399, 0.173, 0.0743, 0.4897...</td>\n",
       "      <td>[0.6949, 0.7706, 0.5601, 0.827, 0.9257, 0.5103...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0073, 0.0077, 0.0227, 0.0158, 0.01, 0.0062,...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3631, 0.3825, 0.8912, 0.7439, 0.3722, 0.626...</td>\n",
       "      <td>[0.6369, 0.6175, 0.1088, 0.2561, 0.6278, 0.373...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.01, 0.0048, 0.0316, 0.0029, 0.001, 0.0017, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1294, 0.0268, 0.5208, 0.4089, 0.0672, 0.490...</td>\n",
       "      <td>[0.8706, 0.9732, 0.4792, 0.5911, 0.9328, 0.509...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0006, 0.0052, 0.0097, 0.0041, 0.009, 0.0063...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7497, 0.5832, 0.5754, 0.7434, 0.8727, 0.868...</td>\n",
       "      <td>[0.2503, 0.4168, 0.4246, 0.2566, 0.1273, 0.131...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.001, 0.0128, 0.0237, 0.0168, 0.0033, 0.0045...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7737, 0.3422, 0.7053, 0.8567, 0.9461, 0.973...</td>\n",
       "      <td>[0.2263, 0.6578, 0.2947, 0.1433, 0.0539, 0.026...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0006, 0.0122, 0.0028, 0.0049, 0.0156, 0.002...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8548, 0.4767, 0.8656, 0.9346, 0.9786, 0.963...</td>\n",
       "      <td>[0.1452, 0.5233, 0.1344, 0.0654, 0.0214, 0.036...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.1472, 0.7432, 0.1173, 0.0666, 0.7337, 0.569...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9787, 0.9877, 0.4688, 0.9805, 1.0, 0.7929, ...</td>\n",
       "      <td>[0.0213, 0.0123, 0.5312, 0.0195, 0.0, 0.2071, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0037, 0.009, 0.0274, 0.0555, 0.0028, 0.0018...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9186, 0.3991, 0.9003, 0.8196, 0.2998, 0.798...</td>\n",
       "      <td>[0.0814, 0.6009, 0.0997, 0.1804, 0.7002, 0.201...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0034, 0.0036, 0.0155, 0.0534, 0.0014, 0.000...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7824, 0.7155, 0.9554, 0.9641, 0.1733, 0.824...</td>\n",
       "      <td>[0.2176, 0.2845, 0.0446, 0.0359, 0.8267, 0.175...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0371, 0.0073, 0.049, 0.0077, 0.0016, 0.0093...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.1682, 0.2768, 0.0758, 0.2071, 0.0905, 0.208...</td>\n",
       "      <td>[0.8318, 0.7232, 0.9242, 0.7929, 0.9095, 0.791...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0587, 0.0373, 0.0232, 0.0267, 0.1129, 0.046...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.843, 0.9436, 0.1012, 0.4775, 0.7979, 0.1794...</td>\n",
       "      <td>[0.157, 0.0564, 0.8988, 0.5225, 0.2021, 0.8206...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0395, 0.0103, 0.0238, 0.012, 0.0011, 0.0061...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2301, 0.1226, 0.4873, 0.4831, 0.0629, 0.579...</td>\n",
       "      <td>[0.7699, 0.8774, 0.5127, 0.5169, 0.9371, 0.420...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0263, 0.0072, 0.0316, 0.0053, 0.0007, 0.004...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0684, 0.0515, 0.2916, 0.3669, 0.028, 0.4671...</td>\n",
       "      <td>[0.9316, 0.9485, 0.7084, 0.6331, 0.972, 0.5329...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0044, 0.0059, 0.0129, 0.0167, 0.0004, 0.000...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0963, 0.1046, 0.7167, 0.8353, 0.0417, 0.968...</td>\n",
       "      <td>[0.9037, 0.8954, 0.2833, 0.1647, 0.9583, 0.031...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0036, 0.001, 0.038, 0.0119, 0.0007, 0.0012,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7447, 0.0574, 0.0492, 0.3922, 0.0857, 0.945...</td>\n",
       "      <td>[0.2553, 0.9426, 0.9508, 0.6078, 0.9143, 0.054...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0089, 0.0017, 0.0306, 0.0106, 0.0006, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.573, 0.0285, 0.0398, 0.4325, 0.0328, 0.6986...</td>\n",
       "      <td>[0.427, 0.9715, 0.9602, 0.5675, 0.9672, 0.3014...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0091, 0.0144, 0.0049, 0.028, 0.0043, 0.0022...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.8646, 0.3618, 0.2944, 0.934, 0.3608, 0.8179...</td>\n",
       "      <td>[0.1354, 0.6382, 0.7056, 0.066, 0.6392, 0.1821...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0248, 0.0027, 0.0622, 0.0107, 0.0007, 0.004...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1854, 0.0296, 0.0392, 0.1112, 0.0495, 0.737...</td>\n",
       "      <td>[0.8146, 0.9704, 0.9608, 0.8888, 0.9505, 0.262...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0142, 0.0047, 0.0163, 0.0057, 0.0003, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1338, 0.051, 0.5341, 0.4632, 0.0221, 0.4949...</td>\n",
       "      <td>[0.8662, 0.949, 0.4659, 0.5368, 0.9779, 0.5051...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.006, 0.0044, 0.0087, 0.0077, 0.0003, 0.0003...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3565, 0.0719, 0.6582, 0.8353, 0.059, 0.9425...</td>\n",
       "      <td>[0.6435, 0.9281, 0.3418, 0.1647, 0.941, 0.0575...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0028, 0.0033, 0.0087, 0.0057, 0.0002, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2322, 0.0317, 0.8762, 0.8006, 0.0558, 0.986...</td>\n",
       "      <td>[0.7678, 0.9683, 0.1238, 0.1994, 0.9442, 0.013...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0072, 0.0031, 0.0087, 0.0053, 0.0002, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2121, 0.0442, 0.5839, 0.5641, 0.0237, 0.715...</td>\n",
       "      <td>[0.7879, 0.9558, 0.4161, 0.4359, 0.9763, 0.284...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0075, 0.0083, 0.1153, 0.0311, 0.0067, 0.026...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.8042, 0.3877, 0.8738, 0.5036, 0.5243, 0.761...</td>\n",
       "      <td>[0.1958, 0.6123, 0.1262, 0.4964, 0.4757, 0.238...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0205, 0.076, 0.0132, 0.0216, 0.017, 0.0107,...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.7753, 0.6066, 0.7721, 0.7917, 0.3158, 0.442...</td>\n",
       "      <td>[0.2247, 0.3934, 0.2279, 0.2083, 0.6842, 0.557...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0081, 0.0082, 0.0083, 0.0005, 0.0089, 0.015...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2195, 0.6517, 0.0109, 0.16, 0.7086, 0.0976,...</td>\n",
       "      <td>[0.7805, 0.3483, 0.9891, 0.84, 0.2914, 0.9024,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0211, 0.0277, 0.01, 0.0092, 0.0486, 0.1121,...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1986, 0.9677, 0.2515, 0.6434, 0.8076, 0.303...</td>\n",
       "      <td>[0.8014, 0.0323, 0.7485, 0.3566, 0.1924, 0.696...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.038, 0.016, 0.012, 0.0161, 0.0069, 0.0066, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4904, 0.4682, 0.779, 0.7106, 0.207, 0.2975,...</td>\n",
       "      <td>[0.5096, 0.5318, 0.221, 0.2894, 0.793, 0.7025,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0053, 0.0056, 0.0055, 0.0173, 0.0303, 0.001...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7168, 0.8394, 0.7948, 0.7871, 0.6694, 0.447...</td>\n",
       "      <td>[0.2832, 0.1606, 0.2052, 0.2129, 0.3306, 0.552...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3682, 0.0908, 0.0197, 0.0308, 0.0137, 0.029...               0   \n",
       "1   [0.7294, 0.4938, 0.0102, 0.0317, 0.1049, 0.031...               0   \n",
       "2   [0.6882, 0.1982, 0.0098, 0.0921, 0.0214, 0.053...               0   \n",
       "3   [0.0401, 0.22, 0.0044, 0.0055, 0.0702, 0.0322,...               1   \n",
       "4   [0.0925, 0.1082, 0.0185, 0.0053, 0.0063, 0.025...               1   \n",
       "5   [0.0193, 0.0143, 0.2418, 0.0092, 0.0058, 0.048...               2   \n",
       "6   [0.0355, 0.005, 0.0013, 0.8325, 0.0033, 0.0006...               3   \n",
       "7   [0.0272, 0.0147, 0.0147, 0.5855, 0.0046, 0.003...               3   \n",
       "8   [0.0388, 0.0105, 0.0253, 0.5189, 0.0104, 0.010...               3   \n",
       "9   [0.0095, 0.0721, 0.0068, 0.0166, 0.6499, 0.106...               4   \n",
       "10  [0.0477, 0.4183, 0.0043, 0.0053, 0.3375, 0.326...               4   \n",
       "11  [0.0052, 0.0244, 0.0052, 0.004, 0.0785, 0.2859...               5   \n",
       "12  [0.0009, 0.0069, 0.0227, 0.0018, 0.0029, 0.003...               6   \n",
       "13  [0.0616, 0.0631, 0.1804, 0.0709, 0.0034, 0.075...               7   \n",
       "14  [0.0206, 0.0176, 0.0012, 0.0372, 0.0249, 0.003...               8   \n",
       "15  [0.051, 0.0193, 0.005, 0.0053, 0.0215, 0.009, ...               8   \n",
       "16  [0.0124, 0.0062, 0.1457, 0.0048, 0.0011, 0.021...               9   \n",
       "17  [0.0196, 0.0081, 0.0276, 0.0128, 0.0039, 0.006...               9   \n",
       "18  [0.0501, 0.0075, 0.0674, 0.0046, 0.0029, 0.017...              10   \n",
       "19  [0.0342, 0.008, 0.0735, 0.0033, 0.0036, 0.0209...              10   \n",
       "20  [0.0629, 0.0267, 0.0217, 0.0475, 0.0031, 0.005...              11   \n",
       "21  [0.0129, 0.0063, 0.0294, 0.0269, 0.0011, 0.002...              11   \n",
       "22  [0.024, 0.0113, 0.029, 0.0191, 0.0025, 0.0142,...              12   \n",
       "23  [0.0301, 0.0121, 0.0255, 0.0364, 0.0244, 0.053...              13   \n",
       "24  [0.0414, 0.0038, 0.0937, 0.0147, 0.0033, 0.015...              13   \n",
       "25  [0.0073, 0.0077, 0.0227, 0.0158, 0.01, 0.0062,...              14   \n",
       "26  [0.01, 0.0048, 0.0316, 0.0029, 0.001, 0.0017, ...              14   \n",
       "27  [0.0006, 0.0052, 0.0097, 0.0041, 0.009, 0.0063...              15   \n",
       "28  [0.001, 0.0128, 0.0237, 0.0168, 0.0033, 0.0045...              15   \n",
       "29  [0.0006, 0.0122, 0.0028, 0.0049, 0.0156, 0.002...              15   \n",
       "30  [0.1472, 0.7432, 0.1173, 0.0666, 0.7337, 0.569...              16   \n",
       "31  [0.0037, 0.009, 0.0274, 0.0555, 0.0028, 0.0018...              17   \n",
       "32  [0.0034, 0.0036, 0.0155, 0.0534, 0.0014, 0.000...              17   \n",
       "33  [0.0371, 0.0073, 0.049, 0.0077, 0.0016, 0.0093...              18   \n",
       "34  [0.0587, 0.0373, 0.0232, 0.0267, 0.1129, 0.046...              19   \n",
       "35  [0.0395, 0.0103, 0.0238, 0.012, 0.0011, 0.0061...              20   \n",
       "36  [0.0263, 0.0072, 0.0316, 0.0053, 0.0007, 0.004...              21   \n",
       "37  [0.0044, 0.0059, 0.0129, 0.0167, 0.0004, 0.000...              21   \n",
       "38  [0.0036, 0.001, 0.038, 0.0119, 0.0007, 0.0012,...              22   \n",
       "39  [0.0089, 0.0017, 0.0306, 0.0106, 0.0006, 0.001...              22   \n",
       "40  [0.0091, 0.0144, 0.0049, 0.028, 0.0043, 0.0022...              22   \n",
       "41  [0.0248, 0.0027, 0.0622, 0.0107, 0.0007, 0.004...              22   \n",
       "42  [0.0142, 0.0047, 0.0163, 0.0057, 0.0003, 0.000...              23   \n",
       "43  [0.006, 0.0044, 0.0087, 0.0077, 0.0003, 0.0003...              23   \n",
       "44  [0.0028, 0.0033, 0.0087, 0.0057, 0.0002, 0.000...              23   \n",
       "45  [0.0072, 0.0031, 0.0087, 0.0053, 0.0002, 0.000...              23   \n",
       "46  [0.0075, 0.0083, 0.1153, 0.0311, 0.0067, 0.026...              24   \n",
       "47  [0.0205, 0.076, 0.0132, 0.0216, 0.017, 0.0107,...              24   \n",
       "48  [0.0081, 0.0082, 0.0083, 0.0005, 0.0089, 0.015...              25   \n",
       "49  [0.0211, 0.0277, 0.01, 0.0092, 0.0486, 0.1121,...              25   \n",
       "50  [0.038, 0.016, 0.012, 0.0161, 0.0069, 0.0066, ...              26   \n",
       "51  [0.0053, 0.0056, 0.0055, 0.0173, 0.0303, 0.001...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.321, 0.4704, 0.0861, 0.4558, 0.2299, 0.0976...   \n",
       "1                  0  [0.78, 0.7369, 0.2266, 0.6595, 0.604, 0.2608, ...   \n",
       "2                  0  [0.5682, 0.6552, 0.1664, 0.6556, 0.2176, 0.130...   \n",
       "3                  1  [0.5918, 0.5159, 0.1583, 0.7254, 0.8278, 0.501...   \n",
       "4                  1  [0.1551, 0.2344, 0.0387, 0.3479, 0.2559, 0.047...   \n",
       "5                  2  [0.2167, 0.0671, 0.1684, 0.2711, 0.1597, 0.576...   \n",
       "6                  3  [0.98, 0.9098, 0.9671, 0.9762, 0.1616, 0.9622,...   \n",
       "7                  3  [0.9198, 0.439, 0.8706, 0.7643, 0.1734, 0.9502...   \n",
       "8                  3  [0.8797, 0.6117, 0.9431, 0.6282, 0.1782, 0.875...   \n",
       "9                  4  [0.8731, 0.9656, 0.8203, 0.8194, 0.9718, 0.796...   \n",
       "10                 1  [0.6594, 0.9536, 0.1233, 0.6019, 0.8656, 0.698...   \n",
       "11                 5  [0.4432, 0.8664, 0.2373, 0.5298, 0.863, 0.588,...   \n",
       "12                 6  [0.6305, 0.6598, 0.8565, 0.9722, 0.8517, 0.687...   \n",
       "13                 7  [0.3395, 0.0794, 0.259, 0.6346, 0.7357, 0.9567...   \n",
       "14                 8  [0.7617, 0.776, 0.5928, 0.9499, 0.6864, 0.5904...   \n",
       "15                 8  [0.2168, 0.8457, 0.41, 0.5371, 0.6446, 0.1302,...   \n",
       "16                 9  [0.2386, 0.1316, 0.1146, 0.336, 0.1777, 0.4176...   \n",
       "17                 9  [0.5646, 0.6049, 0.5304, 0.726, 0.43, 0.6895, ...   \n",
       "18                10  [0.1538, 0.4916, 0.0888, 0.0966, 0.5212, 0.339...   \n",
       "19                10  [0.1847, 0.7255, 0.1056, 0.2398, 0.6792, 0.208...   \n",
       "20                11  [0.4887, 0.3486, 0.6595, 0.8975, 0.123, 0.384,...   \n",
       "21                22  [0.4457, 0.1248, 0.3484, 0.8758, 0.0475, 0.556...   \n",
       "22                12  [0.3538, 0.2582, 0.2281, 0.6908, 0.0893, 0.526...   \n",
       "23                13  [0.7559, 0.7328, 0.723, 0.328, 0.6637, 0.7921,...   \n",
       "24                10  [0.3051, 0.2294, 0.4399, 0.173, 0.0743, 0.4897...   \n",
       "25                14  [0.3631, 0.3825, 0.8912, 0.7439, 0.3722, 0.626...   \n",
       "26                14  [0.1294, 0.0268, 0.5208, 0.4089, 0.0672, 0.490...   \n",
       "27                15  [0.7497, 0.5832, 0.5754, 0.7434, 0.8727, 0.868...   \n",
       "28                15  [0.7737, 0.3422, 0.7053, 0.8567, 0.9461, 0.973...   \n",
       "29                15  [0.8548, 0.4767, 0.8656, 0.9346, 0.9786, 0.963...   \n",
       "30                16  [0.9787, 0.9877, 0.4688, 0.9805, 1.0, 0.7929, ...   \n",
       "31                17  [0.9186, 0.3991, 0.9003, 0.8196, 0.2998, 0.798...   \n",
       "32                17  [0.7824, 0.7155, 0.9554, 0.9641, 0.1733, 0.824...   \n",
       "33                18  [0.1682, 0.2768, 0.0758, 0.2071, 0.0905, 0.208...   \n",
       "34                19  [0.843, 0.9436, 0.1012, 0.4775, 0.7979, 0.1794...   \n",
       "35                10  [0.2301, 0.1226, 0.4873, 0.4831, 0.0629, 0.579...   \n",
       "36                23  [0.0684, 0.0515, 0.2916, 0.3669, 0.028, 0.4671...   \n",
       "37                23  [0.0963, 0.1046, 0.7167, 0.8353, 0.0417, 0.968...   \n",
       "38                22  [0.7447, 0.0574, 0.0492, 0.3922, 0.0857, 0.945...   \n",
       "39                22  [0.573, 0.0285, 0.0398, 0.4325, 0.0328, 0.6986...   \n",
       "40                22  [0.8646, 0.3618, 0.2944, 0.934, 0.3608, 0.8179...   \n",
       "41                22  [0.1854, 0.0296, 0.0392, 0.1112, 0.0495, 0.737...   \n",
       "42                23  [0.1338, 0.051, 0.5341, 0.4632, 0.0221, 0.4949...   \n",
       "43                23  [0.3565, 0.0719, 0.6582, 0.8353, 0.059, 0.9425...   \n",
       "44                23  [0.2322, 0.0317, 0.8762, 0.8006, 0.0558, 0.986...   \n",
       "45                23  [0.2121, 0.0442, 0.5839, 0.5641, 0.0237, 0.715...   \n",
       "46                13  [0.8042, 0.3877, 0.8738, 0.5036, 0.5243, 0.761...   \n",
       "47                24  [0.7753, 0.6066, 0.7721, 0.7917, 0.3158, 0.442...   \n",
       "48                25  [0.2195, 0.6517, 0.0109, 0.16, 0.7086, 0.0976,...   \n",
       "49                25  [0.1986, 0.9677, 0.2515, 0.6434, 0.8076, 0.303...   \n",
       "50                26  [0.4904, 0.4682, 0.779, 0.7106, 0.207, 0.2975,...   \n",
       "51                26  [0.7168, 0.8394, 0.7948, 0.7871, 0.6694, 0.447...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.679, 0.5296, 0.9139, 0.5442, 0.7701, 0.9024...  0.865385  0.065449  \n",
       "1   [0.22, 0.2631, 0.7734, 0.3405, 0.396, 0.7392, ...       NaN       NaN  \n",
       "2   [0.4318, 0.3448, 0.8336, 0.3444, 0.7824, 0.869...       NaN       NaN  \n",
       "3   [0.4082, 0.4841, 0.8417, 0.2746, 0.1722, 0.498...       NaN       NaN  \n",
       "4   [0.8449, 0.7656, 0.9613, 0.6521, 0.7441, 0.952...       NaN       NaN  \n",
       "5   [0.7833, 0.9329, 0.8316, 0.7289, 0.8403, 0.423...       NaN       NaN  \n",
       "6   [0.02, 0.0902, 0.0329, 0.0238, 0.8384, 0.0378,...       NaN       NaN  \n",
       "7   [0.0802, 0.561, 0.1294, 0.2357, 0.8266, 0.0498...       NaN       NaN  \n",
       "8   [0.1203, 0.3883, 0.0569, 0.3718, 0.8218, 0.124...       NaN       NaN  \n",
       "9   [0.1269, 0.0344, 0.1797, 0.1806, 0.0282, 0.203...       NaN       NaN  \n",
       "10  [0.3406, 0.0464, 0.8767, 0.3981, 0.1344, 0.301...       NaN       NaN  \n",
       "11  [0.5568, 0.1336, 0.7627, 0.4702, 0.137, 0.412,...       NaN       NaN  \n",
       "12  [0.3695, 0.3402, 0.1435, 0.0278, 0.1483, 0.312...       NaN       NaN  \n",
       "13  [0.6605, 0.9206, 0.741, 0.3654, 0.2643, 0.0433...       NaN       NaN  \n",
       "14  [0.2383, 0.224, 0.4072, 0.0501, 0.3136, 0.4096...       NaN       NaN  \n",
       "15  [0.7832, 0.1543, 0.59, 0.4629, 0.3554, 0.8698,...       NaN       NaN  \n",
       "16  [0.7614, 0.8684, 0.8854, 0.664, 0.8223, 0.5824...       NaN       NaN  \n",
       "17  [0.4354, 0.3951, 0.4696, 0.274, 0.57, 0.3105, ...       NaN       NaN  \n",
       "18  [0.8462, 0.5084, 0.9112, 0.9034, 0.4788, 0.660...       NaN       NaN  \n",
       "19  [0.8153, 0.2745, 0.8944, 0.7602, 0.3208, 0.791...       NaN       NaN  \n",
       "20  [0.5113, 0.6514, 0.3405, 0.1025, 0.877, 0.616,...       NaN       NaN  \n",
       "21  [0.5543, 0.8752, 0.6516, 0.1242, 0.9525, 0.443...       NaN       NaN  \n",
       "22  [0.6462, 0.7418, 0.7719, 0.3092, 0.9107, 0.473...       NaN       NaN  \n",
       "23  [0.2441, 0.2672, 0.277, 0.672, 0.3363, 0.2079,...       NaN       NaN  \n",
       "24  [0.6949, 0.7706, 0.5601, 0.827, 0.9257, 0.5103...       NaN       NaN  \n",
       "25  [0.6369, 0.6175, 0.1088, 0.2561, 0.6278, 0.373...       NaN       NaN  \n",
       "26  [0.8706, 0.9732, 0.4792, 0.5911, 0.9328, 0.509...       NaN       NaN  \n",
       "27  [0.2503, 0.4168, 0.4246, 0.2566, 0.1273, 0.131...       NaN       NaN  \n",
       "28  [0.2263, 0.6578, 0.2947, 0.1433, 0.0539, 0.026...       NaN       NaN  \n",
       "29  [0.1452, 0.5233, 0.1344, 0.0654, 0.0214, 0.036...       NaN       NaN  \n",
       "30  [0.0213, 0.0123, 0.5312, 0.0195, 0.0, 0.2071, ...       NaN       NaN  \n",
       "31  [0.0814, 0.6009, 0.0997, 0.1804, 0.7002, 0.201...       NaN       NaN  \n",
       "32  [0.2176, 0.2845, 0.0446, 0.0359, 0.8267, 0.175...       NaN       NaN  \n",
       "33  [0.8318, 0.7232, 0.9242, 0.7929, 0.9095, 0.791...       NaN       NaN  \n",
       "34  [0.157, 0.0564, 0.8988, 0.5225, 0.2021, 0.8206...       NaN       NaN  \n",
       "35  [0.7699, 0.8774, 0.5127, 0.5169, 0.9371, 0.420...       NaN       NaN  \n",
       "36  [0.9316, 0.9485, 0.7084, 0.6331, 0.972, 0.5329...       NaN       NaN  \n",
       "37  [0.9037, 0.8954, 0.2833, 0.1647, 0.9583, 0.031...       NaN       NaN  \n",
       "38  [0.2553, 0.9426, 0.9508, 0.6078, 0.9143, 0.054...       NaN       NaN  \n",
       "39  [0.427, 0.9715, 0.9602, 0.5675, 0.9672, 0.3014...       NaN       NaN  \n",
       "40  [0.1354, 0.6382, 0.7056, 0.066, 0.6392, 0.1821...       NaN       NaN  \n",
       "41  [0.8146, 0.9704, 0.9608, 0.8888, 0.9505, 0.262...       NaN       NaN  \n",
       "42  [0.8662, 0.949, 0.4659, 0.5368, 0.9779, 0.5051...       NaN       NaN  \n",
       "43  [0.6435, 0.9281, 0.3418, 0.1647, 0.941, 0.0575...       NaN       NaN  \n",
       "44  [0.7678, 0.9683, 0.1238, 0.1994, 0.9442, 0.013...       NaN       NaN  \n",
       "45  [0.7879, 0.9558, 0.4161, 0.4359, 0.9763, 0.284...       NaN       NaN  \n",
       "46  [0.1958, 0.6123, 0.1262, 0.4964, 0.4757, 0.238...       NaN       NaN  \n",
       "47  [0.2247, 0.3934, 0.2279, 0.2083, 0.6842, 0.557...       NaN       NaN  \n",
       "48  [0.7805, 0.3483, 0.9891, 0.84, 0.2914, 0.9024,...       NaN       NaN  \n",
       "49  [0.8014, 0.0323, 0.7485, 0.3566, 0.1924, 0.696...       NaN       NaN  \n",
       "50  [0.5096, 0.5318, 0.221, 0.2894, 0.793, 0.7025,...       NaN       NaN  \n",
       "51  [0.2832, 0.1606, 0.2052, 0.2129, 0.3306, 0.552...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381 : Training: loss:  0.06065271\n",
      "2382 : Training: loss:  0.06952911\n",
      "2383 : Training: loss:  0.028431825\n",
      "2384 : Training: loss:  0.059465975\n",
      "2385 : Training: loss:  0.027201755\n",
      "2386 : Training: loss:  0.05909323\n",
      "2387 : Training: loss:  0.049243025\n",
      "2388 : Training: loss:  0.04645348\n",
      "2389 : Training: loss:  0.042331774\n",
      "2390 : Training: loss:  0.048819378\n",
      "2391 : Training: loss:  0.05087041\n",
      "2392 : Training: loss:  0.05469296\n",
      "2393 : Training: loss:  0.050868545\n",
      "2394 : Training: loss:  0.027004207\n",
      "2395 : Training: loss:  0.022004724\n",
      "2396 : Training: loss:  0.041657954\n",
      "2397 : Training: loss:  0.030764889\n",
      "2398 : Training: loss:  0.043843776\n",
      "2399 : Training: loss:  0.023540104\n",
      "2400 : Training: loss:  0.041873198\n",
      "Validation: Loss:  0.064801894  Accuracy:  0.86538464\n",
      "2401 : Training: loss:  0.030134961\n",
      "2402 : Training: loss:  0.040191486\n",
      "2403 : Training: loss:  0.05695261\n",
      "2404 : Training: loss:  0.042316824\n",
      "2405 : Training: loss:  0.053900473\n",
      "2406 : Training: loss:  0.038758274\n",
      "2407 : Training: loss:  0.037504464\n",
      "2408 : Training: loss:  0.032491107\n",
      "2409 : Training: loss:  0.08115081\n",
      "2410 : Training: loss:  0.06830095\n",
      "2411 : Training: loss:  0.071357094\n",
      "2412 : Training: loss:  0.044520542\n",
      "2413 : Training: loss:  0.0437454\n",
      "2414 : Training: loss:  0.03662502\n",
      "2415 : Training: loss:  0.045869894\n",
      "2416 : Training: loss:  0.050553445\n",
      "2417 : Training: loss:  0.046754498\n",
      "2418 : Training: loss:  0.03692128\n",
      "2419 : Training: loss:  0.027930688\n",
      "2420 : Training: loss:  0.029751027\n",
      "Validation: Loss:  0.06400759  Accuracy:  0.86538464\n",
      "2421 : Training: loss:  0.048966307\n",
      "2422 : Training: loss:  0.06837935\n",
      "2423 : Training: loss:  0.0479795\n",
      "2424 : Training: loss:  0.054222044\n",
      "2425 : Training: loss:  0.036730856\n",
      "2426 : Training: loss:  0.07084335\n",
      "2427 : Training: loss:  0.031863537\n",
      "2428 : Training: loss:  0.024364589\n",
      "2429 : Training: loss:  0.057012554\n",
      "2430 : Training: loss:  0.027429877\n",
      "2431 : Training: loss:  0.051384944\n",
      "2432 : Training: loss:  0.036995735\n",
      "2433 : Training: loss:  0.03644693\n",
      "2434 : Training: loss:  0.056970928\n",
      "2435 : Training: loss:  0.048680477\n",
      "2436 : Training: loss:  0.07385951\n",
      "2437 : Training: loss:  0.05110806\n",
      "2438 : Training: loss:  0.050983932\n",
      "2439 : Training: loss:  0.02572736\n",
      "2440 : Training: loss:  0.059014842\n",
      "Validation: Loss:  0.063200116  Accuracy:  0.86538464\n",
      "2441 : Training: loss:  0.050113816\n",
      "2442 : Training: loss:  0.049595375\n",
      "2443 : Training: loss:  0.04380708\n",
      "2444 : Training: loss:  0.04104841\n",
      "2445 : Training: loss:  0.04768169\n",
      "2446 : Training: loss:  0.04468487\n",
      "2447 : Training: loss:  0.04216613\n",
      "2448 : Training: loss:  0.052469686\n",
      "2449 : Training: loss:  0.03543296\n",
      "2450 : Training: loss:  0.031158779\n",
      "2451 : Training: loss:  0.034773767\n",
      "2452 : Training: loss:  0.053148184\n",
      "2453 : Training: loss:  0.056476325\n",
      "2454 : Training: loss:  0.023223912\n",
      "2455 : Training: loss:  0.04280328\n",
      "2456 : Training: loss:  0.05601504\n",
      "2457 : Training: loss:  0.05007929\n",
      "2458 : Training: loss:  0.048477013\n",
      "2459 : Training: loss:  0.047802463\n",
      "2460 : Training: loss:  0.043801423\n",
      "Validation: Loss:  0.06230695  Accuracy:  0.90384614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3682, 0.0915, 0.0174, 0.0288, 0.0139, 0.029...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.328, 0.4804, 0.0857, 0.4697, 0.2436, 0.0987...</td>\n",
       "      <td>[0.672, 0.5196, 0.9143, 0.5303, 0.7564, 0.9013...</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.062307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.7395, 0.4955, 0.0091, 0.0304, 0.1096, 0.029...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7849, 0.7413, 0.2286, 0.6618, 0.6232, 0.268...</td>\n",
       "      <td>[0.2151, 0.2587, 0.7714, 0.3382, 0.3768, 0.731...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.7187, 0.2014, 0.0081, 0.0933, 0.0223, 0.055...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5784, 0.6678, 0.167, 0.6734, 0.2312, 0.1315...</td>\n",
       "      <td>[0.4216, 0.3322, 0.833, 0.3266, 0.7688, 0.8685...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0361, 0.2318, 0.0037, 0.0052, 0.0769, 0.032...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.596, 0.5223, 0.16, 0.734, 0.84, 0.5159, 0.5...</td>\n",
       "      <td>[0.404, 0.4777, 0.84, 0.266, 0.16, 0.4841, 0.4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0902, 0.1231, 0.0169, 0.0048, 0.0063, 0.026...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1593, 0.2407, 0.0382, 0.3623, 0.2725, 0.049...</td>\n",
       "      <td>[0.8407, 0.7593, 0.9618, 0.6377, 0.7275, 0.950...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0163, 0.0138, 0.2582, 0.0082, 0.0057, 0.047...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.2235, 0.0701, 0.1725, 0.286, 0.1627, 0.6034...</td>\n",
       "      <td>[0.7765, 0.9299, 0.8275, 0.714, 0.8373, 0.3966...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0324, 0.0038, 0.001, 0.8518, 0.0031, 0.0005...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9813, 0.9192, 0.9689, 0.9781, 0.1599, 0.963...</td>\n",
       "      <td>[0.0187, 0.0808, 0.0311, 0.0219, 0.8401, 0.036...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0249, 0.013, 0.0125, 0.6224, 0.0045, 0.0033...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9227, 0.4564, 0.8766, 0.7726, 0.1722, 0.953...</td>\n",
       "      <td>[0.0773, 0.5436, 0.1234, 0.2274, 0.8278, 0.046...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0364, 0.0091, 0.0216, 0.556, 0.0106, 0.0096...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8856, 0.6351, 0.9477, 0.6353, 0.1756, 0.883...</td>\n",
       "      <td>[0.1144, 0.3649, 0.0523, 0.3647, 0.8244, 0.116...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0079, 0.0624, 0.006, 0.0161, 0.6835, 0.0982...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8757, 0.9667, 0.823, 0.8142, 0.9732, 0.8034...</td>\n",
       "      <td>[0.1243, 0.0333, 0.177, 0.1858, 0.0268, 0.1966...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0436, 0.4229, 0.0036, 0.0049, 0.3732, 0.345...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6639, 0.9556, 0.1192, 0.5898, 0.8743, 0.712...</td>\n",
       "      <td>[0.3361, 0.0444, 0.8808, 0.4102, 0.1257, 0.287...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0044, 0.0227, 0.0044, 0.0035, 0.0918, 0.320...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4484, 0.8739, 0.2366, 0.5275, 0.8747, 0.599...</td>\n",
       "      <td>[0.5516, 0.1261, 0.7634, 0.4725, 0.1253, 0.400...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0007, 0.0061, 0.0199, 0.0014, 0.0025, 0.002...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6226, 0.6583, 0.8555, 0.9732, 0.8595, 0.686...</td>\n",
       "      <td>[0.3774, 0.3417, 0.1445, 0.0268, 0.1405, 0.313...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0557, 0.0607, 0.1631, 0.063, 0.0028, 0.0693...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.3262, 0.0735, 0.2534, 0.6341, 0.7291, 0.959...</td>\n",
       "      <td>[0.6738, 0.9265, 0.7466, 0.3659, 0.2709, 0.040...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.019, 0.0159, 0.001, 0.0351, 0.0283, 0.0036,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.766, 0.7896, 0.5934, 0.9531, 0.6999, 0.5955...</td>\n",
       "      <td>[0.234, 0.2104, 0.4066, 0.0469, 0.3001, 0.4045...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0459, 0.0174, 0.0042, 0.0047, 0.0226, 0.008...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2178, 0.854, 0.4095, 0.5362, 0.6613, 0.1317...</td>\n",
       "      <td>[0.7822, 0.146, 0.5905, 0.4638, 0.3387, 0.8683...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0107, 0.0057, 0.1425, 0.0041, 0.0009, 0.020...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.2471, 0.1388, 0.1163, 0.3489, 0.1808, 0.435...</td>\n",
       "      <td>[0.7529, 0.8612, 0.8837, 0.6511, 0.8192, 0.564...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0178, 0.0071, 0.0245, 0.012, 0.0036, 0.0052...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5768, 0.6206, 0.5355, 0.7332, 0.4269, 0.703...</td>\n",
       "      <td>[0.4232, 0.3794, 0.4645, 0.2668, 0.5731, 0.296...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0446, 0.0071, 0.0636, 0.004, 0.003, 0.0174,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1563, 0.5019, 0.0892, 0.0929, 0.5428, 0.354...</td>\n",
       "      <td>[0.8437, 0.4981, 0.9108, 0.9071, 0.4572, 0.645...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0296, 0.0075, 0.0676, 0.0028, 0.0035, 0.020...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1868, 0.7362, 0.1047, 0.2305, 0.6998, 0.212...</td>\n",
       "      <td>[0.8132, 0.2638, 0.8953, 0.7695, 0.3002, 0.787...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0592, 0.0248, 0.0198, 0.046, 0.0028, 0.0045...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4954, 0.3582, 0.6691, 0.904, 0.1233, 0.3952...</td>\n",
       "      <td>[0.5046, 0.6418, 0.3309, 0.096, 0.8767, 0.6048...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0113, 0.0059, 0.0276, 0.0265, 0.001, 0.0019...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4523, 0.1289, 0.3631, 0.8841, 0.0491, 0.572...</td>\n",
       "      <td>[0.5477, 0.8711, 0.6369, 0.1159, 0.9509, 0.427...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0214, 0.0106, 0.0257, 0.018, 0.0024, 0.0143...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.3613, 0.2697, 0.2352, 0.7049, 0.0929, 0.543...</td>\n",
       "      <td>[0.6387, 0.7303, 0.7648, 0.2951, 0.9071, 0.456...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.028, 0.0107, 0.0215, 0.0361, 0.0266, 0.0526...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.76, 0.7475, 0.7318, 0.3172, 0.6771, 0.8021,...</td>\n",
       "      <td>[0.24, 0.2525, 0.2682, 0.6828, 0.3229, 0.1979,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0368, 0.0033, 0.0921, 0.014, 0.0034, 0.0148...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.3192, 0.2475, 0.4558, 0.1788, 0.0769, 0.512...</td>\n",
       "      <td>[0.6808, 0.7525, 0.5442, 0.8212, 0.9231, 0.488...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0062, 0.0069, 0.0206, 0.015, 0.0098, 0.0055...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3656, 0.3967, 0.8986, 0.7584, 0.3781, 0.645...</td>\n",
       "      <td>[0.6344, 0.6033, 0.1014, 0.2416, 0.6219, 0.355...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0083, 0.0045, 0.029, 0.0025, 0.0009, 0.0014...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1288, 0.0269, 0.5386, 0.4264, 0.0688, 0.512...</td>\n",
       "      <td>[0.8712, 0.9731, 0.4614, 0.5736, 0.9312, 0.487...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0005, 0.0046, 0.0083, 0.0038, 0.0091, 0.005...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7531, 0.5895, 0.5857, 0.7558, 0.876, 0.877,...</td>\n",
       "      <td>[0.2469, 0.4105, 0.4143, 0.2442, 0.124, 0.123,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0008, 0.012, 0.0203, 0.0151, 0.003, 0.0037,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7722, 0.3336, 0.7099, 0.8609, 0.9443, 0.975...</td>\n",
       "      <td>[0.2278, 0.6664, 0.2901, 0.1391, 0.0557, 0.024...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0005, 0.0117, 0.0022, 0.0046, 0.0162, 0.002...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8517, 0.4748, 0.8691, 0.9381, 0.9791, 0.965...</td>\n",
       "      <td>[0.1483, 0.5252, 0.1309, 0.0619, 0.0209, 0.034...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.127, 0.7436, 0.102, 0.0539, 0.7298, 0.5652,...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9751, 0.9855, 0.406, 0.9727, 1.0, 0.7632, 0...</td>\n",
       "      <td>[0.0249, 0.0145, 0.594, 0.0273, 0.0, 0.2368, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0031, 0.0078, 0.0252, 0.0533, 0.0025, 0.001...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9216, 0.405, 0.9082, 0.8289, 0.3012, 0.8073...</td>\n",
       "      <td>[0.0784, 0.595, 0.0918, 0.1711, 0.6988, 0.1927...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0028, 0.003, 0.0139, 0.0513, 0.0012, 0.0007...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.7893, 0.7283, 0.9594, 0.9677, 0.1737, 0.832...</td>\n",
       "      <td>[0.2107, 0.2717, 0.0406, 0.0323, 0.8263, 0.168...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0338, 0.0068, 0.0463, 0.0072, 0.0016, 0.008...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.1751, 0.292, 0.0778, 0.2168, 0.0937, 0.2237...</td>\n",
       "      <td>[0.8249, 0.708, 0.9222, 0.7832, 0.9063, 0.7763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0567, 0.0354, 0.0196, 0.0256, 0.1259, 0.045...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8503, 0.9482, 0.102, 0.4854, 0.8126, 0.1841...</td>\n",
       "      <td>[0.1497, 0.0518, 0.898, 0.5146, 0.1874, 0.8159...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0357, 0.0099, 0.0219, 0.0114, 0.0011, 0.005...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.2358, 0.1274, 0.5055, 0.498, 0.0652, 0.6028...</td>\n",
       "      <td>[0.7642, 0.8726, 0.4945, 0.502, 0.9348, 0.3972...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0245, 0.0067, 0.028, 0.0048, 0.0006, 0.0045...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0689, 0.0529, 0.303, 0.3799, 0.0287, 0.4887...</td>\n",
       "      <td>[0.9311, 0.9471, 0.697, 0.6201, 0.9713, 0.5113...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0039, 0.0053, 0.011, 0.0155, 0.0004, 0.0008...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0919, 0.1026, 0.7223, 0.8371, 0.0414, 0.969...</td>\n",
       "      <td>[0.9081, 0.8974, 0.2777, 0.1629, 0.9586, 0.030...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.003, 0.0009, 0.0352, 0.0105, 0.0006, 0.001,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7482, 0.0573, 0.0482, 0.4069, 0.0864, 0.949...</td>\n",
       "      <td>[0.2518, 0.9427, 0.9518, 0.5931, 0.9136, 0.050...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0076, 0.0015, 0.028, 0.0097, 0.0006, 0.0015...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5818, 0.0297, 0.0404, 0.4509, 0.0344, 0.718...</td>\n",
       "      <td>[0.4182, 0.9703, 0.9596, 0.5491, 0.9656, 0.281...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0075, 0.0128, 0.0043, 0.025, 0.0041, 0.0019...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.867, 0.3618, 0.2987, 0.9389, 0.3667, 0.8241...</td>\n",
       "      <td>[0.133, 0.6382, 0.7013, 0.0611, 0.6333, 0.1759...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0217, 0.0024, 0.0594, 0.0097, 0.0006, 0.004...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1908, 0.0307, 0.0402, 0.1152, 0.0509, 0.759...</td>\n",
       "      <td>[0.8092, 0.9693, 0.9598, 0.8848, 0.9491, 0.240...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0129, 0.0043, 0.0145, 0.0052, 0.0003, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1359, 0.0531, 0.5442, 0.4746, 0.0223, 0.508...</td>\n",
       "      <td>[0.8641, 0.9469, 0.4558, 0.5254, 0.9777, 0.491...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0053, 0.0039, 0.0074, 0.0069, 0.0003, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3525, 0.0721, 0.6609, 0.8349, 0.0585, 0.944...</td>\n",
       "      <td>[0.6475, 0.9279, 0.3391, 0.1651, 0.9415, 0.056...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0023, 0.0029, 0.0075, 0.005, 0.0002, 0.0002...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2254, 0.0307, 0.8797, 0.7998, 0.0549, 0.987...</td>\n",
       "      <td>[0.7746, 0.9693, 0.1203, 0.2002, 0.9451, 0.012...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0065, 0.0028, 0.0075, 0.0049, 0.0002, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2117, 0.045, 0.5938, 0.5744, 0.0234, 0.7247...</td>\n",
       "      <td>[0.7883, 0.955, 0.4062, 0.4256, 0.9766, 0.2753...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0064, 0.0073, 0.1085, 0.0282, 0.0063, 0.023...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.8089, 0.4006, 0.8819, 0.5016, 0.5299, 0.772...</td>\n",
       "      <td>[0.1911, 0.5994, 0.1181, 0.4984, 0.4701, 0.228...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0186, 0.0738, 0.0117, 0.0217, 0.0167, 0.009...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.7864, 0.621, 0.7848, 0.7998, 0.3185, 0.4442...</td>\n",
       "      <td>[0.2136, 0.379, 0.2152, 0.2002, 0.6815, 0.5558...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0069, 0.0079, 0.0069, 0.0004, 0.0097, 0.015...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2203, 0.6595, 0.0107, 0.1642, 0.7292, 0.101...</td>\n",
       "      <td>[0.7797, 0.3405, 0.9893, 0.8358, 0.2708, 0.898...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0182, 0.0255, 0.0081, 0.0087, 0.0513, 0.119...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1966, 0.9689, 0.251, 0.6423, 0.8189, 0.3107...</td>\n",
       "      <td>[0.8034, 0.0311, 0.749, 0.3577, 0.1811, 0.6893...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0345, 0.0145, 0.011, 0.0149, 0.0064, 0.0058...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4913, 0.4688, 0.7842, 0.7195, 0.2107, 0.300...</td>\n",
       "      <td>[0.5087, 0.5312, 0.2158, 0.2805, 0.7893, 0.699...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0045, 0.0047, 0.0048, 0.0169, 0.0316, 0.001...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7261, 0.8475, 0.8019, 0.794, 0.679, 0.4596,...</td>\n",
       "      <td>[0.2739, 0.1525, 0.1981, 0.206, 0.321, 0.5404,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3682, 0.0915, 0.0174, 0.0288, 0.0139, 0.029...               0   \n",
       "1   [0.7395, 0.4955, 0.0091, 0.0304, 0.1096, 0.029...               0   \n",
       "2   [0.7187, 0.2014, 0.0081, 0.0933, 0.0223, 0.055...               0   \n",
       "3   [0.0361, 0.2318, 0.0037, 0.0052, 0.0769, 0.032...               1   \n",
       "4   [0.0902, 0.1231, 0.0169, 0.0048, 0.0063, 0.026...               1   \n",
       "5   [0.0163, 0.0138, 0.2582, 0.0082, 0.0057, 0.047...               2   \n",
       "6   [0.0324, 0.0038, 0.001, 0.8518, 0.0031, 0.0005...               3   \n",
       "7   [0.0249, 0.013, 0.0125, 0.6224, 0.0045, 0.0033...               3   \n",
       "8   [0.0364, 0.0091, 0.0216, 0.556, 0.0106, 0.0096...               3   \n",
       "9   [0.0079, 0.0624, 0.006, 0.0161, 0.6835, 0.0982...               4   \n",
       "10  [0.0436, 0.4229, 0.0036, 0.0049, 0.3732, 0.345...               4   \n",
       "11  [0.0044, 0.0227, 0.0044, 0.0035, 0.0918, 0.320...               5   \n",
       "12  [0.0007, 0.0061, 0.0199, 0.0014, 0.0025, 0.002...               6   \n",
       "13  [0.0557, 0.0607, 0.1631, 0.063, 0.0028, 0.0693...               7   \n",
       "14  [0.019, 0.0159, 0.001, 0.0351, 0.0283, 0.0036,...               8   \n",
       "15  [0.0459, 0.0174, 0.0042, 0.0047, 0.0226, 0.008...               8   \n",
       "16  [0.0107, 0.0057, 0.1425, 0.0041, 0.0009, 0.020...               9   \n",
       "17  [0.0178, 0.0071, 0.0245, 0.012, 0.0036, 0.0052...               9   \n",
       "18  [0.0446, 0.0071, 0.0636, 0.004, 0.003, 0.0174,...              10   \n",
       "19  [0.0296, 0.0075, 0.0676, 0.0028, 0.0035, 0.020...              10   \n",
       "20  [0.0592, 0.0248, 0.0198, 0.046, 0.0028, 0.0045...              11   \n",
       "21  [0.0113, 0.0059, 0.0276, 0.0265, 0.001, 0.0019...              11   \n",
       "22  [0.0214, 0.0106, 0.0257, 0.018, 0.0024, 0.0143...              12   \n",
       "23  [0.028, 0.0107, 0.0215, 0.0361, 0.0266, 0.0526...              13   \n",
       "24  [0.0368, 0.0033, 0.0921, 0.014, 0.0034, 0.0148...              13   \n",
       "25  [0.0062, 0.0069, 0.0206, 0.015, 0.0098, 0.0055...              14   \n",
       "26  [0.0083, 0.0045, 0.029, 0.0025, 0.0009, 0.0014...              14   \n",
       "27  [0.0005, 0.0046, 0.0083, 0.0038, 0.0091, 0.005...              15   \n",
       "28  [0.0008, 0.012, 0.0203, 0.0151, 0.003, 0.0037,...              15   \n",
       "29  [0.0005, 0.0117, 0.0022, 0.0046, 0.0162, 0.002...              15   \n",
       "30  [0.127, 0.7436, 0.102, 0.0539, 0.7298, 0.5652,...              16   \n",
       "31  [0.0031, 0.0078, 0.0252, 0.0533, 0.0025, 0.001...              17   \n",
       "32  [0.0028, 0.003, 0.0139, 0.0513, 0.0012, 0.0007...              17   \n",
       "33  [0.0338, 0.0068, 0.0463, 0.0072, 0.0016, 0.008...              18   \n",
       "34  [0.0567, 0.0354, 0.0196, 0.0256, 0.1259, 0.045...              19   \n",
       "35  [0.0357, 0.0099, 0.0219, 0.0114, 0.0011, 0.005...              20   \n",
       "36  [0.0245, 0.0067, 0.028, 0.0048, 0.0006, 0.0045...              21   \n",
       "37  [0.0039, 0.0053, 0.011, 0.0155, 0.0004, 0.0008...              21   \n",
       "38  [0.003, 0.0009, 0.0352, 0.0105, 0.0006, 0.001,...              22   \n",
       "39  [0.0076, 0.0015, 0.028, 0.0097, 0.0006, 0.0015...              22   \n",
       "40  [0.0075, 0.0128, 0.0043, 0.025, 0.0041, 0.0019...              22   \n",
       "41  [0.0217, 0.0024, 0.0594, 0.0097, 0.0006, 0.004...              22   \n",
       "42  [0.0129, 0.0043, 0.0145, 0.0052, 0.0003, 0.000...              23   \n",
       "43  [0.0053, 0.0039, 0.0074, 0.0069, 0.0003, 0.000...              23   \n",
       "44  [0.0023, 0.0029, 0.0075, 0.005, 0.0002, 0.0002...              23   \n",
       "45  [0.0065, 0.0028, 0.0075, 0.0049, 0.0002, 0.000...              23   \n",
       "46  [0.0064, 0.0073, 0.1085, 0.0282, 0.0063, 0.023...              24   \n",
       "47  [0.0186, 0.0738, 0.0117, 0.0217, 0.0167, 0.009...              24   \n",
       "48  [0.0069, 0.0079, 0.0069, 0.0004, 0.0097, 0.015...              25   \n",
       "49  [0.0182, 0.0255, 0.0081, 0.0087, 0.0513, 0.119...              25   \n",
       "50  [0.0345, 0.0145, 0.011, 0.0149, 0.0064, 0.0058...              26   \n",
       "51  [0.0045, 0.0047, 0.0048, 0.0169, 0.0316, 0.001...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.328, 0.4804, 0.0857, 0.4697, 0.2436, 0.0987...   \n",
       "1                  0  [0.7849, 0.7413, 0.2286, 0.6618, 0.6232, 0.268...   \n",
       "2                  0  [0.5784, 0.6678, 0.167, 0.6734, 0.2312, 0.1315...   \n",
       "3                  1  [0.596, 0.5223, 0.16, 0.734, 0.84, 0.5159, 0.5...   \n",
       "4                  1  [0.1593, 0.2407, 0.0382, 0.3623, 0.2725, 0.049...   \n",
       "5                  2  [0.2235, 0.0701, 0.1725, 0.286, 0.1627, 0.6034...   \n",
       "6                  3  [0.9813, 0.9192, 0.9689, 0.9781, 0.1599, 0.963...   \n",
       "7                  3  [0.9227, 0.4564, 0.8766, 0.7726, 0.1722, 0.953...   \n",
       "8                  3  [0.8856, 0.6351, 0.9477, 0.6353, 0.1756, 0.883...   \n",
       "9                  4  [0.8757, 0.9667, 0.823, 0.8142, 0.9732, 0.8034...   \n",
       "10                 1  [0.6639, 0.9556, 0.1192, 0.5898, 0.8743, 0.712...   \n",
       "11                 5  [0.4484, 0.8739, 0.2366, 0.5275, 0.8747, 0.599...   \n",
       "12                 6  [0.6226, 0.6583, 0.8555, 0.9732, 0.8595, 0.686...   \n",
       "13                 7  [0.3262, 0.0735, 0.2534, 0.6341, 0.7291, 0.959...   \n",
       "14                 8  [0.766, 0.7896, 0.5934, 0.9531, 0.6999, 0.5955...   \n",
       "15                 8  [0.2178, 0.854, 0.4095, 0.5362, 0.6613, 0.1317...   \n",
       "16                 9  [0.2471, 0.1388, 0.1163, 0.3489, 0.1808, 0.435...   \n",
       "17                 9  [0.5768, 0.6206, 0.5355, 0.7332, 0.4269, 0.703...   \n",
       "18                10  [0.1563, 0.5019, 0.0892, 0.0929, 0.5428, 0.354...   \n",
       "19                10  [0.1868, 0.7362, 0.1047, 0.2305, 0.6998, 0.212...   \n",
       "20                11  [0.4954, 0.3582, 0.6691, 0.904, 0.1233, 0.3952...   \n",
       "21                11  [0.4523, 0.1289, 0.3631, 0.8841, 0.0491, 0.572...   \n",
       "22                12  [0.3613, 0.2697, 0.2352, 0.7049, 0.0929, 0.543...   \n",
       "23                13  [0.76, 0.7475, 0.7318, 0.3172, 0.6771, 0.8021,...   \n",
       "24                13  [0.3192, 0.2475, 0.4558, 0.1788, 0.0769, 0.512...   \n",
       "25                14  [0.3656, 0.3967, 0.8986, 0.7584, 0.3781, 0.645...   \n",
       "26                14  [0.1288, 0.0269, 0.5386, 0.4264, 0.0688, 0.512...   \n",
       "27                15  [0.7531, 0.5895, 0.5857, 0.7558, 0.876, 0.877,...   \n",
       "28                15  [0.7722, 0.3336, 0.7099, 0.8609, 0.9443, 0.975...   \n",
       "29                15  [0.8517, 0.4748, 0.8691, 0.9381, 0.9791, 0.965...   \n",
       "30                16  [0.9751, 0.9855, 0.406, 0.9727, 1.0, 0.7632, 0...   \n",
       "31                17  [0.9216, 0.405, 0.9082, 0.8289, 0.3012, 0.8073...   \n",
       "32                17  [0.7893, 0.7283, 0.9594, 0.9677, 0.1737, 0.832...   \n",
       "33                18  [0.1751, 0.292, 0.0778, 0.2168, 0.0937, 0.2237...   \n",
       "34                19  [0.8503, 0.9482, 0.102, 0.4854, 0.8126, 0.1841...   \n",
       "35                10  [0.2358, 0.1274, 0.5055, 0.498, 0.0652, 0.6028...   \n",
       "36                23  [0.0689, 0.0529, 0.303, 0.3799, 0.0287, 0.4887...   \n",
       "37                23  [0.0919, 0.1026, 0.7223, 0.8371, 0.0414, 0.969...   \n",
       "38                22  [0.7482, 0.0573, 0.0482, 0.4069, 0.0864, 0.949...   \n",
       "39                22  [0.5818, 0.0297, 0.0404, 0.4509, 0.0344, 0.718...   \n",
       "40                22  [0.867, 0.3618, 0.2987, 0.9389, 0.3667, 0.8241...   \n",
       "41                22  [0.1908, 0.0307, 0.0402, 0.1152, 0.0509, 0.759...   \n",
       "42                23  [0.1359, 0.0531, 0.5442, 0.4746, 0.0223, 0.508...   \n",
       "43                23  [0.3525, 0.0721, 0.6609, 0.8349, 0.0585, 0.944...   \n",
       "44                23  [0.2254, 0.0307, 0.8797, 0.7998, 0.0549, 0.987...   \n",
       "45                23  [0.2117, 0.045, 0.5938, 0.5744, 0.0234, 0.7247...   \n",
       "46                13  [0.8089, 0.4006, 0.8819, 0.5016, 0.5299, 0.772...   \n",
       "47                24  [0.7864, 0.621, 0.7848, 0.7998, 0.3185, 0.4442...   \n",
       "48                25  [0.2203, 0.6595, 0.0107, 0.1642, 0.7292, 0.101...   \n",
       "49                25  [0.1966, 0.9689, 0.251, 0.6423, 0.8189, 0.3107...   \n",
       "50                26  [0.4913, 0.4688, 0.7842, 0.7195, 0.2107, 0.300...   \n",
       "51                26  [0.7261, 0.8475, 0.8019, 0.794, 0.679, 0.4596,...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.672, 0.5196, 0.9143, 0.5303, 0.7564, 0.9013...  0.903846  0.062307  \n",
       "1   [0.2151, 0.2587, 0.7714, 0.3382, 0.3768, 0.731...       NaN       NaN  \n",
       "2   [0.4216, 0.3322, 0.833, 0.3266, 0.7688, 0.8685...       NaN       NaN  \n",
       "3   [0.404, 0.4777, 0.84, 0.266, 0.16, 0.4841, 0.4...       NaN       NaN  \n",
       "4   [0.8407, 0.7593, 0.9618, 0.6377, 0.7275, 0.950...       NaN       NaN  \n",
       "5   [0.7765, 0.9299, 0.8275, 0.714, 0.8373, 0.3966...       NaN       NaN  \n",
       "6   [0.0187, 0.0808, 0.0311, 0.0219, 0.8401, 0.036...       NaN       NaN  \n",
       "7   [0.0773, 0.5436, 0.1234, 0.2274, 0.8278, 0.046...       NaN       NaN  \n",
       "8   [0.1144, 0.3649, 0.0523, 0.3647, 0.8244, 0.116...       NaN       NaN  \n",
       "9   [0.1243, 0.0333, 0.177, 0.1858, 0.0268, 0.1966...       NaN       NaN  \n",
       "10  [0.3361, 0.0444, 0.8808, 0.4102, 0.1257, 0.287...       NaN       NaN  \n",
       "11  [0.5516, 0.1261, 0.7634, 0.4725, 0.1253, 0.400...       NaN       NaN  \n",
       "12  [0.3774, 0.3417, 0.1445, 0.0268, 0.1405, 0.313...       NaN       NaN  \n",
       "13  [0.6738, 0.9265, 0.7466, 0.3659, 0.2709, 0.040...       NaN       NaN  \n",
       "14  [0.234, 0.2104, 0.4066, 0.0469, 0.3001, 0.4045...       NaN       NaN  \n",
       "15  [0.7822, 0.146, 0.5905, 0.4638, 0.3387, 0.8683...       NaN       NaN  \n",
       "16  [0.7529, 0.8612, 0.8837, 0.6511, 0.8192, 0.564...       NaN       NaN  \n",
       "17  [0.4232, 0.3794, 0.4645, 0.2668, 0.5731, 0.296...       NaN       NaN  \n",
       "18  [0.8437, 0.4981, 0.9108, 0.9071, 0.4572, 0.645...       NaN       NaN  \n",
       "19  [0.8132, 0.2638, 0.8953, 0.7695, 0.3002, 0.787...       NaN       NaN  \n",
       "20  [0.5046, 0.6418, 0.3309, 0.096, 0.8767, 0.6048...       NaN       NaN  \n",
       "21  [0.5477, 0.8711, 0.6369, 0.1159, 0.9509, 0.427...       NaN       NaN  \n",
       "22  [0.6387, 0.7303, 0.7648, 0.2951, 0.9071, 0.456...       NaN       NaN  \n",
       "23  [0.24, 0.2525, 0.2682, 0.6828, 0.3229, 0.1979,...       NaN       NaN  \n",
       "24  [0.6808, 0.7525, 0.5442, 0.8212, 0.9231, 0.488...       NaN       NaN  \n",
       "25  [0.6344, 0.6033, 0.1014, 0.2416, 0.6219, 0.355...       NaN       NaN  \n",
       "26  [0.8712, 0.9731, 0.4614, 0.5736, 0.9312, 0.487...       NaN       NaN  \n",
       "27  [0.2469, 0.4105, 0.4143, 0.2442, 0.124, 0.123,...       NaN       NaN  \n",
       "28  [0.2278, 0.6664, 0.2901, 0.1391, 0.0557, 0.024...       NaN       NaN  \n",
       "29  [0.1483, 0.5252, 0.1309, 0.0619, 0.0209, 0.034...       NaN       NaN  \n",
       "30  [0.0249, 0.0145, 0.594, 0.0273, 0.0, 0.2368, 1...       NaN       NaN  \n",
       "31  [0.0784, 0.595, 0.0918, 0.1711, 0.6988, 0.1927...       NaN       NaN  \n",
       "32  [0.2107, 0.2717, 0.0406, 0.0323, 0.8263, 0.168...       NaN       NaN  \n",
       "33  [0.8249, 0.708, 0.9222, 0.7832, 0.9063, 0.7763...       NaN       NaN  \n",
       "34  [0.1497, 0.0518, 0.898, 0.5146, 0.1874, 0.8159...       NaN       NaN  \n",
       "35  [0.7642, 0.8726, 0.4945, 0.502, 0.9348, 0.3972...       NaN       NaN  \n",
       "36  [0.9311, 0.9471, 0.697, 0.6201, 0.9713, 0.5113...       NaN       NaN  \n",
       "37  [0.9081, 0.8974, 0.2777, 0.1629, 0.9586, 0.030...       NaN       NaN  \n",
       "38  [0.2518, 0.9427, 0.9518, 0.5931, 0.9136, 0.050...       NaN       NaN  \n",
       "39  [0.4182, 0.9703, 0.9596, 0.5491, 0.9656, 0.281...       NaN       NaN  \n",
       "40  [0.133, 0.6382, 0.7013, 0.0611, 0.6333, 0.1759...       NaN       NaN  \n",
       "41  [0.8092, 0.9693, 0.9598, 0.8848, 0.9491, 0.240...       NaN       NaN  \n",
       "42  [0.8641, 0.9469, 0.4558, 0.5254, 0.9777, 0.491...       NaN       NaN  \n",
       "43  [0.6475, 0.9279, 0.3391, 0.1651, 0.9415, 0.056...       NaN       NaN  \n",
       "44  [0.7746, 0.9693, 0.1203, 0.2002, 0.9451, 0.012...       NaN       NaN  \n",
       "45  [0.7883, 0.955, 0.4062, 0.4256, 0.9766, 0.2753...       NaN       NaN  \n",
       "46  [0.1911, 0.5994, 0.1181, 0.4984, 0.4701, 0.228...       NaN       NaN  \n",
       "47  [0.2136, 0.379, 0.2152, 0.2002, 0.6815, 0.5558...       NaN       NaN  \n",
       "48  [0.7797, 0.3405, 0.9893, 0.8358, 0.2708, 0.898...       NaN       NaN  \n",
       "49  [0.8034, 0.0311, 0.749, 0.3577, 0.1811, 0.6893...       NaN       NaN  \n",
       "50  [0.5087, 0.5312, 0.2158, 0.2805, 0.7893, 0.699...       NaN       NaN  \n",
       "51  [0.2739, 0.1525, 0.1981, 0.206, 0.321, 0.5404,...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2461 : Training: loss:  0.056082726\n",
      "2462 : Training: loss:  0.056904845\n",
      "2463 : Training: loss:  0.041903067\n",
      "2464 : Training: loss:  0.043661814\n",
      "2465 : Training: loss:  0.04518865\n",
      "2466 : Training: loss:  0.028710779\n",
      "2467 : Training: loss:  0.08430506\n",
      "2468 : Training: loss:  0.04441898\n",
      "2469 : Training: loss:  0.046012193\n",
      "2470 : Training: loss:  0.049666017\n",
      "2471 : Training: loss:  0.03276232\n",
      "2472 : Training: loss:  0.03654664\n",
      "2473 : Training: loss:  0.029438443\n",
      "2474 : Training: loss:  0.047473874\n",
      "2475 : Training: loss:  0.04424884\n",
      "2476 : Training: loss:  0.04156201\n",
      "2477 : Training: loss:  0.028880075\n",
      "2478 : Training: loss:  0.03867839\n",
      "2479 : Training: loss:  0.07041267\n",
      "2480 : Training: loss:  0.039399147\n",
      "Validation: Loss:  0.061588947  Accuracy:  0.90384614\n",
      "2481 : Training: loss:  0.04406951\n",
      "2482 : Training: loss:  0.042019196\n",
      "2483 : Training: loss:  0.04196403\n",
      "2484 : Training: loss:  0.04654494\n",
      "2485 : Training: loss:  0.05160491\n",
      "2486 : Training: loss:  0.06985291\n",
      "2487 : Training: loss:  0.056637745\n",
      "2488 : Training: loss:  0.040986545\n",
      "2489 : Training: loss:  0.04008785\n",
      "2490 : Training: loss:  0.0498271\n",
      "2491 : Training: loss:  0.029819788\n",
      "2492 : Training: loss:  0.02980675\n",
      "2493 : Training: loss:  0.041800424\n",
      "2494 : Training: loss:  0.036684662\n",
      "2495 : Training: loss:  0.031014025\n",
      "2496 : Training: loss:  0.035377737\n",
      "2497 : Training: loss:  0.033956643\n",
      "2498 : Training: loss:  0.04187718\n",
      "2499 : Training: loss:  0.03803216\n",
      "2500 : Training: loss:  0.03121787\n",
      "Validation: Loss:  0.060785234  Accuracy:  0.90384614\n",
      "2501 : Training: loss:  0.03752804\n",
      "2502 : Training: loss:  0.029328423\n",
      "2503 : Training: loss:  0.036194533\n",
      "2504 : Training: loss:  0.025928678\n",
      "2505 : Training: loss:  0.039092753\n",
      "2506 : Training: loss:  0.028952608\n",
      "2507 : Training: loss:  0.03401473\n",
      "2508 : Training: loss:  0.04946677\n",
      "2509 : Training: loss:  0.06172611\n",
      "2510 : Training: loss:  0.043757025\n",
      "2511 : Training: loss:  0.030346198\n",
      "2512 : Training: loss:  0.03461382\n",
      "2513 : Training: loss:  0.043231457\n",
      "2514 : Training: loss:  0.06318333\n",
      "2515 : Training: loss:  0.04086511\n",
      "2516 : Training: loss:  0.033474896\n",
      "2517 : Training: loss:  0.027909959\n",
      "2518 : Training: loss:  0.039033476\n",
      "2519 : Training: loss:  0.031654242\n",
      "2520 : Training: loss:  0.032350823\n",
      "Validation: Loss:  0.059819527  Accuracy:  0.90384614\n",
      "2521 : Training: loss:  0.04237292\n",
      "2522 : Training: loss:  0.042101376\n",
      "2523 : Training: loss:  0.0321959\n",
      "2524 : Training: loss:  0.047649503\n",
      "2525 : Training: loss:  0.060174156\n",
      "2526 : Training: loss:  0.03633044\n",
      "2527 : Training: loss:  0.041813686\n",
      "2528 : Training: loss:  0.06358794\n",
      "2529 : Training: loss:  0.03992954\n",
      "2530 : Training: loss:  0.034114946\n",
      "2531 : Training: loss:  0.0203525\n",
      "2532 : Training: loss:  0.07659551\n",
      "2533 : Training: loss:  0.055455215\n",
      "2534 : Training: loss:  0.043450966\n",
      "2535 : Training: loss:  0.043533437\n",
      "2536 : Training: loss:  0.034068804\n",
      "2537 : Training: loss:  0.05386638\n",
      "2538 : Training: loss:  0.03609505\n",
      "2539 : Training: loss:  0.04887257\n",
      "2540 : Training: loss:  0.03742397\n",
      "Validation: Loss:  0.058911007  Accuracy:  0.90384614\n",
      "2541 : Training: loss:  0.041993957\n",
      "2542 : Training: loss:  0.022438403\n",
      "2543 : Training: loss:  0.027571864\n",
      "2544 : Training: loss:  0.037015583\n",
      "2545 : Training: loss:  0.054841045\n",
      "2546 : Training: loss:  0.047493596\n",
      "2547 : Training: loss:  0.026454505\n",
      "2548 : Training: loss:  0.03731588\n",
      "2549 : Training: loss:  0.058921807\n",
      "2550 : Training: loss:  0.036618873\n",
      "2551 : Training: loss:  0.052629735\n",
      "2552 : Training: loss:  0.057372827\n",
      "2553 : Training: loss:  0.017826488\n",
      "2554 : Training: loss:  0.04284201\n",
      "2555 : Training: loss:  0.034141522\n",
      "2556 : Training: loss:  0.019344904\n",
      "2557 : Training: loss:  0.030546011\n",
      "2558 : Training: loss:  0.03884863\n",
      "2559 : Training: loss:  0.053861994\n",
      "2560 : Training: loss:  0.06663532\n",
      "Validation: Loss:  0.058215395  Accuracy:  0.9230769\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3758, 0.0843, 0.0157, 0.0228, 0.0112, 0.031...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3339, 0.4919, 0.0841, 0.4845, 0.2606, 0.097...</td>\n",
       "      <td>[0.6661, 0.5081, 0.9159, 0.5155, 0.7394, 0.902...</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.058215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.7733, 0.4887, 0.0082, 0.0243, 0.0891, 0.029...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.785, 0.7464, 0.2227, 0.656, 0.6426, 0.2628,...</td>\n",
       "      <td>[0.215, 0.2536, 0.7773, 0.344, 0.3574, 0.7372,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.7667, 0.1988, 0.007, 0.0784, 0.0182, 0.064,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5797, 0.6813, 0.1666, 0.6879, 0.2461, 0.130...</td>\n",
       "      <td>[0.4203, 0.3187, 0.8334, 0.3121, 0.7539, 0.869...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0325, 0.2263, 0.0032, 0.0039, 0.0625, 0.036...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5821, 0.5287, 0.1589, 0.7356, 0.8521, 0.519...</td>\n",
       "      <td>[0.4179, 0.4713, 0.8411, 0.2644, 0.1479, 0.480...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0863, 0.1222, 0.016, 0.0037, 0.005, 0.0299,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1577, 0.2486, 0.0373, 0.3741, 0.2934, 0.051...</td>\n",
       "      <td>[0.8423, 0.7514, 0.9627, 0.6259, 0.7066, 0.948...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0137, 0.0126, 0.3025, 0.0067, 0.0049, 0.051...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.2287, 0.072, 0.1748, 0.2966, 0.1641, 0.633,...</td>\n",
       "      <td>[0.7713, 0.928, 0.8252, 0.7034, 0.8359, 0.367,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0342, 0.003, 0.0009, 0.8399, 0.0025, 0.0004...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9825, 0.9263, 0.9692, 0.9795, 0.157, 0.9649...</td>\n",
       "      <td>[0.0175, 0.0737, 0.0308, 0.0205, 0.843, 0.0351...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0253, 0.0118, 0.012, 0.6158, 0.0039, 0.003,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9254, 0.4626, 0.8762, 0.7753, 0.1691, 0.956...</td>\n",
       "      <td>[0.0746, 0.5374, 0.1238, 0.2247, 0.8309, 0.043...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0366, 0.0079, 0.0207, 0.5568, 0.0098, 0.009...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8941, 0.6542, 0.9505, 0.6369, 0.1703, 0.890...</td>\n",
       "      <td>[0.1059, 0.3458, 0.0495, 0.3631, 0.8297, 0.109...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0061, 0.0486, 0.0053, 0.0128, 0.6363, 0.101...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8699, 0.9678, 0.8254, 0.8023, 0.9751, 0.800...</td>\n",
       "      <td>[0.1301, 0.0322, 0.1746, 0.1977, 0.0249, 0.199...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0363, 0.3924, 0.0032, 0.0036, 0.3319, 0.421...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.6563, 0.9583, 0.1143, 0.5646, 0.8838, 0.713...</td>\n",
       "      <td>[0.3437, 0.0417, 0.8857, 0.4354, 0.1162, 0.286...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0032, 0.0181, 0.0037, 0.0025, 0.0806, 0.400...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4384, 0.882, 0.2365, 0.5192, 0.8875, 0.6032...</td>\n",
       "      <td>[0.5616, 0.118, 0.7635, 0.4808, 0.1125, 0.3968...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0006, 0.0051, 0.0193, 0.0011, 0.0019, 0.002...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6131, 0.658, 0.8588, 0.9737, 0.8648, 0.6819...</td>\n",
       "      <td>[0.3869, 0.342, 0.1412, 0.0263, 0.1352, 0.3181...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0532, 0.0552, 0.1555, 0.0502, 0.0022, 0.071...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.3056, 0.0654, 0.2494, 0.6314, 0.7263, 0.962...</td>\n",
       "      <td>[0.6944, 0.9346, 0.7506, 0.3686, 0.2737, 0.037...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0174, 0.0134, 0.0007, 0.0276, 0.0234, 0.003...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7615, 0.8016, 0.5952, 0.9546, 0.7136, 0.590...</td>\n",
       "      <td>[0.2385, 0.1984, 0.4048, 0.0454, 0.2864, 0.409...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0404, 0.0136, 0.0035, 0.0036, 0.0179, 0.008...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.2144, 0.8636, 0.4144, 0.5346, 0.6812, 0.130...</td>\n",
       "      <td>[0.7856, 0.1364, 0.5856, 0.4654, 0.3188, 0.869...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0096, 0.0048, 0.1542, 0.0033, 0.0007, 0.021...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.2605, 0.1485, 0.1162, 0.3589, 0.1799, 0.455...</td>\n",
       "      <td>[0.7395, 0.8515, 0.8838, 0.6411, 0.8201, 0.544...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0169, 0.0057, 0.0252, 0.0094, 0.0028, 0.005...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5937, 0.6395, 0.5354, 0.7339, 0.4184, 0.712...</td>\n",
       "      <td>[0.4063, 0.3605, 0.4646, 0.2661, 0.5816, 0.287...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0405, 0.0061, 0.0645, 0.003, 0.0026, 0.0194...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1553, 0.5191, 0.0903, 0.0887, 0.5641, 0.372...</td>\n",
       "      <td>[0.8447, 0.4809, 0.9097, 0.9113, 0.4359, 0.627...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.026, 0.0063, 0.0681, 0.0021, 0.0029, 0.0223...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1817, 0.751, 0.1066, 0.2183, 0.7208, 0.2214...</td>\n",
       "      <td>[0.8183, 0.249, 0.8934, 0.7817, 0.2792, 0.7786...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0584, 0.0229, 0.0197, 0.0398, 0.0022, 0.003...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.5027, 0.3647, 0.6841, 0.9095, 0.1262, 0.406...</td>\n",
       "      <td>[0.4973, 0.6353, 0.3159, 0.0905, 0.8738, 0.593...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0098, 0.0052, 0.0271, 0.0223, 0.0008, 0.001...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4588, 0.1308, 0.3772, 0.8919, 0.0517, 0.591...</td>\n",
       "      <td>[0.5412, 0.8692, 0.6228, 0.1081, 0.9483, 0.408...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0199, 0.0094, 0.0241, 0.0148, 0.002, 0.0156...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.3625, 0.2825, 0.2446, 0.7175, 0.096, 0.5654...</td>\n",
       "      <td>[0.6375, 0.7175, 0.7554, 0.2825, 0.904, 0.4346...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0272, 0.009, 0.0197, 0.0305, 0.025, 0.0599,...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7593, 0.7636, 0.7421, 0.2948, 0.6887, 0.810...</td>\n",
       "      <td>[0.2407, 0.2364, 0.2579, 0.7052, 0.3113, 0.189...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0349, 0.0027, 0.1018, 0.0118, 0.0031, 0.015...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.3348, 0.2682, 0.469, 0.1842, 0.0774, 0.5415...</td>\n",
       "      <td>[0.6652, 0.7318, 0.531, 0.8158, 0.9226, 0.4585...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0051, 0.0056, 0.0196, 0.0126, 0.0078, 0.004...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3658, 0.4099, 0.9066, 0.7713, 0.386, 0.6644...</td>\n",
       "      <td>[0.6342, 0.5901, 0.0934, 0.2287, 0.614, 0.3356...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0071, 0.0039, 0.0282, 0.0019, 0.0007, 0.001...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1279, 0.0271, 0.5574, 0.4426, 0.0698, 0.542...</td>\n",
       "      <td>[0.8721, 0.9729, 0.4426, 0.5574, 0.9302, 0.457...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0004, 0.0037, 0.0074, 0.003, 0.0073, 0.0053...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7524, 0.5994, 0.5892, 0.7639, 0.8783, 0.883...</td>\n",
       "      <td>[0.2476, 0.4006, 0.4108, 0.2361, 0.1217, 0.116...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0006, 0.0106, 0.0191, 0.0118, 0.0023, 0.003...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7651, 0.3227, 0.707, 0.8612, 0.9424, 0.9769...</td>\n",
       "      <td>[0.2349, 0.6773, 0.293, 0.1388, 0.0576, 0.0231...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0004, 0.0101, 0.0018, 0.0035, 0.0129, 0.001...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8453, 0.4738, 0.87, 0.9395, 0.9798, 0.9667,...</td>\n",
       "      <td>[0.1547, 0.5262, 0.13, 0.0605, 0.0202, 0.0333,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0935, 0.7107, 0.0741, 0.0384, 0.6639, 0.53,...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9606, 0.9798, 0.3893, 0.9551, 1.0, 0.7258, ...</td>\n",
       "      <td>[0.0394, 0.0202, 0.6107, 0.0449, 0.0, 0.2742, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0029, 0.0068, 0.0259, 0.0465, 0.002, 0.0012...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9236, 0.4106, 0.911, 0.8355, 0.301, 0.818, ...</td>\n",
       "      <td>[0.0764, 0.5894, 0.089, 0.1645, 0.699, 0.182, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0025, 0.0024, 0.0142, 0.045, 0.001, 0.0006,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.796, 0.745, 0.9626, 0.9709, 0.1729, 0.8404,...</td>\n",
       "      <td>[0.204, 0.255, 0.0374, 0.0291, 0.8271, 0.1596,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.031, 0.0057, 0.0472, 0.006, 0.0013, 0.0088,...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.1856, 0.3139, 0.0791, 0.2275, 0.097, 0.2442...</td>\n",
       "      <td>[0.8144, 0.6861, 0.9209, 0.7725, 0.903, 0.7558...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0525, 0.0303, 0.0161, 0.0203, 0.1119, 0.042...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8561, 0.9531, 0.1026, 0.4857, 0.8277, 0.183...</td>\n",
       "      <td>[0.1439, 0.0469, 0.8974, 0.5143, 0.1723, 0.816...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0346, 0.0089, 0.0217, 0.0097, 0.0009, 0.006...</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.2432, 0.1327, 0.5186, 0.5156, 0.0669, 0.628...</td>\n",
       "      <td>[0.7568, 0.8673, 0.4814, 0.4844, 0.9331, 0.371...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0245, 0.0058, 0.028, 0.0038, 0.0005, 0.005,...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0672, 0.0547, 0.3129, 0.3942, 0.0285, 0.513...</td>\n",
       "      <td>[0.9328, 0.9453, 0.6871, 0.6058, 0.9715, 0.486...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0036, 0.0045, 0.0106, 0.0127, 0.0003, 0.000...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.086, 0.1017, 0.7286, 0.8379, 0.041, 0.972, ...</td>\n",
       "      <td>[0.914, 0.8983, 0.2714, 0.1621, 0.959, 0.028, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0026, 0.0008, 0.0348, 0.008, 0.0005, 0.0009...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7502, 0.0564, 0.0457, 0.4199, 0.0864, 0.954...</td>\n",
       "      <td>[0.2498, 0.9436, 0.9543, 0.5801, 0.9136, 0.045...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0067, 0.0013, 0.0272, 0.0077, 0.0005, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5874, 0.0303, 0.0403, 0.4662, 0.0356, 0.740...</td>\n",
       "      <td>[0.4126, 0.9697, 0.9597, 0.5338, 0.9644, 0.259...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0064, 0.0108, 0.0039, 0.0183, 0.0032, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.8648, 0.3551, 0.299, 0.9432, 0.3764, 0.8283...</td>\n",
       "      <td>[0.1352, 0.6449, 0.701, 0.0568, 0.6236, 0.1717...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0197, 0.002, 0.0621, 0.0077, 0.0005, 0.0047...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1966, 0.0316, 0.0398, 0.1194, 0.0517, 0.784...</td>\n",
       "      <td>[0.8034, 0.9684, 0.9602, 0.8806, 0.9483, 0.215...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0123, 0.0036, 0.0147, 0.0042, 0.0002, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1409, 0.0552, 0.5502, 0.4875, 0.0225, 0.527...</td>\n",
       "      <td>[0.8591, 0.9448, 0.4498, 0.5125, 0.9775, 0.472...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0049, 0.0033, 0.0072, 0.0053, 0.0002, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3478, 0.0694, 0.6588, 0.8312, 0.0577, 0.945...</td>\n",
       "      <td>[0.6522, 0.9306, 0.3412, 0.1688, 0.9423, 0.054...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.002, 0.0024, 0.0074, 0.0039, 1e-04, 1e-04, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2243, 0.0291, 0.8783, 0.7977, 0.0536, 0.988...</td>\n",
       "      <td>[0.7757, 0.9709, 0.1217, 0.2023, 0.9464, 0.012...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0062, 0.0023, 0.0072, 0.004, 1e-04, 0.0002,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2106, 0.0452, 0.5971, 0.5864, 0.0233, 0.739...</td>\n",
       "      <td>[0.7894, 0.9548, 0.4029, 0.4136, 0.9767, 0.261...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0058, 0.0063, 0.1121, 0.0239, 0.0056, 0.024...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.8149, 0.4135, 0.8892, 0.4937, 0.529, 0.7877...</td>\n",
       "      <td>[0.1851, 0.5865, 0.1108, 0.5063, 0.471, 0.2123...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0169, 0.0702, 0.0112, 0.0184, 0.0135, 0.008...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.7975, 0.6335, 0.7908, 0.8033, 0.3277, 0.449...</td>\n",
       "      <td>[0.2025, 0.3665, 0.2092, 0.1967, 0.6723, 0.550...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0056, 0.0064, 0.0061, 0.0003, 0.0081, 0.016...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.208, 0.6738, 0.0104, 0.1614, 0.7499, 0.1067...</td>\n",
       "      <td>[0.792, 0.3262, 0.9896, 0.8386, 0.2501, 0.8933...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.015, 0.0203, 0.0068, 0.007, 0.0408, 0.136, ...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1831, 0.9709, 0.25, 0.6345, 0.8325, 0.3136,...</td>\n",
       "      <td>[0.8169, 0.0291, 0.75, 0.3655, 0.1675, 0.6864,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0313, 0.0117, 0.0099, 0.0122, 0.0049, 0.004...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4926, 0.4699, 0.7951, 0.7308, 0.2189, 0.304...</td>\n",
       "      <td>[0.5074, 0.5301, 0.2049, 0.2692, 0.7811, 0.696...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0038, 0.0035, 0.0042, 0.0136, 0.0247, 0.001...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7298, 0.8579, 0.8108, 0.7977, 0.6872, 0.461...</td>\n",
       "      <td>[0.2702, 0.1421, 0.1892, 0.2023, 0.3128, 0.538...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3758, 0.0843, 0.0157, 0.0228, 0.0112, 0.031...               0   \n",
       "1   [0.7733, 0.4887, 0.0082, 0.0243, 0.0891, 0.029...               0   \n",
       "2   [0.7667, 0.1988, 0.007, 0.0784, 0.0182, 0.064,...               0   \n",
       "3   [0.0325, 0.2263, 0.0032, 0.0039, 0.0625, 0.036...               1   \n",
       "4   [0.0863, 0.1222, 0.016, 0.0037, 0.005, 0.0299,...               1   \n",
       "5   [0.0137, 0.0126, 0.3025, 0.0067, 0.0049, 0.051...               2   \n",
       "6   [0.0342, 0.003, 0.0009, 0.8399, 0.0025, 0.0004...               3   \n",
       "7   [0.0253, 0.0118, 0.012, 0.6158, 0.0039, 0.003,...               3   \n",
       "8   [0.0366, 0.0079, 0.0207, 0.5568, 0.0098, 0.009...               3   \n",
       "9   [0.0061, 0.0486, 0.0053, 0.0128, 0.6363, 0.101...               4   \n",
       "10  [0.0363, 0.3924, 0.0032, 0.0036, 0.3319, 0.421...               4   \n",
       "11  [0.0032, 0.0181, 0.0037, 0.0025, 0.0806, 0.400...               5   \n",
       "12  [0.0006, 0.0051, 0.0193, 0.0011, 0.0019, 0.002...               6   \n",
       "13  [0.0532, 0.0552, 0.1555, 0.0502, 0.0022, 0.071...               7   \n",
       "14  [0.0174, 0.0134, 0.0007, 0.0276, 0.0234, 0.003...               8   \n",
       "15  [0.0404, 0.0136, 0.0035, 0.0036, 0.0179, 0.008...               8   \n",
       "16  [0.0096, 0.0048, 0.1542, 0.0033, 0.0007, 0.021...               9   \n",
       "17  [0.0169, 0.0057, 0.0252, 0.0094, 0.0028, 0.005...               9   \n",
       "18  [0.0405, 0.0061, 0.0645, 0.003, 0.0026, 0.0194...              10   \n",
       "19  [0.026, 0.0063, 0.0681, 0.0021, 0.0029, 0.0223...              10   \n",
       "20  [0.0584, 0.0229, 0.0197, 0.0398, 0.0022, 0.003...              11   \n",
       "21  [0.0098, 0.0052, 0.0271, 0.0223, 0.0008, 0.001...              11   \n",
       "22  [0.0199, 0.0094, 0.0241, 0.0148, 0.002, 0.0156...              12   \n",
       "23  [0.0272, 0.009, 0.0197, 0.0305, 0.025, 0.0599,...              13   \n",
       "24  [0.0349, 0.0027, 0.1018, 0.0118, 0.0031, 0.015...              13   \n",
       "25  [0.0051, 0.0056, 0.0196, 0.0126, 0.0078, 0.004...              14   \n",
       "26  [0.0071, 0.0039, 0.0282, 0.0019, 0.0007, 0.001...              14   \n",
       "27  [0.0004, 0.0037, 0.0074, 0.003, 0.0073, 0.0053...              15   \n",
       "28  [0.0006, 0.0106, 0.0191, 0.0118, 0.0023, 0.003...              15   \n",
       "29  [0.0004, 0.0101, 0.0018, 0.0035, 0.0129, 0.001...              15   \n",
       "30  [0.0935, 0.7107, 0.0741, 0.0384, 0.6639, 0.53,...              16   \n",
       "31  [0.0029, 0.0068, 0.0259, 0.0465, 0.002, 0.0012...              17   \n",
       "32  [0.0025, 0.0024, 0.0142, 0.045, 0.001, 0.0006,...              17   \n",
       "33  [0.031, 0.0057, 0.0472, 0.006, 0.0013, 0.0088,...              18   \n",
       "34  [0.0525, 0.0303, 0.0161, 0.0203, 0.1119, 0.042...              19   \n",
       "35  [0.0346, 0.0089, 0.0217, 0.0097, 0.0009, 0.006...              20   \n",
       "36  [0.0245, 0.0058, 0.028, 0.0038, 0.0005, 0.005,...              21   \n",
       "37  [0.0036, 0.0045, 0.0106, 0.0127, 0.0003, 0.000...              21   \n",
       "38  [0.0026, 0.0008, 0.0348, 0.008, 0.0005, 0.0009...              22   \n",
       "39  [0.0067, 0.0013, 0.0272, 0.0077, 0.0005, 0.001...              22   \n",
       "40  [0.0064, 0.0108, 0.0039, 0.0183, 0.0032, 0.001...              22   \n",
       "41  [0.0197, 0.002, 0.0621, 0.0077, 0.0005, 0.0047...              22   \n",
       "42  [0.0123, 0.0036, 0.0147, 0.0042, 0.0002, 0.000...              23   \n",
       "43  [0.0049, 0.0033, 0.0072, 0.0053, 0.0002, 0.000...              23   \n",
       "44  [0.002, 0.0024, 0.0074, 0.0039, 1e-04, 1e-04, ...              23   \n",
       "45  [0.0062, 0.0023, 0.0072, 0.004, 1e-04, 0.0002,...              23   \n",
       "46  [0.0058, 0.0063, 0.1121, 0.0239, 0.0056, 0.024...              24   \n",
       "47  [0.0169, 0.0702, 0.0112, 0.0184, 0.0135, 0.008...              24   \n",
       "48  [0.0056, 0.0064, 0.0061, 0.0003, 0.0081, 0.016...              25   \n",
       "49  [0.015, 0.0203, 0.0068, 0.007, 0.0408, 0.136, ...              25   \n",
       "50  [0.0313, 0.0117, 0.0099, 0.0122, 0.0049, 0.004...              26   \n",
       "51  [0.0038, 0.0035, 0.0042, 0.0136, 0.0247, 0.001...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.3339, 0.4919, 0.0841, 0.4845, 0.2606, 0.097...   \n",
       "1                  0  [0.785, 0.7464, 0.2227, 0.656, 0.6426, 0.2628,...   \n",
       "2                  0  [0.5797, 0.6813, 0.1666, 0.6879, 0.2461, 0.130...   \n",
       "3                  1  [0.5821, 0.5287, 0.1589, 0.7356, 0.8521, 0.519...   \n",
       "4                  1  [0.1577, 0.2486, 0.0373, 0.3741, 0.2934, 0.051...   \n",
       "5                  2  [0.2287, 0.072, 0.1748, 0.2966, 0.1641, 0.633,...   \n",
       "6                  3  [0.9825, 0.9263, 0.9692, 0.9795, 0.157, 0.9649...   \n",
       "7                  3  [0.9254, 0.4626, 0.8762, 0.7753, 0.1691, 0.956...   \n",
       "8                  3  [0.8941, 0.6542, 0.9505, 0.6369, 0.1703, 0.890...   \n",
       "9                  4  [0.8699, 0.9678, 0.8254, 0.8023, 0.9751, 0.800...   \n",
       "10                 5  [0.6563, 0.9583, 0.1143, 0.5646, 0.8838, 0.713...   \n",
       "11                 5  [0.4384, 0.882, 0.2365, 0.5192, 0.8875, 0.6032...   \n",
       "12                 6  [0.6131, 0.658, 0.8588, 0.9737, 0.8648, 0.6819...   \n",
       "13                 7  [0.3056, 0.0654, 0.2494, 0.6314, 0.7263, 0.962...   \n",
       "14                 8  [0.7615, 0.8016, 0.5952, 0.9546, 0.7136, 0.590...   \n",
       "15                 8  [0.2144, 0.8636, 0.4144, 0.5346, 0.6812, 0.130...   \n",
       "16                 9  [0.2605, 0.1485, 0.1162, 0.3589, 0.1799, 0.455...   \n",
       "17                 9  [0.5937, 0.6395, 0.5354, 0.7339, 0.4184, 0.712...   \n",
       "18                10  [0.1553, 0.5191, 0.0903, 0.0887, 0.5641, 0.372...   \n",
       "19                10  [0.1817, 0.751, 0.1066, 0.2183, 0.7208, 0.2214...   \n",
       "20                11  [0.5027, 0.3647, 0.6841, 0.9095, 0.1262, 0.406...   \n",
       "21                11  [0.4588, 0.1308, 0.3772, 0.8919, 0.0517, 0.591...   \n",
       "22                12  [0.3625, 0.2825, 0.2446, 0.7175, 0.096, 0.5654...   \n",
       "23                13  [0.7593, 0.7636, 0.7421, 0.2948, 0.6887, 0.810...   \n",
       "24                13  [0.3348, 0.2682, 0.469, 0.1842, 0.0774, 0.5415...   \n",
       "25                14  [0.3658, 0.4099, 0.9066, 0.7713, 0.386, 0.6644...   \n",
       "26                14  [0.1279, 0.0271, 0.5574, 0.4426, 0.0698, 0.542...   \n",
       "27                15  [0.7524, 0.5994, 0.5892, 0.7639, 0.8783, 0.883...   \n",
       "28                15  [0.7651, 0.3227, 0.707, 0.8612, 0.9424, 0.9769...   \n",
       "29                15  [0.8453, 0.4738, 0.87, 0.9395, 0.9798, 0.9667,...   \n",
       "30                16  [0.9606, 0.9798, 0.3893, 0.9551, 1.0, 0.7258, ...   \n",
       "31                17  [0.9236, 0.4106, 0.911, 0.8355, 0.301, 0.818, ...   \n",
       "32                17  [0.796, 0.745, 0.9626, 0.9709, 0.1729, 0.8404,...   \n",
       "33                18  [0.1856, 0.3139, 0.0791, 0.2275, 0.097, 0.2442...   \n",
       "34                19  [0.8561, 0.9531, 0.1026, 0.4857, 0.8277, 0.183...   \n",
       "35                21  [0.2432, 0.1327, 0.5186, 0.5156, 0.0669, 0.628...   \n",
       "36                21  [0.0672, 0.0547, 0.3129, 0.3942, 0.0285, 0.513...   \n",
       "37                23  [0.086, 0.1017, 0.7286, 0.8379, 0.041, 0.972, ...   \n",
       "38                22  [0.7502, 0.0564, 0.0457, 0.4199, 0.0864, 0.954...   \n",
       "39                22  [0.5874, 0.0303, 0.0403, 0.4662, 0.0356, 0.740...   \n",
       "40                22  [0.8648, 0.3551, 0.299, 0.9432, 0.3764, 0.8283...   \n",
       "41                22  [0.1966, 0.0316, 0.0398, 0.1194, 0.0517, 0.784...   \n",
       "42                23  [0.1409, 0.0552, 0.5502, 0.4875, 0.0225, 0.527...   \n",
       "43                23  [0.3478, 0.0694, 0.6588, 0.8312, 0.0577, 0.945...   \n",
       "44                23  [0.2243, 0.0291, 0.8783, 0.7977, 0.0536, 0.988...   \n",
       "45                23  [0.2106, 0.0452, 0.5971, 0.5864, 0.0233, 0.739...   \n",
       "46                13  [0.8149, 0.4135, 0.8892, 0.4937, 0.529, 0.7877...   \n",
       "47                24  [0.7975, 0.6335, 0.7908, 0.8033, 0.3277, 0.449...   \n",
       "48                25  [0.208, 0.6738, 0.0104, 0.1614, 0.7499, 0.1067...   \n",
       "49                25  [0.1831, 0.9709, 0.25, 0.6345, 0.8325, 0.3136,...   \n",
       "50                26  [0.4926, 0.4699, 0.7951, 0.7308, 0.2189, 0.304...   \n",
       "51                26  [0.7298, 0.8579, 0.8108, 0.7977, 0.6872, 0.461...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.6661, 0.5081, 0.9159, 0.5155, 0.7394, 0.902...  0.923077  0.058215  \n",
       "1   [0.215, 0.2536, 0.7773, 0.344, 0.3574, 0.7372,...       NaN       NaN  \n",
       "2   [0.4203, 0.3187, 0.8334, 0.3121, 0.7539, 0.869...       NaN       NaN  \n",
       "3   [0.4179, 0.4713, 0.8411, 0.2644, 0.1479, 0.480...       NaN       NaN  \n",
       "4   [0.8423, 0.7514, 0.9627, 0.6259, 0.7066, 0.948...       NaN       NaN  \n",
       "5   [0.7713, 0.928, 0.8252, 0.7034, 0.8359, 0.367,...       NaN       NaN  \n",
       "6   [0.0175, 0.0737, 0.0308, 0.0205, 0.843, 0.0351...       NaN       NaN  \n",
       "7   [0.0746, 0.5374, 0.1238, 0.2247, 0.8309, 0.043...       NaN       NaN  \n",
       "8   [0.1059, 0.3458, 0.0495, 0.3631, 0.8297, 0.109...       NaN       NaN  \n",
       "9   [0.1301, 0.0322, 0.1746, 0.1977, 0.0249, 0.199...       NaN       NaN  \n",
       "10  [0.3437, 0.0417, 0.8857, 0.4354, 0.1162, 0.286...       NaN       NaN  \n",
       "11  [0.5616, 0.118, 0.7635, 0.4808, 0.1125, 0.3968...       NaN       NaN  \n",
       "12  [0.3869, 0.342, 0.1412, 0.0263, 0.1352, 0.3181...       NaN       NaN  \n",
       "13  [0.6944, 0.9346, 0.7506, 0.3686, 0.2737, 0.037...       NaN       NaN  \n",
       "14  [0.2385, 0.1984, 0.4048, 0.0454, 0.2864, 0.409...       NaN       NaN  \n",
       "15  [0.7856, 0.1364, 0.5856, 0.4654, 0.3188, 0.869...       NaN       NaN  \n",
       "16  [0.7395, 0.8515, 0.8838, 0.6411, 0.8201, 0.544...       NaN       NaN  \n",
       "17  [0.4063, 0.3605, 0.4646, 0.2661, 0.5816, 0.287...       NaN       NaN  \n",
       "18  [0.8447, 0.4809, 0.9097, 0.9113, 0.4359, 0.627...       NaN       NaN  \n",
       "19  [0.8183, 0.249, 0.8934, 0.7817, 0.2792, 0.7786...       NaN       NaN  \n",
       "20  [0.4973, 0.6353, 0.3159, 0.0905, 0.8738, 0.593...       NaN       NaN  \n",
       "21  [0.5412, 0.8692, 0.6228, 0.1081, 0.9483, 0.408...       NaN       NaN  \n",
       "22  [0.6375, 0.7175, 0.7554, 0.2825, 0.904, 0.4346...       NaN       NaN  \n",
       "23  [0.2407, 0.2364, 0.2579, 0.7052, 0.3113, 0.189...       NaN       NaN  \n",
       "24  [0.6652, 0.7318, 0.531, 0.8158, 0.9226, 0.4585...       NaN       NaN  \n",
       "25  [0.6342, 0.5901, 0.0934, 0.2287, 0.614, 0.3356...       NaN       NaN  \n",
       "26  [0.8721, 0.9729, 0.4426, 0.5574, 0.9302, 0.457...       NaN       NaN  \n",
       "27  [0.2476, 0.4006, 0.4108, 0.2361, 0.1217, 0.116...       NaN       NaN  \n",
       "28  [0.2349, 0.6773, 0.293, 0.1388, 0.0576, 0.0231...       NaN       NaN  \n",
       "29  [0.1547, 0.5262, 0.13, 0.0605, 0.0202, 0.0333,...       NaN       NaN  \n",
       "30  [0.0394, 0.0202, 0.6107, 0.0449, 0.0, 0.2742, ...       NaN       NaN  \n",
       "31  [0.0764, 0.5894, 0.089, 0.1645, 0.699, 0.182, ...       NaN       NaN  \n",
       "32  [0.204, 0.255, 0.0374, 0.0291, 0.8271, 0.1596,...       NaN       NaN  \n",
       "33  [0.8144, 0.6861, 0.9209, 0.7725, 0.903, 0.7558...       NaN       NaN  \n",
       "34  [0.1439, 0.0469, 0.8974, 0.5143, 0.1723, 0.816...       NaN       NaN  \n",
       "35  [0.7568, 0.8673, 0.4814, 0.4844, 0.9331, 0.371...       NaN       NaN  \n",
       "36  [0.9328, 0.9453, 0.6871, 0.6058, 0.9715, 0.486...       NaN       NaN  \n",
       "37  [0.914, 0.8983, 0.2714, 0.1621, 0.959, 0.028, ...       NaN       NaN  \n",
       "38  [0.2498, 0.9436, 0.9543, 0.5801, 0.9136, 0.045...       NaN       NaN  \n",
       "39  [0.4126, 0.9697, 0.9597, 0.5338, 0.9644, 0.259...       NaN       NaN  \n",
       "40  [0.1352, 0.6449, 0.701, 0.0568, 0.6236, 0.1717...       NaN       NaN  \n",
       "41  [0.8034, 0.9684, 0.9602, 0.8806, 0.9483, 0.215...       NaN       NaN  \n",
       "42  [0.8591, 0.9448, 0.4498, 0.5125, 0.9775, 0.472...       NaN       NaN  \n",
       "43  [0.6522, 0.9306, 0.3412, 0.1688, 0.9423, 0.054...       NaN       NaN  \n",
       "44  [0.7757, 0.9709, 0.1217, 0.2023, 0.9464, 0.012...       NaN       NaN  \n",
       "45  [0.7894, 0.9548, 0.4029, 0.4136, 0.9767, 0.261...       NaN       NaN  \n",
       "46  [0.1851, 0.5865, 0.1108, 0.5063, 0.471, 0.2123...       NaN       NaN  \n",
       "47  [0.2025, 0.3665, 0.2092, 0.1967, 0.6723, 0.550...       NaN       NaN  \n",
       "48  [0.792, 0.3262, 0.9896, 0.8386, 0.2501, 0.8933...       NaN       NaN  \n",
       "49  [0.8169, 0.0291, 0.75, 0.3655, 0.1675, 0.6864,...       NaN       NaN  \n",
       "50  [0.5074, 0.5301, 0.2049, 0.2692, 0.7811, 0.696...       NaN       NaN  \n",
       "51  [0.2702, 0.1421, 0.1892, 0.2023, 0.3128, 0.538...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2561 : Training: loss:  0.060241207\n",
      "2562 : Training: loss:  0.053998943\n",
      "2563 : Training: loss:  0.03704019\n",
      "2564 : Training: loss:  0.03946841\n",
      "2565 : Training: loss:  0.045760527\n",
      "2566 : Training: loss:  0.032210547\n",
      "2567 : Training: loss:  0.037436564\n",
      "2568 : Training: loss:  0.03823828\n",
      "2569 : Training: loss:  0.033802412\n",
      "2570 : Training: loss:  0.04564406\n",
      "2571 : Training: loss:  0.025849843\n",
      "2572 : Training: loss:  0.03394516\n",
      "2573 : Training: loss:  0.04374015\n",
      "2574 : Training: loss:  0.028245844\n",
      "2575 : Training: loss:  0.06381113\n",
      "2576 : Training: loss:  0.03409259\n",
      "2577 : Training: loss:  0.028676007\n",
      "2578 : Training: loss:  0.05941323\n",
      "2579 : Training: loss:  0.034675177\n",
      "2580 : Training: loss:  0.02182199\n",
      "Validation: Loss:  0.05746832  Accuracy:  0.9230769\n",
      "2581 : Training: loss:  0.030450737\n",
      "2582 : Training: loss:  0.027953502\n",
      "2583 : Training: loss:  0.03197281\n",
      "2584 : Training: loss:  0.02413377\n",
      "2585 : Training: loss:  0.05253144\n",
      "2586 : Training: loss:  0.028108396\n",
      "2587 : Training: loss:  0.060627576\n",
      "2588 : Training: loss:  0.043468233\n",
      "2589 : Training: loss:  0.022424353\n",
      "2590 : Training: loss:  0.04509396\n",
      "2591 : Training: loss:  0.035431508\n",
      "2592 : Training: loss:  0.022813378\n",
      "2593 : Training: loss:  0.0411979\n",
      "2594 : Training: loss:  0.037404094\n",
      "2595 : Training: loss:  0.0574694\n",
      "2596 : Training: loss:  0.051422622\n",
      "2597 : Training: loss:  0.055637818\n",
      "2598 : Training: loss:  0.029694133\n",
      "2599 : Training: loss:  0.025639309\n",
      "2600 : Training: loss:  0.037347764\n",
      "Validation: Loss:  0.05689147  Accuracy:  0.9230769\n",
      "2601 : Training: loss:  0.024293836\n",
      "2602 : Training: loss:  0.04791929\n",
      "2603 : Training: loss:  0.03893253\n",
      "2604 : Training: loss:  0.03355191\n",
      "2605 : Training: loss:  0.05362991\n",
      "2606 : Training: loss:  0.025963845\n",
      "2607 : Training: loss:  0.038713887\n",
      "2608 : Training: loss:  0.042877827\n",
      "2609 : Training: loss:  0.02757317\n",
      "2610 : Training: loss:  0.03488515\n",
      "2611 : Training: loss:  0.038951382\n",
      "2612 : Training: loss:  0.025220744\n",
      "2613 : Training: loss:  0.030682502\n",
      "2614 : Training: loss:  0.047789153\n",
      "2615 : Training: loss:  0.051029425\n",
      "2616 : Training: loss:  0.032262746\n",
      "2617 : Training: loss:  0.025102368\n",
      "2618 : Training: loss:  0.043809608\n",
      "2619 : Training: loss:  0.02455967\n",
      "2620 : Training: loss:  0.04599344\n",
      "Validation: Loss:  0.056176472  Accuracy:  0.9423077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3634, 0.0879, 0.0164, 0.0225, 0.01, 0.0319,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.337, 0.4945, 0.0825, 0.4971, 0.2636, 0.0967...</td>\n",
       "      <td>[0.663, 0.5055, 0.9175, 0.5029, 0.7364, 0.9033...</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.056176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.7642, 0.5026, 0.0083, 0.0239, 0.0799, 0.027...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7866, 0.7477, 0.2259, 0.6594, 0.6491, 0.261...</td>\n",
       "      <td>[0.2134, 0.2523, 0.7741, 0.3406, 0.3509, 0.738...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.7702, 0.2109, 0.0072, 0.0814, 0.0163, 0.065...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5834, 0.6871, 0.1666, 0.7028, 0.2497, 0.129...</td>\n",
       "      <td>[0.4166, 0.3129, 0.8334, 0.2972, 0.7503, 0.870...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.027, 0.2335, 0.0032, 0.0038, 0.0561, 0.0351...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5781, 0.5319, 0.1616, 0.7419, 0.8572, 0.521...</td>\n",
       "      <td>[0.4219, 0.4681, 0.8384, 0.2581, 0.1428, 0.478...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0787, 0.133, 0.0176, 0.0034, 0.0045, 0.0287...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1601, 0.2518, 0.0372, 0.3837, 0.3024, 0.051...</td>\n",
       "      <td>[0.8399, 0.7482, 0.9628, 0.6163, 0.6976, 0.948...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0121, 0.0125, 0.37, 0.0062, 0.0046, 0.0499,...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.2325, 0.0717, 0.1688, 0.3056, 0.1657, 0.648...</td>\n",
       "      <td>[0.7675, 0.9283, 0.8312, 0.6944, 0.8343, 0.352...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0304, 0.0028, 0.0009, 0.859, 0.0022, 0.0004...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9833, 0.9305, 0.9698, 0.9806, 0.153, 0.9658...</td>\n",
       "      <td>[0.0167, 0.0695, 0.0302, 0.0194, 0.847, 0.0342...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0216, 0.0111, 0.0129, 0.6401, 0.0035, 0.002...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9271, 0.4672, 0.8773, 0.7801, 0.1669, 0.958...</td>\n",
       "      <td>[0.0729, 0.5328, 0.1227, 0.2199, 0.8331, 0.041...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0323, 0.0073, 0.0224, 0.5872, 0.009, 0.0086...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.8985, 0.6648, 0.9515, 0.6417, 0.1685, 0.895...</td>\n",
       "      <td>[0.1015, 0.3352, 0.0485, 0.3583, 0.8315, 0.104...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0052, 0.0468, 0.0051, 0.0131, 0.5968, 0.100...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8674, 0.968, 0.831, 0.8029, 0.9758, 0.7971,...</td>\n",
       "      <td>[0.1326, 0.032, 0.169, 0.1971, 0.0242, 0.2029,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0319, 0.4089, 0.0034, 0.0035, 0.3035, 0.442...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.6553, 0.9593, 0.1167, 0.5686, 0.8875, 0.708...</td>\n",
       "      <td>[0.3447, 0.0407, 0.8833, 0.4314, 0.1125, 0.291...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0027, 0.0171, 0.0037, 0.0024, 0.0715, 0.426...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4357, 0.8852, 0.2353, 0.5261, 0.891, 0.5998...</td>\n",
       "      <td>[0.5643, 0.1148, 0.7647, 0.4739, 0.109, 0.4002...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0004, 0.0046, 0.0214, 0.0009, 0.0016, 0.002...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6151, 0.6661, 0.8646, 0.9741, 0.8694, 0.681...</td>\n",
       "      <td>[0.3849, 0.3339, 0.1354, 0.0259, 0.1306, 0.318...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0481, 0.0522, 0.1687, 0.0467, 0.0019, 0.064...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.2967, 0.0617, 0.2514, 0.635, 0.7279, 0.9635...</td>\n",
       "      <td>[0.7033, 0.9383, 0.7486, 0.365, 0.2721, 0.0365...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0151, 0.0126, 0.0007, 0.0283, 0.0208, 0.003...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7613, 0.8097, 0.6028, 0.9574, 0.7207, 0.585...</td>\n",
       "      <td>[0.2387, 0.1903, 0.3972, 0.0426, 0.2793, 0.414...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0358, 0.0127, 0.0035, 0.0035, 0.0154, 0.007...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.212, 0.8675, 0.4214, 0.5364, 0.6876, 0.1276...</td>\n",
       "      <td>[0.788, 0.1325, 0.5786, 0.4636, 0.3124, 0.8724...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0087, 0.0045, 0.1794, 0.0031, 0.0006, 0.020...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.2659, 0.1515, 0.1147, 0.3712, 0.1794, 0.467...</td>\n",
       "      <td>[0.7341, 0.8485, 0.8853, 0.6288, 0.8206, 0.532...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0155, 0.0053, 0.0285, 0.0093, 0.0024, 0.004...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.5992, 0.6459, 0.5368, 0.7388, 0.4159, 0.717...</td>\n",
       "      <td>[0.4008, 0.3541, 0.4632, 0.2612, 0.5841, 0.282...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0368, 0.0058, 0.072, 0.0028, 0.0024, 0.0183...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1561, 0.5253, 0.0895, 0.0885, 0.5671, 0.373...</td>\n",
       "      <td>[0.8439, 0.4747, 0.9105, 0.9115, 0.4329, 0.626...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0233, 0.006, 0.0763, 0.0019, 0.0026, 0.0209...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1828, 0.7577, 0.1064, 0.217, 0.726, 0.2193,...</td>\n",
       "      <td>[0.8172, 0.2423, 0.8936, 0.783, 0.274, 0.7807,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0536, 0.0228, 0.0212, 0.0399, 0.0019, 0.003...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.5052, 0.3654, 0.6916, 0.9133, 0.1267, 0.412...</td>\n",
       "      <td>[0.4948, 0.6346, 0.3084, 0.0867, 0.8733, 0.587...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0085, 0.005, 0.03, 0.0226, 0.0008, 0.0014, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4635, 0.1329, 0.3847, 0.8985, 0.0526, 0.602...</td>\n",
       "      <td>[0.5365, 0.8671, 0.6153, 0.1015, 0.9474, 0.397...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.018, 0.0091, 0.0259, 0.0146, 0.0018, 0.0154...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.3639, 0.2873, 0.242, 0.7282, 0.0962, 0.572,...</td>\n",
       "      <td>[0.6361, 0.7127, 0.758, 0.2718, 0.9038, 0.428,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0246, 0.0084, 0.0215, 0.0316, 0.0231, 0.059...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7585, 0.7709, 0.7467, 0.2942, 0.6959, 0.811...</td>\n",
       "      <td>[0.2415, 0.2291, 0.2533, 0.7058, 0.3041, 0.188...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0319, 0.0025, 0.1245, 0.0119, 0.0029, 0.015...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.3426, 0.2787, 0.4633, 0.1917, 0.0772, 0.551...</td>\n",
       "      <td>[0.6574, 0.7213, 0.5367, 0.8083, 0.9228, 0.448...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0042, 0.0053, 0.0212, 0.0124, 0.0069, 0.004...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3645, 0.4171, 0.9107, 0.7793, 0.3916, 0.674...</td>\n",
       "      <td>[0.6355, 0.5829, 0.0893, 0.2207, 0.6084, 0.325...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.006, 0.0037, 0.0323, 0.0017, 0.0007, 0.001,...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1275, 0.0273, 0.5656, 0.4535, 0.0714, 0.557...</td>\n",
       "      <td>[0.8725, 0.9727, 0.4344, 0.5465, 0.9286, 0.442...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0003, 0.0035, 0.0077, 0.0028, 0.0064, 0.004...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7533, 0.6025, 0.5921, 0.7736, 0.8804, 0.886...</td>\n",
       "      <td>[0.2467, 0.3975, 0.4079, 0.2264, 0.1196, 0.113...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0005, 0.0101, 0.0206, 0.0112, 0.0019, 0.002...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7637, 0.3169, 0.715, 0.8664, 0.9432, 0.9776...</td>\n",
       "      <td>[0.2363, 0.6831, 0.285, 0.1336, 0.0568, 0.0224...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0003, 0.0097, 0.0018, 0.0033, 0.0112, 0.001...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.844, 0.4753, 0.8753, 0.9421, 0.9809, 0.9672...</td>\n",
       "      <td>[0.156, 0.5247, 0.1247, 0.0579, 0.0191, 0.0328...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0766, 0.6959, 0.0656, 0.0311, 0.6121, 0.499...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9497, 0.9766, 0.4152, 0.9433, 1.0, 0.6926, ...</td>\n",
       "      <td>[0.0503, 0.0234, 0.5848, 0.0567, 0.0, 0.3074, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0025, 0.0065, 0.0288, 0.0476, 0.0017, 0.001...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9261, 0.4153, 0.9142, 0.8425, 0.3015, 0.825...</td>\n",
       "      <td>[0.0739, 0.5847, 0.0858, 0.1575, 0.6985, 0.174...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0021, 0.0023, 0.0166, 0.0457, 0.0008, 0.000...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.801, 0.7535, 0.9643, 0.973, 0.1706, 0.846, ...</td>\n",
       "      <td>[0.199, 0.2465, 0.0357, 0.027, 0.8294, 0.154, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0273, 0.0055, 0.0563, 0.0058, 0.0012, 0.008...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.1921, 0.3225, 0.079, 0.2351, 0.0984, 0.252,...</td>\n",
       "      <td>[0.8079, 0.6775, 0.921, 0.7649, 0.9016, 0.748,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0455, 0.0288, 0.0151, 0.0201, 0.102, 0.0413...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8604, 0.9554, 0.102, 0.4944, 0.8328, 0.1811...</td>\n",
       "      <td>[0.1396, 0.0446, 0.898, 0.5056, 0.1672, 0.8189...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0312, 0.0089, 0.025, 0.0097, 0.0008, 0.0057...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.2471, 0.1354, 0.5248, 0.5329, 0.0668, 0.639...</td>\n",
       "      <td>[0.7529, 0.8646, 0.4752, 0.4671, 0.9332, 0.360...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0225, 0.0054, 0.0307, 0.0037, 0.0004, 0.004...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0653, 0.0559, 0.3218, 0.4106, 0.0281, 0.522...</td>\n",
       "      <td>[0.9347, 0.9441, 0.6782, 0.5894, 0.9719, 0.477...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0031, 0.0041, 0.0113, 0.0127, 0.0003, 0.000...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0814, 0.103, 0.7404, 0.8442, 0.0403, 0.9726...</td>\n",
       "      <td>[0.9186, 0.897, 0.2596, 0.1558, 0.9597, 0.0274...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0022, 0.0007, 0.0389, 0.0075, 0.0005, 0.000...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7491, 0.0556, 0.0437, 0.4302, 0.0837, 0.956...</td>\n",
       "      <td>[0.2509, 0.9444, 0.9563, 0.5698, 0.9163, 0.043...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0057, 0.0012, 0.0302, 0.0075, 0.0004, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.5941, 0.0314, 0.0402, 0.4797, 0.0359, 0.751...</td>\n",
       "      <td>[0.4059, 0.9686, 0.9598, 0.5203, 0.9641, 0.248...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0053, 0.0103, 0.004, 0.0174, 0.0028, 0.0016...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.865, 0.3525, 0.2984, 0.9458, 0.3766, 0.8321...</td>\n",
       "      <td>[0.135, 0.6475, 0.7016, 0.0542, 0.6234, 0.1679...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0172, 0.0019, 0.0715, 0.0073, 0.0004, 0.004...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1979, 0.0315, 0.0385, 0.1226, 0.0505, 0.795...</td>\n",
       "      <td>[0.8021, 0.9685, 0.9615, 0.8774, 0.9495, 0.204...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0108, 0.0034, 0.0172, 0.0042, 0.0002, 0.000...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1429, 0.0568, 0.5572, 0.4944, 0.0225, 0.535...</td>\n",
       "      <td>[0.8571, 0.9432, 0.4428, 0.5056, 0.9775, 0.464...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0042, 0.003, 0.0081, 0.005, 0.0002, 0.0002,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3506, 0.0711, 0.6664, 0.8305, 0.0577, 0.946...</td>\n",
       "      <td>[0.6494, 0.9289, 0.3336, 0.1695, 0.9423, 0.053...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0017, 0.0021, 0.0086, 0.0036, 1e-04, 1e-04,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2275, 0.0294, 0.8829, 0.7955, 0.0531, 0.988...</td>\n",
       "      <td>[0.7725, 0.9706, 0.1171, 0.2045, 0.9469, 0.011...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0054, 0.0021, 0.008, 0.0039, 1e-04, 1e-04, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2099, 0.0463, 0.6071, 0.5931, 0.023, 0.7452...</td>\n",
       "      <td>[0.7901, 0.9537, 0.3929, 0.4069, 0.977, 0.2548...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0051, 0.0059, 0.1282, 0.0238, 0.005, 0.023,...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.8186, 0.4216, 0.892, 0.4975, 0.5335, 0.7964...</td>\n",
       "      <td>[0.1814, 0.5784, 0.108, 0.5025, 0.4665, 0.2036...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0147, 0.0713, 0.0129, 0.0187, 0.0121, 0.007...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.8042, 0.636, 0.7951, 0.8082, 0.33, 0.4564, ...</td>\n",
       "      <td>[0.1958, 0.364, 0.2049, 0.1918, 0.67, 0.5436, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0046, 0.006, 0.0066, 0.0003, 0.0072, 0.0161...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.2067, 0.6806, 0.0103, 0.163, 0.7596, 0.1065...</td>\n",
       "      <td>[0.7933, 0.3194, 0.9897, 0.837, 0.2404, 0.8935...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0133, 0.0189, 0.0073, 0.0068, 0.035, 0.1372...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1775, 0.9712, 0.253, 0.6382, 0.8348, 0.3096...</td>\n",
       "      <td>[0.8225, 0.0288, 0.747, 0.3618, 0.1652, 0.6904...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0286, 0.0113, 0.0102, 0.0117, 0.0041, 0.004...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4922, 0.4697, 0.8038, 0.7373, 0.2197, 0.305...</td>\n",
       "      <td>[0.5078, 0.5303, 0.1962, 0.2627, 0.7803, 0.695...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0032, 0.0032, 0.0043, 0.014, 0.021, 0.0009,...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7311, 0.8645, 0.8184, 0.8041, 0.691, 0.4603...</td>\n",
       "      <td>[0.2689, 0.1355, 0.1816, 0.1959, 0.309, 0.5397...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3634, 0.0879, 0.0164, 0.0225, 0.01, 0.0319,...               0   \n",
       "1   [0.7642, 0.5026, 0.0083, 0.0239, 0.0799, 0.027...               0   \n",
       "2   [0.7702, 0.2109, 0.0072, 0.0814, 0.0163, 0.065...               0   \n",
       "3   [0.027, 0.2335, 0.0032, 0.0038, 0.0561, 0.0351...               1   \n",
       "4   [0.0787, 0.133, 0.0176, 0.0034, 0.0045, 0.0287...               1   \n",
       "5   [0.0121, 0.0125, 0.37, 0.0062, 0.0046, 0.0499,...               2   \n",
       "6   [0.0304, 0.0028, 0.0009, 0.859, 0.0022, 0.0004...               3   \n",
       "7   [0.0216, 0.0111, 0.0129, 0.6401, 0.0035, 0.002...               3   \n",
       "8   [0.0323, 0.0073, 0.0224, 0.5872, 0.009, 0.0086...               3   \n",
       "9   [0.0052, 0.0468, 0.0051, 0.0131, 0.5968, 0.100...               4   \n",
       "10  [0.0319, 0.4089, 0.0034, 0.0035, 0.3035, 0.442...               4   \n",
       "11  [0.0027, 0.0171, 0.0037, 0.0024, 0.0715, 0.426...               5   \n",
       "12  [0.0004, 0.0046, 0.0214, 0.0009, 0.0016, 0.002...               6   \n",
       "13  [0.0481, 0.0522, 0.1687, 0.0467, 0.0019, 0.064...               7   \n",
       "14  [0.0151, 0.0126, 0.0007, 0.0283, 0.0208, 0.003...               8   \n",
       "15  [0.0358, 0.0127, 0.0035, 0.0035, 0.0154, 0.007...               8   \n",
       "16  [0.0087, 0.0045, 0.1794, 0.0031, 0.0006, 0.020...               9   \n",
       "17  [0.0155, 0.0053, 0.0285, 0.0093, 0.0024, 0.004...               9   \n",
       "18  [0.0368, 0.0058, 0.072, 0.0028, 0.0024, 0.0183...              10   \n",
       "19  [0.0233, 0.006, 0.0763, 0.0019, 0.0026, 0.0209...              10   \n",
       "20  [0.0536, 0.0228, 0.0212, 0.0399, 0.0019, 0.003...              11   \n",
       "21  [0.0085, 0.005, 0.03, 0.0226, 0.0008, 0.0014, ...              11   \n",
       "22  [0.018, 0.0091, 0.0259, 0.0146, 0.0018, 0.0154...              12   \n",
       "23  [0.0246, 0.0084, 0.0215, 0.0316, 0.0231, 0.059...              13   \n",
       "24  [0.0319, 0.0025, 0.1245, 0.0119, 0.0029, 0.015...              13   \n",
       "25  [0.0042, 0.0053, 0.0212, 0.0124, 0.0069, 0.004...              14   \n",
       "26  [0.006, 0.0037, 0.0323, 0.0017, 0.0007, 0.001,...              14   \n",
       "27  [0.0003, 0.0035, 0.0077, 0.0028, 0.0064, 0.004...              15   \n",
       "28  [0.0005, 0.0101, 0.0206, 0.0112, 0.0019, 0.002...              15   \n",
       "29  [0.0003, 0.0097, 0.0018, 0.0033, 0.0112, 0.001...              15   \n",
       "30  [0.0766, 0.6959, 0.0656, 0.0311, 0.6121, 0.499...              16   \n",
       "31  [0.0025, 0.0065, 0.0288, 0.0476, 0.0017, 0.001...              17   \n",
       "32  [0.0021, 0.0023, 0.0166, 0.0457, 0.0008, 0.000...              17   \n",
       "33  [0.0273, 0.0055, 0.0563, 0.0058, 0.0012, 0.008...              18   \n",
       "34  [0.0455, 0.0288, 0.0151, 0.0201, 0.102, 0.0413...              19   \n",
       "35  [0.0312, 0.0089, 0.025, 0.0097, 0.0008, 0.0057...              20   \n",
       "36  [0.0225, 0.0054, 0.0307, 0.0037, 0.0004, 0.004...              21   \n",
       "37  [0.0031, 0.0041, 0.0113, 0.0127, 0.0003, 0.000...              21   \n",
       "38  [0.0022, 0.0007, 0.0389, 0.0075, 0.0005, 0.000...              22   \n",
       "39  [0.0057, 0.0012, 0.0302, 0.0075, 0.0004, 0.001...              22   \n",
       "40  [0.0053, 0.0103, 0.004, 0.0174, 0.0028, 0.0016...              22   \n",
       "41  [0.0172, 0.0019, 0.0715, 0.0073, 0.0004, 0.004...              22   \n",
       "42  [0.0108, 0.0034, 0.0172, 0.0042, 0.0002, 0.000...              23   \n",
       "43  [0.0042, 0.003, 0.0081, 0.005, 0.0002, 0.0002,...              23   \n",
       "44  [0.0017, 0.0021, 0.0086, 0.0036, 1e-04, 1e-04,...              23   \n",
       "45  [0.0054, 0.0021, 0.008, 0.0039, 1e-04, 1e-04, ...              23   \n",
       "46  [0.0051, 0.0059, 0.1282, 0.0238, 0.005, 0.023,...              24   \n",
       "47  [0.0147, 0.0713, 0.0129, 0.0187, 0.0121, 0.007...              24   \n",
       "48  [0.0046, 0.006, 0.0066, 0.0003, 0.0072, 0.0161...              25   \n",
       "49  [0.0133, 0.0189, 0.0073, 0.0068, 0.035, 0.1372...              25   \n",
       "50  [0.0286, 0.0113, 0.0102, 0.0117, 0.0041, 0.004...              26   \n",
       "51  [0.0032, 0.0032, 0.0043, 0.014, 0.021, 0.0009,...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.337, 0.4945, 0.0825, 0.4971, 0.2636, 0.0967...   \n",
       "1                  0  [0.7866, 0.7477, 0.2259, 0.6594, 0.6491, 0.261...   \n",
       "2                  0  [0.5834, 0.6871, 0.1666, 0.7028, 0.2497, 0.129...   \n",
       "3                  1  [0.5781, 0.5319, 0.1616, 0.7419, 0.8572, 0.521...   \n",
       "4                  1  [0.1601, 0.2518, 0.0372, 0.3837, 0.3024, 0.051...   \n",
       "5                  2  [0.2325, 0.0717, 0.1688, 0.3056, 0.1657, 0.648...   \n",
       "6                  3  [0.9833, 0.9305, 0.9698, 0.9806, 0.153, 0.9658...   \n",
       "7                  3  [0.9271, 0.4672, 0.8773, 0.7801, 0.1669, 0.958...   \n",
       "8                  3  [0.8985, 0.6648, 0.9515, 0.6417, 0.1685, 0.895...   \n",
       "9                  4  [0.8674, 0.968, 0.831, 0.8029, 0.9758, 0.7971,...   \n",
       "10                 5  [0.6553, 0.9593, 0.1167, 0.5686, 0.8875, 0.708...   \n",
       "11                 5  [0.4357, 0.8852, 0.2353, 0.5261, 0.891, 0.5998...   \n",
       "12                 6  [0.6151, 0.6661, 0.8646, 0.9741, 0.8694, 0.681...   \n",
       "13                 7  [0.2967, 0.0617, 0.2514, 0.635, 0.7279, 0.9635...   \n",
       "14                 8  [0.7613, 0.8097, 0.6028, 0.9574, 0.7207, 0.585...   \n",
       "15                 8  [0.212, 0.8675, 0.4214, 0.5364, 0.6876, 0.1276...   \n",
       "16                 9  [0.2659, 0.1515, 0.1147, 0.3712, 0.1794, 0.467...   \n",
       "17                 9  [0.5992, 0.6459, 0.5368, 0.7388, 0.4159, 0.717...   \n",
       "18                10  [0.1561, 0.5253, 0.0895, 0.0885, 0.5671, 0.373...   \n",
       "19                10  [0.1828, 0.7577, 0.1064, 0.217, 0.726, 0.2193,...   \n",
       "20                11  [0.5052, 0.3654, 0.6916, 0.9133, 0.1267, 0.412...   \n",
       "21                11  [0.4635, 0.1329, 0.3847, 0.8985, 0.0526, 0.602...   \n",
       "22                12  [0.3639, 0.2873, 0.242, 0.7282, 0.0962, 0.572,...   \n",
       "23                13  [0.7585, 0.7709, 0.7467, 0.2942, 0.6959, 0.811...   \n",
       "24                13  [0.3426, 0.2787, 0.4633, 0.1917, 0.0772, 0.551...   \n",
       "25                14  [0.3645, 0.4171, 0.9107, 0.7793, 0.3916, 0.674...   \n",
       "26                14  [0.1275, 0.0273, 0.5656, 0.4535, 0.0714, 0.557...   \n",
       "27                15  [0.7533, 0.6025, 0.5921, 0.7736, 0.8804, 0.886...   \n",
       "28                15  [0.7637, 0.3169, 0.715, 0.8664, 0.9432, 0.9776...   \n",
       "29                15  [0.844, 0.4753, 0.8753, 0.9421, 0.9809, 0.9672...   \n",
       "30                16  [0.9497, 0.9766, 0.4152, 0.9433, 1.0, 0.6926, ...   \n",
       "31                17  [0.9261, 0.4153, 0.9142, 0.8425, 0.3015, 0.825...   \n",
       "32                17  [0.801, 0.7535, 0.9643, 0.973, 0.1706, 0.846, ...   \n",
       "33                18  [0.1921, 0.3225, 0.079, 0.2351, 0.0984, 0.252,...   \n",
       "34                19  [0.8604, 0.9554, 0.102, 0.4944, 0.8328, 0.1811...   \n",
       "35                20  [0.2471, 0.1354, 0.5248, 0.5329, 0.0668, 0.639...   \n",
       "36                21  [0.0653, 0.0559, 0.3218, 0.4106, 0.0281, 0.522...   \n",
       "37                23  [0.0814, 0.103, 0.7404, 0.8442, 0.0403, 0.9726...   \n",
       "38                22  [0.7491, 0.0556, 0.0437, 0.4302, 0.0837, 0.956...   \n",
       "39                22  [0.5941, 0.0314, 0.0402, 0.4797, 0.0359, 0.751...   \n",
       "40                22  [0.865, 0.3525, 0.2984, 0.9458, 0.3766, 0.8321...   \n",
       "41                22  [0.1979, 0.0315, 0.0385, 0.1226, 0.0505, 0.795...   \n",
       "42                23  [0.1429, 0.0568, 0.5572, 0.4944, 0.0225, 0.535...   \n",
       "43                23  [0.3506, 0.0711, 0.6664, 0.8305, 0.0577, 0.946...   \n",
       "44                23  [0.2275, 0.0294, 0.8829, 0.7955, 0.0531, 0.988...   \n",
       "45                23  [0.2099, 0.0463, 0.6071, 0.5931, 0.023, 0.7452...   \n",
       "46                13  [0.8186, 0.4216, 0.892, 0.4975, 0.5335, 0.7964...   \n",
       "47                24  [0.8042, 0.636, 0.7951, 0.8082, 0.33, 0.4564, ...   \n",
       "48                25  [0.2067, 0.6806, 0.0103, 0.163, 0.7596, 0.1065...   \n",
       "49                25  [0.1775, 0.9712, 0.253, 0.6382, 0.8348, 0.3096...   \n",
       "50                26  [0.4922, 0.4697, 0.8038, 0.7373, 0.2197, 0.305...   \n",
       "51                26  [0.7311, 0.8645, 0.8184, 0.8041, 0.691, 0.4603...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.663, 0.5055, 0.9175, 0.5029, 0.7364, 0.9033...  0.942308  0.056176  \n",
       "1   [0.2134, 0.2523, 0.7741, 0.3406, 0.3509, 0.738...       NaN       NaN  \n",
       "2   [0.4166, 0.3129, 0.8334, 0.2972, 0.7503, 0.870...       NaN       NaN  \n",
       "3   [0.4219, 0.4681, 0.8384, 0.2581, 0.1428, 0.478...       NaN       NaN  \n",
       "4   [0.8399, 0.7482, 0.9628, 0.6163, 0.6976, 0.948...       NaN       NaN  \n",
       "5   [0.7675, 0.9283, 0.8312, 0.6944, 0.8343, 0.352...       NaN       NaN  \n",
       "6   [0.0167, 0.0695, 0.0302, 0.0194, 0.847, 0.0342...       NaN       NaN  \n",
       "7   [0.0729, 0.5328, 0.1227, 0.2199, 0.8331, 0.041...       NaN       NaN  \n",
       "8   [0.1015, 0.3352, 0.0485, 0.3583, 0.8315, 0.104...       NaN       NaN  \n",
       "9   [0.1326, 0.032, 0.169, 0.1971, 0.0242, 0.2029,...       NaN       NaN  \n",
       "10  [0.3447, 0.0407, 0.8833, 0.4314, 0.1125, 0.291...       NaN       NaN  \n",
       "11  [0.5643, 0.1148, 0.7647, 0.4739, 0.109, 0.4002...       NaN       NaN  \n",
       "12  [0.3849, 0.3339, 0.1354, 0.0259, 0.1306, 0.318...       NaN       NaN  \n",
       "13  [0.7033, 0.9383, 0.7486, 0.365, 0.2721, 0.0365...       NaN       NaN  \n",
       "14  [0.2387, 0.1903, 0.3972, 0.0426, 0.2793, 0.414...       NaN       NaN  \n",
       "15  [0.788, 0.1325, 0.5786, 0.4636, 0.3124, 0.8724...       NaN       NaN  \n",
       "16  [0.7341, 0.8485, 0.8853, 0.6288, 0.8206, 0.532...       NaN       NaN  \n",
       "17  [0.4008, 0.3541, 0.4632, 0.2612, 0.5841, 0.282...       NaN       NaN  \n",
       "18  [0.8439, 0.4747, 0.9105, 0.9115, 0.4329, 0.626...       NaN       NaN  \n",
       "19  [0.8172, 0.2423, 0.8936, 0.783, 0.274, 0.7807,...       NaN       NaN  \n",
       "20  [0.4948, 0.6346, 0.3084, 0.0867, 0.8733, 0.587...       NaN       NaN  \n",
       "21  [0.5365, 0.8671, 0.6153, 0.1015, 0.9474, 0.397...       NaN       NaN  \n",
       "22  [0.6361, 0.7127, 0.758, 0.2718, 0.9038, 0.428,...       NaN       NaN  \n",
       "23  [0.2415, 0.2291, 0.2533, 0.7058, 0.3041, 0.188...       NaN       NaN  \n",
       "24  [0.6574, 0.7213, 0.5367, 0.8083, 0.9228, 0.448...       NaN       NaN  \n",
       "25  [0.6355, 0.5829, 0.0893, 0.2207, 0.6084, 0.325...       NaN       NaN  \n",
       "26  [0.8725, 0.9727, 0.4344, 0.5465, 0.9286, 0.442...       NaN       NaN  \n",
       "27  [0.2467, 0.3975, 0.4079, 0.2264, 0.1196, 0.113...       NaN       NaN  \n",
       "28  [0.2363, 0.6831, 0.285, 0.1336, 0.0568, 0.0224...       NaN       NaN  \n",
       "29  [0.156, 0.5247, 0.1247, 0.0579, 0.0191, 0.0328...       NaN       NaN  \n",
       "30  [0.0503, 0.0234, 0.5848, 0.0567, 0.0, 0.3074, ...       NaN       NaN  \n",
       "31  [0.0739, 0.5847, 0.0858, 0.1575, 0.6985, 0.174...       NaN       NaN  \n",
       "32  [0.199, 0.2465, 0.0357, 0.027, 0.8294, 0.154, ...       NaN       NaN  \n",
       "33  [0.8079, 0.6775, 0.921, 0.7649, 0.9016, 0.748,...       NaN       NaN  \n",
       "34  [0.1396, 0.0446, 0.898, 0.5056, 0.1672, 0.8189...       NaN       NaN  \n",
       "35  [0.7529, 0.8646, 0.4752, 0.4671, 0.9332, 0.360...       NaN       NaN  \n",
       "36  [0.9347, 0.9441, 0.6782, 0.5894, 0.9719, 0.477...       NaN       NaN  \n",
       "37  [0.9186, 0.897, 0.2596, 0.1558, 0.9597, 0.0274...       NaN       NaN  \n",
       "38  [0.2509, 0.9444, 0.9563, 0.5698, 0.9163, 0.043...       NaN       NaN  \n",
       "39  [0.4059, 0.9686, 0.9598, 0.5203, 0.9641, 0.248...       NaN       NaN  \n",
       "40  [0.135, 0.6475, 0.7016, 0.0542, 0.6234, 0.1679...       NaN       NaN  \n",
       "41  [0.8021, 0.9685, 0.9615, 0.8774, 0.9495, 0.204...       NaN       NaN  \n",
       "42  [0.8571, 0.9432, 0.4428, 0.5056, 0.9775, 0.464...       NaN       NaN  \n",
       "43  [0.6494, 0.9289, 0.3336, 0.1695, 0.9423, 0.053...       NaN       NaN  \n",
       "44  [0.7725, 0.9706, 0.1171, 0.2045, 0.9469, 0.011...       NaN       NaN  \n",
       "45  [0.7901, 0.9537, 0.3929, 0.4069, 0.977, 0.2548...       NaN       NaN  \n",
       "46  [0.1814, 0.5784, 0.108, 0.5025, 0.4665, 0.2036...       NaN       NaN  \n",
       "47  [0.1958, 0.364, 0.2049, 0.1918, 0.67, 0.5436, ...       NaN       NaN  \n",
       "48  [0.7933, 0.3194, 0.9897, 0.837, 0.2404, 0.8935...       NaN       NaN  \n",
       "49  [0.8225, 0.0288, 0.747, 0.3618, 0.1652, 0.6904...       NaN       NaN  \n",
       "50  [0.5078, 0.5303, 0.1962, 0.2627, 0.7803, 0.695...       NaN       NaN  \n",
       "51  [0.2689, 0.1355, 0.1816, 0.1959, 0.309, 0.5397...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2621 : Training: loss:  0.041609995\n",
      "2622 : Training: loss:  0.02648742\n",
      "2623 : Training: loss:  0.033648882\n",
      "2624 : Training: loss:  0.031233294\n",
      "2625 : Training: loss:  0.021169918\n",
      "2626 : Training: loss:  0.030586474\n",
      "2627 : Training: loss:  0.034341708\n",
      "2628 : Training: loss:  0.027388941\n",
      "2629 : Training: loss:  0.05408223\n",
      "2630 : Training: loss:  0.035767417\n",
      "2631 : Training: loss:  0.02330532\n",
      "2632 : Training: loss:  0.037065897\n",
      "2633 : Training: loss:  0.03574856\n",
      "2634 : Training: loss:  0.050197035\n",
      "2635 : Training: loss:  0.03254642\n",
      "2636 : Training: loss:  0.036102645\n",
      "2637 : Training: loss:  0.020445319\n",
      "2638 : Training: loss:  0.040986516\n",
      "2639 : Training: loss:  0.032707307\n",
      "2640 : Training: loss:  0.033140846\n",
      "Validation: Loss:  0.055351023  Accuracy:  0.9423077\n",
      "2641 : Training: loss:  0.020430319\n",
      "2642 : Training: loss:  0.02796969\n",
      "2643 : Training: loss:  0.025225295\n",
      "2644 : Training: loss:  0.044286475\n",
      "2645 : Training: loss:  0.024936272\n",
      "2646 : Training: loss:  0.039601598\n",
      "2647 : Training: loss:  0.02958841\n",
      "2648 : Training: loss:  0.027797105\n",
      "2649 : Training: loss:  0.029604198\n",
      "2650 : Training: loss:  0.03319961\n",
      "2651 : Training: loss:  0.03306623\n",
      "2652 : Training: loss:  0.038766976\n",
      "2653 : Training: loss:  0.017798094\n",
      "2654 : Training: loss:  0.024579467\n",
      "2655 : Training: loss:  0.034770843\n",
      "2656 : Training: loss:  0.027338006\n",
      "2657 : Training: loss:  0.025060238\n",
      "2658 : Training: loss:  0.0235887\n",
      "2659 : Training: loss:  0.045742895\n",
      "2660 : Training: loss:  0.057000283\n",
      "Validation: Loss:  0.05453637  Accuracy:  0.9423077\n",
      "2661 : Training: loss:  0.027264075\n",
      "2662 : Training: loss:  0.06528531\n",
      "2663 : Training: loss:  0.039336346\n",
      "2664 : Training: loss:  0.055252314\n",
      "2665 : Training: loss:  0.02853444\n",
      "2666 : Training: loss:  0.0553725\n",
      "2667 : Training: loss:  0.044424586\n",
      "2668 : Training: loss:  0.035144273\n",
      "2669 : Training: loss:  0.028127985\n",
      "2670 : Training: loss:  0.046451714\n",
      "2671 : Training: loss:  0.031493433\n",
      "2672 : Training: loss:  0.040780656\n",
      "2673 : Training: loss:  0.041604742\n",
      "2674 : Training: loss:  0.040922593\n",
      "2675 : Training: loss:  0.04026744\n",
      "2676 : Training: loss:  0.03552585\n",
      "2677 : Training: loss:  0.033470985\n",
      "2678 : Training: loss:  0.025944231\n",
      "2679 : Training: loss:  0.02771318\n",
      "2680 : Training: loss:  0.03771941\n",
      "Validation: Loss:  0.053919606  Accuracy:  0.9423077\n",
      "2681 : Training: loss:  0.0400999\n",
      "2682 : Training: loss:  0.02834896\n",
      "2683 : Training: loss:  0.051604006\n",
      "2684 : Training: loss:  0.038518492\n",
      "2685 : Training: loss:  0.054916374\n",
      "2686 : Training: loss:  0.018980335\n",
      "2687 : Training: loss:  0.03552138\n",
      "2688 : Training: loss:  0.022498062\n",
      "2689 : Training: loss:  0.028465709\n",
      "2690 : Training: loss:  0.041269712\n",
      "2691 : Training: loss:  0.021680381\n",
      "2692 : Training: loss:  0.030253394\n",
      "2693 : Training: loss:  0.022864701\n",
      "2694 : Training: loss:  0.04023634\n",
      "2695 : Training: loss:  0.048943974\n",
      "2696 : Training: loss:  0.030275755\n",
      "2697 : Training: loss:  0.030839836\n",
      "2698 : Training: loss:  0.030686079\n",
      "2699 : Training: loss:  0.026819361\n",
      "2700 : Training: loss:  0.032314584\n",
      "Validation: Loss:  0.053278204  Accuracy:  0.9423077\n",
      "2701 : Training: loss:  0.037109572\n",
      "2702 : Training: loss:  0.026124476\n",
      "2703 : Training: loss:  0.035257567\n",
      "2704 : Training: loss:  0.024589993\n",
      "2705 : Training: loss:  0.035222534\n",
      "2706 : Training: loss:  0.024515394\n",
      "2707 : Training: loss:  0.015698763\n",
      "2708 : Training: loss:  0.02242749\n",
      "2709 : Training: loss:  0.026325712\n",
      "2710 : Training: loss:  0.020146897\n",
      "2711 : Training: loss:  0.02683687\n",
      "2712 : Training: loss:  0.031077798\n",
      "2713 : Training: loss:  0.019710174\n",
      "2714 : Training: loss:  0.03667236\n",
      "2715 : Training: loss:  0.023877285\n",
      "2716 : Training: loss:  0.03589569\n",
      "2717 : Training: loss:  0.035371274\n",
      "2718 : Training: loss:  0.021292942\n",
      "2719 : Training: loss:  0.04521737\n",
      "2720 : Training: loss:  0.028368818\n",
      "Validation: Loss:  0.052751586  Accuracy:  0.9423077\n",
      "2721 : Training: loss:  0.042857226\n",
      "2722 : Training: loss:  0.028777346\n",
      "2723 : Training: loss:  0.05609626\n",
      "2724 : Training: loss:  0.0328712\n",
      "2725 : Training: loss:  0.034320038\n",
      "2726 : Training: loss:  0.027785696\n",
      "2727 : Training: loss:  0.025623782\n",
      "2728 : Training: loss:  0.031953715\n",
      "2729 : Training: loss:  0.019410059\n",
      "2730 : Training: loss:  0.050545506\n",
      "2731 : Training: loss:  0.03905215\n",
      "2732 : Training: loss:  0.0264975\n",
      "2733 : Training: loss:  0.02724068\n",
      "2734 : Training: loss:  0.036293402\n",
      "2735 : Training: loss:  0.045191243\n",
      "2736 : Training: loss:  0.035489667\n",
      "2737 : Training: loss:  0.05117363\n",
      "2738 : Training: loss:  0.02314361\n",
      "2739 : Training: loss:  0.028178107\n",
      "2740 : Training: loss:  0.024125649\n",
      "Validation: Loss:  0.05223543  Accuracy:  0.9423077\n",
      "2741 : Training: loss:  0.032367557\n",
      "2742 : Training: loss:  0.019291375\n",
      "2743 : Training: loss:  0.023482438\n",
      "2744 : Training: loss:  0.031077312\n",
      "2745 : Training: loss:  0.036377084\n",
      "2746 : Training: loss:  0.038263127\n",
      "2747 : Training: loss:  0.032856647\n",
      "2748 : Training: loss:  0.028639333\n",
      "2749 : Training: loss:  0.045432895\n",
      "2750 : Training: loss:  0.057894103\n",
      "2751 : Training: loss:  0.022722142\n",
      "2752 : Training: loss:  0.03731449\n",
      "2753 : Training: loss:  0.026108755\n",
      "2754 : Training: loss:  0.039389662\n",
      "2755 : Training: loss:  0.028869227\n",
      "2756 : Training: loss:  0.023513012\n",
      "2757 : Training: loss:  0.020111\n",
      "2758 : Training: loss:  0.05530756\n",
      "2759 : Training: loss:  0.032028\n",
      "2760 : Training: loss:  0.015825216\n",
      "Validation: Loss:  0.051755458  Accuracy:  0.9423077\n",
      "2761 : Training: loss:  0.022551265\n",
      "2762 : Training: loss:  0.026902758\n",
      "2763 : Training: loss:  0.038433287\n",
      "2764 : Training: loss:  0.056810528\n",
      "2765 : Training: loss:  0.039791185\n",
      "2766 : Training: loss:  0.021355089\n",
      "2767 : Training: loss:  0.04113663\n",
      "2768 : Training: loss:  0.050200008\n",
      "2769 : Training: loss:  0.035423882\n",
      "2770 : Training: loss:  0.038560223\n",
      "2771 : Training: loss:  0.046472624\n",
      "2772 : Training: loss:  0.039141487\n",
      "2773 : Training: loss:  0.023368554\n",
      "2774 : Training: loss:  0.066741616\n",
      "2775 : Training: loss:  0.03243665\n",
      "2776 : Training: loss:  0.04258101\n",
      "2777 : Training: loss:  0.038484707\n",
      "2778 : Training: loss:  0.047954805\n",
      "2779 : Training: loss:  0.042917494\n",
      "2780 : Training: loss:  0.028604724\n",
      "Validation: Loss:  0.051097102  Accuracy:  0.9423077\n",
      "2781 : Training: loss:  0.06016708\n",
      "2782 : Training: loss:  0.028679017\n",
      "2783 : Training: loss:  0.021162678\n",
      "2784 : Training: loss:  0.037277628\n",
      "2785 : Training: loss:  0.07445473\n",
      "2786 : Training: loss:  0.032144025\n",
      "2787 : Training: loss:  0.032925323\n",
      "2788 : Training: loss:  0.03761112\n",
      "2789 : Training: loss:  0.03459369\n",
      "2790 : Training: loss:  0.04423677\n",
      "2791 : Training: loss:  0.027475448\n",
      "2792 : Training: loss:  0.012752577\n",
      "2793 : Training: loss:  0.062000442\n",
      "2794 : Training: loss:  0.025229717\n",
      "2795 : Training: loss:  0.03327002\n",
      "2796 : Training: loss:  0.040390834\n",
      "2797 : Training: loss:  0.028312262\n",
      "2798 : Training: loss:  0.038769517\n",
      "2799 : Training: loss:  0.041828938\n",
      "2800 : Training: loss:  0.03285398\n",
      "Validation: Loss:  0.050451506  Accuracy:  0.9423077\n",
      "2801 : Training: loss:  0.04285336\n",
      "2802 : Training: loss:  0.027151523\n",
      "2803 : Training: loss:  0.053617485\n",
      "2804 : Training: loss:  0.03860433\n",
      "2805 : Training: loss:  0.036035616\n",
      "2806 : Training: loss:  0.027808027\n",
      "2807 : Training: loss:  0.039863918\n",
      "2808 : Training: loss:  0.03927154\n",
      "2809 : Training: loss:  0.030507132\n",
      "2810 : Training: loss:  0.029730607\n",
      "2811 : Training: loss:  0.027902992\n",
      "2812 : Training: loss:  0.026591532\n",
      "2813 : Training: loss:  0.033232447\n",
      "2814 : Training: loss:  0.022999708\n",
      "2815 : Training: loss:  0.017244762\n",
      "2816 : Training: loss:  0.019200383\n",
      "2817 : Training: loss:  0.049227566\n",
      "2818 : Training: loss:  0.018285548\n",
      "2819 : Training: loss:  0.031603534\n",
      "2820 : Training: loss:  0.032279275\n",
      "Validation: Loss:  0.049926713  Accuracy:  0.96153843\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.4318, 0.0744, 0.0111, 0.0179, 0.0102, 0.023...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.343, 0.5173, 0.0806, 0.5219, 0.2807, 0.0964...</td>\n",
       "      <td>[0.657, 0.4827, 0.9194, 0.4781, 0.7193, 0.9036...</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.049927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.8212, 0.4673, 0.0061, 0.0196, 0.0992, 0.018...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7945, 0.7637, 0.2216, 0.66, 0.6742, 0.2715,...</td>\n",
       "      <td>[0.2055, 0.2363, 0.7784, 0.34, 0.3258, 0.7285,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.8517, 0.1834, 0.0045, 0.0748, 0.0183, 0.05,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5952, 0.7192, 0.1664, 0.7278, 0.2671, 0.130...</td>\n",
       "      <td>[0.4048, 0.2808, 0.8336, 0.2722, 0.7329, 0.869...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0237, 0.2141, 0.0019, 0.0029, 0.079, 0.0257...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5789, 0.5439, 0.1595, 0.7527, 0.8757, 0.546...</td>\n",
       "      <td>[0.4211, 0.4561, 0.8405, 0.2473, 0.1243, 0.453...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.075, 0.1307, 0.0125, 0.0022, 0.0042, 0.0216...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1625, 0.2707, 0.0365, 0.411, 0.3376, 0.0544...</td>\n",
       "      <td>[0.8375, 0.7293, 0.9635, 0.589, 0.6624, 0.9456...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.008, 0.0093, 0.3897, 0.0041, 0.004, 0.0363,...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.2448, 0.0797, 0.1728, 0.3347, 0.1786, 0.688...</td>\n",
       "      <td>[0.7552, 0.9203, 0.8272, 0.6653, 0.8214, 0.311...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0324, 0.0015, 0.0005, 0.8802, 0.0024, 0.000...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9848, 0.9452, 0.9711, 0.9833, 0.1432, 0.968...</td>\n",
       "      <td>[0.0152, 0.0548, 0.0289, 0.0167, 0.8568, 0.031...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0169, 0.0078, 0.009, 0.6738, 0.0035, 0.0017...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9312, 0.4926, 0.8803, 0.7908, 0.1669, 0.963...</td>\n",
       "      <td>[0.0688, 0.5074, 0.1197, 0.2092, 0.8331, 0.036...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0258, 0.0046, 0.015, 0.6253, 0.0093, 0.0058...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9064, 0.7028, 0.9566, 0.6554, 0.1687, 0.906...</td>\n",
       "      <td>[0.0936, 0.2972, 0.0434, 0.3446, 0.8313, 0.093...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0042, 0.0296, 0.0034, 0.0113, 0.7278, 0.070...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8687, 0.9693, 0.835, 0.7878, 0.9779, 0.812,...</td>\n",
       "      <td>[0.1313, 0.0307, 0.165, 0.2122, 0.0221, 0.188,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0299, 0.3611, 0.0021, 0.0027, 0.4451, 0.42,...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.6689, 0.9639, 0.1133, 0.543, 0.9017, 0.7388...</td>\n",
       "      <td>[0.3311, 0.0361, 0.8867, 0.457, 0.0983, 0.2612...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0019, 0.0109, 0.0022, 0.0017, 0.1056, 0.419...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.4281, 0.895, 0.2356, 0.512, 0.9068, 0.6161,...</td>\n",
       "      <td>[0.5719, 0.105, 0.7644, 0.488, 0.0932, 0.3839,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0003, 0.0034, 0.0155, 0.0006, 0.0014, 0.001...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.6401, 0.6866, 0.8715, 0.9767, 0.8857, 0.696...</td>\n",
       "      <td>[0.3599, 0.3134, 0.1285, 0.0233, 0.1143, 0.303...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0461, 0.044, 0.1366, 0.0377, 0.0014, 0.0506...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.2879, 0.054, 0.2415, 0.6254, 0.7315, 0.9659...</td>\n",
       "      <td>[0.7121, 0.946, 0.7585, 0.3746, 0.2685, 0.0341...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.016, 0.0085, 0.0004, 0.0249, 0.0325, 0.0026...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7714, 0.8287, 0.6069, 0.9615, 0.7357, 0.596...</td>\n",
       "      <td>[0.2286, 0.1713, 0.3931, 0.0385, 0.2643, 0.403...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0367, 0.008, 0.0022, 0.0025, 0.0186, 0.0046...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.207, 0.8771, 0.425, 0.5312, 0.7012, 0.1266,...</td>\n",
       "      <td>[0.793, 0.1229, 0.575, 0.4688, 0.2988, 0.8734,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.007, 0.0031, 0.152, 0.002, 0.0004, 0.0154, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.2797, 0.1728, 0.117, 0.4003, 0.1921, 0.4952...</td>\n",
       "      <td>[0.7203, 0.8272, 0.883, 0.5997, 0.8079, 0.5048...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0152, 0.0035, 0.0207, 0.0077, 0.0022, 0.003...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.6217, 0.6819, 0.5444, 0.7522, 0.4172, 0.739...</td>\n",
       "      <td>[0.3783, 0.3181, 0.4556, 0.2478, 0.5828, 0.260...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0314, 0.0042, 0.0549, 0.0019, 0.0023, 0.014...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1516, 0.5473, 0.0899, 0.0822, 0.6049, 0.410...</td>\n",
       "      <td>[0.8484, 0.4527, 0.9101, 0.9178, 0.3951, 0.589...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0195, 0.0043, 0.058, 0.0013, 0.0024, 0.0157...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.179, 0.7796, 0.1073, 0.2002, 0.7607, 0.2376...</td>\n",
       "      <td>[0.821, 0.2204, 0.8927, 0.7998, 0.2393, 0.7624...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0502, 0.0186, 0.0155, 0.034, 0.0017, 0.0019...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.5158, 0.3834, 0.7209, 0.9255, 0.1298, 0.431...</td>\n",
       "      <td>[0.4842, 0.6166, 0.2791, 0.0745, 0.8702, 0.568...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0064, 0.0041, 0.0227, 0.0201, 0.0007, 0.000...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.4774, 0.1413, 0.424, 0.9133, 0.0571, 0.633,...</td>\n",
       "      <td>[0.5226, 0.8587, 0.576, 0.0867, 0.9429, 0.367,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0159, 0.0069, 0.0178, 0.0123, 0.0017, 0.013...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.3699, 0.3153, 0.257, 0.7525, 0.1027, 0.5982...</td>\n",
       "      <td>[0.6301, 0.6847, 0.743, 0.2475, 0.8973, 0.4018...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0206, 0.0051, 0.0136, 0.0279, 0.0297, 0.046...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7627, 0.7962, 0.7616, 0.2715, 0.7232, 0.829...</td>\n",
       "      <td>[0.2373, 0.2038, 0.2384, 0.7285, 0.2768, 0.170...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0262, 0.0014, 0.1057, 0.0095, 0.0027, 0.010...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.3561, 0.327, 0.4872, 0.2039, 0.0831, 0.5932...</td>\n",
       "      <td>[0.6439, 0.673, 0.5128, 0.7961, 0.9169, 0.4068...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0031, 0.0035, 0.0148, 0.0095, 0.007, 0.0022...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3607, 0.4438, 0.921, 0.8053, 0.4078, 0.6988...</td>\n",
       "      <td>[0.6393, 0.5562, 0.079, 0.1947, 0.5922, 0.3012...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0042, 0.0028, 0.0239, 0.0011, 0.0006, 0.000...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.128, 0.0278, 0.5978, 0.4958, 0.0762, 0.599,...</td>\n",
       "      <td>[0.872, 0.9722, 0.4022, 0.5042, 0.9238, 0.401,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0002, 0.0024, 0.0051, 0.002, 0.0077, 0.0031...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7559, 0.619, 0.592, 0.7936, 0.8885, 0.8994,...</td>\n",
       "      <td>[0.2441, 0.381, 0.408, 0.2064, 0.1115, 0.1006,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0003, 0.0078, 0.0146, 0.008, 0.0017, 0.0018...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7665, 0.3105, 0.7163, 0.8728, 0.9444, 0.979...</td>\n",
       "      <td>[0.2335, 0.6895, 0.2837, 0.1272, 0.0556, 0.020...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0002, 0.0076, 0.001, 0.0024, 0.0148, 0.001,...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8428, 0.4726, 0.8752, 0.9477, 0.983, 0.97, ...</td>\n",
       "      <td>[0.1572, 0.5274, 0.1248, 0.0523, 0.017, 0.03, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.053, 0.6215, 0.0335, 0.016, 0.5874, 0.3538,...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.9305, 0.9561, 0.3923, 0.873, 1.0, 0.6106, 0...</td>\n",
       "      <td>[0.0695, 0.0439, 0.6077, 0.127, 0.0, 0.3894, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0018, 0.0049, 0.0227, 0.0409, 0.0014, 0.000...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9309, 0.4415, 0.9239, 0.8615, 0.3134, 0.843...</td>\n",
       "      <td>[0.0691, 0.5585, 0.0761, 0.1385, 0.6866, 0.156...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0016, 0.0015, 0.0125, 0.0403, 0.0007, 0.000...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.8164, 0.7909, 0.9697, 0.979, 0.1696, 0.8627...</td>\n",
       "      <td>[0.1836, 0.2091, 0.0303, 0.021, 0.8304, 0.1373...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0227, 0.0038, 0.0439, 0.0042, 0.001, 0.0055...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.2042, 0.3617, 0.0826, 0.2602, 0.1061, 0.282...</td>\n",
       "      <td>[0.7958, 0.6383, 0.9174, 0.7398, 0.8939, 0.717...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0445, 0.0208, 0.0091, 0.0156, 0.1272, 0.028...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8696, 0.9617, 0.1011, 0.493, 0.8534, 0.1878...</td>\n",
       "      <td>[0.1304, 0.0383, 0.8989, 0.507, 0.1466, 0.8122...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0316, 0.0074, 0.0187, 0.0084, 0.0007, 0.004...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.2538, 0.1534, 0.5575, 0.5732, 0.0693, 0.672...</td>\n",
       "      <td>[0.7462, 0.8466, 0.4425, 0.4268, 0.9307, 0.327...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0219, 0.0039, 0.0199, 0.0028, 0.0004, 0.003...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0633, 0.0606, 0.3485, 0.4452, 0.0291, 0.561...</td>\n",
       "      <td>[0.9367, 0.9394, 0.6515, 0.5548, 0.9709, 0.438...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0028, 0.0031, 0.0074, 0.0118, 0.0002, 0.000...</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0783, 0.1021, 0.7607, 0.8528, 0.0395, 0.976...</td>\n",
       "      <td>[0.9217, 0.8979, 0.2393, 0.1472, 0.9605, 0.023...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0016, 0.0005, 0.029, 0.0054, 0.0004, 0.0005...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7494, 0.055, 0.0406, 0.4545, 0.0842, 0.9616...</td>\n",
       "      <td>[0.2506, 0.945, 0.9594, 0.5455, 0.9158, 0.0384...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0042, 0.0008, 0.021, 0.0055, 0.0003, 0.0008...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6058, 0.0341, 0.041, 0.5164, 0.0399, 0.7822...</td>\n",
       "      <td>[0.3942, 0.9659, 0.959, 0.4836, 0.9601, 0.2178...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0042, 0.0079, 0.0027, 0.0129, 0.0027, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.8659, 0.3523, 0.3015, 0.9526, 0.3884, 0.840...</td>\n",
       "      <td>[0.1341, 0.6477, 0.6985, 0.0474, 0.6116, 0.159...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0136, 0.0013, 0.0544, 0.005, 0.0003, 0.0031...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1986, 0.0337, 0.0385, 0.1315, 0.0528, 0.822...</td>\n",
       "      <td>[0.8014, 0.9663, 0.9615, 0.8685, 0.9472, 0.177...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0096, 0.0026, 0.0118, 0.0032, 1e-04, 0.0002...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1463, 0.0609, 0.584, 0.5236, 0.0232, 0.5615...</td>\n",
       "      <td>[0.8537, 0.9391, 0.416, 0.4764, 0.9768, 0.4385...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0036, 0.0022, 0.0055, 0.0041, 0.0002, 1e-04...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.367, 0.0713, 0.6832, 0.8268, 0.0578, 0.9518...</td>\n",
       "      <td>[0.633, 0.9287, 0.3168, 0.1732, 0.9422, 0.0482...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0013, 0.0016, 0.0061, 0.003, 1e-04, 1e-04, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2372, 0.0282, 0.8925, 0.7945, 0.0523, 0.989...</td>\n",
       "      <td>[0.7628, 0.9718, 0.1075, 0.2055, 0.9477, 0.010...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0049, 0.0016, 0.0052, 0.0032, 1e-04, 1e-04,...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2079, 0.047, 0.6329, 0.618, 0.0228, 0.7661,...</td>\n",
       "      <td>[0.7921, 0.953, 0.3671, 0.382, 0.9772, 0.2339,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0036, 0.0041, 0.1074, 0.0201, 0.0045, 0.017...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.8277, 0.4565, 0.9054, 0.4997, 0.5611, 0.816...</td>\n",
       "      <td>[0.1723, 0.5435, 0.0946, 0.5003, 0.4389, 0.183...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0117, 0.0594, 0.0103, 0.0155, 0.0138, 0.004...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.8175, 0.6584, 0.8159, 0.8187, 0.3459, 0.467...</td>\n",
       "      <td>[0.1825, 0.3416, 0.1841, 0.1813, 0.6541, 0.532...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0033, 0.004, 0.004, 0.0002, 0.0092, 0.0114,...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.207, 0.7007, 0.0097, 0.1625, 0.7931, 0.1179...</td>\n",
       "      <td>[0.793, 0.2993, 0.9903, 0.8375, 0.2069, 0.8821...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0117, 0.0126, 0.0048, 0.0054, 0.0478, 0.104...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1711, 0.9736, 0.2563, 0.633, 0.8453, 0.3289...</td>\n",
       "      <td>[0.8289, 0.0264, 0.7437, 0.367, 0.1547, 0.6711...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0285, 0.0079, 0.0075, 0.0093, 0.0034, 0.002...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4844, 0.4738, 0.8222, 0.754, 0.2229, 0.2984...</td>\n",
       "      <td>[0.5156, 0.5262, 0.1778, 0.246, 0.7771, 0.7016...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0029, 0.0019, 0.0027, 0.0124, 0.026, 0.0005...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7399, 0.88, 0.8312, 0.8145, 0.6997, 0.4759,...</td>\n",
       "      <td>[0.2601, 0.12, 0.1688, 0.1855, 0.3003, 0.5241,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.4318, 0.0744, 0.0111, 0.0179, 0.0102, 0.023...               0   \n",
       "1   [0.8212, 0.4673, 0.0061, 0.0196, 0.0992, 0.018...               0   \n",
       "2   [0.8517, 0.1834, 0.0045, 0.0748, 0.0183, 0.05,...               0   \n",
       "3   [0.0237, 0.2141, 0.0019, 0.0029, 0.079, 0.0257...               1   \n",
       "4   [0.075, 0.1307, 0.0125, 0.0022, 0.0042, 0.0216...               1   \n",
       "5   [0.008, 0.0093, 0.3897, 0.0041, 0.004, 0.0363,...               2   \n",
       "6   [0.0324, 0.0015, 0.0005, 0.8802, 0.0024, 0.000...               3   \n",
       "7   [0.0169, 0.0078, 0.009, 0.6738, 0.0035, 0.0017...               3   \n",
       "8   [0.0258, 0.0046, 0.015, 0.6253, 0.0093, 0.0058...               3   \n",
       "9   [0.0042, 0.0296, 0.0034, 0.0113, 0.7278, 0.070...               4   \n",
       "10  [0.0299, 0.3611, 0.0021, 0.0027, 0.4451, 0.42,...               4   \n",
       "11  [0.0019, 0.0109, 0.0022, 0.0017, 0.1056, 0.419...               5   \n",
       "12  [0.0003, 0.0034, 0.0155, 0.0006, 0.0014, 0.001...               6   \n",
       "13  [0.0461, 0.044, 0.1366, 0.0377, 0.0014, 0.0506...               7   \n",
       "14  [0.016, 0.0085, 0.0004, 0.0249, 0.0325, 0.0026...               8   \n",
       "15  [0.0367, 0.008, 0.0022, 0.0025, 0.0186, 0.0046...               8   \n",
       "16  [0.007, 0.0031, 0.152, 0.002, 0.0004, 0.0154, ...               9   \n",
       "17  [0.0152, 0.0035, 0.0207, 0.0077, 0.0022, 0.003...               9   \n",
       "18  [0.0314, 0.0042, 0.0549, 0.0019, 0.0023, 0.014...              10   \n",
       "19  [0.0195, 0.0043, 0.058, 0.0013, 0.0024, 0.0157...              10   \n",
       "20  [0.0502, 0.0186, 0.0155, 0.034, 0.0017, 0.0019...              11   \n",
       "21  [0.0064, 0.0041, 0.0227, 0.0201, 0.0007, 0.000...              11   \n",
       "22  [0.0159, 0.0069, 0.0178, 0.0123, 0.0017, 0.013...              12   \n",
       "23  [0.0206, 0.0051, 0.0136, 0.0279, 0.0297, 0.046...              13   \n",
       "24  [0.0262, 0.0014, 0.1057, 0.0095, 0.0027, 0.010...              13   \n",
       "25  [0.0031, 0.0035, 0.0148, 0.0095, 0.007, 0.0022...              14   \n",
       "26  [0.0042, 0.0028, 0.0239, 0.0011, 0.0006, 0.000...              14   \n",
       "27  [0.0002, 0.0024, 0.0051, 0.002, 0.0077, 0.0031...              15   \n",
       "28  [0.0003, 0.0078, 0.0146, 0.008, 0.0017, 0.0018...              15   \n",
       "29  [0.0002, 0.0076, 0.001, 0.0024, 0.0148, 0.001,...              15   \n",
       "30  [0.053, 0.6215, 0.0335, 0.016, 0.5874, 0.3538,...              16   \n",
       "31  [0.0018, 0.0049, 0.0227, 0.0409, 0.0014, 0.000...              17   \n",
       "32  [0.0016, 0.0015, 0.0125, 0.0403, 0.0007, 0.000...              17   \n",
       "33  [0.0227, 0.0038, 0.0439, 0.0042, 0.001, 0.0055...              18   \n",
       "34  [0.0445, 0.0208, 0.0091, 0.0156, 0.1272, 0.028...              19   \n",
       "35  [0.0316, 0.0074, 0.0187, 0.0084, 0.0007, 0.004...              20   \n",
       "36  [0.0219, 0.0039, 0.0199, 0.0028, 0.0004, 0.003...              21   \n",
       "37  [0.0028, 0.0031, 0.0074, 0.0118, 0.0002, 0.000...              21   \n",
       "38  [0.0016, 0.0005, 0.029, 0.0054, 0.0004, 0.0005...              22   \n",
       "39  [0.0042, 0.0008, 0.021, 0.0055, 0.0003, 0.0008...              22   \n",
       "40  [0.0042, 0.0079, 0.0027, 0.0129, 0.0027, 0.001...              22   \n",
       "41  [0.0136, 0.0013, 0.0544, 0.005, 0.0003, 0.0031...              22   \n",
       "42  [0.0096, 0.0026, 0.0118, 0.0032, 1e-04, 0.0002...              23   \n",
       "43  [0.0036, 0.0022, 0.0055, 0.0041, 0.0002, 1e-04...              23   \n",
       "44  [0.0013, 0.0016, 0.0061, 0.003, 1e-04, 1e-04, ...              23   \n",
       "45  [0.0049, 0.0016, 0.0052, 0.0032, 1e-04, 1e-04,...              23   \n",
       "46  [0.0036, 0.0041, 0.1074, 0.0201, 0.0045, 0.017...              24   \n",
       "47  [0.0117, 0.0594, 0.0103, 0.0155, 0.0138, 0.004...              24   \n",
       "48  [0.0033, 0.004, 0.004, 0.0002, 0.0092, 0.0114,...              25   \n",
       "49  [0.0117, 0.0126, 0.0048, 0.0054, 0.0478, 0.104...              25   \n",
       "50  [0.0285, 0.0079, 0.0075, 0.0093, 0.0034, 0.002...              26   \n",
       "51  [0.0029, 0.0019, 0.0027, 0.0124, 0.026, 0.0005...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.343, 0.5173, 0.0806, 0.5219, 0.2807, 0.0964...   \n",
       "1                  0  [0.7945, 0.7637, 0.2216, 0.66, 0.6742, 0.2715,...   \n",
       "2                  0  [0.5952, 0.7192, 0.1664, 0.7278, 0.2671, 0.130...   \n",
       "3                  1  [0.5789, 0.5439, 0.1595, 0.7527, 0.8757, 0.546...   \n",
       "4                  1  [0.1625, 0.2707, 0.0365, 0.411, 0.3376, 0.0544...   \n",
       "5                  2  [0.2448, 0.0797, 0.1728, 0.3347, 0.1786, 0.688...   \n",
       "6                  3  [0.9848, 0.9452, 0.9711, 0.9833, 0.1432, 0.968...   \n",
       "7                  3  [0.9312, 0.4926, 0.8803, 0.7908, 0.1669, 0.963...   \n",
       "8                  3  [0.9064, 0.7028, 0.9566, 0.6554, 0.1687, 0.906...   \n",
       "9                  4  [0.8687, 0.9693, 0.835, 0.7878, 0.9779, 0.812,...   \n",
       "10                 4  [0.6689, 0.9639, 0.1133, 0.543, 0.9017, 0.7388...   \n",
       "11                 5  [0.4281, 0.895, 0.2356, 0.512, 0.9068, 0.6161,...   \n",
       "12                 6  [0.6401, 0.6866, 0.8715, 0.9767, 0.8857, 0.696...   \n",
       "13                 7  [0.2879, 0.054, 0.2415, 0.6254, 0.7315, 0.9659...   \n",
       "14                 8  [0.7714, 0.8287, 0.6069, 0.9615, 0.7357, 0.596...   \n",
       "15                 8  [0.207, 0.8771, 0.425, 0.5312, 0.7012, 0.1266,...   \n",
       "16                 9  [0.2797, 0.1728, 0.117, 0.4003, 0.1921, 0.4952...   \n",
       "17                 9  [0.6217, 0.6819, 0.5444, 0.7522, 0.4172, 0.739...   \n",
       "18                10  [0.1516, 0.5473, 0.0899, 0.0822, 0.6049, 0.410...   \n",
       "19                10  [0.179, 0.7796, 0.1073, 0.2002, 0.7607, 0.2376...   \n",
       "20                11  [0.5158, 0.3834, 0.7209, 0.9255, 0.1298, 0.431...   \n",
       "21                11  [0.4774, 0.1413, 0.424, 0.9133, 0.0571, 0.633,...   \n",
       "22                12  [0.3699, 0.3153, 0.257, 0.7525, 0.1027, 0.5982...   \n",
       "23                13  [0.7627, 0.7962, 0.7616, 0.2715, 0.7232, 0.829...   \n",
       "24                13  [0.3561, 0.327, 0.4872, 0.2039, 0.0831, 0.5932...   \n",
       "25                14  [0.3607, 0.4438, 0.921, 0.8053, 0.4078, 0.6988...   \n",
       "26                14  [0.128, 0.0278, 0.5978, 0.4958, 0.0762, 0.599,...   \n",
       "27                15  [0.7559, 0.619, 0.592, 0.7936, 0.8885, 0.8994,...   \n",
       "28                15  [0.7665, 0.3105, 0.7163, 0.8728, 0.9444, 0.979...   \n",
       "29                15  [0.8428, 0.4726, 0.8752, 0.9477, 0.983, 0.97, ...   \n",
       "30                16  [0.9305, 0.9561, 0.3923, 0.873, 1.0, 0.6106, 0...   \n",
       "31                17  [0.9309, 0.4415, 0.9239, 0.8615, 0.3134, 0.843...   \n",
       "32                17  [0.8164, 0.7909, 0.9697, 0.979, 0.1696, 0.8627...   \n",
       "33                18  [0.2042, 0.3617, 0.0826, 0.2602, 0.1061, 0.282...   \n",
       "34                19  [0.8696, 0.9617, 0.1011, 0.493, 0.8534, 0.1878...   \n",
       "35                20  [0.2538, 0.1534, 0.5575, 0.5732, 0.0693, 0.672...   \n",
       "36                21  [0.0633, 0.0606, 0.3485, 0.4452, 0.0291, 0.561...   \n",
       "37                23  [0.0783, 0.1021, 0.7607, 0.8528, 0.0395, 0.976...   \n",
       "38                22  [0.7494, 0.055, 0.0406, 0.4545, 0.0842, 0.9616...   \n",
       "39                22  [0.6058, 0.0341, 0.041, 0.5164, 0.0399, 0.7822...   \n",
       "40                22  [0.8659, 0.3523, 0.3015, 0.9526, 0.3884, 0.840...   \n",
       "41                22  [0.1986, 0.0337, 0.0385, 0.1315, 0.0528, 0.822...   \n",
       "42                23  [0.1463, 0.0609, 0.584, 0.5236, 0.0232, 0.5615...   \n",
       "43                23  [0.367, 0.0713, 0.6832, 0.8268, 0.0578, 0.9518...   \n",
       "44                23  [0.2372, 0.0282, 0.8925, 0.7945, 0.0523, 0.989...   \n",
       "45                23  [0.2079, 0.047, 0.6329, 0.618, 0.0228, 0.7661,...   \n",
       "46                13  [0.8277, 0.4565, 0.9054, 0.4997, 0.5611, 0.816...   \n",
       "47                24  [0.8175, 0.6584, 0.8159, 0.8187, 0.3459, 0.467...   \n",
       "48                25  [0.207, 0.7007, 0.0097, 0.1625, 0.7931, 0.1179...   \n",
       "49                25  [0.1711, 0.9736, 0.2563, 0.633, 0.8453, 0.3289...   \n",
       "50                26  [0.4844, 0.4738, 0.8222, 0.754, 0.2229, 0.2984...   \n",
       "51                26  [0.7399, 0.88, 0.8312, 0.8145, 0.6997, 0.4759,...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.657, 0.4827, 0.9194, 0.4781, 0.7193, 0.9036...  0.961538  0.049927  \n",
       "1   [0.2055, 0.2363, 0.7784, 0.34, 0.3258, 0.7285,...       NaN       NaN  \n",
       "2   [0.4048, 0.2808, 0.8336, 0.2722, 0.7329, 0.869...       NaN       NaN  \n",
       "3   [0.4211, 0.4561, 0.8405, 0.2473, 0.1243, 0.453...       NaN       NaN  \n",
       "4   [0.8375, 0.7293, 0.9635, 0.589, 0.6624, 0.9456...       NaN       NaN  \n",
       "5   [0.7552, 0.9203, 0.8272, 0.6653, 0.8214, 0.311...       NaN       NaN  \n",
       "6   [0.0152, 0.0548, 0.0289, 0.0167, 0.8568, 0.031...       NaN       NaN  \n",
       "7   [0.0688, 0.5074, 0.1197, 0.2092, 0.8331, 0.036...       NaN       NaN  \n",
       "8   [0.0936, 0.2972, 0.0434, 0.3446, 0.8313, 0.093...       NaN       NaN  \n",
       "9   [0.1313, 0.0307, 0.165, 0.2122, 0.0221, 0.188,...       NaN       NaN  \n",
       "10  [0.3311, 0.0361, 0.8867, 0.457, 0.0983, 0.2612...       NaN       NaN  \n",
       "11  [0.5719, 0.105, 0.7644, 0.488, 0.0932, 0.3839,...       NaN       NaN  \n",
       "12  [0.3599, 0.3134, 0.1285, 0.0233, 0.1143, 0.303...       NaN       NaN  \n",
       "13  [0.7121, 0.946, 0.7585, 0.3746, 0.2685, 0.0341...       NaN       NaN  \n",
       "14  [0.2286, 0.1713, 0.3931, 0.0385, 0.2643, 0.403...       NaN       NaN  \n",
       "15  [0.793, 0.1229, 0.575, 0.4688, 0.2988, 0.8734,...       NaN       NaN  \n",
       "16  [0.7203, 0.8272, 0.883, 0.5997, 0.8079, 0.5048...       NaN       NaN  \n",
       "17  [0.3783, 0.3181, 0.4556, 0.2478, 0.5828, 0.260...       NaN       NaN  \n",
       "18  [0.8484, 0.4527, 0.9101, 0.9178, 0.3951, 0.589...       NaN       NaN  \n",
       "19  [0.821, 0.2204, 0.8927, 0.7998, 0.2393, 0.7624...       NaN       NaN  \n",
       "20  [0.4842, 0.6166, 0.2791, 0.0745, 0.8702, 0.568...       NaN       NaN  \n",
       "21  [0.5226, 0.8587, 0.576, 0.0867, 0.9429, 0.367,...       NaN       NaN  \n",
       "22  [0.6301, 0.6847, 0.743, 0.2475, 0.8973, 0.4018...       NaN       NaN  \n",
       "23  [0.2373, 0.2038, 0.2384, 0.7285, 0.2768, 0.170...       NaN       NaN  \n",
       "24  [0.6439, 0.673, 0.5128, 0.7961, 0.9169, 0.4068...       NaN       NaN  \n",
       "25  [0.6393, 0.5562, 0.079, 0.1947, 0.5922, 0.3012...       NaN       NaN  \n",
       "26  [0.872, 0.9722, 0.4022, 0.5042, 0.9238, 0.401,...       NaN       NaN  \n",
       "27  [0.2441, 0.381, 0.408, 0.2064, 0.1115, 0.1006,...       NaN       NaN  \n",
       "28  [0.2335, 0.6895, 0.2837, 0.1272, 0.0556, 0.020...       NaN       NaN  \n",
       "29  [0.1572, 0.5274, 0.1248, 0.0523, 0.017, 0.03, ...       NaN       NaN  \n",
       "30  [0.0695, 0.0439, 0.6077, 0.127, 0.0, 0.3894, 1...       NaN       NaN  \n",
       "31  [0.0691, 0.5585, 0.0761, 0.1385, 0.6866, 0.156...       NaN       NaN  \n",
       "32  [0.1836, 0.2091, 0.0303, 0.021, 0.8304, 0.1373...       NaN       NaN  \n",
       "33  [0.7958, 0.6383, 0.9174, 0.7398, 0.8939, 0.717...       NaN       NaN  \n",
       "34  [0.1304, 0.0383, 0.8989, 0.507, 0.1466, 0.8122...       NaN       NaN  \n",
       "35  [0.7462, 0.8466, 0.4425, 0.4268, 0.9307, 0.327...       NaN       NaN  \n",
       "36  [0.9367, 0.9394, 0.6515, 0.5548, 0.9709, 0.438...       NaN       NaN  \n",
       "37  [0.9217, 0.8979, 0.2393, 0.1472, 0.9605, 0.023...       NaN       NaN  \n",
       "38  [0.2506, 0.945, 0.9594, 0.5455, 0.9158, 0.0384...       NaN       NaN  \n",
       "39  [0.3942, 0.9659, 0.959, 0.4836, 0.9601, 0.2178...       NaN       NaN  \n",
       "40  [0.1341, 0.6477, 0.6985, 0.0474, 0.6116, 0.159...       NaN       NaN  \n",
       "41  [0.8014, 0.9663, 0.9615, 0.8685, 0.9472, 0.177...       NaN       NaN  \n",
       "42  [0.8537, 0.9391, 0.416, 0.4764, 0.9768, 0.4385...       NaN       NaN  \n",
       "43  [0.633, 0.9287, 0.3168, 0.1732, 0.9422, 0.0482...       NaN       NaN  \n",
       "44  [0.7628, 0.9718, 0.1075, 0.2055, 0.9477, 0.010...       NaN       NaN  \n",
       "45  [0.7921, 0.953, 0.3671, 0.382, 0.9772, 0.2339,...       NaN       NaN  \n",
       "46  [0.1723, 0.5435, 0.0946, 0.5003, 0.4389, 0.183...       NaN       NaN  \n",
       "47  [0.1825, 0.3416, 0.1841, 0.1813, 0.6541, 0.532...       NaN       NaN  \n",
       "48  [0.793, 0.2993, 0.9903, 0.8375, 0.2069, 0.8821...       NaN       NaN  \n",
       "49  [0.8289, 0.0264, 0.7437, 0.367, 0.1547, 0.6711...       NaN       NaN  \n",
       "50  [0.5156, 0.5262, 0.1778, 0.246, 0.7771, 0.7016...       NaN       NaN  \n",
       "51  [0.2601, 0.12, 0.1688, 0.1855, 0.3003, 0.5241,...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2821 : Training: loss:  0.02864153\n",
      "2822 : Training: loss:  0.041340802\n",
      "2823 : Training: loss:  0.033962023\n",
      "2824 : Training: loss:  0.03815687\n",
      "2825 : Training: loss:  0.057748746\n",
      "2826 : Training: loss:  0.021790592\n",
      "2827 : Training: loss:  0.026995787\n",
      "2828 : Training: loss:  0.026946403\n",
      "2829 : Training: loss:  0.03537367\n",
      "2830 : Training: loss:  0.029019712\n",
      "2831 : Training: loss:  0.033380624\n",
      "2832 : Training: loss:  0.027671596\n",
      "2833 : Training: loss:  0.022740426\n",
      "2834 : Training: loss:  0.018429581\n",
      "2835 : Training: loss:  0.03836676\n",
      "2836 : Training: loss:  0.033690445\n",
      "2837 : Training: loss:  0.038231306\n",
      "2838 : Training: loss:  0.03168446\n",
      "2839 : Training: loss:  0.03638738\n",
      "2840 : Training: loss:  0.04697024\n",
      "Validation: Loss:  0.049438227  Accuracy:  0.96153843\n",
      "2841 : Training: loss:  0.035351012\n",
      "2842 : Training: loss:  0.03716292\n",
      "2843 : Training: loss:  0.023892883\n",
      "2844 : Training: loss:  0.038743135\n",
      "2845 : Training: loss:  0.02138364\n",
      "2846 : Training: loss:  0.034426544\n",
      "2847 : Training: loss:  0.04059697\n",
      "2848 : Training: loss:  0.031557146\n",
      "2849 : Training: loss:  0.02384479\n",
      "2850 : Training: loss:  0.03592351\n",
      "2851 : Training: loss:  0.03260197\n",
      "2852 : Training: loss:  0.015445755\n",
      "2853 : Training: loss:  0.03564709\n",
      "2854 : Training: loss:  0.030998198\n",
      "2855 : Training: loss:  0.021434091\n",
      "2856 : Training: loss:  0.026562603\n",
      "2857 : Training: loss:  0.013594007\n",
      "2858 : Training: loss:  0.022350209\n",
      "2859 : Training: loss:  0.022154013\n",
      "2860 : Training: loss:  0.027130282\n",
      "Validation: Loss:  0.048990093  Accuracy:  0.96153843\n",
      "2861 : Training: loss:  0.035461638\n",
      "2862 : Training: loss:  0.028209306\n",
      "2863 : Training: loss:  0.021086121\n",
      "2864 : Training: loss:  0.01970732\n",
      "2865 : Training: loss:  0.026365768\n",
      "2866 : Training: loss:  0.033761214\n",
      "2867 : Training: loss:  0.02182285\n",
      "2868 : Training: loss:  0.054393236\n",
      "2869 : Training: loss:  0.039797228\n",
      "2870 : Training: loss:  0.03757851\n",
      "2871 : Training: loss:  0.027830783\n",
      "2872 : Training: loss:  0.031448815\n",
      "2873 : Training: loss:  0.041667517\n",
      "2874 : Training: loss:  0.030363359\n",
      "2875 : Training: loss:  0.02032943\n",
      "2876 : Training: loss:  0.051463656\n",
      "2877 : Training: loss:  0.021551473\n",
      "2878 : Training: loss:  0.043651238\n",
      "2879 : Training: loss:  0.0598371\n",
      "2880 : Training: loss:  0.017639993\n",
      "Validation: Loss:  0.0484521  Accuracy:  0.96153843\n",
      "2881 : Training: loss:  0.027343385\n",
      "2882 : Training: loss:  0.025199048\n",
      "2883 : Training: loss:  0.016967041\n",
      "2884 : Training: loss:  0.026369117\n",
      "2885 : Training: loss:  0.023512984\n",
      "2886 : Training: loss:  0.034375254\n",
      "2887 : Training: loss:  0.01681114\n",
      "2888 : Training: loss:  0.031917088\n",
      "2889 : Training: loss:  0.020547671\n",
      "2890 : Training: loss:  0.0342987\n",
      "2891 : Training: loss:  0.024864923\n",
      "2892 : Training: loss:  0.022099234\n",
      "2893 : Training: loss:  0.0334648\n",
      "2894 : Training: loss:  0.033219527\n",
      "2895 : Training: loss:  0.011922139\n",
      "2896 : Training: loss:  0.025039412\n",
      "2897 : Training: loss:  0.035338346\n",
      "2898 : Training: loss:  0.045423985\n",
      "2899 : Training: loss:  0.015949957\n",
      "2900 : Training: loss:  0.024876967\n",
      "Validation: Loss:  0.047727916  Accuracy:  0.9423077\n",
      "2901 : Training: loss:  0.02260791\n",
      "2902 : Training: loss:  0.019046476\n",
      "2903 : Training: loss:  0.038043294\n",
      "2904 : Training: loss:  0.013816097\n",
      "2905 : Training: loss:  0.013282708\n",
      "2906 : Training: loss:  0.028314387\n",
      "2907 : Training: loss:  0.041177098\n",
      "2908 : Training: loss:  0.021996154\n",
      "2909 : Training: loss:  0.029551039\n",
      "2910 : Training: loss:  0.057654813\n",
      "2911 : Training: loss:  0.042016864\n",
      "2912 : Training: loss:  0.026776902\n",
      "2913 : Training: loss:  0.05447776\n",
      "2914 : Training: loss:  0.02616931\n",
      "2915 : Training: loss:  0.045553755\n",
      "2916 : Training: loss:  0.028819539\n",
      "2917 : Training: loss:  0.022616372\n",
      "2918 : Training: loss:  0.016023777\n",
      "2919 : Training: loss:  0.014998639\n",
      "2920 : Training: loss:  0.024835285\n",
      "Validation: Loss:  0.0471752  Accuracy:  0.9423077\n",
      "2921 : Training: loss:  0.02513323\n",
      "2922 : Training: loss:  0.03072758\n",
      "2923 : Training: loss:  0.024855932\n",
      "2924 : Training: loss:  0.02433224\n",
      "2925 : Training: loss:  0.031896196\n",
      "2926 : Training: loss:  0.049714487\n",
      "2927 : Training: loss:  0.020699546\n",
      "2928 : Training: loss:  0.027808718\n",
      "2929 : Training: loss:  0.030266618\n",
      "2930 : Training: loss:  0.04412438\n",
      "2931 : Training: loss:  0.027014451\n",
      "2932 : Training: loss:  0.02299708\n",
      "2933 : Training: loss:  0.023616318\n",
      "2934 : Training: loss:  0.023191039\n",
      "2935 : Training: loss:  0.026617944\n",
      "2936 : Training: loss:  0.025150092\n",
      "2937 : Training: loss:  0.03702837\n",
      "2938 : Training: loss:  0.032168772\n",
      "2939 : Training: loss:  0.03508856\n",
      "2940 : Training: loss:  0.029021012\n",
      "Validation: Loss:  0.046520375  Accuracy:  0.9423077\n",
      "2941 : Training: loss:  0.032772426\n",
      "2942 : Training: loss:  0.03336361\n",
      "2943 : Training: loss:  0.024481133\n",
      "2944 : Training: loss:  0.019169748\n",
      "2945 : Training: loss:  0.021965392\n",
      "2946 : Training: loss:  0.028620722\n",
      "2947 : Training: loss:  0.02401618\n",
      "2948 : Training: loss:  0.012160249\n",
      "2949 : Training: loss:  0.0365789\n",
      "2950 : Training: loss:  0.021721248\n",
      "2951 : Training: loss:  0.031124203\n",
      "2952 : Training: loss:  0.030018095\n",
      "2953 : Training: loss:  0.028244521\n",
      "2954 : Training: loss:  0.025049495\n",
      "2955 : Training: loss:  0.017407741\n",
      "2956 : Training: loss:  0.03841386\n",
      "2957 : Training: loss:  0.029016968\n",
      "2958 : Training: loss:  0.023853293\n",
      "2959 : Training: loss:  0.02770834\n",
      "2960 : Training: loss:  0.026299944\n",
      "Validation: Loss:  0.045847613  Accuracy:  0.9423077\n",
      "2961 : Training: loss:  0.03290494\n",
      "2962 : Training: loss:  0.029615207\n",
      "2963 : Training: loss:  0.019651258\n",
      "2964 : Training: loss:  0.039979983\n",
      "2965 : Training: loss:  0.04035222\n",
      "2966 : Training: loss:  0.024497624\n",
      "2967 : Training: loss:  0.018911278\n",
      "2968 : Training: loss:  0.021126758\n",
      "2969 : Training: loss:  0.031129828\n",
      "2970 : Training: loss:  0.04157927\n",
      "2971 : Training: loss:  0.029820891\n",
      "2972 : Training: loss:  0.026629051\n",
      "2973 : Training: loss:  0.025539646\n",
      "2974 : Training: loss:  0.021250539\n",
      "2975 : Training: loss:  0.028883386\n",
      "2976 : Training: loss:  0.040608615\n",
      "2977 : Training: loss:  0.012444095\n",
      "2978 : Training: loss:  0.042786963\n",
      "2979 : Training: loss:  0.031771403\n",
      "2980 : Training: loss:  0.031605966\n",
      "Validation: Loss:  0.04534492  Accuracy:  0.9423077\n",
      "2981 : Training: loss:  0.019415762\n",
      "2982 : Training: loss:  0.018974796\n",
      "2983 : Training: loss:  0.02582004\n",
      "2984 : Training: loss:  0.042002037\n",
      "2985 : Training: loss:  0.046673346\n",
      "2986 : Training: loss:  0.035095897\n",
      "2987 : Training: loss:  0.03938036\n",
      "2988 : Training: loss:  0.035525713\n",
      "2989 : Training: loss:  0.036330435\n",
      "2990 : Training: loss:  0.044819478\n",
      "2991 : Training: loss:  0.03767476\n",
      "2992 : Training: loss:  0.022618147\n",
      "2993 : Training: loss:  0.021950303\n",
      "2994 : Training: loss:  0.020530526\n",
      "2995 : Training: loss:  0.03408082\n",
      "2996 : Training: loss:  0.03583472\n",
      "2997 : Training: loss:  0.02616823\n",
      "2998 : Training: loss:  0.023268268\n",
      "2999 : Training: loss:  0.027469007\n",
      "3000 : Training: loss:  0.030990887\n",
      "Validation: Loss:  0.04479157  Accuracy:  0.9423077\n",
      "3001 : Training: loss:  0.030002322\n",
      "3002 : Training: loss:  0.037316926\n",
      "3003 : Training: loss:  0.018555634\n",
      "3004 : Training: loss:  0.01900632\n",
      "3005 : Training: loss:  0.038237993\n",
      "3006 : Training: loss:  0.020925852\n",
      "3007 : Training: loss:  0.024793454\n",
      "3008 : Training: loss:  0.034128815\n",
      "3009 : Training: loss:  0.039570954\n",
      "3010 : Training: loss:  0.023793904\n",
      "3011 : Training: loss:  0.025134178\n",
      "3012 : Training: loss:  0.028781874\n",
      "3013 : Training: loss:  0.031073932\n",
      "3014 : Training: loss:  0.052442018\n",
      "3015 : Training: loss:  0.022242138\n",
      "3016 : Training: loss:  0.032843217\n",
      "3017 : Training: loss:  0.015880536\n",
      "3018 : Training: loss:  0.020119796\n",
      "3019 : Training: loss:  0.024052147\n",
      "3020 : Training: loss:  0.024406927\n",
      "Validation: Loss:  0.044332925  Accuracy:  0.9423077\n",
      "3021 : Training: loss:  0.06261941\n",
      "3022 : Training: loss:  0.028923059\n",
      "3023 : Training: loss:  0.017638845\n",
      "3024 : Training: loss:  0.017675439\n",
      "3025 : Training: loss:  0.027324168\n",
      "3026 : Training: loss:  0.020438658\n",
      "3027 : Training: loss:  0.014064153\n",
      "3028 : Training: loss:  0.028936569\n",
      "3029 : Training: loss:  0.020125829\n",
      "3030 : Training: loss:  0.031163909\n",
      "3031 : Training: loss:  0.028962346\n",
      "3032 : Training: loss:  0.02277436\n",
      "3033 : Training: loss:  0.04521748\n",
      "3034 : Training: loss:  0.025303662\n",
      "3035 : Training: loss:  0.035016354\n",
      "3036 : Training: loss:  0.027936151\n",
      "3037 : Training: loss:  0.029071696\n",
      "3038 : Training: loss:  0.02830964\n",
      "3039 : Training: loss:  0.021157358\n",
      "3040 : Training: loss:  0.011890162\n",
      "Validation: Loss:  0.044028085  Accuracy:  0.9423077\n",
      "3041 : Training: loss:  0.038028438\n",
      "3042 : Training: loss:  0.029140834\n",
      "3043 : Training: loss:  0.03813571\n",
      "3044 : Training: loss:  0.041222453\n",
      "3045 : Training: loss:  0.017806437\n",
      "3046 : Training: loss:  0.034791\n",
      "3047 : Training: loss:  0.035218325\n",
      "3048 : Training: loss:  0.014688525\n",
      "3049 : Training: loss:  0.01879405\n",
      "3050 : Training: loss:  0.030929804\n",
      "3051 : Training: loss:  0.033276994\n",
      "3052 : Training: loss:  0.031987585\n",
      "3053 : Training: loss:  0.021682015\n",
      "3054 : Training: loss:  0.029305665\n",
      "3055 : Training: loss:  0.027115742\n",
      "3056 : Training: loss:  0.026577577\n",
      "3057 : Training: loss:  0.028356994\n",
      "3058 : Training: loss:  0.021566289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3059 : Training: loss:  0.028405858\n",
      "3060 : Training: loss:  0.022045696\n",
      "Validation: Loss:  0.043497123  Accuracy:  0.9423077\n",
      "3061 : Training: loss:  0.030742355\n",
      "3062 : Training: loss:  0.03335932\n",
      "3063 : Training: loss:  0.029264264\n",
      "3064 : Training: loss:  0.02250178\n",
      "3065 : Training: loss:  0.0344182\n",
      "3066 : Training: loss:  0.026409766\n",
      "3067 : Training: loss:  0.025621178\n",
      "3068 : Training: loss:  0.013296311\n",
      "3069 : Training: loss:  0.016966514\n",
      "3070 : Training: loss:  0.02711637\n",
      "3071 : Training: loss:  0.023393868\n",
      "3072 : Training: loss:  0.025979124\n",
      "3073 : Training: loss:  0.030720167\n",
      "3074 : Training: loss:  0.027610827\n",
      "3075 : Training: loss:  0.035220355\n",
      "3076 : Training: loss:  0.022140529\n",
      "3077 : Training: loss:  0.035376377\n",
      "3078 : Training: loss:  0.020770391\n",
      "3079 : Training: loss:  0.030348599\n",
      "3080 : Training: loss:  0.027374342\n",
      "Validation: Loss:  0.042879604  Accuracy:  0.9423077\n",
      "3081 : Training: loss:  0.02656423\n",
      "3082 : Training: loss:  0.042414736\n",
      "3083 : Training: loss:  0.01088614\n",
      "3084 : Training: loss:  0.03341939\n",
      "3085 : Training: loss:  0.020960663\n",
      "3086 : Training: loss:  0.020107063\n",
      "3087 : Training: loss:  0.018890437\n",
      "3088 : Training: loss:  0.024032319\n",
      "3089 : Training: loss:  0.015603507\n",
      "3090 : Training: loss:  0.016587393\n",
      "3091 : Training: loss:  0.025722314\n",
      "3092 : Training: loss:  0.02087009\n",
      "3093 : Training: loss:  0.015442092\n",
      "3094 : Training: loss:  0.02333378\n",
      "3095 : Training: loss:  0.01975501\n",
      "3096 : Training: loss:  0.029691914\n",
      "3097 : Training: loss:  0.029695157\n",
      "3098 : Training: loss:  0.015821628\n",
      "3099 : Training: loss:  0.013418417\n",
      "3100 : Training: loss:  0.038934823\n",
      "Validation: Loss:  0.04241716  Accuracy:  0.9423077\n",
      "3101 : Training: loss:  0.011210824\n",
      "3102 : Training: loss:  0.027640928\n",
      "3103 : Training: loss:  0.018656813\n",
      "3104 : Training: loss:  0.03699396\n",
      "3105 : Training: loss:  0.031568725\n",
      "3106 : Training: loss:  0.032952182\n",
      "3107 : Training: loss:  0.024545494\n",
      "3108 : Training: loss:  0.022148632\n",
      "3109 : Training: loss:  0.030840566\n",
      "3110 : Training: loss:  0.021135606\n",
      "3111 : Training: loss:  0.023135154\n",
      "3112 : Training: loss:  0.034323446\n",
      "3113 : Training: loss:  0.017680023\n",
      "3114 : Training: loss:  0.022459915\n",
      "3115 : Training: loss:  0.024800852\n",
      "3116 : Training: loss:  0.029008968\n",
      "3117 : Training: loss:  0.023671003\n",
      "3118 : Training: loss:  0.022089852\n",
      "3119 : Training: loss:  0.016422506\n",
      "3120 : Training: loss:  0.020037886\n",
      "Validation: Loss:  0.041858874  Accuracy:  0.9423077\n",
      "3121 : Training: loss:  0.03655905\n",
      "3122 : Training: loss:  0.024225099\n",
      "3123 : Training: loss:  0.014801377\n",
      "3124 : Training: loss:  0.03732776\n",
      "3125 : Training: loss:  0.028580401\n",
      "3126 : Training: loss:  0.031188343\n",
      "3127 : Training: loss:  0.02180382\n",
      "3128 : Training: loss:  0.048085086\n",
      "3129 : Training: loss:  0.017935123\n",
      "3130 : Training: loss:  0.021103336\n",
      "3131 : Training: loss:  0.017471038\n",
      "3132 : Training: loss:  0.023669124\n",
      "3133 : Training: loss:  0.028254567\n",
      "3134 : Training: loss:  0.018196935\n",
      "3135 : Training: loss:  0.025922213\n",
      "3136 : Training: loss:  0.020266326\n",
      "3137 : Training: loss:  0.023835253\n",
      "3138 : Training: loss:  0.013906127\n",
      "3139 : Training: loss:  0.01878747\n",
      "3140 : Training: loss:  0.03127336\n",
      "Validation: Loss:  0.04136292  Accuracy:  0.9423077\n",
      "3141 : Training: loss:  0.017607005\n",
      "3142 : Training: loss:  0.031276565\n",
      "3143 : Training: loss:  0.028470168\n",
      "3144 : Training: loss:  0.021589858\n",
      "3145 : Training: loss:  0.0218387\n",
      "3146 : Training: loss:  0.026684657\n",
      "3147 : Training: loss:  0.025198461\n",
      "3148 : Training: loss:  0.024501262\n",
      "3149 : Training: loss:  0.017146833\n",
      "3150 : Training: loss:  0.018835345\n",
      "3151 : Training: loss:  0.02598953\n",
      "3152 : Training: loss:  0.013641738\n",
      "3153 : Training: loss:  0.015586161\n",
      "3154 : Training: loss:  0.020061504\n",
      "3155 : Training: loss:  0.023670912\n",
      "3156 : Training: loss:  0.024851719\n",
      "3157 : Training: loss:  0.03564423\n",
      "3158 : Training: loss:  0.023953177\n",
      "3159 : Training: loss:  0.019832542\n",
      "3160 : Training: loss:  0.020143148\n",
      "Validation: Loss:  0.040823195  Accuracy:  0.9423077\n",
      "3161 : Training: loss:  0.028977059\n",
      "3162 : Training: loss:  0.0151925\n",
      "3163 : Training: loss:  0.018858345\n",
      "3164 : Training: loss:  0.012272888\n",
      "3165 : Training: loss:  0.022228284\n",
      "3166 : Training: loss:  0.021840466\n",
      "3167 : Training: loss:  0.03187939\n",
      "3168 : Training: loss:  0.024592116\n",
      "3169 : Training: loss:  0.023050595\n",
      "3170 : Training: loss:  0.017324159\n",
      "3171 : Training: loss:  0.021863673\n",
      "3172 : Training: loss:  0.023258297\n",
      "3173 : Training: loss:  0.015778873\n",
      "3174 : Training: loss:  0.032795362\n",
      "3175 : Training: loss:  0.028605295\n",
      "3176 : Training: loss:  0.019523673\n",
      "3177 : Training: loss:  0.024097908\n",
      "3178 : Training: loss:  0.02914432\n",
      "3179 : Training: loss:  0.0197422\n",
      "3180 : Training: loss:  0.014144138\n",
      "Validation: Loss:  0.0403993  Accuracy:  0.96153843\n",
      "3181 : Training: loss:  0.021105079\n",
      "3182 : Training: loss:  0.025660105\n",
      "3183 : Training: loss:  0.022413267\n",
      "3184 : Training: loss:  0.02580893\n",
      "3185 : Training: loss:  0.012524109\n",
      "3186 : Training: loss:  0.015366658\n",
      "3187 : Training: loss:  0.029592764\n",
      "3188 : Training: loss:  0.039812468\n",
      "3189 : Training: loss:  0.0154305585\n",
      "3190 : Training: loss:  0.01758036\n",
      "3191 : Training: loss:  0.029009974\n",
      "3192 : Training: loss:  0.014308699\n",
      "3193 : Training: loss:  0.049215335\n",
      "3194 : Training: loss:  0.01404284\n",
      "3195 : Training: loss:  0.03067153\n",
      "3196 : Training: loss:  0.016240753\n",
      "3197 : Training: loss:  0.015189103\n",
      "3198 : Training: loss:  0.018217899\n",
      "3199 : Training: loss:  0.030862125\n",
      "3200 : Training: loss:  0.026777864\n",
      "Validation: Loss:  0.04002163  Accuracy:  0.96153843\n",
      "3201 : Training: loss:  0.025199309\n",
      "3202 : Training: loss:  0.009642835\n",
      "3203 : Training: loss:  0.01613385\n",
      "3204 : Training: loss:  0.036764737\n",
      "3205 : Training: loss:  0.031734195\n",
      "3206 : Training: loss:  0.01874286\n",
      "3207 : Training: loss:  0.021221634\n",
      "3208 : Training: loss:  0.017344676\n",
      "3209 : Training: loss:  0.024033299\n",
      "3210 : Training: loss:  0.022255989\n",
      "3211 : Training: loss:  0.027934479\n",
      "3212 : Training: loss:  0.04077696\n",
      "3213 : Training: loss:  0.022006169\n",
      "3214 : Training: loss:  0.013883644\n",
      "3215 : Training: loss:  0.029123006\n",
      "3216 : Training: loss:  0.015748447\n",
      "3217 : Training: loss:  0.010322757\n",
      "3218 : Training: loss:  0.020363001\n",
      "3219 : Training: loss:  0.016085176\n",
      "3220 : Training: loss:  0.024912428\n",
      "Validation: Loss:  0.0396763  Accuracy:  0.9423077\n",
      "3221 : Training: loss:  0.025135318\n",
      "3222 : Training: loss:  0.015800482\n",
      "3223 : Training: loss:  0.018298643\n",
      "3224 : Training: loss:  0.026238026\n",
      "3225 : Training: loss:  0.021208676\n",
      "3226 : Training: loss:  0.02626379\n",
      "3227 : Training: loss:  0.026461137\n",
      "3228 : Training: loss:  0.020867398\n",
      "3229 : Training: loss:  0.016387869\n",
      "3230 : Training: loss:  0.019663695\n",
      "3231 : Training: loss:  0.009768121\n",
      "3232 : Training: loss:  0.02256907\n",
      "3233 : Training: loss:  0.037681226\n",
      "3234 : Training: loss:  0.021322612\n",
      "3235 : Training: loss:  0.021394031\n",
      "3236 : Training: loss:  0.013100721\n",
      "3237 : Training: loss:  0.024665477\n",
      "3238 : Training: loss:  0.016381195\n",
      "3239 : Training: loss:  0.028700337\n",
      "3240 : Training: loss:  0.02430893\n",
      "Validation: Loss:  0.039360885  Accuracy:  0.9423077\n",
      "3241 : Training: loss:  0.033288106\n",
      "3242 : Training: loss:  0.022557758\n",
      "3243 : Training: loss:  0.029427223\n",
      "3244 : Training: loss:  0.042379647\n",
      "3245 : Training: loss:  0.037730526\n",
      "3246 : Training: loss:  0.019456888\n",
      "3247 : Training: loss:  0.012589413\n",
      "3248 : Training: loss:  0.023558108\n",
      "3249 : Training: loss:  0.013797696\n",
      "3250 : Training: loss:  0.03309705\n",
      "3251 : Training: loss:  0.029551635\n",
      "3252 : Training: loss:  0.020128526\n",
      "3253 : Training: loss:  0.020743854\n",
      "3254 : Training: loss:  0.016070947\n",
      "3255 : Training: loss:  0.025763879\n",
      "3256 : Training: loss:  0.019190917\n",
      "3257 : Training: loss:  0.013861369\n",
      "3258 : Training: loss:  0.020632682\n",
      "3259 : Training: loss:  0.012859348\n",
      "3260 : Training: loss:  0.024103234\n",
      "Validation: Loss:  0.03901097  Accuracy:  0.9423077\n",
      "3261 : Training: loss:  0.029100358\n",
      "3262 : Training: loss:  0.026544409\n",
      "3263 : Training: loss:  0.025676945\n",
      "3264 : Training: loss:  0.019309577\n",
      "3265 : Training: loss:  0.026607059\n",
      "3266 : Training: loss:  0.014287383\n",
      "3267 : Training: loss:  0.015774474\n",
      "3268 : Training: loss:  0.015730701\n",
      "3269 : Training: loss:  0.015177257\n",
      "3270 : Training: loss:  0.01161449\n",
      "3271 : Training: loss:  0.020268748\n",
      "3272 : Training: loss:  0.018622926\n",
      "3273 : Training: loss:  0.010880817\n",
      "3274 : Training: loss:  0.01922768\n",
      "3275 : Training: loss:  0.011084052\n",
      "3276 : Training: loss:  0.011926746\n",
      "3277 : Training: loss:  0.024388323\n",
      "3278 : Training: loss:  0.023397274\n",
      "3279 : Training: loss:  0.012229144\n",
      "3280 : Training: loss:  0.022814844\n",
      "Validation: Loss:  0.03868769  Accuracy:  0.9423077\n",
      "3281 : Training: loss:  0.026291192\n",
      "3282 : Training: loss:  0.014923932\n",
      "3283 : Training: loss:  0.02852103\n",
      "3284 : Training: loss:  0.036186535\n",
      "3285 : Training: loss:  0.015743492\n",
      "3286 : Training: loss:  0.027585967\n",
      "3287 : Training: loss:  0.02385407\n",
      "3288 : Training: loss:  0.025348185\n",
      "3289 : Training: loss:  0.023488533\n",
      "3290 : Training: loss:  0.030465318\n",
      "3291 : Training: loss:  0.0243512\n",
      "3292 : Training: loss:  0.024416275\n",
      "3293 : Training: loss:  0.020637043\n",
      "3294 : Training: loss:  0.01909049\n",
      "3295 : Training: loss:  0.014492997\n",
      "3296 : Training: loss:  0.023974523\n",
      "3297 : Training: loss:  0.02226592\n",
      "3298 : Training: loss:  0.028162373\n",
      "3299 : Training: loss:  0.014788552\n",
      "3300 : Training: loss:  0.01371997\n",
      "Validation: Loss:  0.03831107  Accuracy:  0.96153843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3301 : Training: loss:  0.026013114\n",
      "3302 : Training: loss:  0.011661364\n",
      "3303 : Training: loss:  0.030104546\n",
      "3304 : Training: loss:  0.02015966\n",
      "3305 : Training: loss:  0.021265913\n",
      "3306 : Training: loss:  0.016722193\n",
      "3307 : Training: loss:  0.01452182\n",
      "3308 : Training: loss:  0.025408857\n",
      "3309 : Training: loss:  0.02241924\n",
      "3310 : Training: loss:  0.025026055\n",
      "3311 : Training: loss:  0.017672027\n",
      "3312 : Training: loss:  0.033648457\n",
      "3313 : Training: loss:  0.02541809\n",
      "3314 : Training: loss:  0.011671002\n",
      "3315 : Training: loss:  0.02258367\n",
      "3316 : Training: loss:  0.016449163\n",
      "3317 : Training: loss:  0.04694069\n",
      "3318 : Training: loss:  0.018995307\n",
      "3319 : Training: loss:  0.033525065\n",
      "3320 : Training: loss:  0.013254343\n",
      "Validation: Loss:  0.037912544  Accuracy:  0.9807692\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.4609, 0.058, 0.0063, 0.0092, 0.006, 0.0138,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.361, 0.5908, 0.0765, 0.5805, 0.3347, 0.0899...</td>\n",
       "      <td>[0.639, 0.4092, 0.9235, 0.4195, 0.6653, 0.9101...</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.037913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.8734, 0.4524, 0.0034, 0.0114, 0.0909, 0.008...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8191, 0.8044, 0.2125, 0.6701, 0.7356, 0.296...</td>\n",
       "      <td>[0.1809, 0.1956, 0.7875, 0.3299, 0.2644, 0.703...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.918, 0.1442, 0.0021, 0.0456, 0.011, 0.0319,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6065, 0.7906, 0.1616, 0.7766, 0.3144, 0.122...</td>\n",
       "      <td>[0.3935, 0.2094, 0.8384, 0.2234, 0.6856, 0.877...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0153, 0.2875, 0.0007, 0.0015, 0.0748, 0.017...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5726, 0.5727, 0.15, 0.7775, 0.9088, 0.5808,...</td>\n",
       "      <td>[0.4274, 0.4273, 0.85, 0.2225, 0.0912, 0.4192,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0657, 0.2713, 0.0091, 0.001, 0.0025, 0.0135...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1844, 0.3368, 0.0346, 0.4789, 0.4318, 0.062...</td>\n",
       "      <td>[0.8156, 0.6632, 0.9654, 0.5211, 0.5682, 0.937...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0031, 0.0086, 0.6096, 0.0018, 0.0023, 0.024...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.2701, 0.1007, 0.1551, 0.4064, 0.2079, 0.775...</td>\n",
       "      <td>[0.7299, 0.8993, 0.8449, 0.5936, 0.7921, 0.224...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0267, 0.0004, 0.0002, 0.9088, 0.0014, 1e-04...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9868, 0.9653, 0.9725, 0.9878, 0.1262, 0.971...</td>\n",
       "      <td>[0.0132, 0.0347, 0.0275, 0.0122, 0.8738, 0.028...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0101, 0.0041, 0.0056, 0.75, 0.0023, 0.0007,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9362, 0.5376, 0.877, 0.8059, 0.1603, 0.9723...</td>\n",
       "      <td>[0.0638, 0.4624, 0.123, 0.1941, 0.8397, 0.0277...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0164, 0.0019, 0.0096, 0.7245, 0.0081, 0.003...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9165, 0.7779, 0.9633, 0.6783, 0.1664, 0.928...</td>\n",
       "      <td>[0.0835, 0.2221, 0.0367, 0.3217, 0.8336, 0.071...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0027, 0.0152, 0.0015, 0.0081, 0.8323, 0.056...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8742, 0.972, 0.8371, 0.7608, 0.9823, 0.841,...</td>\n",
       "      <td>[0.1258, 0.028, 0.1629, 0.2392, 0.0177, 0.159,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0294, 0.4598, 0.0011, 0.0017, 0.5338, 0.472...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.6777, 0.9702, 0.1051, 0.5131, 0.9282, 0.788...</td>\n",
       "      <td>[0.3223, 0.0298, 0.8949, 0.4869, 0.0718, 0.211...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0009, 0.0065, 0.0009, 0.0007, 0.0844, 0.542...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.3921, 0.9081, 0.2239, 0.5041, 0.9362, 0.605...</td>\n",
       "      <td>[0.6079, 0.0919, 0.7761, 0.4959, 0.0638, 0.394...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1e-04, 0.0021, 0.0106, 0.0002, 0.0008, 0.0007...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.619, 0.73, 0.8784, 0.9797, 0.9122, 0.6563, ...</td>\n",
       "      <td>[0.381, 0.27, 0.1216, 0.0203, 0.0878, 0.3437, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0317, 0.0346, 0.1019, 0.0174, 0.0005, 0.026...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.2383, 0.0368, 0.1995, 0.624, 0.7097, 0.9694...</td>\n",
       "      <td>[0.7617, 0.9632, 0.8005, 0.376, 0.2903, 0.0306...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0126, 0.004, 1e-04, 0.0152, 0.0314, 0.0017,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.7662, 0.8621, 0.6152, 0.9668, 0.7676, 0.580...</td>\n",
       "      <td>[0.2338, 0.1379, 0.3848, 0.0332, 0.2324, 0.419...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0262, 0.0036, 0.0009, 0.0011, 0.0142, 0.001...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1939, 0.9026, 0.4444, 0.5333, 0.7421, 0.108...</td>\n",
       "      <td>[0.8061, 0.0974, 0.5556, 0.4667, 0.2579, 0.892...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.004, 0.0019, 0.169, 0.0009, 1e-04, 0.0104, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.2998, 0.2167, 0.1086, 0.4784, 0.2151, 0.578...</td>\n",
       "      <td>[0.7002, 0.7833, 0.8914, 0.5216, 0.7849, 0.421...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.0117, 0.0017, 0.0163, 0.005, 0.0013, 0.0016...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.6563, 0.7437, 0.5313, 0.7877, 0.4207, 0.797...</td>\n",
       "      <td>[0.3437, 0.2563, 0.4687, 0.2123, 0.5793, 0.202...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0178, 0.0027, 0.0427, 0.0007, 0.0015, 0.010...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1494, 0.5841, 0.088, 0.0726, 0.6884, 0.4532...</td>\n",
       "      <td>[0.8506, 0.4159, 0.912, 0.9274, 0.3116, 0.5468...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0107, 0.003, 0.0459, 0.0005, 0.0012, 0.0112...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1775, 0.8221, 0.1067, 0.1679, 0.8292, 0.241...</td>\n",
       "      <td>[0.8225, 0.1779, 0.8933, 0.8321, 0.1708, 0.758...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0333, 0.0128, 0.0107, 0.0251, 0.001, 0.0007...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.5462, 0.4627, 0.7669, 0.9429, 0.1443, 0.471...</td>\n",
       "      <td>[0.4538, 0.5373, 0.2331, 0.0571, 0.8557, 0.528...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0027, 0.0026, 0.0148, 0.0129, 0.0004, 0.000...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.5021, 0.1693, 0.4909, 0.9349, 0.0697, 0.681...</td>\n",
       "      <td>[0.4979, 0.8307, 0.5091, 0.0651, 0.9303, 0.318...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0097, 0.0044, 0.0102, 0.006, 0.001, 0.0127,...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.3653, 0.3831, 0.2729, 0.801, 0.1179, 0.6255...</td>\n",
       "      <td>[0.6347, 0.6169, 0.7271, 0.199, 0.8821, 0.3745...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0137, 0.0021, 0.0075, 0.0186, 0.0305, 0.038...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7624, 0.8433, 0.7794, 0.2285, 0.7788, 0.859...</td>\n",
       "      <td>[0.2376, 0.1567, 0.2206, 0.7715, 0.2212, 0.141...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.0155, 0.0006, 0.1337, 0.0062, 0.0021, 0.006...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.3837, 0.4339, 0.5034, 0.2371, 0.0962, 0.680...</td>\n",
       "      <td>[0.6163, 0.5661, 0.4966, 0.7629, 0.9038, 0.319...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0012, 0.002, 0.009, 0.0062, 0.0063, 0.0009,...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.3742, 0.5249, 0.9374, 0.8547, 0.464, 0.7649...</td>\n",
       "      <td>[0.6258, 0.4751, 0.0626, 0.1453, 0.536, 0.2351...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0016, 0.002, 0.0162, 0.0004, 0.0004, 0.0002...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1311, 0.0323, 0.6472, 0.5698, 0.0904, 0.683...</td>\n",
       "      <td>[0.8689, 0.9677, 0.3528, 0.4302, 0.9096, 0.316...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[1e-04, 0.0013, 0.0025, 0.0009, 0.0052, 0.0016...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7517, 0.6439, 0.5691, 0.8369, 0.9025, 0.920...</td>\n",
       "      <td>[0.2483, 0.3561, 0.4309, 0.1631, 0.0975, 0.079...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[1e-04, 0.0076, 0.0093, 0.0038, 0.0008, 0.0007...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7587, 0.278, 0.6916, 0.8923, 0.9395, 0.9853...</td>\n",
       "      <td>[0.2413, 0.722, 0.3084, 0.1077, 0.0605, 0.0147...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[1e-04, 0.0067, 0.0003, 0.0011, 0.0133, 0.0005...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8291, 0.4546, 0.8666, 0.9569, 0.9864, 0.975...</td>\n",
       "      <td>[0.1709, 0.5454, 0.1334, 0.0431, 0.0136, 0.024...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.0251, 0.4841, 0.0071, 0.0027, 0.2936, 0.138...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.8704, 0.8884, 0.3538, 0.5524, 1.0, 0.3298, ...</td>\n",
       "      <td>[0.1296, 0.1116, 0.6462, 0.4476, 0.0, 0.6702, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0008, 0.0028, 0.016, 0.0299, 0.0008, 0.0002...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.942, 0.5167, 0.933, 0.8933, 0.3307, 0.8902,...</td>\n",
       "      <td>[0.058, 0.4833, 0.067, 0.1067, 0.6693, 0.1098,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0006, 0.0006, 0.0107, 0.0259, 0.0004, 1e-04...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.8286, 0.863, 0.9764, 0.9872, 0.1739, 0.8902...</td>\n",
       "      <td>[0.1714, 0.137, 0.0236, 0.0128, 0.8261, 0.1098...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0122, 0.0022, 0.0401, 0.0023, 0.0006, 0.002...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.2263, 0.4621, 0.0853, 0.3342, 0.1232, 0.344...</td>\n",
       "      <td>[0.7737, 0.5379, 0.9147, 0.6658, 0.8768, 0.655...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0344, 0.0102, 0.0035, 0.0078, 0.0947, 0.014...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.8794, 0.9706, 0.094, 0.5078, 0.8884, 0.1725...</td>\n",
       "      <td>[0.1206, 0.0294, 0.906, 0.4922, 0.1116, 0.8275...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0222, 0.0058, 0.0134, 0.0052, 0.0004, 0.002...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.2658, 0.2157, 0.6172, 0.6602, 0.0752, 0.730...</td>\n",
       "      <td>[0.7342, 0.7843, 0.3828, 0.3398, 0.9248, 0.269...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0149, 0.0022, 0.0106, 0.0014, 0.0002, 0.002...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0587, 0.0735, 0.3913, 0.5189, 0.032, 0.6559...</td>\n",
       "      <td>[0.9413, 0.9265, 0.6087, 0.4811, 0.968, 0.3441...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0015, 0.0014, 0.0039, 0.0071, 1e-04, 0.0003...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0532, 0.0909, 0.7893, 0.8652, 0.0383, 0.980...</td>\n",
       "      <td>[0.9468, 0.9091, 0.2107, 0.1348, 0.9617, 0.019...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0007, 0.0003, 0.0201, 0.0023, 0.0002, 0.000...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7351, 0.0484, 0.0316, 0.5223, 0.0798, 0.968...</td>\n",
       "      <td>[0.2649, 0.9516, 0.9684, 0.4777, 0.9202, 0.031...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0016, 0.0004, 0.0131, 0.0026, 0.0002, 0.000...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.625, 0.0391, 0.0398, 0.5868, 0.048, 0.8246,...</td>\n",
       "      <td>[0.375, 0.9609, 0.9602, 0.4132, 0.952, 0.1754,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0018, 0.0047, 0.0013, 0.0053, 0.0016, 0.000...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.8696, 0.3545, 0.2948, 0.9648, 0.4165, 0.843...</td>\n",
       "      <td>[0.1304, 0.6455, 0.7052, 0.0352, 0.5835, 0.156...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0067, 0.0007, 0.0456, 0.0023, 0.0002, 0.001...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1965, 0.0371, 0.0356, 0.1616, 0.0566, 0.870...</td>\n",
       "      <td>[0.8035, 0.9629, 0.9644, 0.8384, 0.9434, 0.129...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0053, 0.0017, 0.0081, 0.0018, 1e-04, 0.0, 0...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1508, 0.0741, 0.6237, 0.5814, 0.0258, 0.614...</td>\n",
       "      <td>[0.8492, 0.9259, 0.3763, 0.4186, 0.9742, 0.385...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0019, 0.0012, 0.0034, 0.0017, 1e-04, 0.0, 0...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.3312, 0.0641, 0.6955, 0.8068, 0.0553, 0.948...</td>\n",
       "      <td>[0.6688, 0.9359, 0.3045, 0.1932, 0.9447, 0.051...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0006, 0.0009, 0.0038, 0.0013, 0.0, 0.0, 0.0...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.2023, 0.0234, 0.8999, 0.7889, 0.0482, 0.989...</td>\n",
       "      <td>[0.7977, 0.9766, 0.1001, 0.2111, 0.9518, 0.010...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0026, 0.0009, 0.0028, 0.0019, 1e-04, 0.0, 0...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.199, 0.0512, 0.6745, 0.6615, 0.0228, 0.8021...</td>\n",
       "      <td>[0.801, 0.9488, 0.3255, 0.3385, 0.9772, 0.1979...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0017, 0.0022, 0.1046, 0.0126, 0.0029, 0.012...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.837, 0.5367, 0.9182, 0.5021, 0.5917, 0.8569...</td>\n",
       "      <td>[0.163, 0.4633, 0.0818, 0.4979, 0.4083, 0.1431...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0063, 0.0542, 0.008, 0.0112, 0.0106, 0.0022...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.8505, 0.732, 0.8417, 0.8416, 0.3808, 0.5043...</td>\n",
       "      <td>[0.1495, 0.268, 0.1583, 0.1584, 0.6192, 0.4957...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0016, 0.0026, 0.0019, 1e-04, 0.0081, 0.0086...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1998, 0.7358, 0.0083, 0.1748, 0.8544, 0.133...</td>\n",
       "      <td>[0.8002, 0.2642, 0.9917, 0.8252, 0.1456, 0.866...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0078, 0.0071, 0.0023, 0.003, 0.0416, 0.1035...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.1465, 0.9782, 0.2596, 0.6415, 0.8756, 0.342...</td>\n",
       "      <td>[0.8535, 0.0218, 0.7404, 0.3585, 0.1244, 0.657...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0189, 0.0034, 0.0042, 0.0056, 0.0016, 0.000...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.4884, 0.5302, 0.8588, 0.798, 0.2392, 0.2739...</td>\n",
       "      <td>[0.5116, 0.4698, 0.1412, 0.202, 0.7608, 0.7261...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0014, 0.0007, 0.0012, 0.008, 0.0231, 0.0002...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.7592, 0.9099, 0.8492, 0.8453, 0.7271, 0.493...</td>\n",
       "      <td>[0.2408, 0.0901, 0.1508, 0.1547, 0.2729, 0.506...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.4609, 0.058, 0.0063, 0.0092, 0.006, 0.0138,...               0   \n",
       "1   [0.8734, 0.4524, 0.0034, 0.0114, 0.0909, 0.008...               0   \n",
       "2   [0.918, 0.1442, 0.0021, 0.0456, 0.011, 0.0319,...               0   \n",
       "3   [0.0153, 0.2875, 0.0007, 0.0015, 0.0748, 0.017...               1   \n",
       "4   [0.0657, 0.2713, 0.0091, 0.001, 0.0025, 0.0135...               1   \n",
       "5   [0.0031, 0.0086, 0.6096, 0.0018, 0.0023, 0.024...               2   \n",
       "6   [0.0267, 0.0004, 0.0002, 0.9088, 0.0014, 1e-04...               3   \n",
       "7   [0.0101, 0.0041, 0.0056, 0.75, 0.0023, 0.0007,...               3   \n",
       "8   [0.0164, 0.0019, 0.0096, 0.7245, 0.0081, 0.003...               3   \n",
       "9   [0.0027, 0.0152, 0.0015, 0.0081, 0.8323, 0.056...               4   \n",
       "10  [0.0294, 0.4598, 0.0011, 0.0017, 0.5338, 0.472...               4   \n",
       "11  [0.0009, 0.0065, 0.0009, 0.0007, 0.0844, 0.542...               5   \n",
       "12  [1e-04, 0.0021, 0.0106, 0.0002, 0.0008, 0.0007...               6   \n",
       "13  [0.0317, 0.0346, 0.1019, 0.0174, 0.0005, 0.026...               7   \n",
       "14  [0.0126, 0.004, 1e-04, 0.0152, 0.0314, 0.0017,...               8   \n",
       "15  [0.0262, 0.0036, 0.0009, 0.0011, 0.0142, 0.001...               8   \n",
       "16  [0.004, 0.0019, 0.169, 0.0009, 1e-04, 0.0104, ...               9   \n",
       "17  [0.0117, 0.0017, 0.0163, 0.005, 0.0013, 0.0016...               9   \n",
       "18  [0.0178, 0.0027, 0.0427, 0.0007, 0.0015, 0.010...              10   \n",
       "19  [0.0107, 0.003, 0.0459, 0.0005, 0.0012, 0.0112...              10   \n",
       "20  [0.0333, 0.0128, 0.0107, 0.0251, 0.001, 0.0007...              11   \n",
       "21  [0.0027, 0.0026, 0.0148, 0.0129, 0.0004, 0.000...              11   \n",
       "22  [0.0097, 0.0044, 0.0102, 0.006, 0.001, 0.0127,...              12   \n",
       "23  [0.0137, 0.0021, 0.0075, 0.0186, 0.0305, 0.038...              13   \n",
       "24  [0.0155, 0.0006, 0.1337, 0.0062, 0.0021, 0.006...              13   \n",
       "25  [0.0012, 0.002, 0.009, 0.0062, 0.0063, 0.0009,...              14   \n",
       "26  [0.0016, 0.002, 0.0162, 0.0004, 0.0004, 0.0002...              14   \n",
       "27  [1e-04, 0.0013, 0.0025, 0.0009, 0.0052, 0.0016...              15   \n",
       "28  [1e-04, 0.0076, 0.0093, 0.0038, 0.0008, 0.0007...              15   \n",
       "29  [1e-04, 0.0067, 0.0003, 0.0011, 0.0133, 0.0005...              15   \n",
       "30  [0.0251, 0.4841, 0.0071, 0.0027, 0.2936, 0.138...              16   \n",
       "31  [0.0008, 0.0028, 0.016, 0.0299, 0.0008, 0.0002...              17   \n",
       "32  [0.0006, 0.0006, 0.0107, 0.0259, 0.0004, 1e-04...              17   \n",
       "33  [0.0122, 0.0022, 0.0401, 0.0023, 0.0006, 0.002...              18   \n",
       "34  [0.0344, 0.0102, 0.0035, 0.0078, 0.0947, 0.014...              19   \n",
       "35  [0.0222, 0.0058, 0.0134, 0.0052, 0.0004, 0.002...              20   \n",
       "36  [0.0149, 0.0022, 0.0106, 0.0014, 0.0002, 0.002...              21   \n",
       "37  [0.0015, 0.0014, 0.0039, 0.0071, 1e-04, 0.0003...              21   \n",
       "38  [0.0007, 0.0003, 0.0201, 0.0023, 0.0002, 0.000...              22   \n",
       "39  [0.0016, 0.0004, 0.0131, 0.0026, 0.0002, 0.000...              22   \n",
       "40  [0.0018, 0.0047, 0.0013, 0.0053, 0.0016, 0.000...              22   \n",
       "41  [0.0067, 0.0007, 0.0456, 0.0023, 0.0002, 0.001...              22   \n",
       "42  [0.0053, 0.0017, 0.0081, 0.0018, 1e-04, 0.0, 0...              23   \n",
       "43  [0.0019, 0.0012, 0.0034, 0.0017, 1e-04, 0.0, 0...              23   \n",
       "44  [0.0006, 0.0009, 0.0038, 0.0013, 0.0, 0.0, 0.0...              23   \n",
       "45  [0.0026, 0.0009, 0.0028, 0.0019, 1e-04, 0.0, 0...              23   \n",
       "46  [0.0017, 0.0022, 0.1046, 0.0126, 0.0029, 0.012...              24   \n",
       "47  [0.0063, 0.0542, 0.008, 0.0112, 0.0106, 0.0022...              24   \n",
       "48  [0.0016, 0.0026, 0.0019, 1e-04, 0.0081, 0.0086...              25   \n",
       "49  [0.0078, 0.0071, 0.0023, 0.003, 0.0416, 0.1035...              25   \n",
       "50  [0.0189, 0.0034, 0.0042, 0.0056, 0.0016, 0.000...              26   \n",
       "51  [0.0014, 0.0007, 0.0012, 0.008, 0.0231, 0.0002...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.361, 0.5908, 0.0765, 0.5805, 0.3347, 0.0899...   \n",
       "1                  0  [0.8191, 0.8044, 0.2125, 0.6701, 0.7356, 0.296...   \n",
       "2                  0  [0.6065, 0.7906, 0.1616, 0.7766, 0.3144, 0.122...   \n",
       "3                  1  [0.5726, 0.5727, 0.15, 0.7775, 0.9088, 0.5808,...   \n",
       "4                  1  [0.1844, 0.3368, 0.0346, 0.4789, 0.4318, 0.062...   \n",
       "5                  2  [0.2701, 0.1007, 0.1551, 0.4064, 0.2079, 0.775...   \n",
       "6                  3  [0.9868, 0.9653, 0.9725, 0.9878, 0.1262, 0.971...   \n",
       "7                  3  [0.9362, 0.5376, 0.877, 0.8059, 0.1603, 0.9723...   \n",
       "8                  3  [0.9165, 0.7779, 0.9633, 0.6783, 0.1664, 0.928...   \n",
       "9                  4  [0.8742, 0.972, 0.8371, 0.7608, 0.9823, 0.841,...   \n",
       "10                 4  [0.6777, 0.9702, 0.1051, 0.5131, 0.9282, 0.788...   \n",
       "11                 5  [0.3921, 0.9081, 0.2239, 0.5041, 0.9362, 0.605...   \n",
       "12                 6  [0.619, 0.73, 0.8784, 0.9797, 0.9122, 0.6563, ...   \n",
       "13                 7  [0.2383, 0.0368, 0.1995, 0.624, 0.7097, 0.9694...   \n",
       "14                 8  [0.7662, 0.8621, 0.6152, 0.9668, 0.7676, 0.580...   \n",
       "15                 8  [0.1939, 0.9026, 0.4444, 0.5333, 0.7421, 0.108...   \n",
       "16                 9  [0.2998, 0.2167, 0.1086, 0.4784, 0.2151, 0.578...   \n",
       "17                 9  [0.6563, 0.7437, 0.5313, 0.7877, 0.4207, 0.797...   \n",
       "18                10  [0.1494, 0.5841, 0.088, 0.0726, 0.6884, 0.4532...   \n",
       "19                10  [0.1775, 0.8221, 0.1067, 0.1679, 0.8292, 0.241...   \n",
       "20                11  [0.5462, 0.4627, 0.7669, 0.9429, 0.1443, 0.471...   \n",
       "21                11  [0.5021, 0.1693, 0.4909, 0.9349, 0.0697, 0.681...   \n",
       "22                12  [0.3653, 0.3831, 0.2729, 0.801, 0.1179, 0.6255...   \n",
       "23                13  [0.7624, 0.8433, 0.7794, 0.2285, 0.7788, 0.859...   \n",
       "24                13  [0.3837, 0.4339, 0.5034, 0.2371, 0.0962, 0.680...   \n",
       "25                14  [0.3742, 0.5249, 0.9374, 0.8547, 0.464, 0.7649...   \n",
       "26                14  [0.1311, 0.0323, 0.6472, 0.5698, 0.0904, 0.683...   \n",
       "27                15  [0.7517, 0.6439, 0.5691, 0.8369, 0.9025, 0.920...   \n",
       "28                15  [0.7587, 0.278, 0.6916, 0.8923, 0.9395, 0.9853...   \n",
       "29                15  [0.8291, 0.4546, 0.8666, 0.9569, 0.9864, 0.975...   \n",
       "30                16  [0.8704, 0.8884, 0.3538, 0.5524, 1.0, 0.3298, ...   \n",
       "31                17  [0.942, 0.5167, 0.933, 0.8933, 0.3307, 0.8902,...   \n",
       "32                17  [0.8286, 0.863, 0.9764, 0.9872, 0.1739, 0.8902...   \n",
       "33                18  [0.2263, 0.4621, 0.0853, 0.3342, 0.1232, 0.344...   \n",
       "34                19  [0.8794, 0.9706, 0.094, 0.5078, 0.8884, 0.1725...   \n",
       "35                20  [0.2658, 0.2157, 0.6172, 0.6602, 0.0752, 0.730...   \n",
       "36                21  [0.0587, 0.0735, 0.3913, 0.5189, 0.032, 0.6559...   \n",
       "37                21  [0.0532, 0.0909, 0.7893, 0.8652, 0.0383, 0.980...   \n",
       "38                22  [0.7351, 0.0484, 0.0316, 0.5223, 0.0798, 0.968...   \n",
       "39                22  [0.625, 0.0391, 0.0398, 0.5868, 0.048, 0.8246,...   \n",
       "40                22  [0.8696, 0.3545, 0.2948, 0.9648, 0.4165, 0.843...   \n",
       "41                22  [0.1965, 0.0371, 0.0356, 0.1616, 0.0566, 0.870...   \n",
       "42                23  [0.1508, 0.0741, 0.6237, 0.5814, 0.0258, 0.614...   \n",
       "43                23  [0.3312, 0.0641, 0.6955, 0.8068, 0.0553, 0.948...   \n",
       "44                23  [0.2023, 0.0234, 0.8999, 0.7889, 0.0482, 0.989...   \n",
       "45                23  [0.199, 0.0512, 0.6745, 0.6615, 0.0228, 0.8021...   \n",
       "46                13  [0.837, 0.5367, 0.9182, 0.5021, 0.5917, 0.8569...   \n",
       "47                24  [0.8505, 0.732, 0.8417, 0.8416, 0.3808, 0.5043...   \n",
       "48                25  [0.1998, 0.7358, 0.0083, 0.1748, 0.8544, 0.133...   \n",
       "49                25  [0.1465, 0.9782, 0.2596, 0.6415, 0.8756, 0.342...   \n",
       "50                26  [0.4884, 0.5302, 0.8588, 0.798, 0.2392, 0.2739...   \n",
       "51                26  [0.7592, 0.9099, 0.8492, 0.8453, 0.7271, 0.493...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.639, 0.4092, 0.9235, 0.4195, 0.6653, 0.9101...  0.980769  0.037913  \n",
       "1   [0.1809, 0.1956, 0.7875, 0.3299, 0.2644, 0.703...       NaN       NaN  \n",
       "2   [0.3935, 0.2094, 0.8384, 0.2234, 0.6856, 0.877...       NaN       NaN  \n",
       "3   [0.4274, 0.4273, 0.85, 0.2225, 0.0912, 0.4192,...       NaN       NaN  \n",
       "4   [0.8156, 0.6632, 0.9654, 0.5211, 0.5682, 0.937...       NaN       NaN  \n",
       "5   [0.7299, 0.8993, 0.8449, 0.5936, 0.7921, 0.224...       NaN       NaN  \n",
       "6   [0.0132, 0.0347, 0.0275, 0.0122, 0.8738, 0.028...       NaN       NaN  \n",
       "7   [0.0638, 0.4624, 0.123, 0.1941, 0.8397, 0.0277...       NaN       NaN  \n",
       "8   [0.0835, 0.2221, 0.0367, 0.3217, 0.8336, 0.071...       NaN       NaN  \n",
       "9   [0.1258, 0.028, 0.1629, 0.2392, 0.0177, 0.159,...       NaN       NaN  \n",
       "10  [0.3223, 0.0298, 0.8949, 0.4869, 0.0718, 0.211...       NaN       NaN  \n",
       "11  [0.6079, 0.0919, 0.7761, 0.4959, 0.0638, 0.394...       NaN       NaN  \n",
       "12  [0.381, 0.27, 0.1216, 0.0203, 0.0878, 0.3437, ...       NaN       NaN  \n",
       "13  [0.7617, 0.9632, 0.8005, 0.376, 0.2903, 0.0306...       NaN       NaN  \n",
       "14  [0.2338, 0.1379, 0.3848, 0.0332, 0.2324, 0.419...       NaN       NaN  \n",
       "15  [0.8061, 0.0974, 0.5556, 0.4667, 0.2579, 0.892...       NaN       NaN  \n",
       "16  [0.7002, 0.7833, 0.8914, 0.5216, 0.7849, 0.421...       NaN       NaN  \n",
       "17  [0.3437, 0.2563, 0.4687, 0.2123, 0.5793, 0.202...       NaN       NaN  \n",
       "18  [0.8506, 0.4159, 0.912, 0.9274, 0.3116, 0.5468...       NaN       NaN  \n",
       "19  [0.8225, 0.1779, 0.8933, 0.8321, 0.1708, 0.758...       NaN       NaN  \n",
       "20  [0.4538, 0.5373, 0.2331, 0.0571, 0.8557, 0.528...       NaN       NaN  \n",
       "21  [0.4979, 0.8307, 0.5091, 0.0651, 0.9303, 0.318...       NaN       NaN  \n",
       "22  [0.6347, 0.6169, 0.7271, 0.199, 0.8821, 0.3745...       NaN       NaN  \n",
       "23  [0.2376, 0.1567, 0.2206, 0.7715, 0.2212, 0.141...       NaN       NaN  \n",
       "24  [0.6163, 0.5661, 0.4966, 0.7629, 0.9038, 0.319...       NaN       NaN  \n",
       "25  [0.6258, 0.4751, 0.0626, 0.1453, 0.536, 0.2351...       NaN       NaN  \n",
       "26  [0.8689, 0.9677, 0.3528, 0.4302, 0.9096, 0.316...       NaN       NaN  \n",
       "27  [0.2483, 0.3561, 0.4309, 0.1631, 0.0975, 0.079...       NaN       NaN  \n",
       "28  [0.2413, 0.722, 0.3084, 0.1077, 0.0605, 0.0147...       NaN       NaN  \n",
       "29  [0.1709, 0.5454, 0.1334, 0.0431, 0.0136, 0.024...       NaN       NaN  \n",
       "30  [0.1296, 0.1116, 0.6462, 0.4476, 0.0, 0.6702, ...       NaN       NaN  \n",
       "31  [0.058, 0.4833, 0.067, 0.1067, 0.6693, 0.1098,...       NaN       NaN  \n",
       "32  [0.1714, 0.137, 0.0236, 0.0128, 0.8261, 0.1098...       NaN       NaN  \n",
       "33  [0.7737, 0.5379, 0.9147, 0.6658, 0.8768, 0.655...       NaN       NaN  \n",
       "34  [0.1206, 0.0294, 0.906, 0.4922, 0.1116, 0.8275...       NaN       NaN  \n",
       "35  [0.7342, 0.7843, 0.3828, 0.3398, 0.9248, 0.269...       NaN       NaN  \n",
       "36  [0.9413, 0.9265, 0.6087, 0.4811, 0.968, 0.3441...       NaN       NaN  \n",
       "37  [0.9468, 0.9091, 0.2107, 0.1348, 0.9617, 0.019...       NaN       NaN  \n",
       "38  [0.2649, 0.9516, 0.9684, 0.4777, 0.9202, 0.031...       NaN       NaN  \n",
       "39  [0.375, 0.9609, 0.9602, 0.4132, 0.952, 0.1754,...       NaN       NaN  \n",
       "40  [0.1304, 0.6455, 0.7052, 0.0352, 0.5835, 0.156...       NaN       NaN  \n",
       "41  [0.8035, 0.9629, 0.9644, 0.8384, 0.9434, 0.129...       NaN       NaN  \n",
       "42  [0.8492, 0.9259, 0.3763, 0.4186, 0.9742, 0.385...       NaN       NaN  \n",
       "43  [0.6688, 0.9359, 0.3045, 0.1932, 0.9447, 0.051...       NaN       NaN  \n",
       "44  [0.7977, 0.9766, 0.1001, 0.2111, 0.9518, 0.010...       NaN       NaN  \n",
       "45  [0.801, 0.9488, 0.3255, 0.3385, 0.9772, 0.1979...       NaN       NaN  \n",
       "46  [0.163, 0.4633, 0.0818, 0.4979, 0.4083, 0.1431...       NaN       NaN  \n",
       "47  [0.1495, 0.268, 0.1583, 0.1584, 0.6192, 0.4957...       NaN       NaN  \n",
       "48  [0.8002, 0.2642, 0.9917, 0.8252, 0.1456, 0.866...       NaN       NaN  \n",
       "49  [0.8535, 0.0218, 0.7404, 0.3585, 0.1244, 0.657...       NaN       NaN  \n",
       "50  [0.5116, 0.4698, 0.1412, 0.202, 0.7608, 0.7261...       NaN       NaN  \n",
       "51  [0.2408, 0.0901, 0.1508, 0.1547, 0.2729, 0.506...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321 : Training: loss:  0.02009047\n",
      "3322 : Training: loss:  0.016411241\n",
      "3323 : Training: loss:  0.026662895\n",
      "3324 : Training: loss:  0.010281165\n",
      "3325 : Training: loss:  0.019568112\n",
      "3326 : Training: loss:  0.01805347\n",
      "3327 : Training: loss:  0.04739145\n",
      "3328 : Training: loss:  0.019734493\n",
      "3329 : Training: loss:  0.021555863\n",
      "3330 : Training: loss:  0.020936808\n",
      "3331 : Training: loss:  0.03718493\n",
      "3332 : Training: loss:  0.022093616\n",
      "3333 : Training: loss:  0.0244395\n",
      "3334 : Training: loss:  0.017634228\n",
      "3335 : Training: loss:  0.025541712\n",
      "3336 : Training: loss:  0.01542452\n",
      "3337 : Training: loss:  0.022417018\n",
      "3338 : Training: loss:  0.016818604\n",
      "3339 : Training: loss:  0.017014949\n",
      "3340 : Training: loss:  0.019314423\n",
      "Validation: Loss:  0.03748376  Accuracy:  0.96153843\n",
      "3341 : Training: loss:  0.027764203\n",
      "3342 : Training: loss:  0.01773198\n",
      "3343 : Training: loss:  0.020327406\n",
      "3344 : Training: loss:  0.01675977\n",
      "3345 : Training: loss:  0.04609125\n",
      "3346 : Training: loss:  0.010774696\n",
      "3347 : Training: loss:  0.020818012\n",
      "3348 : Training: loss:  0.022426363\n",
      "3349 : Training: loss:  0.016139783\n",
      "3350 : Training: loss:  0.01951677\n",
      "3351 : Training: loss:  0.020234691\n",
      "3352 : Training: loss:  0.012578569\n",
      "3353 : Training: loss:  0.04399888\n",
      "3354 : Training: loss:  0.017869432\n",
      "3355 : Training: loss:  0.030881954\n",
      "3356 : Training: loss:  0.011875715\n",
      "3357 : Training: loss:  0.018214595\n",
      "3358 : Training: loss:  0.017565392\n",
      "3359 : Training: loss:  0.015289156\n",
      "3360 : Training: loss:  0.024404278\n",
      "Validation: Loss:  0.037061214  Accuracy:  0.96153843\n",
      "3361 : Training: loss:  0.02163435\n",
      "3362 : Training: loss:  0.013655893\n",
      "3363 : Training: loss:  0.02517449\n",
      "3364 : Training: loss:  0.02429499\n",
      "3365 : Training: loss:  0.026071358\n",
      "3366 : Training: loss:  0.014962963\n",
      "3367 : Training: loss:  0.021887233\n",
      "3368 : Training: loss:  0.025675472\n",
      "3369 : Training: loss:  0.010942979\n",
      "3370 : Training: loss:  0.023159888\n",
      "3371 : Training: loss:  0.021442404\n",
      "3372 : Training: loss:  0.018188173\n",
      "3373 : Training: loss:  0.017845497\n",
      "3374 : Training: loss:  0.012101708\n",
      "3375 : Training: loss:  0.017438276\n",
      "3376 : Training: loss:  0.023724113\n",
      "3377 : Training: loss:  0.012901663\n",
      "3378 : Training: loss:  0.010906213\n",
      "3379 : Training: loss:  0.012169903\n",
      "3380 : Training: loss:  0.034875955\n",
      "Validation: Loss:  0.03677178  Accuracy:  0.96153843\n",
      "3381 : Training: loss:  0.01310026\n",
      "3382 : Training: loss:  0.026843762\n",
      "3383 : Training: loss:  0.034240182\n",
      "3384 : Training: loss:  0.02165596\n",
      "3385 : Training: loss:  0.02469701\n",
      "3386 : Training: loss:  0.028540507\n",
      "3387 : Training: loss:  0.017461343\n",
      "3388 : Training: loss:  0.018491456\n",
      "3389 : Training: loss:  0.020954812\n",
      "3390 : Training: loss:  0.024204902\n",
      "3391 : Training: loss:  0.022767536\n",
      "3392 : Training: loss:  0.014236604\n",
      "3393 : Training: loss:  0.03783158\n",
      "3394 : Training: loss:  0.022218635\n",
      "3395 : Training: loss:  0.026067682\n",
      "3396 : Training: loss:  0.01559475\n",
      "3397 : Training: loss:  0.016045097\n",
      "3398 : Training: loss:  0.017779546\n",
      "3399 : Training: loss:  0.018249614\n",
      "3400 : Training: loss:  0.015904026\n",
      "Validation: Loss:  0.03629456  Accuracy:  0.96153843\n",
      "3401 : Training: loss:  0.010677196\n",
      "3402 : Training: loss:  0.024084723\n",
      "3403 : Training: loss:  0.020915594\n",
      "3404 : Training: loss:  0.015585414\n",
      "3405 : Training: loss:  0.013633298\n",
      "3406 : Training: loss:  0.03605142\n",
      "3407 : Training: loss:  0.02748274\n",
      "3408 : Training: loss:  0.019243905\n",
      "3409 : Training: loss:  0.021414138\n",
      "3410 : Training: loss:  0.011878641\n",
      "3411 : Training: loss:  0.014232051\n",
      "3412 : Training: loss:  0.02229183\n",
      "3413 : Training: loss:  0.019556526\n",
      "3414 : Training: loss:  0.015477461\n",
      "3415 : Training: loss:  0.02230594\n",
      "3416 : Training: loss:  0.017016435\n",
      "3417 : Training: loss:  0.015952483\n",
      "3418 : Training: loss:  0.022080146\n",
      "3419 : Training: loss:  0.023269475\n",
      "3420 : Training: loss:  0.0072306483\n",
      "Validation: Loss:  0.036006305  Accuracy:  0.9423077\n",
      "3421 : Training: loss:  0.016643642\n",
      "3422 : Training: loss:  0.029671097\n",
      "3423 : Training: loss:  0.02610567\n",
      "3424 : Training: loss:  0.034136076\n",
      "3425 : Training: loss:  0.009640855\n",
      "3426 : Training: loss:  0.016835216\n",
      "3427 : Training: loss:  0.015884684\n",
      "3428 : Training: loss:  0.023275081\n",
      "3429 : Training: loss:  0.021941217\n",
      "3430 : Training: loss:  0.025394704\n",
      "3431 : Training: loss:  0.019956766\n",
      "3432 : Training: loss:  0.016092697\n",
      "3433 : Training: loss:  0.016824704\n",
      "3434 : Training: loss:  0.019455327\n",
      "3435 : Training: loss:  0.03830116\n",
      "3436 : Training: loss:  0.034545824\n",
      "3437 : Training: loss:  0.035750803\n",
      "3438 : Training: loss:  0.016634766\n",
      "3439 : Training: loss:  0.012696834\n",
      "3440 : Training: loss:  0.024378927\n",
      "Validation: Loss:  0.0355444  Accuracy:  0.96153843\n",
      "3441 : Training: loss:  0.017779842\n",
      "3442 : Training: loss:  0.0117527265\n",
      "3443 : Training: loss:  0.021396978\n",
      "3444 : Training: loss:  0.021338446\n",
      "3445 : Training: loss:  0.018639266\n",
      "3446 : Training: loss:  0.020407459\n",
      "3447 : Training: loss:  0.016182052\n",
      "3448 : Training: loss:  0.01640814\n",
      "3449 : Training: loss:  0.024554856\n",
      "3450 : Training: loss:  0.021968529\n",
      "3451 : Training: loss:  0.016164243\n",
      "3452 : Training: loss:  0.013709883\n",
      "3453 : Training: loss:  0.02124722\n",
      "3454 : Training: loss:  0.020182397\n",
      "3455 : Training: loss:  0.013655051\n",
      "3456 : Training: loss:  0.020531777\n",
      "3457 : Training: loss:  0.021944296\n",
      "3458 : Training: loss:  0.026836963\n",
      "3459 : Training: loss:  0.024423037\n",
      "3460 : Training: loss:  0.015817516\n",
      "Validation: Loss:  0.03509745  Accuracy:  0.96153843\n",
      "3461 : Training: loss:  0.009948618\n",
      "3462 : Training: loss:  0.022632837\n",
      "3463 : Training: loss:  0.013724778\n",
      "3464 : Training: loss:  0.016786005\n",
      "3465 : Training: loss:  0.012829315\n",
      "3466 : Training: loss:  0.0208048\n",
      "3467 : Training: loss:  0.029163662\n",
      "3468 : Training: loss:  0.01386392\n",
      "3469 : Training: loss:  0.014865114\n",
      "3470 : Training: loss:  0.013986285\n",
      "3471 : Training: loss:  0.018977128\n",
      "3472 : Training: loss:  0.037040297\n",
      "3473 : Training: loss:  0.017150292\n",
      "3474 : Training: loss:  0.01593979\n",
      "3475 : Training: loss:  0.024146486\n",
      "3476 : Training: loss:  0.03147417\n",
      "3477 : Training: loss:  0.011125169\n",
      "3478 : Training: loss:  0.014030025\n",
      "3479 : Training: loss:  0.021893095\n",
      "3480 : Training: loss:  0.021969471\n",
      "Validation: Loss:  0.034671057  Accuracy:  0.9807692\n",
      "3481 : Training: loss:  0.02331824\n",
      "3482 : Training: loss:  0.014933551\n",
      "3483 : Training: loss:  0.010432195\n",
      "3484 : Training: loss:  0.013497818\n",
      "3485 : Training: loss:  0.015747443\n",
      "3486 : Training: loss:  0.024534216\n",
      "3487 : Training: loss:  0.01633139\n",
      "3488 : Training: loss:  0.017393919\n",
      "3489 : Training: loss:  0.023045579\n",
      "3490 : Training: loss:  0.028530907\n",
      "3491 : Training: loss:  0.009375167\n",
      "3492 : Training: loss:  0.020006835\n",
      "3493 : Training: loss:  0.026952522\n",
      "3494 : Training: loss:  0.01962427\n",
      "3495 : Training: loss:  0.016719354\n",
      "3496 : Training: loss:  0.032162413\n",
      "3497 : Training: loss:  0.008510987\n",
      "3498 : Training: loss:  0.011273188\n",
      "3499 : Training: loss:  0.020688014\n",
      "3500 : Training: loss:  0.010744838\n",
      "Validation: Loss:  0.034452513  Accuracy:  0.9807692\n",
      "3501 : Training: loss:  0.016734684\n",
      "3502 : Training: loss:  0.025356181\n",
      "3503 : Training: loss:  0.018938376\n",
      "3504 : Training: loss:  0.014721332\n",
      "3505 : Training: loss:  0.02600153\n",
      "3506 : Training: loss:  0.01455258\n",
      "3507 : Training: loss:  0.0184104\n",
      "3508 : Training: loss:  0.01985949\n",
      "3509 : Training: loss:  0.016650122\n",
      "3510 : Training: loss:  0.015902834\n",
      "3511 : Training: loss:  0.015196986\n",
      "3512 : Training: loss:  0.014911753\n",
      "3513 : Training: loss:  0.018626412\n",
      "3514 : Training: loss:  0.01224916\n",
      "3515 : Training: loss:  0.023558132\n",
      "3516 : Training: loss:  0.024097323\n",
      "3517 : Training: loss:  0.012294097\n",
      "3518 : Training: loss:  0.017115131\n",
      "3519 : Training: loss:  0.015020975\n",
      "3520 : Training: loss:  0.014506034\n",
      "Validation: Loss:  0.03415841  Accuracy:  0.9807692\n",
      "3521 : Training: loss:  0.024768082\n",
      "3522 : Training: loss:  0.015409458\n",
      "3523 : Training: loss:  0.025056683\n",
      "3524 : Training: loss:  0.015501745\n",
      "3525 : Training: loss:  0.023774996\n",
      "3526 : Training: loss:  0.009708548\n",
      "3527 : Training: loss:  0.013562125\n",
      "3528 : Training: loss:  0.024671046\n",
      "3529 : Training: loss:  0.015910404\n",
      "3530 : Training: loss:  0.015146239\n",
      "3531 : Training: loss:  0.025864037\n",
      "3532 : Training: loss:  0.01343372\n",
      "3533 : Training: loss:  0.037495017\n",
      "3534 : Training: loss:  0.016051056\n",
      "3535 : Training: loss:  0.018157555\n",
      "3536 : Training: loss:  0.024556143\n",
      "3537 : Training: loss:  0.017335981\n",
      "3538 : Training: loss:  0.009534026\n",
      "3539 : Training: loss:  0.017615134\n",
      "3540 : Training: loss:  0.0166133\n",
      "Validation: Loss:  0.033830047  Accuracy:  0.9807692\n",
      "3541 : Training: loss:  0.021614\n",
      "3542 : Training: loss:  0.01688635\n",
      "3543 : Training: loss:  0.013283809\n",
      "3544 : Training: loss:  0.0150852995\n",
      "3545 : Training: loss:  0.01974481\n",
      "3546 : Training: loss:  0.015852546\n",
      "3547 : Training: loss:  0.020483075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548 : Training: loss:  0.023569958\n",
      "3549 : Training: loss:  0.024596957\n",
      "3550 : Training: loss:  0.017647807\n",
      "3551 : Training: loss:  0.014525033\n",
      "3552 : Training: loss:  0.0141292475\n",
      "3553 : Training: loss:  0.012213277\n",
      "3554 : Training: loss:  0.015596431\n",
      "3555 : Training: loss:  0.012626224\n",
      "3556 : Training: loss:  0.015883727\n",
      "3557 : Training: loss:  0.014502318\n",
      "3558 : Training: loss:  0.022607824\n",
      "3559 : Training: loss:  0.013367546\n",
      "3560 : Training: loss:  0.02108487\n",
      "Validation: Loss:  0.033499405  Accuracy:  0.96153843\n",
      "3561 : Training: loss:  0.022153903\n",
      "3562 : Training: loss:  0.027243057\n",
      "3563 : Training: loss:  0.026393546\n",
      "3564 : Training: loss:  0.0142869875\n",
      "3565 : Training: loss:  0.016587572\n",
      "3566 : Training: loss:  0.019103026\n",
      "3567 : Training: loss:  0.020766702\n",
      "3568 : Training: loss:  0.010662405\n",
      "3569 : Training: loss:  0.012014664\n",
      "3570 : Training: loss:  0.017287204\n",
      "3571 : Training: loss:  0.013038456\n",
      "3572 : Training: loss:  0.020338848\n",
      "3573 : Training: loss:  0.03669991\n",
      "3574 : Training: loss:  0.020770017\n",
      "3575 : Training: loss:  0.013542491\n",
      "3576 : Training: loss:  0.019044055\n",
      "3577 : Training: loss:  0.015253192\n",
      "3578 : Training: loss:  0.01803109\n",
      "3579 : Training: loss:  0.019843329\n",
      "3580 : Training: loss:  0.009285922\n",
      "Validation: Loss:  0.03319253  Accuracy:  0.96153843\n",
      "3581 : Training: loss:  0.027111182\n",
      "3582 : Training: loss:  0.011611063\n",
      "3583 : Training: loss:  0.017426338\n",
      "3584 : Training: loss:  0.017142268\n",
      "3585 : Training: loss:  0.020132545\n",
      "3586 : Training: loss:  0.015209401\n",
      "3587 : Training: loss:  0.013770596\n",
      "3588 : Training: loss:  0.033811305\n",
      "3589 : Training: loss:  0.028084135\n",
      "3590 : Training: loss:  0.024396284\n",
      "3591 : Training: loss:  0.011712319\n",
      "3592 : Training: loss:  0.021830967\n",
      "3593 : Training: loss:  0.014570687\n",
      "3594 : Training: loss:  0.012960231\n",
      "3595 : Training: loss:  0.018839844\n",
      "3596 : Training: loss:  0.012591965\n",
      "3597 : Training: loss:  0.018159\n",
      "3598 : Training: loss:  0.011601658\n",
      "3599 : Training: loss:  0.020566173\n",
      "3600 : Training: loss:  0.016956381\n",
      "Validation: Loss:  0.03298107  Accuracy:  0.96153843\n",
      "3601 : Training: loss:  0.01458499\n",
      "3602 : Training: loss:  0.020566745\n",
      "3603 : Training: loss:  0.016538218\n",
      "3604 : Training: loss:  0.010417348\n",
      "3605 : Training: loss:  0.011389718\n",
      "3606 : Training: loss:  0.0114277955\n",
      "3607 : Training: loss:  0.01453866\n",
      "3608 : Training: loss:  0.009753978\n",
      "3609 : Training: loss:  0.014177119\n",
      "3610 : Training: loss:  0.021763345\n",
      "3611 : Training: loss:  0.0123009635\n",
      "3612 : Training: loss:  0.01687328\n",
      "3613 : Training: loss:  0.015184955\n",
      "3614 : Training: loss:  0.031003831\n",
      "3615 : Training: loss:  0.016297996\n",
      "3616 : Training: loss:  0.014180912\n",
      "3617 : Training: loss:  0.012259115\n",
      "3618 : Training: loss:  0.017832467\n",
      "3619 : Training: loss:  0.018082287\n",
      "3620 : Training: loss:  0.022083947\n",
      "Validation: Loss:  0.032694276  Accuracy:  0.9423077\n",
      "3621 : Training: loss:  0.018743508\n",
      "3622 : Training: loss:  0.015161734\n",
      "3623 : Training: loss:  0.01918063\n",
      "3624 : Training: loss:  0.020324172\n",
      "3625 : Training: loss:  0.015661607\n",
      "3626 : Training: loss:  0.015277608\n",
      "3627 : Training: loss:  0.017277\n",
      "3628 : Training: loss:  0.018145205\n",
      "3629 : Training: loss:  0.017277608\n",
      "3630 : Training: loss:  0.028966678\n",
      "3631 : Training: loss:  0.0103684785\n",
      "3632 : Training: loss:  0.015682425\n",
      "3633 : Training: loss:  0.010824952\n",
      "3634 : Training: loss:  0.017934902\n",
      "3635 : Training: loss:  0.011358988\n",
      "3636 : Training: loss:  0.008414631\n",
      "3637 : Training: loss:  0.015473433\n",
      "3638 : Training: loss:  0.019704485\n",
      "3639 : Training: loss:  0.015614797\n",
      "3640 : Training: loss:  0.013002986\n",
      "Validation: Loss:  0.032350034  Accuracy:  0.9423077\n",
      "3641 : Training: loss:  0.013993477\n",
      "3642 : Training: loss:  0.014754846\n",
      "3643 : Training: loss:  0.01586946\n",
      "3644 : Training: loss:  0.02135453\n",
      "3645 : Training: loss:  0.009651891\n",
      "3646 : Training: loss:  0.024029097\n",
      "3647 : Training: loss:  0.023459442\n",
      "3648 : Training: loss:  0.018245531\n",
      "3649 : Training: loss:  0.013542215\n",
      "3650 : Training: loss:  0.022042103\n",
      "3651 : Training: loss:  0.011982114\n",
      "3652 : Training: loss:  0.025480082\n",
      "3653 : Training: loss:  0.028352764\n",
      "3654 : Training: loss:  0.013211161\n",
      "3655 : Training: loss:  0.00716274\n",
      "3656 : Training: loss:  0.015135595\n",
      "3657 : Training: loss:  0.02441584\n",
      "3658 : Training: loss:  0.013964845\n",
      "3659 : Training: loss:  0.012092901\n",
      "3660 : Training: loss:  0.023946807\n",
      "Validation: Loss:  0.032120857  Accuracy:  0.9423077\n",
      "3661 : Training: loss:  0.011663978\n",
      "3662 : Training: loss:  0.020611448\n",
      "3663 : Training: loss:  0.013641761\n",
      "3664 : Training: loss:  0.017808972\n",
      "3665 : Training: loss:  0.018906996\n",
      "3666 : Training: loss:  0.011388194\n",
      "3667 : Training: loss:  0.016449643\n",
      "3668 : Training: loss:  0.02622904\n",
      "3669 : Training: loss:  0.025999192\n",
      "3670 : Training: loss:  0.020307438\n",
      "3671 : Training: loss:  0.0165101\n",
      "3672 : Training: loss:  0.0092870975\n",
      "3673 : Training: loss:  0.018796027\n",
      "3674 : Training: loss:  0.02433757\n",
      "3675 : Training: loss:  0.0060610343\n",
      "3676 : Training: loss:  0.020363536\n",
      "3677 : Training: loss:  0.015603834\n",
      "3678 : Training: loss:  0.009500642\n",
      "3679 : Training: loss:  0.019362375\n",
      "3680 : Training: loss:  0.01794881\n",
      "Validation: Loss:  0.031795464  Accuracy:  0.9423077\n",
      "3681 : Training: loss:  0.022245364\n",
      "3682 : Training: loss:  0.017501263\n",
      "3683 : Training: loss:  0.011636167\n",
      "3684 : Training: loss:  0.014427245\n",
      "3685 : Training: loss:  0.014460559\n",
      "3686 : Training: loss:  0.0071665654\n",
      "3687 : Training: loss:  0.01615468\n",
      "3688 : Training: loss:  0.01904377\n",
      "3689 : Training: loss:  0.010168601\n",
      "3690 : Training: loss:  0.013463826\n",
      "3691 : Training: loss:  0.011549096\n",
      "3692 : Training: loss:  0.02307292\n",
      "3693 : Training: loss:  0.021763077\n",
      "3694 : Training: loss:  0.015406937\n",
      "3695 : Training: loss:  0.015370016\n",
      "3696 : Training: loss:  0.016754923\n",
      "3697 : Training: loss:  0.013815963\n",
      "3698 : Training: loss:  0.021042371\n",
      "3699 : Training: loss:  0.0101586\n",
      "3700 : Training: loss:  0.021015381\n",
      "Validation: Loss:  0.031477697  Accuracy:  0.9423077\n",
      "3701 : Training: loss:  0.013963031\n",
      "3702 : Training: loss:  0.01460277\n",
      "3703 : Training: loss:  0.018072331\n",
      "3704 : Training: loss:  0.007403687\n",
      "3705 : Training: loss:  0.015788307\n",
      "3706 : Training: loss:  0.010901071\n",
      "3707 : Training: loss:  0.007595808\n",
      "3708 : Training: loss:  0.022998536\n",
      "3709 : Training: loss:  0.007209131\n",
      "3710 : Training: loss:  0.009857695\n",
      "3711 : Training: loss:  0.014417749\n",
      "3712 : Training: loss:  0.019699015\n",
      "3713 : Training: loss:  0.011611885\n",
      "3714 : Training: loss:  0.019819032\n",
      "3715 : Training: loss:  0.010102211\n",
      "3716 : Training: loss:  0.020202385\n",
      "3717 : Training: loss:  0.01818523\n",
      "3718 : Training: loss:  0.017176688\n",
      "3719 : Training: loss:  0.018259462\n",
      "3720 : Training: loss:  0.012100809\n",
      "Validation: Loss:  0.031208908  Accuracy:  0.9423077\n",
      "3721 : Training: loss:  0.011879957\n",
      "3722 : Training: loss:  0.032615587\n",
      "3723 : Training: loss:  0.020956708\n",
      "3724 : Training: loss:  0.015024532\n",
      "3725 : Training: loss:  0.014520459\n",
      "3726 : Training: loss:  0.008538287\n",
      "3727 : Training: loss:  0.015866382\n",
      "3728 : Training: loss:  0.020802332\n",
      "3729 : Training: loss:  0.018429024\n",
      "3730 : Training: loss:  0.040298816\n",
      "3731 : Training: loss:  0.023265447\n",
      "3732 : Training: loss:  0.021024108\n",
      "3733 : Training: loss:  0.010336977\n",
      "3734 : Training: loss:  0.01996168\n",
      "3735 : Training: loss:  0.012006731\n",
      "3736 : Training: loss:  0.0139241805\n",
      "3737 : Training: loss:  0.009414755\n",
      "3738 : Training: loss:  0.01372122\n",
      "3739 : Training: loss:  0.010418331\n",
      "3740 : Training: loss:  0.010234751\n",
      "Validation: Loss:  0.031092029  Accuracy:  0.9423077\n",
      "3741 : Training: loss:  0.016108863\n",
      "3742 : Training: loss:  0.012150651\n",
      "3743 : Training: loss:  0.01120472\n",
      "3744 : Training: loss:  0.012838234\n",
      "3745 : Training: loss:  0.016159581\n",
      "3746 : Training: loss:  0.019040748\n",
      "3747 : Training: loss:  0.022927068\n",
      "3748 : Training: loss:  0.009403666\n",
      "3749 : Training: loss:  0.011175569\n",
      "3750 : Training: loss:  0.011898237\n",
      "3751 : Training: loss:  0.014269901\n",
      "3752 : Training: loss:  0.011430855\n",
      "3753 : Training: loss:  0.018318811\n",
      "3754 : Training: loss:  0.027368715\n",
      "3755 : Training: loss:  0.022495607\n",
      "3756 : Training: loss:  0.011820727\n",
      "3757 : Training: loss:  0.01429499\n",
      "3758 : Training: loss:  0.011862416\n",
      "3759 : Training: loss:  0.012024595\n",
      "3760 : Training: loss:  0.012214749\n",
      "Validation: Loss:  0.030933475  Accuracy:  0.9423077\n",
      "3761 : Training: loss:  0.013519136\n",
      "3762 : Training: loss:  0.016989328\n",
      "3763 : Training: loss:  0.018629149\n",
      "3764 : Training: loss:  0.021124825\n",
      "3765 : Training: loss:  0.01804865\n",
      "3766 : Training: loss:  0.006845989\n",
      "3767 : Training: loss:  0.016713543\n",
      "3768 : Training: loss:  0.007820761\n",
      "3769 : Training: loss:  0.01895649\n",
      "3770 : Training: loss:  0.016083984\n",
      "3771 : Training: loss:  0.02602023\n",
      "3772 : Training: loss:  0.015195374\n",
      "3773 : Training: loss:  0.018154075\n",
      "3774 : Training: loss:  0.015518237\n",
      "3775 : Training: loss:  0.026753722\n",
      "3776 : Training: loss:  0.013638997\n",
      "3777 : Training: loss:  0.015571934\n",
      "3778 : Training: loss:  0.01930846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3779 : Training: loss:  0.012108629\n",
      "3780 : Training: loss:  0.019004608\n",
      "Validation: Loss:  0.030680697  Accuracy:  0.96153843\n",
      "3781 : Training: loss:  0.019785069\n",
      "3782 : Training: loss:  0.0127401035\n",
      "3783 : Training: loss:  0.015051669\n",
      "3784 : Training: loss:  0.013033789\n",
      "3785 : Training: loss:  0.01100645\n",
      "3786 : Training: loss:  0.014841225\n",
      "3787 : Training: loss:  0.013915527\n",
      "3788 : Training: loss:  0.010311903\n",
      "3789 : Training: loss:  0.014211444\n",
      "3790 : Training: loss:  0.010812761\n",
      "3791 : Training: loss:  0.024050899\n",
      "3792 : Training: loss:  0.013395765\n",
      "3793 : Training: loss:  0.015778296\n",
      "3794 : Training: loss:  0.010284778\n",
      "3795 : Training: loss:  0.010799194\n",
      "3796 : Training: loss:  0.014708781\n",
      "3797 : Training: loss:  0.017124262\n",
      "3798 : Training: loss:  0.018555414\n",
      "3799 : Training: loss:  0.011818358\n",
      "3800 : Training: loss:  0.009011543\n",
      "Validation: Loss:  0.030323721  Accuracy:  0.96153843\n",
      "3801 : Training: loss:  0.016026838\n",
      "3802 : Training: loss:  0.020085843\n",
      "3803 : Training: loss:  0.018017597\n",
      "3804 : Training: loss:  0.0065255235\n",
      "3805 : Training: loss:  0.00785312\n",
      "3806 : Training: loss:  0.012028161\n",
      "3807 : Training: loss:  0.014102391\n",
      "3808 : Training: loss:  0.012733595\n",
      "3809 : Training: loss:  0.01569024\n",
      "3810 : Training: loss:  0.008986547\n",
      "3811 : Training: loss:  0.015704922\n",
      "3812 : Training: loss:  0.018261591\n",
      "3813 : Training: loss:  0.015920402\n",
      "3814 : Training: loss:  0.017039604\n",
      "3815 : Training: loss:  0.011549912\n",
      "3816 : Training: loss:  0.020160342\n",
      "3817 : Training: loss:  0.0086436635\n",
      "3818 : Training: loss:  0.009721972\n",
      "3819 : Training: loss:  0.025630929\n",
      "3820 : Training: loss:  0.0115654515\n",
      "Validation: Loss:  0.029960208  Accuracy:  0.96153843\n",
      "3821 : Training: loss:  0.012204621\n",
      "3822 : Training: loss:  0.014790332\n",
      "3823 : Training: loss:  0.019905653\n",
      "3824 : Training: loss:  0.012392451\n",
      "3825 : Training: loss:  0.014822282\n",
      "3826 : Training: loss:  0.015101314\n",
      "3827 : Training: loss:  0.02132731\n",
      "3828 : Training: loss:  0.01309652\n",
      "3829 : Training: loss:  0.013047857\n",
      "3830 : Training: loss:  0.017515888\n",
      "3831 : Training: loss:  0.010403268\n",
      "3832 : Training: loss:  0.011921698\n",
      "3833 : Training: loss:  0.01712847\n",
      "3834 : Training: loss:  0.011728179\n",
      "3835 : Training: loss:  0.010663781\n",
      "3836 : Training: loss:  0.009499615\n",
      "3837 : Training: loss:  0.015155063\n",
      "3838 : Training: loss:  0.0165096\n",
      "3839 : Training: loss:  0.014306477\n",
      "3840 : Training: loss:  0.010785911\n",
      "Validation: Loss:  0.029718377  Accuracy:  0.96153843\n",
      "3841 : Training: loss:  0.011765825\n",
      "3842 : Training: loss:  0.011642377\n",
      "3843 : Training: loss:  0.011066226\n",
      "3844 : Training: loss:  0.013644082\n",
      "3845 : Training: loss:  0.017065883\n",
      "3846 : Training: loss:  0.010332667\n",
      "3847 : Training: loss:  0.017434575\n",
      "3848 : Training: loss:  0.021619035\n",
      "3849 : Training: loss:  0.01131835\n",
      "3850 : Training: loss:  0.010375925\n",
      "3851 : Training: loss:  0.0187413\n",
      "3852 : Training: loss:  0.009827965\n",
      "3853 : Training: loss:  0.014385411\n",
      "3854 : Training: loss:  0.009432899\n",
      "3855 : Training: loss:  0.019217135\n",
      "3856 : Training: loss:  0.01203944\n",
      "3857 : Training: loss:  0.014537877\n",
      "3858 : Training: loss:  0.014995809\n",
      "3859 : Training: loss:  0.013143598\n",
      "3860 : Training: loss:  0.008463925\n",
      "Validation: Loss:  0.029526751  Accuracy:  0.9423077\n",
      "3861 : Training: loss:  0.009173032\n",
      "3862 : Training: loss:  0.012035387\n",
      "3863 : Training: loss:  0.01838402\n",
      "3864 : Training: loss:  0.011367949\n",
      "3865 : Training: loss:  0.02527335\n",
      "3866 : Training: loss:  0.015146919\n",
      "3867 : Training: loss:  0.02435077\n",
      "3868 : Training: loss:  0.017512515\n",
      "3869 : Training: loss:  0.014922941\n",
      "3870 : Training: loss:  0.009034197\n",
      "3871 : Training: loss:  0.010425639\n",
      "3872 : Training: loss:  0.010686741\n",
      "3873 : Training: loss:  0.007931802\n",
      "3874 : Training: loss:  0.008217925\n",
      "3875 : Training: loss:  0.011829907\n",
      "3876 : Training: loss:  0.010747171\n",
      "3877 : Training: loss:  0.012118194\n",
      "3878 : Training: loss:  0.013290784\n",
      "3879 : Training: loss:  0.0106243305\n",
      "3880 : Training: loss:  0.016799385\n",
      "Validation: Loss:  0.029351063  Accuracy:  0.96153843\n",
      "3881 : Training: loss:  0.0081912065\n",
      "3882 : Training: loss:  0.010756619\n",
      "3883 : Training: loss:  0.011733371\n",
      "3884 : Training: loss:  0.0052795103\n",
      "3885 : Training: loss:  0.01555683\n",
      "3886 : Training: loss:  0.015264685\n",
      "3887 : Training: loss:  0.010915176\n",
      "3888 : Training: loss:  0.009767209\n",
      "3889 : Training: loss:  0.01600136\n",
      "3890 : Training: loss:  0.016433217\n",
      "3891 : Training: loss:  0.020246295\n",
      "3892 : Training: loss:  0.015118821\n",
      "3893 : Training: loss:  0.017794194\n",
      "3894 : Training: loss:  0.011477972\n",
      "3895 : Training: loss:  0.027029242\n",
      "3896 : Training: loss:  0.018557169\n",
      "3897 : Training: loss:  0.02006091\n",
      "3898 : Training: loss:  0.032995634\n",
      "3899 : Training: loss:  0.011097274\n",
      "3900 : Training: loss:  0.024927007\n",
      "Validation: Loss:  0.02908166  Accuracy:  0.96153843\n",
      "3901 : Training: loss:  0.012210132\n",
      "3902 : Training: loss:  0.010772402\n",
      "3903 : Training: loss:  0.0068103573\n",
      "3904 : Training: loss:  0.017177055\n",
      "3905 : Training: loss:  0.011314922\n",
      "3906 : Training: loss:  0.01430748\n",
      "3907 : Training: loss:  0.022771733\n",
      "3908 : Training: loss:  0.014904226\n",
      "3909 : Training: loss:  0.010573023\n",
      "3910 : Training: loss:  0.018127475\n",
      "3911 : Training: loss:  0.010697549\n",
      "3912 : Training: loss:  0.009826257\n",
      "3913 : Training: loss:  0.012440098\n",
      "3914 : Training: loss:  0.009683825\n",
      "3915 : Training: loss:  0.033654135\n",
      "3916 : Training: loss:  0.013488795\n",
      "3917 : Training: loss:  0.020888729\n",
      "3918 : Training: loss:  0.019580804\n",
      "3919 : Training: loss:  0.010739588\n",
      "3920 : Training: loss:  0.008506586\n",
      "Validation: Loss:  0.028801007  Accuracy:  0.96153843\n",
      "3921 : Training: loss:  0.017797552\n",
      "3922 : Training: loss:  0.012882764\n",
      "3923 : Training: loss:  0.007453475\n",
      "3924 : Training: loss:  0.014648672\n",
      "3925 : Training: loss:  0.019350497\n",
      "3926 : Training: loss:  0.00726308\n",
      "3927 : Training: loss:  0.026194878\n",
      "3928 : Training: loss:  0.007349783\n",
      "3929 : Training: loss:  0.009215881\n",
      "3930 : Training: loss:  0.008234904\n",
      "3931 : Training: loss:  0.018801756\n",
      "3932 : Training: loss:  0.012309589\n",
      "3933 : Training: loss:  0.019478854\n",
      "3934 : Training: loss:  0.011263883\n",
      "3935 : Training: loss:  0.016406538\n",
      "3936 : Training: loss:  0.017180545\n",
      "3937 : Training: loss:  0.013529448\n",
      "3938 : Training: loss:  0.017894272\n",
      "3939 : Training: loss:  0.017025111\n",
      "3940 : Training: loss:  0.018711312\n",
      "Validation: Loss:  0.028399218  Accuracy:  0.96153843\n",
      "3941 : Training: loss:  0.009963643\n",
      "3942 : Training: loss:  0.017068325\n",
      "3943 : Training: loss:  0.013856609\n",
      "3944 : Training: loss:  0.016074637\n",
      "3945 : Training: loss:  0.012147022\n",
      "3946 : Training: loss:  0.009678072\n",
      "3947 : Training: loss:  0.011984899\n",
      "3948 : Training: loss:  0.019380415\n",
      "3949 : Training: loss:  0.01474553\n",
      "3950 : Training: loss:  0.014162081\n",
      "3951 : Training: loss:  0.016992332\n",
      "3952 : Training: loss:  0.010994215\n",
      "3953 : Training: loss:  0.014287968\n",
      "3954 : Training: loss:  0.0117403725\n",
      "3955 : Training: loss:  0.009305323\n",
      "3956 : Training: loss:  0.011387516\n",
      "3957 : Training: loss:  0.009429859\n",
      "3958 : Training: loss:  0.00993058\n",
      "3959 : Training: loss:  0.013025392\n",
      "3960 : Training: loss:  0.010855608\n",
      "Validation: Loss:  0.028133001  Accuracy:  0.96153843\n",
      "3961 : Training: loss:  0.010070722\n",
      "3962 : Training: loss:  0.0144688105\n",
      "3963 : Training: loss:  0.011773782\n",
      "3964 : Training: loss:  0.006915349\n",
      "3965 : Training: loss:  0.011321402\n",
      "3966 : Training: loss:  0.015362482\n",
      "3967 : Training: loss:  0.010994877\n",
      "3968 : Training: loss:  0.019872887\n",
      "3969 : Training: loss:  0.008374722\n",
      "3970 : Training: loss:  0.014427189\n",
      "3971 : Training: loss:  0.009786839\n",
      "3972 : Training: loss:  0.02530524\n",
      "3973 : Training: loss:  0.01083737\n",
      "3974 : Training: loss:  0.012388148\n",
      "3975 : Training: loss:  0.009842188\n",
      "3976 : Training: loss:  0.018179554\n",
      "3977 : Training: loss:  0.014441339\n",
      "3978 : Training: loss:  0.019805744\n",
      "3979 : Training: loss:  0.0072492925\n",
      "3980 : Training: loss:  0.0140756145\n",
      "Validation: Loss:  0.027992362  Accuracy:  0.96153843\n",
      "3981 : Training: loss:  0.0072797514\n",
      "3982 : Training: loss:  0.0060611884\n",
      "3983 : Training: loss:  0.008586139\n",
      "3984 : Training: loss:  0.011916279\n",
      "3985 : Training: loss:  0.019573212\n",
      "3986 : Training: loss:  0.01205662\n",
      "3987 : Training: loss:  0.007768685\n",
      "3988 : Training: loss:  0.012003209\n",
      "3989 : Training: loss:  0.006626124\n",
      "3990 : Training: loss:  0.009564591\n",
      "3991 : Training: loss:  0.008421243\n",
      "3992 : Training: loss:  0.008799543\n",
      "3993 : Training: loss:  0.008938873\n",
      "3994 : Training: loss:  0.0082674995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3995 : Training: loss:  0.02125767\n",
      "3996 : Training: loss:  0.019603996\n",
      "3997 : Training: loss:  0.006692597\n",
      "3998 : Training: loss:  0.009322847\n",
      "3999 : Training: loss:  0.013190899\n",
      "4000 : Training: loss:  0.017603362\n",
      "Validation: Loss:  0.027929317  Accuracy:  0.96153843\n",
      "4001 : Training: loss:  0.020266691\n",
      "4002 : Training: loss:  0.012403201\n",
      "4003 : Training: loss:  0.014599921\n",
      "4004 : Training: loss:  0.016278101\n",
      "4005 : Training: loss:  0.013255678\n",
      "4006 : Training: loss:  0.013149977\n",
      "4007 : Training: loss:  0.016140234\n",
      "4008 : Training: loss:  0.014842144\n",
      "4009 : Training: loss:  0.008151408\n",
      "4010 : Training: loss:  0.010078492\n",
      "4011 : Training: loss:  0.014120693\n",
      "4012 : Training: loss:  0.012414426\n",
      "4013 : Training: loss:  0.012914997\n",
      "4014 : Training: loss:  0.015194835\n",
      "4015 : Training: loss:  0.011384159\n",
      "4016 : Training: loss:  0.009231244\n",
      "4017 : Training: loss:  0.017123984\n",
      "4018 : Training: loss:  0.02342466\n",
      "4019 : Training: loss:  0.011584027\n",
      "4020 : Training: loss:  0.017223375\n",
      "Validation: Loss:  0.027896158  Accuracy:  0.96153843\n",
      "4021 : Training: loss:  0.016256709\n",
      "4022 : Training: loss:  0.010596416\n",
      "4023 : Training: loss:  0.016060034\n",
      "4024 : Training: loss:  0.010322948\n",
      "4025 : Training: loss:  0.017959429\n",
      "4026 : Training: loss:  0.011548291\n",
      "4027 : Training: loss:  0.01329875\n",
      "4028 : Training: loss:  0.013209565\n",
      "4029 : Training: loss:  0.01592462\n",
      "4030 : Training: loss:  0.023940291\n",
      "4031 : Training: loss:  0.010530032\n",
      "4032 : Training: loss:  0.01942511\n",
      "4033 : Training: loss:  0.008878167\n",
      "4034 : Training: loss:  0.008025969\n",
      "4035 : Training: loss:  0.011100692\n",
      "4036 : Training: loss:  0.016853107\n",
      "4037 : Training: loss:  0.021176083\n",
      "4038 : Training: loss:  0.008455789\n",
      "4039 : Training: loss:  0.013818602\n",
      "4040 : Training: loss:  0.013838566\n",
      "Validation: Loss:  0.027626203  Accuracy:  0.96153843\n",
      "4041 : Training: loss:  0.011502304\n",
      "4042 : Training: loss:  0.005834208\n",
      "4043 : Training: loss:  0.0091845095\n",
      "4044 : Training: loss:  0.010567008\n",
      "4045 : Training: loss:  0.010604646\n",
      "4046 : Training: loss:  0.006918747\n",
      "4047 : Training: loss:  0.016747024\n",
      "4048 : Training: loss:  0.01591989\n",
      "4049 : Training: loss:  0.01576167\n",
      "4050 : Training: loss:  0.0104518635\n",
      "4051 : Training: loss:  0.016102104\n",
      "4052 : Training: loss:  0.014732946\n",
      "4053 : Training: loss:  0.0125213945\n",
      "4054 : Training: loss:  0.015595353\n",
      "4055 : Training: loss:  0.01289526\n",
      "4056 : Training: loss:  0.0151954405\n",
      "4057 : Training: loss:  0.013745362\n",
      "4058 : Training: loss:  0.016844096\n",
      "4059 : Training: loss:  0.011874205\n",
      "4060 : Training: loss:  0.012180572\n",
      "Validation: Loss:  0.027326073  Accuracy:  0.96153843\n",
      "4061 : Training: loss:  0.015714673\n",
      "4062 : Training: loss:  0.0087793125\n",
      "4063 : Training: loss:  0.01131814\n",
      "4064 : Training: loss:  0.010674621\n",
      "4065 : Training: loss:  0.02027075\n",
      "4066 : Training: loss:  0.008114814\n",
      "4067 : Training: loss:  0.009982251\n",
      "4068 : Training: loss:  0.007900385\n",
      "4069 : Training: loss:  0.010744563\n",
      "4070 : Training: loss:  0.0077039986\n",
      "4071 : Training: loss:  0.018108927\n",
      "4072 : Training: loss:  0.010810851\n",
      "4073 : Training: loss:  0.0094284015\n",
      "4074 : Training: loss:  0.0076946104\n",
      "4075 : Training: loss:  0.008190835\n",
      "4076 : Training: loss:  0.003911647\n",
      "4077 : Training: loss:  0.019017253\n",
      "4078 : Training: loss:  0.011437273\n",
      "4079 : Training: loss:  0.01584918\n",
      "4080 : Training: loss:  0.008830238\n",
      "Validation: Loss:  0.027115181  Accuracy:  0.96153843\n",
      "4081 : Training: loss:  0.01591676\n",
      "4082 : Training: loss:  0.024761006\n",
      "4083 : Training: loss:  0.015748708\n",
      "4084 : Training: loss:  0.0094159655\n",
      "4085 : Training: loss:  0.013521809\n",
      "4086 : Training: loss:  0.009383955\n",
      "4087 : Training: loss:  0.013272833\n",
      "4088 : Training: loss:  0.007940887\n",
      "4089 : Training: loss:  0.0116764\n",
      "4090 : Training: loss:  0.010662281\n",
      "4091 : Training: loss:  0.008586187\n",
      "4092 : Training: loss:  0.015627023\n",
      "4093 : Training: loss:  0.0054846765\n",
      "4094 : Training: loss:  0.009713749\n",
      "4095 : Training: loss:  0.0048107277\n",
      "4096 : Training: loss:  0.011450565\n",
      "4097 : Training: loss:  0.010571816\n",
      "4098 : Training: loss:  0.008924474\n",
      "4099 : Training: loss:  0.010174553\n",
      "4100 : Training: loss:  0.011022091\n",
      "Validation: Loss:  0.026941562  Accuracy:  0.96153843\n",
      "4101 : Training: loss:  0.013053097\n",
      "4102 : Training: loss:  0.011347079\n",
      "4103 : Training: loss:  0.012475047\n",
      "4104 : Training: loss:  0.007923466\n",
      "4105 : Training: loss:  0.007966109\n",
      "4106 : Training: loss:  0.014307813\n",
      "4107 : Training: loss:  0.018025808\n",
      "4108 : Training: loss:  0.009358584\n",
      "4109 : Training: loss:  0.010926842\n",
      "4110 : Training: loss:  0.0064443266\n",
      "4111 : Training: loss:  0.0031755418\n",
      "4112 : Training: loss:  0.0075878347\n",
      "4113 : Training: loss:  0.012308738\n",
      "4114 : Training: loss:  0.010637481\n",
      "4115 : Training: loss:  0.016254123\n",
      "4116 : Training: loss:  0.009720129\n",
      "4117 : Training: loss:  0.008374806\n",
      "4118 : Training: loss:  0.013178901\n",
      "4119 : Training: loss:  0.02228504\n",
      "4120 : Training: loss:  0.014456536\n",
      "Validation: Loss:  0.026827516  Accuracy:  0.96153843\n",
      "4121 : Training: loss:  0.0126395\n",
      "4122 : Training: loss:  0.01071781\n",
      "4123 : Training: loss:  0.013387937\n",
      "4124 : Training: loss:  0.011673157\n",
      "4125 : Training: loss:  0.010116946\n",
      "4126 : Training: loss:  0.009229968\n",
      "4127 : Training: loss:  0.010381111\n",
      "4128 : Training: loss:  0.010052893\n",
      "4129 : Training: loss:  0.0056060553\n",
      "4130 : Training: loss:  0.024534652\n",
      "4131 : Training: loss:  0.010643705\n",
      "4132 : Training: loss:  0.009387803\n",
      "4133 : Training: loss:  0.012956316\n",
      "4134 : Training: loss:  0.017663077\n",
      "4135 : Training: loss:  0.011349571\n",
      "4136 : Training: loss:  0.016697463\n",
      "4137 : Training: loss:  0.017963108\n",
      "4138 : Training: loss:  0.011951463\n",
      "4139 : Training: loss:  0.007945605\n",
      "4140 : Training: loss:  0.010678268\n",
      "Validation: Loss:  0.0267335  Accuracy:  0.96153843\n",
      "4141 : Training: loss:  0.009698774\n",
      "4142 : Training: loss:  0.008455341\n",
      "4143 : Training: loss:  0.010162192\n",
      "4144 : Training: loss:  0.015299601\n",
      "4145 : Training: loss:  0.020962115\n",
      "4146 : Training: loss:  0.00960061\n",
      "4147 : Training: loss:  0.010814708\n",
      "4148 : Training: loss:  0.008516743\n",
      "4149 : Training: loss:  0.015745226\n",
      "4150 : Training: loss:  0.017836958\n",
      "4151 : Training: loss:  0.009167249\n",
      "4152 : Training: loss:  0.015654916\n",
      "4153 : Training: loss:  0.017029986\n",
      "4154 : Training: loss:  0.014766998\n",
      "4155 : Training: loss:  0.010479694\n",
      "4156 : Training: loss:  0.013762582\n",
      "4157 : Training: loss:  0.008768094\n",
      "4158 : Training: loss:  0.015452654\n",
      "4159 : Training: loss:  0.008953882\n",
      "4160 : Training: loss:  0.012403607\n",
      "Validation: Loss:  0.026659671  Accuracy:  0.96153843\n",
      "4161 : Training: loss:  0.011509485\n",
      "4162 : Training: loss:  0.00920595\n",
      "4163 : Training: loss:  0.00905397\n",
      "4164 : Training: loss:  0.011284901\n",
      "4165 : Training: loss:  0.017207535\n",
      "4166 : Training: loss:  0.012992159\n",
      "4167 : Training: loss:  0.0082939435\n",
      "4168 : Training: loss:  0.009101372\n",
      "4169 : Training: loss:  0.011571684\n",
      "4170 : Training: loss:  0.014493779\n",
      "4171 : Training: loss:  0.011122169\n",
      "4172 : Training: loss:  0.012543093\n",
      "4173 : Training: loss:  0.008092982\n",
      "4174 : Training: loss:  0.005843114\n",
      "4175 : Training: loss:  0.011122103\n",
      "4176 : Training: loss:  0.011320019\n",
      "4177 : Training: loss:  0.009334154\n",
      "4178 : Training: loss:  0.008404429\n",
      "4179 : Training: loss:  0.016140439\n",
      "4180 : Training: loss:  0.008756603\n",
      "Validation: Loss:  0.02652012  Accuracy:  0.96153843\n",
      "4181 : Training: loss:  0.009048196\n",
      "4182 : Training: loss:  0.009617841\n",
      "4183 : Training: loss:  0.0067043463\n",
      "4184 : Training: loss:  0.01300648\n",
      "4185 : Training: loss:  0.014467733\n",
      "4186 : Training: loss:  0.025496498\n",
      "4187 : Training: loss:  0.007608687\n",
      "4188 : Training: loss:  0.010850876\n",
      "4189 : Training: loss:  0.011900813\n",
      "4190 : Training: loss:  0.008757566\n",
      "4191 : Training: loss:  0.008850447\n",
      "4192 : Training: loss:  0.007153934\n",
      "4193 : Training: loss:  0.0108573595\n",
      "4194 : Training: loss:  0.012292955\n",
      "4195 : Training: loss:  0.011821218\n",
      "4196 : Training: loss:  0.0049717776\n",
      "4197 : Training: loss:  0.013365425\n",
      "4198 : Training: loss:  0.016465072\n",
      "4199 : Training: loss:  0.014813169\n",
      "4200 : Training: loss:  0.01263034\n",
      "Validation: Loss:  0.026225215  Accuracy:  0.96153843\n",
      "4201 : Training: loss:  0.014492086\n",
      "4202 : Training: loss:  0.011953966\n",
      "4203 : Training: loss:  0.011150943\n",
      "4204 : Training: loss:  0.021671377\n",
      "4205 : Training: loss:  0.01563681\n",
      "4206 : Training: loss:  0.012349919\n",
      "4207 : Training: loss:  0.0074430383\n",
      "4208 : Training: loss:  0.007902664\n",
      "4209 : Training: loss:  0.010825455\n",
      "4210 : Training: loss:  0.009438348\n",
      "4211 : Training: loss:  0.0120948255\n",
      "4212 : Training: loss:  0.010545786\n",
      "4213 : Training: loss:  0.029637657\n",
      "4214 : Training: loss:  0.009734536\n",
      "4215 : Training: loss:  0.010382856\n",
      "4216 : Training: loss:  0.007336363\n",
      "4217 : Training: loss:  0.007297656\n",
      "4218 : Training: loss:  0.0125323525\n",
      "4219 : Training: loss:  0.013164953\n",
      "4220 : Training: loss:  0.00813147\n",
      "Validation: Loss:  0.026053084  Accuracy:  0.96153843\n",
      "4221 : Training: loss:  0.014223834\n",
      "4222 : Training: loss:  0.00830281\n",
      "4223 : Training: loss:  0.0074985977\n",
      "4224 : Training: loss:  0.009779554\n",
      "4225 : Training: loss:  0.01719063\n",
      "4226 : Training: loss:  0.0160641\n",
      "4227 : Training: loss:  0.01215229\n",
      "4228 : Training: loss:  0.01435124\n",
      "4229 : Training: loss:  0.011064065\n",
      "4230 : Training: loss:  0.008377595\n",
      "4231 : Training: loss:  0.007522828\n",
      "4232 : Training: loss:  0.009221382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4233 : Training: loss:  0.0074387025\n",
      "4234 : Training: loss:  0.009456003\n",
      "4235 : Training: loss:  0.0060368422\n",
      "4236 : Training: loss:  0.009743417\n",
      "4237 : Training: loss:  0.013310425\n",
      "4238 : Training: loss:  0.0063800253\n",
      "4239 : Training: loss:  0.0056875907\n",
      "4240 : Training: loss:  0.015414869\n",
      "Validation: Loss:  0.025881222  Accuracy:  0.96153843\n",
      "4241 : Training: loss:  0.0087793935\n",
      "4242 : Training: loss:  0.013466185\n",
      "4243 : Training: loss:  0.01450869\n",
      "4244 : Training: loss:  0.012681197\n",
      "4245 : Training: loss:  0.012584449\n",
      "4246 : Training: loss:  0.011072544\n",
      "4247 : Training: loss:  0.009634602\n",
      "4248 : Training: loss:  0.008546572\n",
      "4249 : Training: loss:  0.011745253\n",
      "4250 : Training: loss:  0.018091869\n",
      "4251 : Training: loss:  0.014087302\n",
      "4252 : Training: loss:  0.008088862\n",
      "4253 : Training: loss:  0.008141831\n",
      "4254 : Training: loss:  0.010778344\n",
      "4255 : Training: loss:  0.009613756\n",
      "4256 : Training: loss:  0.011373542\n",
      "4257 : Training: loss:  0.009019984\n",
      "4258 : Training: loss:  0.01763129\n",
      "4259 : Training: loss:  0.011697663\n",
      "4260 : Training: loss:  0.006905521\n",
      "Validation: Loss:  0.025696099  Accuracy:  0.96153843\n",
      "4261 : Training: loss:  0.013640468\n",
      "4262 : Training: loss:  0.008497669\n",
      "4263 : Training: loss:  0.010842772\n",
      "4264 : Training: loss:  0.01388457\n",
      "4265 : Training: loss:  0.019209005\n",
      "4266 : Training: loss:  0.009671474\n",
      "4267 : Training: loss:  0.007061189\n",
      "4268 : Training: loss:  0.008882825\n",
      "4269 : Training: loss:  0.008966781\n",
      "4270 : Training: loss:  0.009020945\n",
      "4271 : Training: loss:  0.009782499\n",
      "4272 : Training: loss:  0.008381889\n",
      "4273 : Training: loss:  0.009478154\n",
      "4274 : Training: loss:  0.011219826\n",
      "4275 : Training: loss:  0.014318577\n",
      "4276 : Training: loss:  0.0103867715\n",
      "4277 : Training: loss:  0.015523271\n",
      "4278 : Training: loss:  0.009405853\n",
      "4279 : Training: loss:  0.0069372077\n",
      "4280 : Training: loss:  0.008769583\n",
      "Validation: Loss:  0.025496654  Accuracy:  0.96153843\n",
      "4281 : Training: loss:  0.013649283\n",
      "4282 : Training: loss:  0.01448106\n",
      "4283 : Training: loss:  0.0084298225\n",
      "4284 : Training: loss:  0.011198943\n",
      "4285 : Training: loss:  0.0057858583\n",
      "4286 : Training: loss:  0.010504974\n",
      "4287 : Training: loss:  0.0050751027\n",
      "4288 : Training: loss:  0.012778773\n",
      "4289 : Training: loss:  0.009283676\n",
      "4290 : Training: loss:  0.008318211\n",
      "4291 : Training: loss:  0.0060658194\n",
      "4292 : Training: loss:  0.010516898\n",
      "4293 : Training: loss:  0.008707917\n",
      "4294 : Training: loss:  0.0086781755\n",
      "4295 : Training: loss:  0.010508591\n",
      "4296 : Training: loss:  0.0060738395\n",
      "4297 : Training: loss:  0.010650795\n",
      "4298 : Training: loss:  0.012061425\n",
      "4299 : Training: loss:  0.013863381\n",
      "4300 : Training: loss:  0.0061306246\n",
      "Validation: Loss:  0.02527132  Accuracy:  0.96153843\n",
      "4301 : Training: loss:  0.014410338\n",
      "4302 : Training: loss:  0.010339727\n",
      "4303 : Training: loss:  0.009876132\n",
      "4304 : Training: loss:  0.008610882\n",
      "4305 : Training: loss:  0.008022343\n",
      "4306 : Training: loss:  0.007184336\n",
      "4307 : Training: loss:  0.015930302\n",
      "4308 : Training: loss:  0.0073585296\n",
      "4309 : Training: loss:  0.010650706\n",
      "4310 : Training: loss:  0.009905804\n",
      "4311 : Training: loss:  0.008435309\n",
      "4312 : Training: loss:  0.007070245\n",
      "4313 : Training: loss:  0.012838365\n",
      "4314 : Training: loss:  0.008358477\n",
      "4315 : Training: loss:  0.0119797345\n",
      "4316 : Training: loss:  0.010902487\n",
      "4317 : Training: loss:  0.0063576708\n",
      "4318 : Training: loss:  0.01238389\n",
      "4319 : Training: loss:  0.008670177\n",
      "4320 : Training: loss:  0.008209009\n",
      "Validation: Loss:  0.025137676  Accuracy:  0.96153843\n",
      "4321 : Training: loss:  0.010504109\n",
      "4322 : Training: loss:  0.017926447\n",
      "4323 : Training: loss:  0.011360024\n",
      "4324 : Training: loss:  0.005855048\n",
      "4325 : Training: loss:  0.016604127\n",
      "4326 : Training: loss:  0.01071346\n",
      "4327 : Training: loss:  0.008507724\n",
      "4328 : Training: loss:  0.007912591\n",
      "4329 : Training: loss:  0.008655564\n",
      "4330 : Training: loss:  0.007397194\n",
      "4331 : Training: loss:  0.00881952\n",
      "4332 : Training: loss:  0.008562328\n",
      "4333 : Training: loss:  0.007521895\n",
      "4334 : Training: loss:  0.0069980347\n",
      "4335 : Training: loss:  0.009717662\n",
      "4336 : Training: loss:  0.009168561\n",
      "4337 : Training: loss:  0.009248167\n",
      "4338 : Training: loss:  0.00792863\n",
      "4339 : Training: loss:  0.013900287\n",
      "4340 : Training: loss:  0.013184854\n",
      "Validation: Loss:  0.02495546  Accuracy:  0.96153843\n",
      "4341 : Training: loss:  0.012912576\n",
      "4342 : Training: loss:  0.0073959236\n",
      "4343 : Training: loss:  0.011503637\n",
      "4344 : Training: loss:  0.008037725\n",
      "4345 : Training: loss:  0.016537696\n",
      "4346 : Training: loss:  0.014233494\n",
      "4347 : Training: loss:  0.007062372\n",
      "4348 : Training: loss:  0.005169206\n",
      "4349 : Training: loss:  0.008783083\n",
      "4350 : Training: loss:  0.008495967\n",
      "4351 : Training: loss:  0.011375941\n",
      "4352 : Training: loss:  0.009511084\n",
      "4353 : Training: loss:  0.013132298\n",
      "4354 : Training: loss:  0.017495664\n",
      "4355 : Training: loss:  0.016891936\n",
      "4356 : Training: loss:  0.0135271065\n",
      "4357 : Training: loss:  0.0087351175\n",
      "4358 : Training: loss:  0.009285433\n",
      "4359 : Training: loss:  0.012294512\n",
      "4360 : Training: loss:  0.009408602\n",
      "Validation: Loss:  0.024871316  Accuracy:  0.96153843\n",
      "4361 : Training: loss:  0.006234765\n",
      "4362 : Training: loss:  0.014524372\n",
      "4363 : Training: loss:  0.017023748\n",
      "4364 : Training: loss:  0.009163635\n",
      "4365 : Training: loss:  0.006629785\n",
      "4366 : Training: loss:  0.0076729762\n",
      "4367 : Training: loss:  0.024637153\n",
      "4368 : Training: loss:  0.008386313\n",
      "4369 : Training: loss:  0.0060225436\n",
      "4370 : Training: loss:  0.009769176\n",
      "4371 : Training: loss:  0.0102838045\n",
      "4372 : Training: loss:  0.02532696\n",
      "4373 : Training: loss:  0.0055089463\n",
      "4374 : Training: loss:  0.00650237\n",
      "4375 : Training: loss:  0.008099353\n",
      "4376 : Training: loss:  0.008524255\n",
      "4377 : Training: loss:  0.010155894\n",
      "4378 : Training: loss:  0.00950569\n",
      "4379 : Training: loss:  0.009201847\n",
      "4380 : Training: loss:  0.011920668\n",
      "Validation: Loss:  0.02482983  Accuracy:  0.96153843\n",
      "4381 : Training: loss:  0.007426668\n",
      "4382 : Training: loss:  0.015438329\n",
      "4383 : Training: loss:  0.0068849977\n",
      "4384 : Training: loss:  0.015691431\n",
      "4385 : Training: loss:  0.008496493\n",
      "4386 : Training: loss:  0.0068115923\n",
      "4387 : Training: loss:  0.0069863074\n",
      "4388 : Training: loss:  0.009277851\n",
      "4389 : Training: loss:  0.00869659\n",
      "4390 : Training: loss:  0.012748206\n",
      "4391 : Training: loss:  0.010643414\n",
      "4392 : Training: loss:  0.006336605\n",
      "4393 : Training: loss:  0.011446256\n",
      "4394 : Training: loss:  0.012383987\n",
      "4395 : Training: loss:  0.011080686\n",
      "4396 : Training: loss:  0.011265446\n",
      "4397 : Training: loss:  0.011440088\n",
      "4398 : Training: loss:  0.008357055\n",
      "4399 : Training: loss:  0.008506103\n",
      "4400 : Training: loss:  0.010816935\n",
      "Validation: Loss:  0.024715962  Accuracy:  0.96153843\n",
      "4401 : Training: loss:  0.007353767\n",
      "4402 : Training: loss:  0.00598891\n",
      "4403 : Training: loss:  0.013328664\n",
      "4404 : Training: loss:  0.011618358\n",
      "4405 : Training: loss:  0.009761792\n",
      "4406 : Training: loss:  0.015283494\n",
      "4407 : Training: loss:  0.0068265707\n",
      "4408 : Training: loss:  0.012689366\n",
      "4409 : Training: loss:  0.011870015\n",
      "4410 : Training: loss:  0.010646348\n",
      "4411 : Training: loss:  0.007965885\n",
      "4412 : Training: loss:  0.008571221\n",
      "4413 : Training: loss:  0.011000925\n",
      "4414 : Training: loss:  0.0123103615\n",
      "4415 : Training: loss:  0.014778207\n",
      "4416 : Training: loss:  0.0142657515\n",
      "4417 : Training: loss:  0.011062461\n",
      "4418 : Training: loss:  0.0067075337\n",
      "4419 : Training: loss:  0.004044089\n",
      "4420 : Training: loss:  0.009047723\n",
      "Validation: Loss:  0.024549823  Accuracy:  0.96153843\n",
      "4421 : Training: loss:  0.011489954\n",
      "4422 : Training: loss:  0.0069502955\n",
      "4423 : Training: loss:  0.009061234\n",
      "4424 : Training: loss:  0.013568394\n",
      "4425 : Training: loss:  0.009046409\n",
      "4426 : Training: loss:  0.010173166\n",
      "4427 : Training: loss:  0.019422023\n",
      "4428 : Training: loss:  0.014167083\n",
      "4429 : Training: loss:  0.013026007\n",
      "4430 : Training: loss:  0.004878381\n",
      "4431 : Training: loss:  0.009751589\n",
      "4432 : Training: loss:  0.0062983395\n",
      "4433 : Training: loss:  0.004809519\n",
      "4434 : Training: loss:  0.006677104\n",
      "4435 : Training: loss:  0.009557806\n",
      "4436 : Training: loss:  0.009288014\n",
      "4437 : Training: loss:  0.008276896\n",
      "4438 : Training: loss:  0.011274008\n",
      "4439 : Training: loss:  0.005650229\n",
      "4440 : Training: loss:  0.009797167\n",
      "Validation: Loss:  0.024456188  Accuracy:  0.96153843\n",
      "4441 : Training: loss:  0.011955691\n",
      "4442 : Training: loss:  0.006153617\n",
      "4443 : Training: loss:  0.009532668\n",
      "4444 : Training: loss:  0.009994526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445 : Training: loss:  0.007812046\n",
      "4446 : Training: loss:  0.008250676\n",
      "4447 : Training: loss:  0.008863037\n",
      "4448 : Training: loss:  0.009309744\n",
      "4449 : Training: loss:  0.0062846835\n",
      "4450 : Training: loss:  0.007307101\n",
      "4451 : Training: loss:  0.011060834\n",
      "4452 : Training: loss:  0.0073620705\n",
      "4453 : Training: loss:  0.012487369\n",
      "4454 : Training: loss:  0.009009808\n",
      "4455 : Training: loss:  0.012770081\n",
      "4456 : Training: loss:  0.009701124\n",
      "4457 : Training: loss:  0.00535043\n",
      "4458 : Training: loss:  0.008599513\n",
      "4459 : Training: loss:  0.00917051\n",
      "4460 : Training: loss:  0.0056732683\n",
      "Validation: Loss:  0.024445817  Accuracy:  0.96153843\n",
      "4461 : Training: loss:  0.009123898\n",
      "4462 : Training: loss:  0.016572384\n",
      "4463 : Training: loss:  0.007950534\n",
      "4464 : Training: loss:  0.0063011255\n",
      "4465 : Training: loss:  0.012742299\n",
      "4466 : Training: loss:  0.010979717\n",
      "4467 : Training: loss:  0.009945538\n",
      "4468 : Training: loss:  0.017611584\n",
      "4469 : Training: loss:  0.006910399\n",
      "4470 : Training: loss:  0.008017286\n",
      "4471 : Training: loss:  0.014204157\n",
      "4472 : Training: loss:  0.0066928295\n",
      "4473 : Training: loss:  0.0075446623\n",
      "4474 : Training: loss:  0.008545231\n",
      "4475 : Training: loss:  0.006239465\n",
      "4476 : Training: loss:  0.01106412\n",
      "4477 : Training: loss:  0.0117461085\n",
      "4478 : Training: loss:  0.010361594\n",
      "4479 : Training: loss:  0.006622969\n",
      "4480 : Training: loss:  0.008319396\n",
      "Validation: Loss:  0.024278445  Accuracy:  0.96153843\n",
      "4481 : Training: loss:  0.0130800055\n",
      "4482 : Training: loss:  0.006014488\n",
      "4483 : Training: loss:  0.0141403545\n",
      "4484 : Training: loss:  0.0052173254\n",
      "4485 : Training: loss:  0.0063090003\n",
      "4486 : Training: loss:  0.0064015323\n",
      "4487 : Training: loss:  0.010468686\n",
      "4488 : Training: loss:  0.008973431\n",
      "4489 : Training: loss:  0.0126526905\n",
      "4490 : Training: loss:  0.0052396143\n",
      "4491 : Training: loss:  0.013272811\n",
      "4492 : Training: loss:  0.011856699\n",
      "4493 : Training: loss:  0.020017011\n",
      "4494 : Training: loss:  0.008997326\n",
      "4495 : Training: loss:  0.0091822725\n",
      "4496 : Training: loss:  0.009435364\n",
      "4497 : Training: loss:  0.008690195\n",
      "4498 : Training: loss:  0.012015324\n",
      "4499 : Training: loss:  0.0073433155\n",
      "4500 : Training: loss:  0.009593534\n",
      "Validation: Loss:  0.024045637  Accuracy:  0.96153843\n",
      "4501 : Training: loss:  0.008358707\n",
      "4502 : Training: loss:  0.010019832\n",
      "4503 : Training: loss:  0.011042186\n",
      "4504 : Training: loss:  0.021040436\n",
      "4505 : Training: loss:  0.011335916\n",
      "4506 : Training: loss:  0.008441893\n",
      "4507 : Training: loss:  0.008865625\n",
      "4508 : Training: loss:  0.0075868517\n",
      "4509 : Training: loss:  0.0074082557\n",
      "4510 : Training: loss:  0.008451639\n",
      "4511 : Training: loss:  0.0067660147\n",
      "4512 : Training: loss:  0.012980769\n",
      "4513 : Training: loss:  0.006368729\n",
      "4514 : Training: loss:  0.005217733\n",
      "4515 : Training: loss:  0.008731898\n",
      "4516 : Training: loss:  0.010248665\n",
      "4517 : Training: loss:  0.0077654156\n",
      "4518 : Training: loss:  0.008172269\n",
      "4519 : Training: loss:  0.01148886\n",
      "4520 : Training: loss:  0.004954701\n",
      "Validation: Loss:  0.02390583  Accuracy:  0.96153843\n",
      "4521 : Training: loss:  0.0045532677\n",
      "4522 : Training: loss:  0.007185604\n",
      "4523 : Training: loss:  0.008895605\n",
      "4524 : Training: loss:  0.0049127853\n",
      "4525 : Training: loss:  0.014476321\n",
      "4526 : Training: loss:  0.011795617\n",
      "4527 : Training: loss:  0.0063095056\n",
      "4528 : Training: loss:  0.008813673\n",
      "4529 : Training: loss:  0.0111222565\n",
      "4530 : Training: loss:  0.008270275\n",
      "4531 : Training: loss:  0.0063963174\n",
      "4532 : Training: loss:  0.013222561\n",
      "4533 : Training: loss:  0.012206979\n",
      "4534 : Training: loss:  0.008248015\n",
      "4535 : Training: loss:  0.008318379\n",
      "4536 : Training: loss:  0.008294178\n",
      "4537 : Training: loss:  0.0053025577\n",
      "4538 : Training: loss:  0.0071246265\n",
      "4539 : Training: loss:  0.007117964\n",
      "4540 : Training: loss:  0.011382578\n",
      "Validation: Loss:  0.023720346  Accuracy:  0.96153843\n",
      "4541 : Training: loss:  0.009463275\n",
      "4542 : Training: loss:  0.008459641\n",
      "4543 : Training: loss:  0.010578961\n",
      "4544 : Training: loss:  0.010089767\n",
      "4545 : Training: loss:  0.008839187\n",
      "4546 : Training: loss:  0.0061033047\n",
      "4547 : Training: loss:  0.0092853615\n",
      "4548 : Training: loss:  0.0051377537\n",
      "4549 : Training: loss:  0.011419355\n",
      "4550 : Training: loss:  0.0073516886\n",
      "4551 : Training: loss:  0.00565764\n",
      "4552 : Training: loss:  0.009587844\n",
      "4553 : Training: loss:  0.013290034\n",
      "4554 : Training: loss:  0.013947891\n",
      "4555 : Training: loss:  0.009618084\n",
      "4556 : Training: loss:  0.007177994\n",
      "4557 : Training: loss:  0.009418656\n",
      "4558 : Training: loss:  0.012207697\n",
      "4559 : Training: loss:  0.014003964\n",
      "4560 : Training: loss:  0.0071567176\n",
      "Validation: Loss:  0.023541166  Accuracy:  0.96153843\n",
      "4561 : Training: loss:  0.0071730325\n",
      "4562 : Training: loss:  0.00844397\n",
      "4563 : Training: loss:  0.0056901164\n",
      "4564 : Training: loss:  0.0069268304\n",
      "4565 : Training: loss:  0.006767409\n",
      "4566 : Training: loss:  0.00682997\n",
      "4567 : Training: loss:  0.00935547\n",
      "4568 : Training: loss:  0.005318624\n",
      "4569 : Training: loss:  0.0070679877\n",
      "4570 : Training: loss:  0.007565326\n",
      "4571 : Training: loss:  0.0070720543\n",
      "4572 : Training: loss:  0.0033322158\n",
      "4573 : Training: loss:  0.01506579\n",
      "4574 : Training: loss:  0.009623929\n",
      "4575 : Training: loss:  0.007108066\n",
      "4576 : Training: loss:  0.009852592\n",
      "4577 : Training: loss:  0.014923233\n",
      "4578 : Training: loss:  0.013004863\n",
      "4579 : Training: loss:  0.0062199477\n",
      "4580 : Training: loss:  0.0046019563\n",
      "Validation: Loss:  0.02320654  Accuracy:  0.96153843\n",
      "4581 : Training: loss:  0.0061957063\n",
      "4582 : Training: loss:  0.009642563\n",
      "4583 : Training: loss:  0.008716763\n",
      "4584 : Training: loss:  0.0048197038\n",
      "4585 : Training: loss:  0.0063450765\n",
      "4586 : Training: loss:  0.0069230734\n",
      "4587 : Training: loss:  0.010356353\n",
      "4588 : Training: loss:  0.01024966\n",
      "4589 : Training: loss:  0.0055227596\n",
      "4590 : Training: loss:  0.009783132\n",
      "4591 : Training: loss:  0.018964818\n",
      "4592 : Training: loss:  0.0059208395\n",
      "4593 : Training: loss:  0.011695934\n",
      "4594 : Training: loss:  0.007991784\n",
      "4595 : Training: loss:  0.0055093938\n",
      "4596 : Training: loss:  0.0065385913\n",
      "4597 : Training: loss:  0.012298537\n",
      "4598 : Training: loss:  0.0066950656\n",
      "4599 : Training: loss:  0.008292034\n",
      "4600 : Training: loss:  0.008861609\n",
      "Validation: Loss:  0.022968987  Accuracy:  0.96153843\n",
      "4601 : Training: loss:  0.0072525865\n",
      "4602 : Training: loss:  0.007141616\n",
      "4603 : Training: loss:  0.009666305\n",
      "4604 : Training: loss:  0.011251013\n",
      "4605 : Training: loss:  0.013189769\n",
      "4606 : Training: loss:  0.011729473\n",
      "4607 : Training: loss:  0.009284783\n",
      "4608 : Training: loss:  0.0074152206\n",
      "4609 : Training: loss:  0.008424236\n",
      "4610 : Training: loss:  0.009389599\n",
      "4611 : Training: loss:  0.008311285\n",
      "4612 : Training: loss:  0.0050302227\n",
      "4613 : Training: loss:  0.0056196987\n",
      "4614 : Training: loss:  0.009120585\n",
      "4615 : Training: loss:  0.0069475416\n",
      "4616 : Training: loss:  0.0075047547\n",
      "4617 : Training: loss:  0.011513277\n",
      "4618 : Training: loss:  0.0039396724\n",
      "4619 : Training: loss:  0.008531145\n",
      "4620 : Training: loss:  0.009748589\n",
      "Validation: Loss:  0.02285813  Accuracy:  0.96153843\n",
      "4621 : Training: loss:  0.011315017\n",
      "4622 : Training: loss:  0.008140852\n",
      "4623 : Training: loss:  0.009383293\n",
      "4624 : Training: loss:  0.003294987\n",
      "4625 : Training: loss:  0.010658872\n",
      "4626 : Training: loss:  0.006772401\n",
      "4627 : Training: loss:  0.005559085\n",
      "4628 : Training: loss:  0.0059410324\n",
      "4629 : Training: loss:  0.008236851\n",
      "4630 : Training: loss:  0.017648054\n",
      "4631 : Training: loss:  0.0096593965\n",
      "4632 : Training: loss:  0.009181303\n",
      "4633 : Training: loss:  0.013145037\n",
      "4634 : Training: loss:  0.005887171\n",
      "4635 : Training: loss:  0.0071433536\n",
      "4636 : Training: loss:  0.006336647\n",
      "4637 : Training: loss:  0.0076271915\n",
      "4638 : Training: loss:  0.0052998\n",
      "4639 : Training: loss:  0.00726174\n",
      "4640 : Training: loss:  0.010971301\n",
      "Validation: Loss:  0.022782939  Accuracy:  0.96153843\n",
      "4641 : Training: loss:  0.011850841\n",
      "4642 : Training: loss:  0.012148168\n",
      "4643 : Training: loss:  0.010074\n",
      "4644 : Training: loss:  0.011722281\n",
      "4645 : Training: loss:  0.006032508\n",
      "4646 : Training: loss:  0.010895121\n",
      "4647 : Training: loss:  0.013611662\n",
      "4648 : Training: loss:  0.00954879\n",
      "4649 : Training: loss:  0.007577049\n",
      "4650 : Training: loss:  0.015107819\n",
      "4651 : Training: loss:  0.01066917\n",
      "4652 : Training: loss:  0.011389601\n",
      "4653 : Training: loss:  0.011812258\n",
      "4654 : Training: loss:  0.0077768774\n",
      "4655 : Training: loss:  0.006820897\n",
      "4656 : Training: loss:  0.0038823495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4657 : Training: loss:  0.009896449\n",
      "4658 : Training: loss:  0.008821509\n",
      "4659 : Training: loss:  0.0043199635\n",
      "4660 : Training: loss:  0.009351353\n",
      "Validation: Loss:  0.02251609  Accuracy:  0.96153843\n",
      "4661 : Training: loss:  0.0063729864\n",
      "4662 : Training: loss:  0.0070102606\n",
      "4663 : Training: loss:  0.0070532155\n",
      "4664 : Training: loss:  0.008376232\n",
      "4665 : Training: loss:  0.005208481\n",
      "4666 : Training: loss:  0.010256426\n",
      "4667 : Training: loss:  0.00927397\n",
      "4668 : Training: loss:  0.009490273\n",
      "4669 : Training: loss:  0.0044485154\n",
      "4670 : Training: loss:  0.0074352683\n",
      "4671 : Training: loss:  0.015457244\n",
      "4672 : Training: loss:  0.0066896197\n",
      "4673 : Training: loss:  0.010024512\n",
      "4674 : Training: loss:  0.008135352\n",
      "4675 : Training: loss:  0.008645943\n",
      "4676 : Training: loss:  0.008141064\n",
      "4677 : Training: loss:  0.0093127405\n",
      "4678 : Training: loss:  0.0088767195\n",
      "4679 : Training: loss:  0.005311031\n",
      "4680 : Training: loss:  0.009283519\n",
      "Validation: Loss:  0.022415826  Accuracy:  0.96153843\n",
      "4681 : Training: loss:  0.009625286\n",
      "4682 : Training: loss:  0.0073994333\n",
      "4683 : Training: loss:  0.006853759\n",
      "4684 : Training: loss:  0.00885454\n",
      "4685 : Training: loss:  0.009592872\n",
      "4686 : Training: loss:  0.00598545\n",
      "4687 : Training: loss:  0.009413047\n",
      "4688 : Training: loss:  0.012772233\n",
      "4689 : Training: loss:  0.011664514\n",
      "4690 : Training: loss:  0.006674994\n",
      "4691 : Training: loss:  0.010552848\n",
      "4692 : Training: loss:  0.0070791324\n",
      "4693 : Training: loss:  0.0062317085\n",
      "4694 : Training: loss:  0.007669411\n",
      "4695 : Training: loss:  0.007914525\n",
      "4696 : Training: loss:  0.0063743466\n",
      "4697 : Training: loss:  0.008322484\n",
      "4698 : Training: loss:  0.014674658\n",
      "4699 : Training: loss:  0.0064664693\n",
      "4700 : Training: loss:  0.009994082\n",
      "Validation: Loss:  0.022219371  Accuracy:  0.96153843\n",
      "4701 : Training: loss:  0.004580277\n",
      "4702 : Training: loss:  0.0068159145\n",
      "4703 : Training: loss:  0.011134935\n",
      "4704 : Training: loss:  0.008779728\n",
      "4705 : Training: loss:  0.0057079145\n",
      "4706 : Training: loss:  0.005223751\n",
      "4707 : Training: loss:  0.007208055\n",
      "4708 : Training: loss:  0.00790374\n",
      "4709 : Training: loss:  0.014652539\n",
      "4710 : Training: loss:  0.0065323403\n",
      "4711 : Training: loss:  0.009119313\n",
      "4712 : Training: loss:  0.0063251457\n",
      "4713 : Training: loss:  0.0049594995\n",
      "4714 : Training: loss:  0.005562221\n",
      "4715 : Training: loss:  0.011663161\n",
      "4716 : Training: loss:  0.0062050233\n",
      "4717 : Training: loss:  0.0053450754\n",
      "4718 : Training: loss:  0.0053922622\n",
      "4719 : Training: loss:  0.010483481\n",
      "4720 : Training: loss:  0.004811382\n",
      "Validation: Loss:  0.02194042  Accuracy:  0.96153843\n",
      "4721 : Training: loss:  0.006745565\n",
      "4722 : Training: loss:  0.0066096596\n",
      "4723 : Training: loss:  0.007470485\n",
      "4724 : Training: loss:  0.010736015\n",
      "4725 : Training: loss:  0.008977829\n",
      "4726 : Training: loss:  0.00708596\n",
      "4727 : Training: loss:  0.0098440545\n",
      "4728 : Training: loss:  0.0044326317\n",
      "4729 : Training: loss:  0.0049920413\n",
      "4730 : Training: loss:  0.007322635\n",
      "4731 : Training: loss:  0.006330653\n",
      "4732 : Training: loss:  0.008242006\n",
      "4733 : Training: loss:  0.01027271\n",
      "4734 : Training: loss:  0.0058668135\n",
      "4735 : Training: loss:  0.008083812\n",
      "4736 : Training: loss:  0.004868105\n",
      "4737 : Training: loss:  0.0037959104\n",
      "4738 : Training: loss:  0.006286671\n",
      "4739 : Training: loss:  0.007778517\n",
      "4740 : Training: loss:  0.010720748\n",
      "Validation: Loss:  0.021786174  Accuracy:  0.96153843\n",
      "4741 : Training: loss:  0.005244494\n",
      "4742 : Training: loss:  0.0053094127\n",
      "4743 : Training: loss:  0.008711688\n",
      "4744 : Training: loss:  0.0052684397\n",
      "4745 : Training: loss:  0.0074475724\n",
      "4746 : Training: loss:  0.010496699\n",
      "4747 : Training: loss:  0.007985218\n",
      "4748 : Training: loss:  0.005460682\n",
      "4749 : Training: loss:  0.003857043\n",
      "4750 : Training: loss:  0.0048815818\n",
      "4751 : Training: loss:  0.009124531\n",
      "4752 : Training: loss:  0.0059664943\n",
      "4753 : Training: loss:  0.0046540895\n",
      "4754 : Training: loss:  0.017729484\n",
      "4755 : Training: loss:  0.0051824274\n",
      "4756 : Training: loss:  0.006579583\n",
      "4757 : Training: loss:  0.006366153\n",
      "4758 : Training: loss:  0.004338574\n",
      "4759 : Training: loss:  0.005125304\n",
      "4760 : Training: loss:  0.007427954\n",
      "Validation: Loss:  0.021689199  Accuracy:  0.96153843\n",
      "4761 : Training: loss:  0.0034150525\n",
      "4762 : Training: loss:  0.0067294203\n",
      "4763 : Training: loss:  0.0065945536\n",
      "4764 : Training: loss:  0.013200662\n",
      "4765 : Training: loss:  0.008438446\n",
      "4766 : Training: loss:  0.0055174343\n",
      "4767 : Training: loss:  0.010863044\n",
      "4768 : Training: loss:  0.0074293227\n",
      "4769 : Training: loss:  0.005071423\n",
      "4770 : Training: loss:  0.00599561\n",
      "4771 : Training: loss:  0.008505757\n",
      "4772 : Training: loss:  0.0074397586\n",
      "4773 : Training: loss:  0.0058570863\n",
      "4774 : Training: loss:  0.0062633785\n",
      "4775 : Training: loss:  0.003566125\n",
      "4776 : Training: loss:  0.004135639\n",
      "4777 : Training: loss:  0.0034577188\n",
      "4778 : Training: loss:  0.010955334\n",
      "4779 : Training: loss:  0.0066193053\n",
      "4780 : Training: loss:  0.0054538734\n",
      "Validation: Loss:  0.021670947  Accuracy:  0.96153843\n",
      "4781 : Training: loss:  0.008723445\n",
      "4782 : Training: loss:  0.0059907935\n",
      "4783 : Training: loss:  0.00622791\n",
      "4784 : Training: loss:  0.0055556353\n",
      "4785 : Training: loss:  0.017787313\n",
      "4786 : Training: loss:  0.003912698\n",
      "4787 : Training: loss:  0.006703511\n",
      "4788 : Training: loss:  0.0085196365\n",
      "4789 : Training: loss:  0.00958045\n",
      "4790 : Training: loss:  0.0072658453\n",
      "4791 : Training: loss:  0.011251249\n",
      "4792 : Training: loss:  0.008554094\n",
      "4793 : Training: loss:  0.0059380154\n",
      "4794 : Training: loss:  0.010539966\n",
      "4795 : Training: loss:  0.0116732465\n",
      "4796 : Training: loss:  0.006446787\n",
      "4797 : Training: loss:  0.009486453\n",
      "4798 : Training: loss:  0.008479672\n",
      "4799 : Training: loss:  0.0029789018\n",
      "4800 : Training: loss:  0.0034113762\n",
      "Validation: Loss:  0.021649875  Accuracy:  0.96153843\n",
      "4801 : Training: loss:  0.003634423\n",
      "4802 : Training: loss:  0.0053717266\n",
      "4803 : Training: loss:  0.0068000313\n",
      "4804 : Training: loss:  0.008450132\n",
      "4805 : Training: loss:  0.025440538\n",
      "4806 : Training: loss:  0.003951796\n",
      "4807 : Training: loss:  0.012038655\n",
      "4808 : Training: loss:  0.0052844416\n",
      "4809 : Training: loss:  0.011286272\n",
      "4810 : Training: loss:  0.005512175\n",
      "4811 : Training: loss:  0.009313695\n",
      "4812 : Training: loss:  0.0063648615\n",
      "4813 : Training: loss:  0.0072495113\n",
      "4814 : Training: loss:  0.007988992\n",
      "4815 : Training: loss:  0.005576801\n",
      "4816 : Training: loss:  0.002125052\n",
      "4817 : Training: loss:  0.0057734945\n",
      "4818 : Training: loss:  0.011734629\n",
      "4819 : Training: loss:  0.0073259957\n",
      "4820 : Training: loss:  0.0075292015\n",
      "Validation: Loss:  0.021615077  Accuracy:  0.96153843\n",
      "4821 : Training: loss:  0.016070602\n",
      "4822 : Training: loss:  0.008081243\n",
      "4823 : Training: loss:  0.0040333704\n",
      "4824 : Training: loss:  0.004828125\n",
      "4825 : Training: loss:  0.009217179\n",
      "4826 : Training: loss:  0.006224797\n",
      "4827 : Training: loss:  0.011331407\n",
      "4828 : Training: loss:  0.004979025\n",
      "4829 : Training: loss:  0.0048438013\n",
      "4830 : Training: loss:  0.010295064\n",
      "4831 : Training: loss:  0.005770266\n",
      "4832 : Training: loss:  0.0059568486\n",
      "4833 : Training: loss:  0.0077147656\n",
      "4834 : Training: loss:  0.0074610375\n",
      "4835 : Training: loss:  0.00930769\n",
      "4836 : Training: loss:  0.007656191\n",
      "4837 : Training: loss:  0.007691512\n",
      "4838 : Training: loss:  0.011038636\n",
      "4839 : Training: loss:  0.015526563\n",
      "4840 : Training: loss:  0.005137557\n",
      "Validation: Loss:  0.021562448  Accuracy:  0.96153843\n",
      "4841 : Training: loss:  0.008042875\n",
      "4842 : Training: loss:  0.00843619\n",
      "4843 : Training: loss:  0.0071809595\n",
      "4844 : Training: loss:  0.008094973\n",
      "4845 : Training: loss:  0.009906571\n",
      "4846 : Training: loss:  0.009559178\n",
      "4847 : Training: loss:  0.0074683796\n",
      "4848 : Training: loss:  0.0073438566\n",
      "4849 : Training: loss:  0.008200436\n",
      "4850 : Training: loss:  0.008889702\n",
      "4851 : Training: loss:  0.008114613\n",
      "4852 : Training: loss:  0.014759809\n",
      "4853 : Training: loss:  0.010296522\n",
      "4854 : Training: loss:  0.006517668\n",
      "4855 : Training: loss:  0.0036129786\n",
      "4856 : Training: loss:  0.009426402\n",
      "4857 : Training: loss:  0.003768423\n",
      "4858 : Training: loss:  0.006210021\n",
      "4859 : Training: loss:  0.0047574216\n",
      "4860 : Training: loss:  0.006077657\n",
      "Validation: Loss:  0.021410411  Accuracy:  0.96153843\n",
      "4861 : Training: loss:  0.007906663\n",
      "4862 : Training: loss:  0.017739184\n",
      "4863 : Training: loss:  0.0067950995\n",
      "4864 : Training: loss:  0.0077502904\n",
      "4865 : Training: loss:  0.009119811\n",
      "4866 : Training: loss:  0.007956891\n",
      "4867 : Training: loss:  0.006526565\n",
      "4868 : Training: loss:  0.009985725\n",
      "4869 : Training: loss:  0.0072942036\n",
      "4870 : Training: loss:  0.010271734\n",
      "4871 : Training: loss:  0.0048962855\n",
      "4872 : Training: loss:  0.007939346\n",
      "4873 : Training: loss:  0.0055160103\n",
      "4874 : Training: loss:  0.0065598837\n",
      "4875 : Training: loss:  0.004043046\n",
      "4876 : Training: loss:  0.012462647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4877 : Training: loss:  0.005559053\n",
      "4878 : Training: loss:  0.006168774\n",
      "4879 : Training: loss:  0.007832899\n",
      "4880 : Training: loss:  0.008172804\n",
      "Validation: Loss:  0.02133636  Accuracy:  0.96153843\n",
      "4881 : Training: loss:  0.008122753\n",
      "4882 : Training: loss:  0.008666111\n",
      "4883 : Training: loss:  0.006406067\n",
      "4884 : Training: loss:  0.011817051\n",
      "4885 : Training: loss:  0.009006705\n",
      "4886 : Training: loss:  0.005345238\n",
      "4887 : Training: loss:  0.004219127\n",
      "4888 : Training: loss:  0.0040416536\n",
      "4889 : Training: loss:  0.008473206\n",
      "4890 : Training: loss:  0.009540128\n",
      "4891 : Training: loss:  0.0065455055\n",
      "4892 : Training: loss:  0.01566158\n",
      "4893 : Training: loss:  0.0058185398\n",
      "4894 : Training: loss:  0.008708806\n",
      "4895 : Training: loss:  0.0066428464\n",
      "4896 : Training: loss:  0.0041673663\n",
      "4897 : Training: loss:  0.010086459\n",
      "4898 : Training: loss:  0.006892116\n",
      "4899 : Training: loss:  0.008160747\n",
      "4900 : Training: loss:  0.005627976\n",
      "Validation: Loss:  0.021291083  Accuracy:  0.96153843\n",
      "4901 : Training: loss:  0.0037418292\n",
      "4902 : Training: loss:  0.006862882\n",
      "4903 : Training: loss:  0.00831076\n",
      "4904 : Training: loss:  0.009820951\n",
      "4905 : Training: loss:  0.0052927346\n",
      "4906 : Training: loss:  0.011924298\n",
      "4907 : Training: loss:  0.0045825336\n",
      "4908 : Training: loss:  0.00613446\n",
      "4909 : Training: loss:  0.006225387\n",
      "4910 : Training: loss:  0.0055900356\n",
      "4911 : Training: loss:  0.0075324024\n",
      "4912 : Training: loss:  0.00611007\n",
      "4913 : Training: loss:  0.008278989\n",
      "4914 : Training: loss:  0.011714\n",
      "4915 : Training: loss:  0.0046094633\n",
      "4916 : Training: loss:  0.0119439475\n",
      "4917 : Training: loss:  0.008775325\n",
      "4918 : Training: loss:  0.0050399844\n",
      "4919 : Training: loss:  0.004053224\n",
      "4920 : Training: loss:  0.011238645\n",
      "Validation: Loss:  0.021233616  Accuracy:  0.96153843\n",
      "4921 : Training: loss:  0.011276178\n",
      "4922 : Training: loss:  0.00552108\n",
      "4923 : Training: loss:  0.0062742024\n",
      "4924 : Training: loss:  0.006350066\n",
      "4925 : Training: loss:  0.004721475\n",
      "4926 : Training: loss:  0.010964336\n",
      "4927 : Training: loss:  0.005424729\n",
      "4928 : Training: loss:  0.008595013\n",
      "4929 : Training: loss:  0.0075281076\n",
      "4930 : Training: loss:  0.012185813\n",
      "4931 : Training: loss:  0.008484606\n",
      "4932 : Training: loss:  0.006652214\n",
      "4933 : Training: loss:  0.0048697437\n",
      "4934 : Training: loss:  0.009690344\n",
      "4935 : Training: loss:  0.005679295\n",
      "4936 : Training: loss:  0.007009533\n",
      "4937 : Training: loss:  0.004828692\n",
      "4938 : Training: loss:  0.0064373254\n",
      "4939 : Training: loss:  0.004418894\n",
      "4940 : Training: loss:  0.005625413\n",
      "Validation: Loss:  0.0211625  Accuracy:  0.96153843\n",
      "4941 : Training: loss:  0.00572241\n",
      "4942 : Training: loss:  0.007972913\n",
      "4943 : Training: loss:  0.0043837246\n",
      "4944 : Training: loss:  0.004163763\n",
      "4945 : Training: loss:  0.009375988\n",
      "4946 : Training: loss:  0.008920576\n",
      "4947 : Training: loss:  0.0069500445\n",
      "4948 : Training: loss:  0.0051625227\n",
      "4949 : Training: loss:  0.004026768\n",
      "4950 : Training: loss:  0.008528133\n",
      "4951 : Training: loss:  0.0066122795\n",
      "4952 : Training: loss:  0.007507045\n",
      "4953 : Training: loss:  0.005521062\n",
      "4954 : Training: loss:  0.011760951\n",
      "4955 : Training: loss:  0.008364864\n",
      "4956 : Training: loss:  0.006361491\n",
      "4957 : Training: loss:  0.0048796874\n",
      "4958 : Training: loss:  0.0076425076\n",
      "4959 : Training: loss:  0.0064897547\n",
      "4960 : Training: loss:  0.005615198\n",
      "Validation: Loss:  0.020989554  Accuracy:  0.96153843\n",
      "4961 : Training: loss:  0.007888227\n",
      "4962 : Training: loss:  0.008299742\n",
      "4963 : Training: loss:  0.007455927\n",
      "4964 : Training: loss:  0.010375528\n",
      "4965 : Training: loss:  0.009261168\n",
      "4966 : Training: loss:  0.011602291\n",
      "4967 : Training: loss:  0.0050834324\n",
      "4968 : Training: loss:  0.007822498\n",
      "4969 : Training: loss:  0.0039172038\n",
      "4970 : Training: loss:  0.007594025\n",
      "4971 : Training: loss:  0.005516192\n",
      "4972 : Training: loss:  0.00541921\n",
      "4973 : Training: loss:  0.0052484395\n",
      "4974 : Training: loss:  0.004866148\n",
      "4975 : Training: loss:  0.004243445\n",
      "4976 : Training: loss:  0.005132935\n",
      "4977 : Training: loss:  0.00871631\n",
      "4978 : Training: loss:  0.0048439796\n",
      "4979 : Training: loss:  0.0050135087\n",
      "4980 : Training: loss:  0.0057292134\n",
      "Validation: Loss:  0.020886725  Accuracy:  0.96153843\n",
      "4981 : Training: loss:  0.0050001387\n",
      "4982 : Training: loss:  0.0044866325\n",
      "4983 : Training: loss:  0.006669155\n",
      "4984 : Training: loss:  0.0044872975\n",
      "4985 : Training: loss:  0.0071889316\n",
      "4986 : Training: loss:  0.0056499136\n",
      "4987 : Training: loss:  0.013783734\n",
      "4988 : Training: loss:  0.0033019157\n",
      "4989 : Training: loss:  0.007637351\n",
      "4990 : Training: loss:  0.0074396785\n",
      "4991 : Training: loss:  0.011647026\n",
      "4992 : Training: loss:  0.0066776876\n",
      "4993 : Training: loss:  0.005104599\n",
      "4994 : Training: loss:  0.0061755087\n",
      "4995 : Training: loss:  0.0051425267\n",
      "4996 : Training: loss:  0.0048705824\n",
      "4997 : Training: loss:  0.009481089\n",
      "4998 : Training: loss:  0.0028165374\n",
      "4999 : Training: loss:  0.014115403\n",
      "5000 : Training: loss:  0.009035094\n",
      "Validation: Loss:  0.020850861  Accuracy:  0.96153843\n",
      "5001 : Training: loss:  0.005020557\n",
      "5002 : Training: loss:  0.013373756\n",
      "5003 : Training: loss:  0.0060534263\n",
      "5004 : Training: loss:  0.0055742157\n",
      "5005 : Training: loss:  0.008275737\n",
      "5006 : Training: loss:  0.0067536435\n",
      "5007 : Training: loss:  0.0067491136\n",
      "5008 : Training: loss:  0.0041977814\n",
      "5009 : Training: loss:  0.004701514\n",
      "5010 : Training: loss:  0.009191857\n",
      "5011 : Training: loss:  0.0064471904\n",
      "5012 : Training: loss:  0.0071229436\n",
      "5013 : Training: loss:  0.004927267\n",
      "5014 : Training: loss:  0.00964093\n",
      "5015 : Training: loss:  0.0049691265\n",
      "5016 : Training: loss:  0.0070018717\n",
      "5017 : Training: loss:  0.0026200968\n",
      "5018 : Training: loss:  0.008011815\n",
      "5019 : Training: loss:  0.0058077546\n",
      "5020 : Training: loss:  0.0067137894\n",
      "Validation: Loss:  0.020750241  Accuracy:  0.96153843\n",
      "5021 : Training: loss:  0.0068448656\n",
      "5022 : Training: loss:  0.0066829068\n",
      "5023 : Training: loss:  0.0031791378\n",
      "5024 : Training: loss:  0.0049845525\n",
      "5025 : Training: loss:  0.0077061653\n",
      "5026 : Training: loss:  0.006863671\n",
      "5027 : Training: loss:  0.007633845\n",
      "5028 : Training: loss:  0.006247183\n",
      "5029 : Training: loss:  0.012800735\n",
      "5030 : Training: loss:  0.006223476\n",
      "5031 : Training: loss:  0.0059205303\n",
      "5032 : Training: loss:  0.007050606\n",
      "5033 : Training: loss:  0.012035138\n",
      "5034 : Training: loss:  0.006503067\n",
      "5035 : Training: loss:  0.010370516\n",
      "5036 : Training: loss:  0.009703629\n",
      "5037 : Training: loss:  0.008001369\n",
      "5038 : Training: loss:  0.006288543\n",
      "5039 : Training: loss:  0.012168925\n",
      "5040 : Training: loss:  0.00958774\n",
      "Validation: Loss:  0.02070369  Accuracy:  0.96153843\n",
      "5041 : Training: loss:  0.008250279\n",
      "5042 : Training: loss:  0.0072396863\n",
      "5043 : Training: loss:  0.0066236584\n",
      "5044 : Training: loss:  0.005393756\n",
      "5045 : Training: loss:  0.0064743725\n",
      "5046 : Training: loss:  0.0055647814\n",
      "5047 : Training: loss:  0.009149841\n",
      "5048 : Training: loss:  0.006172382\n",
      "5049 : Training: loss:  0.006347699\n",
      "5050 : Training: loss:  0.0076056244\n",
      "5051 : Training: loss:  0.005454379\n",
      "5052 : Training: loss:  0.0048752096\n",
      "5053 : Training: loss:  0.0068539404\n",
      "5054 : Training: loss:  0.0044745407\n",
      "5055 : Training: loss:  0.0071068793\n",
      "5056 : Training: loss:  0.006626049\n",
      "5057 : Training: loss:  0.00458909\n",
      "5058 : Training: loss:  0.005626573\n",
      "5059 : Training: loss:  0.0044901883\n",
      "5060 : Training: loss:  0.0057572457\n",
      "Validation: Loss:  0.020619545  Accuracy:  0.96153843\n",
      "5061 : Training: loss:  0.006901461\n",
      "5062 : Training: loss:  0.011450307\n",
      "5063 : Training: loss:  0.006510934\n",
      "5064 : Training: loss:  0.010230657\n",
      "5065 : Training: loss:  0.0068299216\n",
      "5066 : Training: loss:  0.006563019\n",
      "5067 : Training: loss:  0.0046099266\n",
      "5068 : Training: loss:  0.0055045355\n",
      "5069 : Training: loss:  0.004043307\n",
      "5070 : Training: loss:  0.0035525428\n",
      "5071 : Training: loss:  0.006806066\n",
      "5072 : Training: loss:  0.0053947326\n",
      "5073 : Training: loss:  0.005373302\n",
      "5074 : Training: loss:  0.006358927\n",
      "5075 : Training: loss:  0.0040478115\n",
      "5076 : Training: loss:  0.009218666\n",
      "5077 : Training: loss:  0.0072097504\n",
      "5078 : Training: loss:  0.0074630897\n",
      "5079 : Training: loss:  0.0076947245\n",
      "5080 : Training: loss:  0.0033597979\n",
      "Validation: Loss:  0.020520855  Accuracy:  0.96153843\n",
      "5081 : Training: loss:  0.004928867\n",
      "5082 : Training: loss:  0.007450005\n",
      "5083 : Training: loss:  0.0041918852\n",
      "5084 : Training: loss:  0.0048824693\n",
      "5085 : Training: loss:  0.0048952852\n",
      "5086 : Training: loss:  0.008533733\n",
      "5087 : Training: loss:  0.0049578133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5088 : Training: loss:  0.0034424106\n",
      "5089 : Training: loss:  0.0057458254\n",
      "5090 : Training: loss:  0.004771456\n",
      "5091 : Training: loss:  0.0046149013\n",
      "5092 : Training: loss:  0.007936866\n",
      "5093 : Training: loss:  0.00657664\n",
      "5094 : Training: loss:  0.0043777353\n",
      "5095 : Training: loss:  0.005935637\n",
      "5096 : Training: loss:  0.005638364\n",
      "5097 : Training: loss:  0.0043071033\n",
      "5098 : Training: loss:  0.0058553135\n",
      "5099 : Training: loss:  0.0047691585\n",
      "5100 : Training: loss:  0.0061377734\n",
      "Validation: Loss:  0.020399995  Accuracy:  0.96153843\n",
      "5101 : Training: loss:  0.0061785053\n",
      "5102 : Training: loss:  0.0079669105\n",
      "5103 : Training: loss:  0.007665664\n",
      "5104 : Training: loss:  0.0068076397\n",
      "5105 : Training: loss:  0.0078033865\n",
      "5106 : Training: loss:  0.008472537\n",
      "5107 : Training: loss:  0.004593226\n",
      "5108 : Training: loss:  0.0029251177\n",
      "5109 : Training: loss:  0.007466043\n",
      "5110 : Training: loss:  0.0041353013\n",
      "5111 : Training: loss:  0.0039296383\n",
      "5112 : Training: loss:  0.011528534\n",
      "5113 : Training: loss:  0.00805829\n",
      "5114 : Training: loss:  0.009023809\n",
      "5115 : Training: loss:  0.006161547\n",
      "5116 : Training: loss:  0.0043193623\n",
      "5117 : Training: loss:  0.0059965593\n",
      "5118 : Training: loss:  0.00448257\n",
      "5119 : Training: loss:  0.008469262\n",
      "5120 : Training: loss:  0.007414976\n",
      "Validation: Loss:  0.020377623  Accuracy:  0.96153843\n",
      "5121 : Training: loss:  0.00687792\n",
      "5122 : Training: loss:  0.004085378\n",
      "5123 : Training: loss:  0.004948791\n",
      "5124 : Training: loss:  0.011775805\n",
      "5125 : Training: loss:  0.009787331\n",
      "5126 : Training: loss:  0.0037648843\n",
      "5127 : Training: loss:  0.0046236464\n",
      "5128 : Training: loss:  0.0052482504\n",
      "5129 : Training: loss:  0.0077756005\n",
      "5130 : Training: loss:  0.0040503107\n",
      "5131 : Training: loss:  0.005130097\n",
      "5132 : Training: loss:  0.0043871407\n",
      "5133 : Training: loss:  0.00361308\n",
      "5134 : Training: loss:  0.0059792018\n",
      "5135 : Training: loss:  0.007174004\n",
      "5136 : Training: loss:  0.006908694\n",
      "5137 : Training: loss:  0.0055849045\n",
      "5138 : Training: loss:  0.0065201437\n",
      "5139 : Training: loss:  0.005413428\n",
      "5140 : Training: loss:  0.0058559948\n",
      "Validation: Loss:  0.020346468  Accuracy:  0.96153843\n",
      "5141 : Training: loss:  0.0050317715\n",
      "5142 : Training: loss:  0.0077811684\n",
      "5143 : Training: loss:  0.0030862847\n",
      "5144 : Training: loss:  0.010544501\n",
      "5145 : Training: loss:  0.005140973\n",
      "5146 : Training: loss:  0.0031896145\n",
      "5147 : Training: loss:  0.005273542\n",
      "5148 : Training: loss:  0.004493853\n",
      "5149 : Training: loss:  0.0064975065\n",
      "5150 : Training: loss:  0.005385795\n",
      "5151 : Training: loss:  0.0074014845\n",
      "5152 : Training: loss:  0.004440377\n",
      "5153 : Training: loss:  0.007332951\n",
      "5154 : Training: loss:  0.0065696123\n",
      "5155 : Training: loss:  0.0060367356\n",
      "5156 : Training: loss:  0.003025801\n",
      "5157 : Training: loss:  0.0035812\n",
      "5158 : Training: loss:  0.003314804\n",
      "5159 : Training: loss:  0.0029598866\n",
      "5160 : Training: loss:  0.005344004\n",
      "Validation: Loss:  0.020262681  Accuracy:  0.96153843\n",
      "5161 : Training: loss:  0.005310593\n",
      "5162 : Training: loss:  0.00585999\n",
      "5163 : Training: loss:  0.0068571065\n",
      "5164 : Training: loss:  0.009993397\n",
      "5165 : Training: loss:  0.011199008\n",
      "5166 : Training: loss:  0.011567979\n",
      "5167 : Training: loss:  0.005094299\n",
      "5168 : Training: loss:  0.0070533385\n",
      "5169 : Training: loss:  0.0076078186\n",
      "5170 : Training: loss:  0.003547278\n",
      "5171 : Training: loss:  0.0075681168\n",
      "5172 : Training: loss:  0.0045583313\n",
      "5173 : Training: loss:  0.0043416806\n",
      "5174 : Training: loss:  0.0039007126\n",
      "5175 : Training: loss:  0.0053933985\n",
      "5176 : Training: loss:  0.0060419384\n",
      "5177 : Training: loss:  0.010763688\n",
      "5178 : Training: loss:  0.0081574535\n",
      "5179 : Training: loss:  0.0068103555\n",
      "5180 : Training: loss:  0.0016300575\n",
      "Validation: Loss:  0.020228269  Accuracy:  0.96153843\n",
      "5181 : Training: loss:  0.006900879\n",
      "5182 : Training: loss:  0.006011986\n",
      "5183 : Training: loss:  0.007972975\n",
      "5184 : Training: loss:  0.010876006\n",
      "5185 : Training: loss:  0.0038785338\n",
      "5186 : Training: loss:  0.0046597887\n",
      "5187 : Training: loss:  0.007375886\n",
      "5188 : Training: loss:  0.005697332\n",
      "5189 : Training: loss:  0.006935331\n",
      "5190 : Training: loss:  0.00676984\n",
      "5191 : Training: loss:  0.006516496\n",
      "5192 : Training: loss:  0.0049285265\n",
      "5193 : Training: loss:  0.0076478124\n",
      "5194 : Training: loss:  0.004937913\n",
      "5195 : Training: loss:  0.0049418844\n",
      "5196 : Training: loss:  0.007532314\n",
      "5197 : Training: loss:  0.0062348484\n",
      "5198 : Training: loss:  0.0077367253\n",
      "5199 : Training: loss:  0.0067240093\n",
      "5200 : Training: loss:  0.0037964126\n",
      "Validation: Loss:  0.020093692  Accuracy:  0.96153843\n",
      "5201 : Training: loss:  0.0023317155\n",
      "5202 : Training: loss:  0.005427923\n",
      "5203 : Training: loss:  0.009169554\n",
      "5204 : Training: loss:  0.005522648\n",
      "5205 : Training: loss:  0.008029104\n",
      "5206 : Training: loss:  0.009197886\n",
      "5207 : Training: loss:  0.0073202704\n",
      "5208 : Training: loss:  0.009021524\n",
      "5209 : Training: loss:  0.006167727\n",
      "5210 : Training: loss:  0.0060981046\n",
      "5211 : Training: loss:  0.00588643\n",
      "5212 : Training: loss:  0.0077848257\n",
      "5213 : Training: loss:  0.007016787\n",
      "5214 : Training: loss:  0.003001373\n",
      "5215 : Training: loss:  0.005517879\n",
      "5216 : Training: loss:  0.0048407046\n",
      "5217 : Training: loss:  0.004495392\n",
      "5218 : Training: loss:  0.004046124\n",
      "5219 : Training: loss:  0.0068283677\n",
      "5220 : Training: loss:  0.007879902\n",
      "Validation: Loss:  0.020034576  Accuracy:  0.96153843\n",
      "5221 : Training: loss:  0.0069563\n",
      "5222 : Training: loss:  0.009620594\n",
      "5223 : Training: loss:  0.0044996617\n",
      "5224 : Training: loss:  0.004934625\n",
      "5225 : Training: loss:  0.004683716\n",
      "5226 : Training: loss:  0.002768767\n",
      "5227 : Training: loss:  0.00541436\n",
      "5228 : Training: loss:  0.0037717822\n",
      "5229 : Training: loss:  0.0051315133\n",
      "5230 : Training: loss:  0.0059649386\n",
      "5231 : Training: loss:  0.0038001253\n",
      "5232 : Training: loss:  0.0065139267\n",
      "5233 : Training: loss:  0.007959614\n",
      "5234 : Training: loss:  0.006412098\n",
      "5235 : Training: loss:  0.009215929\n",
      "5236 : Training: loss:  0.0066254456\n",
      "5237 : Training: loss:  0.005588368\n",
      "5238 : Training: loss:  0.0053153303\n",
      "5239 : Training: loss:  0.005835465\n",
      "5240 : Training: loss:  0.011778991\n",
      "Validation: Loss:  0.019927723  Accuracy:  0.96153843\n",
      "5241 : Training: loss:  0.0059224535\n",
      "5242 : Training: loss:  0.0041438406\n",
      "5243 : Training: loss:  0.006367171\n",
      "5244 : Training: loss:  0.008876674\n",
      "5245 : Training: loss:  0.004139383\n",
      "5246 : Training: loss:  0.0061378223\n",
      "5247 : Training: loss:  0.008627917\n",
      "5248 : Training: loss:  0.004548884\n",
      "5249 : Training: loss:  0.011414658\n",
      "5250 : Training: loss:  0.00498487\n",
      "5251 : Training: loss:  0.0053373287\n",
      "5252 : Training: loss:  0.004095338\n",
      "5253 : Training: loss:  0.005310048\n",
      "5254 : Training: loss:  0.0066947727\n",
      "5255 : Training: loss:  0.0045800786\n",
      "5256 : Training: loss:  0.007651969\n",
      "5257 : Training: loss:  0.0040856856\n",
      "5258 : Training: loss:  0.0056922673\n",
      "5259 : Training: loss:  0.004675266\n",
      "5260 : Training: loss:  0.0034203785\n",
      "Validation: Loss:  0.019897873  Accuracy:  0.96153843\n",
      "5261 : Training: loss:  0.002734744\n",
      "5262 : Training: loss:  0.0042254278\n",
      "5263 : Training: loss:  0.0037556333\n",
      "5264 : Training: loss:  0.006584108\n",
      "5265 : Training: loss:  0.007349775\n",
      "5266 : Training: loss:  0.005035971\n",
      "5267 : Training: loss:  0.007165045\n",
      "5268 : Training: loss:  0.0029153703\n",
      "5269 : Training: loss:  0.0045946054\n",
      "5270 : Training: loss:  0.0042241435\n",
      "5271 : Training: loss:  0.0037949737\n",
      "5272 : Training: loss:  0.008092913\n",
      "5273 : Training: loss:  0.0041184626\n",
      "5274 : Training: loss:  0.0049749315\n",
      "5275 : Training: loss:  0.007014597\n",
      "5276 : Training: loss:  0.0064634564\n",
      "5277 : Training: loss:  0.004467375\n",
      "5278 : Training: loss:  0.0027161243\n",
      "5279 : Training: loss:  0.0065305084\n",
      "5280 : Training: loss:  0.005667596\n",
      "Validation: Loss:  0.019705655  Accuracy:  0.96153843\n",
      "5281 : Training: loss:  0.0046905694\n",
      "5282 : Training: loss:  0.009445816\n",
      "5283 : Training: loss:  0.007546699\n",
      "5284 : Training: loss:  0.0067838887\n",
      "5285 : Training: loss:  0.005600339\n",
      "5286 : Training: loss:  0.0024693476\n",
      "5287 : Training: loss:  0.005035033\n",
      "5288 : Training: loss:  0.004203517\n",
      "5289 : Training: loss:  0.008103819\n",
      "5290 : Training: loss:  0.0071522533\n",
      "5291 : Training: loss:  0.009294138\n",
      "5292 : Training: loss:  0.0068807444\n",
      "5293 : Training: loss:  0.0033308268\n",
      "5294 : Training: loss:  0.0054212767\n",
      "5295 : Training: loss:  0.0036621208\n",
      "5296 : Training: loss:  0.004648774\n",
      "5297 : Training: loss:  0.0060993754\n",
      "5298 : Training: loss:  0.0020018993\n",
      "5299 : Training: loss:  0.0026548477\n",
      "5300 : Training: loss:  0.004545625\n",
      "Validation: Loss:  0.01953708  Accuracy:  0.96153843\n",
      "5301 : Training: loss:  0.00748866\n",
      "5302 : Training: loss:  0.004105587\n",
      "5303 : Training: loss:  0.0054822317\n",
      "5304 : Training: loss:  0.0071973912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5305 : Training: loss:  0.004571222\n",
      "5306 : Training: loss:  0.0042901253\n",
      "5307 : Training: loss:  0.0029805216\n",
      "5308 : Training: loss:  0.008166451\n",
      "5309 : Training: loss:  0.003649515\n",
      "5310 : Training: loss:  0.007493233\n",
      "5311 : Training: loss:  0.0051695853\n",
      "5312 : Training: loss:  0.004060518\n",
      "5313 : Training: loss:  0.0070227627\n",
      "5314 : Training: loss:  0.0054726037\n",
      "5315 : Training: loss:  0.0058339485\n",
      "5316 : Training: loss:  0.0077119777\n",
      "5317 : Training: loss:  0.00830802\n",
      "5318 : Training: loss:  0.005064441\n",
      "5319 : Training: loss:  0.005473995\n",
      "5320 : Training: loss:  0.0056794416\n",
      "Validation: Loss:  0.019530468  Accuracy:  0.96153843\n",
      "5321 : Training: loss:  0.004460466\n",
      "5322 : Training: loss:  0.0041882955\n",
      "5323 : Training: loss:  0.0053945393\n",
      "5324 : Training: loss:  0.0042111487\n",
      "5325 : Training: loss:  0.0040529873\n",
      "5326 : Training: loss:  0.010323484\n",
      "5327 : Training: loss:  0.009260673\n",
      "5328 : Training: loss:  0.005841705\n",
      "5329 : Training: loss:  0.006584695\n",
      "5330 : Training: loss:  0.0047347634\n",
      "5331 : Training: loss:  0.010886716\n",
      "5332 : Training: loss:  0.002973256\n",
      "5333 : Training: loss:  0.0063120574\n",
      "5334 : Training: loss:  0.0072564986\n",
      "5335 : Training: loss:  0.004750363\n",
      "5336 : Training: loss:  0.0048446334\n",
      "5337 : Training: loss:  0.005366795\n",
      "5338 : Training: loss:  0.011335026\n",
      "5339 : Training: loss:  0.0049265423\n",
      "5340 : Training: loss:  0.003926345\n",
      "Validation: Loss:  0.019469656  Accuracy:  0.96153843\n",
      "5341 : Training: loss:  0.011628364\n",
      "5342 : Training: loss:  0.0070216954\n",
      "5343 : Training: loss:  0.004639304\n",
      "5344 : Training: loss:  0.005291361\n",
      "5345 : Training: loss:  0.0058428114\n",
      "5346 : Training: loss:  0.005585695\n",
      "5347 : Training: loss:  0.0051413206\n",
      "5348 : Training: loss:  0.0024390607\n",
      "5349 : Training: loss:  0.00461228\n",
      "5350 : Training: loss:  0.004468948\n",
      "5351 : Training: loss:  0.0036774704\n",
      "5352 : Training: loss:  0.0024843065\n",
      "5353 : Training: loss:  0.006660116\n",
      "5354 : Training: loss:  0.008036915\n",
      "5355 : Training: loss:  0.006523598\n",
      "5356 : Training: loss:  0.0044951756\n",
      "5357 : Training: loss:  0.007589758\n",
      "5358 : Training: loss:  0.005355366\n",
      "5359 : Training: loss:  0.0038688849\n",
      "5360 : Training: loss:  0.0045661945\n",
      "Validation: Loss:  0.019317847  Accuracy:  0.96153843\n",
      "5361 : Training: loss:  0.009380979\n",
      "5362 : Training: loss:  0.007106411\n",
      "5363 : Training: loss:  0.0045425724\n",
      "5364 : Training: loss:  0.005241217\n",
      "5365 : Training: loss:  0.0045833597\n",
      "5366 : Training: loss:  0.0032479747\n",
      "5367 : Training: loss:  0.0040004663\n",
      "5368 : Training: loss:  0.0058216597\n",
      "5369 : Training: loss:  0.0044355174\n",
      "5370 : Training: loss:  0.00798565\n",
      "5371 : Training: loss:  0.0045572063\n",
      "5372 : Training: loss:  0.0074632685\n",
      "5373 : Training: loss:  0.0036794285\n",
      "5374 : Training: loss:  0.004031424\n",
      "5375 : Training: loss:  0.0048768516\n",
      "5376 : Training: loss:  0.0066775414\n",
      "5377 : Training: loss:  0.0035785756\n",
      "5378 : Training: loss:  0.0039379885\n",
      "5379 : Training: loss:  0.00427311\n",
      "5380 : Training: loss:  0.0053846766\n",
      "Validation: Loss:  0.019169345  Accuracy:  0.96153843\n",
      "5381 : Training: loss:  0.007633416\n",
      "5382 : Training: loss:  0.0060902643\n",
      "5383 : Training: loss:  0.0053569796\n",
      "5384 : Training: loss:  0.00853606\n",
      "5385 : Training: loss:  0.011120511\n",
      "5386 : Training: loss:  0.004025582\n",
      "5387 : Training: loss:  0.0020741401\n",
      "5388 : Training: loss:  0.0038447012\n",
      "5389 : Training: loss:  0.0032089811\n",
      "5390 : Training: loss:  0.006093416\n",
      "5391 : Training: loss:  0.006447772\n",
      "5392 : Training: loss:  0.0042680087\n",
      "5393 : Training: loss:  0.002371819\n",
      "5394 : Training: loss:  0.0067376923\n",
      "5395 : Training: loss:  0.006801284\n",
      "5396 : Training: loss:  0.0047956486\n",
      "5397 : Training: loss:  0.010212315\n",
      "5398 : Training: loss:  0.005873116\n",
      "5399 : Training: loss:  0.006895792\n",
      "5400 : Training: loss:  0.0044595036\n",
      "Validation: Loss:  0.019268611  Accuracy:  0.96153843\n",
      "5401 : Training: loss:  0.0074199787\n",
      "5402 : Training: loss:  0.003024379\n",
      "5403 : Training: loss:  0.0036792385\n",
      "5404 : Training: loss:  0.0058709295\n",
      "5405 : Training: loss:  0.005682167\n",
      "5406 : Training: loss:  0.0037714196\n",
      "5407 : Training: loss:  0.007794627\n",
      "5408 : Training: loss:  0.0075096483\n",
      "5409 : Training: loss:  0.004475539\n",
      "5410 : Training: loss:  0.0049744346\n",
      "5411 : Training: loss:  0.0021284802\n",
      "5412 : Training: loss:  0.0047906064\n",
      "5413 : Training: loss:  0.00417965\n",
      "5414 : Training: loss:  0.00268096\n",
      "5415 : Training: loss:  0.008248831\n",
      "5416 : Training: loss:  0.0122017395\n",
      "5417 : Training: loss:  0.007415648\n",
      "5418 : Training: loss:  0.0043345094\n",
      "5419 : Training: loss:  0.0052761612\n",
      "5420 : Training: loss:  0.005332753\n",
      "Validation: Loss:  0.019257769  Accuracy:  0.96153843\n",
      "5421 : Training: loss:  0.00474726\n",
      "5422 : Training: loss:  0.0051731043\n",
      "5423 : Training: loss:  0.004771164\n",
      "5424 : Training: loss:  0.003977074\n",
      "5425 : Training: loss:  0.0046030544\n",
      "5426 : Training: loss:  0.0053023277\n",
      "5427 : Training: loss:  0.007932617\n",
      "5428 : Training: loss:  0.010513394\n",
      "5429 : Training: loss:  0.0051691225\n",
      "5430 : Training: loss:  0.004481614\n",
      "5431 : Training: loss:  0.004382663\n",
      "5432 : Training: loss:  0.008207003\n",
      "5433 : Training: loss:  0.003143772\n",
      "5434 : Training: loss:  0.004366708\n",
      "5435 : Training: loss:  0.00414516\n",
      "5436 : Training: loss:  0.005473422\n",
      "5437 : Training: loss:  0.0083553465\n",
      "5438 : Training: loss:  0.0054822187\n",
      "5439 : Training: loss:  0.005266567\n",
      "5440 : Training: loss:  0.0072022406\n",
      "Validation: Loss:  0.019169621  Accuracy:  0.96153843\n",
      "5441 : Training: loss:  0.007896317\n",
      "5442 : Training: loss:  0.007398716\n",
      "5443 : Training: loss:  0.0035953426\n",
      "5444 : Training: loss:  0.007505538\n",
      "5445 : Training: loss:  0.006164532\n",
      "5446 : Training: loss:  0.002525006\n",
      "5447 : Training: loss:  0.0021993627\n",
      "5448 : Training: loss:  0.0052091903\n",
      "5449 : Training: loss:  0.008353257\n",
      "5450 : Training: loss:  0.0068468205\n",
      "5451 : Training: loss:  0.0053343712\n",
      "5452 : Training: loss:  0.004079085\n",
      "5453 : Training: loss:  0.006388301\n",
      "5454 : Training: loss:  0.006560716\n",
      "5455 : Training: loss:  0.0062239645\n",
      "5456 : Training: loss:  0.009671432\n",
      "5457 : Training: loss:  0.0033938643\n",
      "5458 : Training: loss:  0.0076460093\n",
      "5459 : Training: loss:  0.0050889137\n",
      "5460 : Training: loss:  0.0060302354\n",
      "Validation: Loss:  0.019044086  Accuracy:  0.96153843\n",
      "5461 : Training: loss:  0.004315753\n",
      "5462 : Training: loss:  0.0034870158\n",
      "5463 : Training: loss:  0.00886815\n",
      "5464 : Training: loss:  0.0043225465\n",
      "5465 : Training: loss:  0.005110421\n",
      "5466 : Training: loss:  0.0043697483\n",
      "5467 : Training: loss:  0.0063373228\n",
      "5468 : Training: loss:  0.008390879\n",
      "5469 : Training: loss:  0.005511721\n",
      "5470 : Training: loss:  0.0032184285\n",
      "5471 : Training: loss:  0.0069277356\n",
      "5472 : Training: loss:  0.0034998262\n",
      "5473 : Training: loss:  0.005376663\n",
      "5474 : Training: loss:  0.0066556204\n",
      "5475 : Training: loss:  0.00462303\n",
      "5476 : Training: loss:  0.008528597\n",
      "5477 : Training: loss:  0.0052819215\n",
      "5478 : Training: loss:  0.0071427445\n",
      "5479 : Training: loss:  0.004724801\n",
      "5480 : Training: loss:  0.0054894183\n",
      "Validation: Loss:  0.019013857  Accuracy:  0.96153843\n",
      "5481 : Training: loss:  0.0030422658\n",
      "5482 : Training: loss:  0.004661406\n",
      "5483 : Training: loss:  0.0058849812\n",
      "5484 : Training: loss:  0.004047169\n",
      "5485 : Training: loss:  0.005136782\n",
      "5486 : Training: loss:  0.003681365\n",
      "5487 : Training: loss:  0.00436543\n",
      "5488 : Training: loss:  0.0063322717\n",
      "5489 : Training: loss:  0.008381807\n",
      "5490 : Training: loss:  0.0037748066\n",
      "5491 : Training: loss:  0.0043549486\n",
      "5492 : Training: loss:  0.0047319504\n",
      "5493 : Training: loss:  0.004133078\n",
      "5494 : Training: loss:  0.0072310055\n",
      "5495 : Training: loss:  0.0034007689\n",
      "5496 : Training: loss:  0.0057814387\n",
      "5497 : Training: loss:  0.0043463456\n",
      "5498 : Training: loss:  0.009220911\n",
      "5499 : Training: loss:  0.0044947094\n",
      "5500 : Training: loss:  0.007152778\n",
      "Validation: Loss:  0.018901922  Accuracy:  0.96153843\n",
      "5501 : Training: loss:  0.0038757604\n",
      "5502 : Training: loss:  0.009707205\n",
      "5503 : Training: loss:  0.0043358137\n",
      "5504 : Training: loss:  0.0076888083\n",
      "5505 : Training: loss:  0.006520644\n",
      "5506 : Training: loss:  0.005110223\n",
      "5507 : Training: loss:  0.010474715\n",
      "5508 : Training: loss:  0.0033260088\n",
      "5509 : Training: loss:  0.006431902\n",
      "5510 : Training: loss:  0.0064555584\n",
      "5511 : Training: loss:  0.004246235\n",
      "5512 : Training: loss:  0.004709057\n",
      "5513 : Training: loss:  0.004645113\n",
      "5514 : Training: loss:  0.0022905732\n",
      "5515 : Training: loss:  0.0045031575\n",
      "5516 : Training: loss:  0.0044984585\n",
      "5517 : Training: loss:  0.0056735906\n",
      "5518 : Training: loss:  0.005219734\n",
      "5519 : Training: loss:  0.0063376906\n",
      "5520 : Training: loss:  0.004712563\n",
      "Validation: Loss:  0.018746113  Accuracy:  0.96153843\n",
      "5521 : Training: loss:  0.0053870995\n",
      "5522 : Training: loss:  0.0053170067\n",
      "5523 : Training: loss:  0.003503247\n",
      "5524 : Training: loss:  0.0033662198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5525 : Training: loss:  0.006021766\n",
      "5526 : Training: loss:  0.008461903\n",
      "5527 : Training: loss:  0.005302778\n",
      "5528 : Training: loss:  0.004590466\n",
      "5529 : Training: loss:  0.008316843\n",
      "5530 : Training: loss:  0.005378435\n",
      "5531 : Training: loss:  0.005054712\n",
      "5532 : Training: loss:  0.005695079\n",
      "5533 : Training: loss:  0.004548848\n",
      "5534 : Training: loss:  0.0040407064\n",
      "5535 : Training: loss:  0.0056528086\n",
      "5536 : Training: loss:  0.004161508\n",
      "5537 : Training: loss:  0.006583875\n",
      "5538 : Training: loss:  0.004040387\n",
      "5539 : Training: loss:  0.0042754817\n",
      "5540 : Training: loss:  0.0036722908\n",
      "Validation: Loss:  0.018677702  Accuracy:  0.96153843\n",
      "5541 : Training: loss:  0.009736175\n",
      "5542 : Training: loss:  0.004845247\n",
      "5543 : Training: loss:  0.004958981\n",
      "5544 : Training: loss:  0.0038954674\n",
      "5545 : Training: loss:  0.0040782504\n",
      "5546 : Training: loss:  0.003317785\n",
      "5547 : Training: loss:  0.0040905126\n",
      "5548 : Training: loss:  0.0030619549\n",
      "5549 : Training: loss:  0.004031363\n",
      "5550 : Training: loss:  0.0034413112\n",
      "5551 : Training: loss:  0.005380305\n",
      "5552 : Training: loss:  0.005165572\n",
      "5553 : Training: loss:  0.002812193\n",
      "5554 : Training: loss:  0.0047277533\n",
      "5555 : Training: loss:  0.006386778\n",
      "5556 : Training: loss:  0.00476229\n",
      "5557 : Training: loss:  0.0053765457\n",
      "5558 : Training: loss:  0.002220702\n",
      "5559 : Training: loss:  0.006886256\n",
      "5560 : Training: loss:  0.006337088\n",
      "Validation: Loss:  0.018694008  Accuracy:  0.96153843\n",
      "5561 : Training: loss:  0.0050171427\n",
      "5562 : Training: loss:  0.005359554\n",
      "5563 : Training: loss:  0.0069942586\n",
      "5564 : Training: loss:  0.0068481225\n",
      "5565 : Training: loss:  0.0054478636\n",
      "5566 : Training: loss:  0.00430229\n",
      "5567 : Training: loss:  0.0044775037\n",
      "5568 : Training: loss:  0.0036338565\n",
      "5569 : Training: loss:  0.0055317343\n",
      "5570 : Training: loss:  0.0029871492\n",
      "5571 : Training: loss:  0.006645847\n",
      "5572 : Training: loss:  0.0045256456\n",
      "5573 : Training: loss:  0.003379525\n",
      "5574 : Training: loss:  0.0040258514\n",
      "5575 : Training: loss:  0.0038435105\n",
      "5576 : Training: loss:  0.0047518197\n",
      "5577 : Training: loss:  0.004882988\n",
      "5578 : Training: loss:  0.0036881703\n",
      "5579 : Training: loss:  0.0043177227\n",
      "5580 : Training: loss:  0.003901187\n",
      "Validation: Loss:  0.018671913  Accuracy:  0.96153843\n",
      "5581 : Training: loss:  0.005829257\n",
      "5582 : Training: loss:  0.0031707748\n",
      "5583 : Training: loss:  0.005219697\n",
      "5584 : Training: loss:  0.0060748104\n",
      "5585 : Training: loss:  0.006185423\n",
      "5586 : Training: loss:  0.004120136\n",
      "5587 : Training: loss:  0.008824217\n",
      "5588 : Training: loss:  0.004451654\n",
      "5589 : Training: loss:  0.0061871344\n",
      "5590 : Training: loss:  0.004945351\n",
      "5591 : Training: loss:  0.0056708124\n",
      "5592 : Training: loss:  0.004354208\n",
      "5593 : Training: loss:  0.003703226\n",
      "5594 : Training: loss:  0.0022843943\n",
      "5595 : Training: loss:  0.00180949\n",
      "5596 : Training: loss:  0.0021380868\n",
      "5597 : Training: loss:  0.005354039\n",
      "5598 : Training: loss:  0.0035395056\n",
      "5599 : Training: loss:  0.0046703727\n",
      "5600 : Training: loss:  0.004112057\n",
      "Validation: Loss:  0.01866236  Accuracy:  0.96153843\n",
      "5601 : Training: loss:  0.0069938237\n",
      "5602 : Training: loss:  0.006610862\n",
      "5603 : Training: loss:  0.002950954\n",
      "5604 : Training: loss:  0.004305028\n",
      "5605 : Training: loss:  0.0043570288\n",
      "5606 : Training: loss:  0.0029270067\n",
      "5607 : Training: loss:  0.0052551837\n",
      "5608 : Training: loss:  0.0040073213\n",
      "5609 : Training: loss:  0.0045210687\n",
      "5610 : Training: loss:  0.0054157237\n",
      "5611 : Training: loss:  0.0039944733\n",
      "5612 : Training: loss:  0.0027218384\n",
      "5613 : Training: loss:  0.0017963378\n",
      "5614 : Training: loss:  0.0060092737\n",
      "5615 : Training: loss:  0.00334138\n",
      "5616 : Training: loss:  0.003615582\n",
      "5617 : Training: loss:  0.0042236936\n",
      "5618 : Training: loss:  0.0045878817\n",
      "5619 : Training: loss:  0.0054606874\n",
      "5620 : Training: loss:  0.003945565\n",
      "Validation: Loss:  0.018578516  Accuracy:  0.96153843\n",
      "5621 : Training: loss:  0.0045744213\n",
      "5622 : Training: loss:  0.003599064\n",
      "5623 : Training: loss:  0.0024380942\n",
      "5624 : Training: loss:  0.003374454\n",
      "5625 : Training: loss:  0.0035805884\n",
      "5626 : Training: loss:  0.007873317\n",
      "5627 : Training: loss:  0.004571297\n",
      "5628 : Training: loss:  0.003373463\n",
      "5629 : Training: loss:  0.0047783786\n",
      "5630 : Training: loss:  0.0048031695\n",
      "5631 : Training: loss:  0.005109621\n",
      "5632 : Training: loss:  0.0050619775\n",
      "5633 : Training: loss:  0.0064686723\n",
      "5634 : Training: loss:  0.0041676126\n",
      "5635 : Training: loss:  0.0054151947\n",
      "5636 : Training: loss:  0.005541514\n",
      "5637 : Training: loss:  0.0028021603\n",
      "5638 : Training: loss:  0.0050362027\n",
      "5639 : Training: loss:  0.003937263\n",
      "5640 : Training: loss:  0.002978688\n",
      "Validation: Loss:  0.018556511  Accuracy:  0.96153843\n",
      "5641 : Training: loss:  0.0040645055\n",
      "5642 : Training: loss:  0.0049786237\n",
      "5643 : Training: loss:  0.005380081\n",
      "5644 : Training: loss:  0.005994897\n",
      "5645 : Training: loss:  0.0042555644\n",
      "5646 : Training: loss:  0.0036567831\n",
      "5647 : Training: loss:  0.0047500036\n",
      "5648 : Training: loss:  0.0046871523\n",
      "5649 : Training: loss:  0.0055758688\n",
      "5650 : Training: loss:  0.006452856\n",
      "5651 : Training: loss:  0.0035337433\n",
      "5652 : Training: loss:  0.010644005\n",
      "5653 : Training: loss:  0.005466694\n",
      "5654 : Training: loss:  0.0061043417\n",
      "5655 : Training: loss:  0.0037909243\n",
      "5656 : Training: loss:  0.00790558\n",
      "5657 : Training: loss:  0.0049633137\n",
      "5658 : Training: loss:  0.004288346\n",
      "5659 : Training: loss:  0.004518955\n",
      "5660 : Training: loss:  0.006184868\n",
      "Validation: Loss:  0.018505359  Accuracy:  0.96153843\n",
      "5661 : Training: loss:  0.0021633897\n",
      "5662 : Training: loss:  0.00560625\n",
      "5663 : Training: loss:  0.00647722\n",
      "5664 : Training: loss:  0.0074092415\n",
      "5665 : Training: loss:  0.0025739314\n",
      "5666 : Training: loss:  0.005410617\n",
      "5667 : Training: loss:  0.0055711735\n",
      "5668 : Training: loss:  0.0070211757\n",
      "5669 : Training: loss:  0.0037138413\n",
      "5670 : Training: loss:  0.005305677\n",
      "5671 : Training: loss:  0.0021899964\n",
      "5672 : Training: loss:  0.0022166132\n",
      "5673 : Training: loss:  0.0016787217\n",
      "5674 : Training: loss:  0.0064821113\n",
      "5675 : Training: loss:  0.006641215\n",
      "5676 : Training: loss:  0.0035131993\n",
      "5677 : Training: loss:  0.0024359939\n",
      "5678 : Training: loss:  0.005474022\n",
      "5679 : Training: loss:  0.0059431153\n",
      "5680 : Training: loss:  0.0033524786\n",
      "Validation: Loss:  0.018432291  Accuracy:  0.96153843\n",
      "5681 : Training: loss:  0.0046114507\n",
      "5682 : Training: loss:  0.004968987\n",
      "5683 : Training: loss:  0.005471521\n",
      "5684 : Training: loss:  0.007005173\n",
      "5685 : Training: loss:  0.008798612\n",
      "5686 : Training: loss:  0.0021509149\n",
      "5687 : Training: loss:  0.0040271636\n",
      "5688 : Training: loss:  0.0032300083\n",
      "5689 : Training: loss:  0.006173398\n",
      "5690 : Training: loss:  0.0031384623\n",
      "5691 : Training: loss:  0.005362639\n",
      "5692 : Training: loss:  0.0017050961\n",
      "5693 : Training: loss:  0.0050533856\n",
      "5694 : Training: loss:  0.0044235834\n",
      "5695 : Training: loss:  0.0036631136\n",
      "5696 : Training: loss:  0.005227791\n",
      "5697 : Training: loss:  0.004607282\n",
      "5698 : Training: loss:  0.0052597076\n",
      "5699 : Training: loss:  0.0047443477\n",
      "5700 : Training: loss:  0.0034955037\n",
      "Validation: Loss:  0.018352458  Accuracy:  0.96153843\n",
      "5701 : Training: loss:  0.00746935\n",
      "5702 : Training: loss:  0.0045037204\n",
      "5703 : Training: loss:  0.0050775427\n",
      "5704 : Training: loss:  0.0029652563\n",
      "5705 : Training: loss:  0.006331903\n",
      "5706 : Training: loss:  0.0042282017\n",
      "5707 : Training: loss:  0.0031333345\n",
      "5708 : Training: loss:  0.0027285109\n",
      "5709 : Training: loss:  0.0050186357\n",
      "5710 : Training: loss:  0.005732971\n",
      "5711 : Training: loss:  0.0033050196\n",
      "5712 : Training: loss:  0.0037438395\n",
      "5713 : Training: loss:  0.0035960707\n",
      "5714 : Training: loss:  0.0044745174\n",
      "5715 : Training: loss:  0.0023821818\n",
      "5716 : Training: loss:  0.004550935\n",
      "5717 : Training: loss:  0.0029504746\n",
      "5718 : Training: loss:  0.0030675707\n",
      "5719 : Training: loss:  0.006311864\n",
      "5720 : Training: loss:  0.0045274603\n",
      "Validation: Loss:  0.018285222  Accuracy:  0.96153843\n",
      "5721 : Training: loss:  0.00557627\n",
      "5722 : Training: loss:  0.0067627253\n",
      "5723 : Training: loss:  0.003034189\n",
      "5724 : Training: loss:  0.0028828816\n",
      "5725 : Training: loss:  0.0035544243\n",
      "5726 : Training: loss:  0.0031785655\n",
      "5727 : Training: loss:  0.0025317275\n",
      "5728 : Training: loss:  0.0031263118\n",
      "5729 : Training: loss:  0.002612328\n",
      "5730 : Training: loss:  0.005334028\n",
      "5731 : Training: loss:  0.0041550444\n",
      "5732 : Training: loss:  0.004082944\n",
      "5733 : Training: loss:  0.004776582\n",
      "5734 : Training: loss:  0.005545855\n",
      "5735 : Training: loss:  0.005362762\n",
      "5736 : Training: loss:  0.005903832\n",
      "5737 : Training: loss:  0.003718963\n",
      "5738 : Training: loss:  0.004800081\n",
      "5739 : Training: loss:  0.0033926538\n",
      "5740 : Training: loss:  0.0053213243\n",
      "Validation: Loss:  0.018262533  Accuracy:  0.96153843\n",
      "5741 : Training: loss:  0.005009171\n",
      "5742 : Training: loss:  0.004728075\n",
      "5743 : Training: loss:  0.004187824\n",
      "5744 : Training: loss:  0.0034965167\n",
      "5745 : Training: loss:  0.003915699\n",
      "5746 : Training: loss:  0.0024554804\n",
      "5747 : Training: loss:  0.0042883116\n",
      "5748 : Training: loss:  0.005773426\n",
      "5749 : Training: loss:  0.0037930773\n",
      "5750 : Training: loss:  0.005449069\n",
      "5751 : Training: loss:  0.008893333\n",
      "5752 : Training: loss:  0.0052063293\n",
      "5753 : Training: loss:  0.0059821033\n",
      "5754 : Training: loss:  0.003791741\n",
      "5755 : Training: loss:  0.0039156335\n",
      "5756 : Training: loss:  0.0051017413\n",
      "5757 : Training: loss:  0.0028754836\n",
      "5758 : Training: loss:  0.0049917586\n",
      "5759 : Training: loss:  0.0051325685\n",
      "5760 : Training: loss:  0.00266491\n",
      "Validation: Loss:  0.018297896  Accuracy:  0.96153843\n",
      "5761 : Training: loss:  0.0077575627\n",
      "5762 : Training: loss:  0.0044867992\n",
      "5763 : Training: loss:  0.004430247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5764 : Training: loss:  0.0043774284\n",
      "5765 : Training: loss:  0.0032297496\n",
      "5766 : Training: loss:  0.004369886\n",
      "5767 : Training: loss:  0.0027379754\n",
      "5768 : Training: loss:  0.0042469827\n",
      "5769 : Training: loss:  0.0044638664\n",
      "5770 : Training: loss:  0.006278495\n",
      "5771 : Training: loss:  0.0049875733\n",
      "5772 : Training: loss:  0.0021586942\n",
      "5773 : Training: loss:  0.0021554278\n",
      "5774 : Training: loss:  0.0033458886\n",
      "5775 : Training: loss:  0.0044986894\n",
      "5776 : Training: loss:  0.0037415877\n",
      "5777 : Training: loss:  0.0030214968\n",
      "5778 : Training: loss:  0.0060754973\n",
      "5779 : Training: loss:  0.006145277\n",
      "5780 : Training: loss:  0.005076179\n",
      "Validation: Loss:  0.01818813  Accuracy:  0.96153843\n",
      "5781 : Training: loss:  0.0049033095\n",
      "5782 : Training: loss:  0.0048442604\n",
      "5783 : Training: loss:  0.0024127094\n",
      "5784 : Training: loss:  0.0029336095\n",
      "5785 : Training: loss:  0.0039089685\n",
      "5786 : Training: loss:  0.0037556246\n",
      "5787 : Training: loss:  0.004234045\n",
      "5788 : Training: loss:  0.0047548153\n",
      "5789 : Training: loss:  0.003916492\n",
      "5790 : Training: loss:  0.002818871\n",
      "5791 : Training: loss:  0.0022945446\n",
      "5792 : Training: loss:  0.0024806177\n",
      "5793 : Training: loss:  0.0050026127\n",
      "5794 : Training: loss:  0.0054834154\n",
      "5795 : Training: loss:  0.0022779508\n",
      "5796 : Training: loss:  0.0040990193\n",
      "5797 : Training: loss:  0.0033134932\n",
      "5798 : Training: loss:  0.004818315\n",
      "5799 : Training: loss:  0.0039713\n",
      "5800 : Training: loss:  0.0036863228\n",
      "Validation: Loss:  0.01813153  Accuracy:  0.96153843\n",
      "5801 : Training: loss:  0.0037879213\n",
      "5802 : Training: loss:  0.0078003425\n",
      "5803 : Training: loss:  0.0073988\n",
      "5804 : Training: loss:  0.002397998\n",
      "5805 : Training: loss:  0.0036896786\n",
      "5806 : Training: loss:  0.0035641403\n",
      "5807 : Training: loss:  0.0025190483\n",
      "5808 : Training: loss:  0.0034760924\n",
      "5809 : Training: loss:  0.0023827127\n",
      "5810 : Training: loss:  0.0059302542\n",
      "5811 : Training: loss:  0.0031168533\n",
      "5812 : Training: loss:  0.0044402312\n",
      "5813 : Training: loss:  0.0028650304\n",
      "5814 : Training: loss:  0.002391519\n",
      "5815 : Training: loss:  0.0016266892\n",
      "5816 : Training: loss:  0.0047135185\n",
      "5817 : Training: loss:  0.0017649139\n",
      "5818 : Training: loss:  0.003824159\n",
      "5819 : Training: loss:  0.0047042193\n",
      "5820 : Training: loss:  0.004138943\n",
      "Validation: Loss:  0.018200807  Accuracy:  0.96153843\n",
      "5821 : Training: loss:  0.0050754095\n",
      "5822 : Training: loss:  0.0022183638\n",
      "5823 : Training: loss:  0.0030861\n",
      "5824 : Training: loss:  0.0044021257\n",
      "5825 : Training: loss:  0.006287819\n",
      "5826 : Training: loss:  0.0032385427\n",
      "5827 : Training: loss:  0.0031117708\n",
      "5828 : Training: loss:  0.004154366\n",
      "5829 : Training: loss:  0.00469032\n",
      "5830 : Training: loss:  0.0024919068\n",
      "5831 : Training: loss:  0.004118448\n",
      "5832 : Training: loss:  0.0024588963\n",
      "5833 : Training: loss:  0.0063027195\n",
      "5834 : Training: loss:  0.0044996175\n",
      "5835 : Training: loss:  0.005908049\n",
      "5836 : Training: loss:  0.003577881\n",
      "5837 : Training: loss:  0.0055544027\n",
      "5838 : Training: loss:  0.0034672006\n",
      "5839 : Training: loss:  0.0040754764\n",
      "5840 : Training: loss:  0.0040531456\n",
      "Validation: Loss:  0.018164773  Accuracy:  0.96153843\n",
      "5841 : Training: loss:  0.003023341\n",
      "5842 : Training: loss:  0.0035756065\n",
      "5843 : Training: loss:  0.002282669\n",
      "5844 : Training: loss:  0.005473291\n",
      "5845 : Training: loss:  0.0028247514\n",
      "5846 : Training: loss:  0.004596976\n",
      "5847 : Training: loss:  0.0045327535\n",
      "5848 : Training: loss:  0.0045465915\n",
      "5849 : Training: loss:  0.0019037431\n",
      "5850 : Training: loss:  0.0041884608\n",
      "5851 : Training: loss:  0.005217471\n",
      "5852 : Training: loss:  0.0038976495\n",
      "5853 : Training: loss:  0.0035750489\n",
      "5854 : Training: loss:  0.0048451596\n",
      "5855 : Training: loss:  0.0040083393\n",
      "5856 : Training: loss:  0.0067287153\n",
      "5857 : Training: loss:  0.0039331517\n",
      "5858 : Training: loss:  0.003355678\n",
      "5859 : Training: loss:  0.0036888612\n",
      "5860 : Training: loss:  0.0033242088\n",
      "Validation: Loss:  0.01808578  Accuracy:  0.96153843\n",
      "5861 : Training: loss:  0.0077444254\n",
      "5862 : Training: loss:  0.00506979\n",
      "5863 : Training: loss:  0.004833997\n",
      "5864 : Training: loss:  0.0030805424\n",
      "5865 : Training: loss:  0.0021848474\n",
      "5866 : Training: loss:  0.0019793783\n",
      "5867 : Training: loss:  0.0042253593\n",
      "5868 : Training: loss:  0.0041038864\n",
      "5869 : Training: loss:  0.0030977547\n",
      "5870 : Training: loss:  0.004325735\n",
      "5871 : Training: loss:  0.003515649\n",
      "5872 : Training: loss:  0.0052974015\n",
      "5873 : Training: loss:  0.0022769456\n",
      "5874 : Training: loss:  0.004405796\n",
      "5875 : Training: loss:  0.006388173\n",
      "5876 : Training: loss:  0.0054938444\n",
      "5877 : Training: loss:  0.004778854\n",
      "5878 : Training: loss:  0.0050908914\n",
      "5879 : Training: loss:  0.005119411\n",
      "5880 : Training: loss:  0.004936626\n",
      "Validation: Loss:  0.018117903  Accuracy:  0.96153843\n",
      "5881 : Training: loss:  0.0037135947\n",
      "5882 : Training: loss:  0.0026649823\n",
      "5883 : Training: loss:  0.004767395\n",
      "5884 : Training: loss:  0.004942049\n",
      "5885 : Training: loss:  0.002500978\n",
      "5886 : Training: loss:  0.002638583\n",
      "5887 : Training: loss:  0.0035022232\n",
      "5888 : Training: loss:  0.00418433\n",
      "5889 : Training: loss:  0.005605704\n",
      "5890 : Training: loss:  0.0023262198\n",
      "5891 : Training: loss:  0.0070578316\n",
      "5892 : Training: loss:  0.004536015\n",
      "5893 : Training: loss:  0.0071966387\n",
      "5894 : Training: loss:  0.002213937\n",
      "5895 : Training: loss:  0.003920815\n",
      "5896 : Training: loss:  0.004194625\n",
      "5897 : Training: loss:  0.0048849317\n",
      "5898 : Training: loss:  0.003933731\n",
      "5899 : Training: loss:  0.00626353\n",
      "5900 : Training: loss:  0.00533101\n",
      "Validation: Loss:  0.018138304  Accuracy:  0.96153843\n",
      "5901 : Training: loss:  0.0039025059\n",
      "5902 : Training: loss:  0.0029707095\n",
      "5903 : Training: loss:  0.003922242\n",
      "5904 : Training: loss:  0.0035604495\n",
      "5905 : Training: loss:  0.0033933907\n",
      "5906 : Training: loss:  0.0029343674\n",
      "5907 : Training: loss:  0.002834448\n",
      "5908 : Training: loss:  0.003251822\n",
      "5909 : Training: loss:  0.0022100206\n",
      "5910 : Training: loss:  0.0038634478\n",
      "5911 : Training: loss:  0.007048102\n",
      "5912 : Training: loss:  0.005830566\n",
      "5913 : Training: loss:  0.0073559503\n",
      "5914 : Training: loss:  0.00166184\n",
      "5915 : Training: loss:  0.0038644897\n",
      "5916 : Training: loss:  0.00692105\n",
      "5917 : Training: loss:  0.0056362683\n",
      "5918 : Training: loss:  0.0035904052\n",
      "5919 : Training: loss:  0.0039186846\n",
      "5920 : Training: loss:  0.0043119434\n",
      "Validation: Loss:  0.018045982  Accuracy:  0.96153843\n",
      "5921 : Training: loss:  0.0051076035\n",
      "5922 : Training: loss:  0.004724318\n",
      "5923 : Training: loss:  0.00475478\n",
      "5924 : Training: loss:  0.005636288\n",
      "5925 : Training: loss:  0.0065507\n",
      "5926 : Training: loss:  0.0023785045\n",
      "5927 : Training: loss:  0.0010808423\n",
      "5928 : Training: loss:  0.0051021073\n",
      "5929 : Training: loss:  0.0036063564\n",
      "5930 : Training: loss:  0.0032274995\n",
      "5931 : Training: loss:  0.0026234966\n",
      "5932 : Training: loss:  0.0046862476\n",
      "5933 : Training: loss:  0.0056163096\n",
      "5934 : Training: loss:  0.0034507273\n",
      "5935 : Training: loss:  0.0021568416\n",
      "5936 : Training: loss:  0.002549261\n",
      "5937 : Training: loss:  0.0046545747\n",
      "5938 : Training: loss:  0.0042025335\n",
      "5939 : Training: loss:  0.003977151\n",
      "5940 : Training: loss:  0.0028097495\n",
      "Validation: Loss:  0.017992012  Accuracy:  0.96153843\n",
      "5941 : Training: loss:  0.0033051793\n",
      "5942 : Training: loss:  0.0022666126\n",
      "5943 : Training: loss:  0.004631085\n",
      "5944 : Training: loss:  0.0023708537\n",
      "5945 : Training: loss:  0.0042698774\n",
      "5946 : Training: loss:  0.004465169\n",
      "5947 : Training: loss:  0.0047393846\n",
      "5948 : Training: loss:  0.00293635\n",
      "5949 : Training: loss:  0.0034626294\n",
      "5950 : Training: loss:  0.0037060215\n",
      "5951 : Training: loss:  0.0029735912\n",
      "5952 : Training: loss:  0.0033966978\n",
      "5953 : Training: loss:  0.0026462881\n",
      "5954 : Training: loss:  0.003139825\n",
      "5955 : Training: loss:  0.0036839002\n",
      "5956 : Training: loss:  0.0045986767\n",
      "5957 : Training: loss:  0.0038309598\n",
      "5958 : Training: loss:  0.0048468765\n",
      "5959 : Training: loss:  0.004182062\n",
      "5960 : Training: loss:  0.002956806\n",
      "Validation: Loss:  0.017911436  Accuracy:  0.96153843\n",
      "5961 : Training: loss:  0.0048202802\n",
      "5962 : Training: loss:  0.0039091515\n",
      "5963 : Training: loss:  0.0027832473\n",
      "5964 : Training: loss:  0.0045927106\n",
      "5965 : Training: loss:  0.0037003814\n",
      "5966 : Training: loss:  0.0045838617\n",
      "5967 : Training: loss:  0.0038842827\n",
      "5968 : Training: loss:  0.0017243072\n",
      "5969 : Training: loss:  0.0025354412\n",
      "5970 : Training: loss:  0.0025684491\n",
      "5971 : Training: loss:  0.006862237\n",
      "5972 : Training: loss:  0.004258206\n",
      "5973 : Training: loss:  0.0031971822\n",
      "5974 : Training: loss:  0.004270717\n",
      "5975 : Training: loss:  0.0043605473\n",
      "5976 : Training: loss:  0.0025430142\n",
      "5977 : Training: loss:  0.006031462\n",
      "5978 : Training: loss:  0.0029745135\n",
      "5979 : Training: loss:  0.0032890583\n",
      "5980 : Training: loss:  0.0029176804\n",
      "Validation: Loss:  0.017804353  Accuracy:  0.96153843\n",
      "5981 : Training: loss:  0.0030739098\n",
      "5982 : Training: loss:  0.004161532\n",
      "5983 : Training: loss:  0.004278191\n",
      "5984 : Training: loss:  0.004651418\n",
      "5985 : Training: loss:  0.002902191\n",
      "5986 : Training: loss:  0.003060986\n",
      "5987 : Training: loss:  0.004294225\n",
      "5988 : Training: loss:  0.00322265\n",
      "5989 : Training: loss:  0.0040520094\n",
      "5990 : Training: loss:  0.0025890244\n",
      "5991 : Training: loss:  0.0063782237\n",
      "5992 : Training: loss:  0.004483614\n",
      "5993 : Training: loss:  0.003792836\n",
      "5994 : Training: loss:  0.003052946\n",
      "5995 : Training: loss:  0.004080155\n",
      "5996 : Training: loss:  0.0036596556\n",
      "5997 : Training: loss:  0.0044932533\n",
      "5998 : Training: loss:  0.0018322789\n",
      "5999 : Training: loss:  0.00428789\n",
      "6000 : Training: loss:  0.0038901942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss:  0.017722972  Accuracy:  0.96153843\n",
      "6001 : Training: loss:  0.0049230885\n",
      "6002 : Training: loss:  0.0029003308\n",
      "6003 : Training: loss:  0.003988989\n",
      "6004 : Training: loss:  0.0027457622\n",
      "6005 : Training: loss:  0.0029962366\n",
      "6006 : Training: loss:  0.0034741228\n",
      "6007 : Training: loss:  0.0036306253\n",
      "6008 : Training: loss:  0.0028277244\n",
      "6009 : Training: loss:  0.0040160245\n",
      "6010 : Training: loss:  0.0024871412\n",
      "6011 : Training: loss:  0.0032970263\n",
      "6012 : Training: loss:  0.0048012016\n",
      "6013 : Training: loss:  0.0036130773\n",
      "6014 : Training: loss:  0.003900283\n",
      "6015 : Training: loss:  0.003989709\n",
      "6016 : Training: loss:  0.0027601747\n",
      "6017 : Training: loss:  0.003645652\n",
      "6018 : Training: loss:  0.0024640558\n",
      "6019 : Training: loss:  0.003529365\n",
      "6020 : Training: loss:  0.0034722146\n",
      "Validation: Loss:  0.017690029  Accuracy:  0.96153843\n",
      "6021 : Training: loss:  0.0040044216\n",
      "6022 : Training: loss:  0.003204626\n",
      "6023 : Training: loss:  0.0028890339\n",
      "6024 : Training: loss:  0.0024625545\n",
      "6025 : Training: loss:  0.0020433269\n",
      "6026 : Training: loss:  0.0045539495\n",
      "6027 : Training: loss:  0.003567363\n",
      "6028 : Training: loss:  0.002650163\n",
      "6029 : Training: loss:  0.0038890045\n",
      "6030 : Training: loss:  0.0021053152\n",
      "6031 : Training: loss:  0.0047781556\n",
      "6032 : Training: loss:  0.006388327\n",
      "6033 : Training: loss:  0.0046171597\n",
      "6034 : Training: loss:  0.0053999997\n",
      "6035 : Training: loss:  0.002977831\n",
      "6036 : Training: loss:  0.0048660524\n",
      "6037 : Training: loss:  0.004508761\n",
      "6038 : Training: loss:  0.0030635444\n",
      "6039 : Training: loss:  0.0020782028\n",
      "6040 : Training: loss:  0.0064622276\n",
      "Validation: Loss:  0.017650189  Accuracy:  0.96153843\n",
      "6041 : Training: loss:  0.005723012\n",
      "6042 : Training: loss:  0.0029942254\n",
      "6043 : Training: loss:  0.0039851535\n",
      "6044 : Training: loss:  0.004329073\n",
      "6045 : Training: loss:  0.0031733613\n",
      "6046 : Training: loss:  0.0012436879\n",
      "6047 : Training: loss:  0.0035771935\n",
      "6048 : Training: loss:  0.0024797006\n",
      "6049 : Training: loss:  0.0046728374\n",
      "6050 : Training: loss:  0.0026189068\n",
      "6051 : Training: loss:  0.001898668\n",
      "6052 : Training: loss:  0.005901931\n",
      "6053 : Training: loss:  0.0033238542\n",
      "6054 : Training: loss:  0.0025376396\n",
      "6055 : Training: loss:  0.0031968628\n",
      "6056 : Training: loss:  0.0043209214\n",
      "6057 : Training: loss:  0.004072835\n",
      "6058 : Training: loss:  0.001917121\n",
      "6059 : Training: loss:  0.0039142827\n",
      "6060 : Training: loss:  0.0021567543\n",
      "Validation: Loss:  0.017651405  Accuracy:  0.96153843\n",
      "6061 : Training: loss:  0.003009549\n",
      "6062 : Training: loss:  0.0026104345\n",
      "6063 : Training: loss:  0.004310422\n",
      "6064 : Training: loss:  0.0019509338\n",
      "6065 : Training: loss:  0.0032847305\n",
      "6066 : Training: loss:  0.0030699675\n",
      "6067 : Training: loss:  0.0031952958\n",
      "6068 : Training: loss:  0.0035887575\n",
      "6069 : Training: loss:  0.0029627145\n",
      "6070 : Training: loss:  0.0039841216\n",
      "6071 : Training: loss:  0.004445685\n",
      "6072 : Training: loss:  0.002025256\n",
      "6073 : Training: loss:  0.002274108\n",
      "6074 : Training: loss:  0.002009302\n",
      "6075 : Training: loss:  0.0020106295\n",
      "6076 : Training: loss:  0.003768164\n",
      "6077 : Training: loss:  0.0036601364\n",
      "6078 : Training: loss:  0.0034295898\n",
      "6079 : Training: loss:  0.0025375169\n",
      "6080 : Training: loss:  0.002633942\n",
      "Validation: Loss:  0.017559564  Accuracy:  0.96153843\n",
      "6081 : Training: loss:  0.0025893832\n",
      "6082 : Training: loss:  0.0034133375\n",
      "6083 : Training: loss:  0.0026328003\n",
      "6084 : Training: loss:  0.0033465049\n",
      "6085 : Training: loss:  0.004350143\n",
      "6086 : Training: loss:  0.003040732\n",
      "6087 : Training: loss:  0.0042859665\n",
      "6088 : Training: loss:  0.004740245\n",
      "6089 : Training: loss:  0.0039408584\n",
      "6090 : Training: loss:  0.0018261404\n",
      "6091 : Training: loss:  0.002443138\n",
      "6092 : Training: loss:  0.0040256153\n",
      "6093 : Training: loss:  0.0033073574\n",
      "6094 : Training: loss:  0.00396715\n",
      "6095 : Training: loss:  0.0029040442\n",
      "6096 : Training: loss:  0.0048708585\n",
      "6097 : Training: loss:  0.0033084962\n",
      "6098 : Training: loss:  0.0044678547\n",
      "6099 : Training: loss:  0.0028831018\n",
      "6100 : Training: loss:  0.0030572542\n",
      "Validation: Loss:  0.017488522  Accuracy:  0.96153843\n",
      "6101 : Training: loss:  0.0038408968\n",
      "6102 : Training: loss:  0.0033707812\n",
      "6103 : Training: loss:  0.002013374\n",
      "6104 : Training: loss:  0.0046970774\n",
      "6105 : Training: loss:  0.0034848107\n",
      "6106 : Training: loss:  0.0030628203\n",
      "6107 : Training: loss:  0.0031425091\n",
      "6108 : Training: loss:  0.003732476\n",
      "6109 : Training: loss:  0.0030740495\n",
      "6110 : Training: loss:  0.003933074\n",
      "6111 : Training: loss:  0.0037885278\n",
      "6112 : Training: loss:  0.004115261\n",
      "6113 : Training: loss:  0.0043224506\n",
      "6114 : Training: loss:  0.003551952\n",
      "6115 : Training: loss:  0.0014605538\n",
      "6116 : Training: loss:  0.0043338398\n",
      "6117 : Training: loss:  0.003250039\n",
      "6118 : Training: loss:  0.003616176\n",
      "6119 : Training: loss:  0.0061225328\n",
      "6120 : Training: loss:  0.0045868605\n",
      "Validation: Loss:  0.017491858  Accuracy:  0.96153843\n",
      "6121 : Training: loss:  0.0021895973\n",
      "6122 : Training: loss:  0.002932215\n",
      "6123 : Training: loss:  0.0026054636\n",
      "6124 : Training: loss:  0.0024138463\n",
      "6125 : Training: loss:  0.0020245016\n",
      "6126 : Training: loss:  0.0035781083\n",
      "6127 : Training: loss:  0.0024178745\n",
      "6128 : Training: loss:  0.0032865524\n",
      "6129 : Training: loss:  0.002040592\n",
      "6130 : Training: loss:  0.0022120352\n",
      "6131 : Training: loss:  0.0041638766\n",
      "6132 : Training: loss:  0.004247613\n",
      "6133 : Training: loss:  0.0028587033\n",
      "6134 : Training: loss:  0.004197816\n",
      "6135 : Training: loss:  0.008824854\n",
      "6136 : Training: loss:  0.0054203244\n",
      "6137 : Training: loss:  0.0028432715\n",
      "6138 : Training: loss:  0.005249748\n",
      "6139 : Training: loss:  0.002707826\n",
      "6140 : Training: loss:  0.0030557106\n",
      "Validation: Loss:  0.017444422  Accuracy:  0.96153843\n",
      "6141 : Training: loss:  0.0029460092\n",
      "6142 : Training: loss:  0.0017572818\n",
      "6143 : Training: loss:  0.0029712121\n",
      "6144 : Training: loss:  0.0030344983\n",
      "6145 : Training: loss:  0.002818827\n",
      "6146 : Training: loss:  0.0032380868\n",
      "6147 : Training: loss:  0.003322029\n",
      "6148 : Training: loss:  0.0041779107\n",
      "6149 : Training: loss:  0.0017960853\n",
      "6150 : Training: loss:  0.0036631546\n",
      "6151 : Training: loss:  0.0040411265\n",
      "6152 : Training: loss:  0.0016860524\n",
      "6153 : Training: loss:  0.0041261963\n",
      "6154 : Training: loss:  0.0049209925\n",
      "6155 : Training: loss:  0.004287121\n",
      "6156 : Training: loss:  0.0025067972\n",
      "6157 : Training: loss:  0.0037458153\n",
      "6158 : Training: loss:  0.003084369\n",
      "6159 : Training: loss:  0.003280781\n",
      "6160 : Training: loss:  0.0016219766\n",
      "Validation: Loss:  0.017467868  Accuracy:  0.96153843\n",
      "6161 : Training: loss:  0.0027813509\n",
      "6162 : Training: loss:  0.0029476176\n",
      "6163 : Training: loss:  0.0015357217\n",
      "6164 : Training: loss:  0.0023585393\n",
      "6165 : Training: loss:  0.0027236934\n",
      "6166 : Training: loss:  0.004887741\n",
      "6167 : Training: loss:  0.0026913064\n",
      "6168 : Training: loss:  0.0028556054\n",
      "6169 : Training: loss:  0.003858985\n",
      "6170 : Training: loss:  0.0020211919\n",
      "6171 : Training: loss:  0.002914691\n",
      "6172 : Training: loss:  0.002916379\n",
      "6173 : Training: loss:  0.0039559957\n",
      "6174 : Training: loss:  0.0025831775\n",
      "6175 : Training: loss:  0.001713346\n",
      "6176 : Training: loss:  0.006516729\n",
      "6177 : Training: loss:  0.0044315094\n",
      "6178 : Training: loss:  0.0041022412\n",
      "6179 : Training: loss:  0.0019273828\n",
      "6180 : Training: loss:  0.0040500886\n",
      "Validation: Loss:  0.017445868  Accuracy:  0.96153843\n",
      "6181 : Training: loss:  0.0028622495\n",
      "6182 : Training: loss:  0.002149444\n",
      "6183 : Training: loss:  0.0024403548\n",
      "6184 : Training: loss:  0.0028284611\n",
      "6185 : Training: loss:  0.0029258027\n",
      "6186 : Training: loss:  0.003507875\n",
      "6187 : Training: loss:  0.0029679148\n",
      "6188 : Training: loss:  0.005752025\n",
      "6189 : Training: loss:  0.0029108447\n",
      "6190 : Training: loss:  0.0044640456\n",
      "6191 : Training: loss:  0.0037858053\n",
      "6192 : Training: loss:  0.0031470188\n",
      "6193 : Training: loss:  0.005066717\n",
      "6194 : Training: loss:  0.004136417\n",
      "6195 : Training: loss:  0.004778278\n",
      "6196 : Training: loss:  0.005094101\n",
      "6197 : Training: loss:  0.0029911592\n",
      "6198 : Training: loss:  0.004092964\n",
      "6199 : Training: loss:  0.004666746\n",
      "6200 : Training: loss:  0.0060060006\n",
      "Validation: Loss:  0.01740369  Accuracy:  0.96153843\n",
      "6201 : Training: loss:  0.0031296266\n",
      "6202 : Training: loss:  0.0022473603\n",
      "6203 : Training: loss:  0.003407972\n",
      "6204 : Training: loss:  0.002922353\n",
      "6205 : Training: loss:  0.0035369198\n",
      "6206 : Training: loss:  0.0028648456\n",
      "6207 : Training: loss:  0.004315291\n",
      "6208 : Training: loss:  0.0031910867\n",
      "6209 : Training: loss:  0.0026302931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6210 : Training: loss:  0.0029318582\n",
      "6211 : Training: loss:  0.0024345254\n",
      "6212 : Training: loss:  0.0041583814\n",
      "6213 : Training: loss:  0.002907206\n",
      "6214 : Training: loss:  0.0037417274\n",
      "6215 : Training: loss:  0.0043804576\n",
      "6216 : Training: loss:  0.0025084\n",
      "6217 : Training: loss:  0.0017025992\n",
      "6218 : Training: loss:  0.00269304\n",
      "6219 : Training: loss:  0.0035779446\n",
      "6220 : Training: loss:  0.0032060505\n",
      "Validation: Loss:  0.017394884  Accuracy:  0.96153843\n",
      "6221 : Training: loss:  0.0021275813\n",
      "6222 : Training: loss:  0.0027785534\n",
      "6223 : Training: loss:  0.0025864372\n",
      "6224 : Training: loss:  0.0022580419\n",
      "6225 : Training: loss:  0.0030873911\n",
      "6226 : Training: loss:  0.0036270365\n",
      "6227 : Training: loss:  0.0040135463\n",
      "6228 : Training: loss:  0.0029744927\n",
      "6229 : Training: loss:  0.0023432241\n",
      "6230 : Training: loss:  0.003610376\n",
      "6231 : Training: loss:  0.0023796486\n",
      "6232 : Training: loss:  0.0038650462\n",
      "6233 : Training: loss:  0.0027406334\n",
      "6234 : Training: loss:  0.0011057825\n",
      "6235 : Training: loss:  0.0038954148\n",
      "6236 : Training: loss:  0.005089146\n",
      "6237 : Training: loss:  0.0031334849\n",
      "6238 : Training: loss:  0.0031586846\n",
      "6239 : Training: loss:  0.0022237925\n",
      "6240 : Training: loss:  0.0030481012\n",
      "Validation: Loss:  0.017310336  Accuracy:  0.96153843\n",
      "6241 : Training: loss:  0.0045599053\n",
      "6242 : Training: loss:  0.002411063\n",
      "6243 : Training: loss:  0.0029491899\n",
      "6244 : Training: loss:  0.00394854\n",
      "6245 : Training: loss:  0.0028618395\n",
      "6246 : Training: loss:  0.0033392\n",
      "6247 : Training: loss:  0.0013732921\n",
      "6248 : Training: loss:  0.0025456022\n",
      "6249 : Training: loss:  0.001876388\n",
      "6250 : Training: loss:  0.0021308544\n",
      "6251 : Training: loss:  0.0034750004\n",
      "6252 : Training: loss:  0.0016338826\n",
      "6253 : Training: loss:  0.0025179058\n",
      "6254 : Training: loss:  0.0049551926\n",
      "6255 : Training: loss:  0.0034124374\n",
      "6256 : Training: loss:  0.0023947144\n",
      "6257 : Training: loss:  0.004702905\n",
      "6258 : Training: loss:  0.0028875272\n",
      "6259 : Training: loss:  0.003020397\n",
      "6260 : Training: loss:  0.0047209305\n",
      "Validation: Loss:  0.017222743  Accuracy:  0.96153843\n",
      "6261 : Training: loss:  0.0045883902\n",
      "6262 : Training: loss:  0.0026363598\n",
      "6263 : Training: loss:  0.0025361453\n",
      "6264 : Training: loss:  0.0027163748\n",
      "6265 : Training: loss:  0.0031228275\n",
      "6266 : Training: loss:  0.008511536\n",
      "6267 : Training: loss:  0.0017179104\n",
      "6268 : Training: loss:  0.0032970896\n",
      "6269 : Training: loss:  0.0022612547\n",
      "6270 : Training: loss:  0.003664499\n",
      "6271 : Training: loss:  0.0037665109\n",
      "6272 : Training: loss:  0.003834012\n",
      "6273 : Training: loss:  0.0030596787\n",
      "6274 : Training: loss:  0.0052830754\n",
      "6275 : Training: loss:  0.0013228416\n",
      "6276 : Training: loss:  0.0027427445\n",
      "6277 : Training: loss:  0.0014552297\n",
      "6278 : Training: loss:  0.004197706\n",
      "6279 : Training: loss:  0.0030343968\n",
      "6280 : Training: loss:  0.0021789817\n",
      "Validation: Loss:  0.017177325  Accuracy:  0.96153843\n",
      "6281 : Training: loss:  0.0022596943\n",
      "6282 : Training: loss:  0.0023289842\n",
      "6283 : Training: loss:  0.0015362443\n",
      "6284 : Training: loss:  0.003308429\n",
      "6285 : Training: loss:  0.0042744945\n",
      "6286 : Training: loss:  0.0034496665\n",
      "6287 : Training: loss:  0.0019765298\n",
      "6288 : Training: loss:  0.0022695945\n",
      "6289 : Training: loss:  0.002164784\n",
      "6290 : Training: loss:  0.0035345408\n",
      "6291 : Training: loss:  0.0030109554\n",
      "6292 : Training: loss:  0.0027285307\n",
      "6293 : Training: loss:  0.0023461387\n",
      "6294 : Training: loss:  0.0055465256\n",
      "6295 : Training: loss:  0.0050511304\n",
      "6296 : Training: loss:  0.0020838033\n",
      "6297 : Training: loss:  0.0027800906\n",
      "6298 : Training: loss:  0.0028811004\n",
      "6299 : Training: loss:  0.0031731871\n",
      "6300 : Training: loss:  0.0033650494\n",
      "Validation: Loss:  0.017188782  Accuracy:  0.96153843\n",
      "6301 : Training: loss:  0.0035602346\n",
      "6302 : Training: loss:  0.0026790737\n",
      "6303 : Training: loss:  0.0054188212\n",
      "6304 : Training: loss:  0.005946801\n",
      "6305 : Training: loss:  0.003447515\n",
      "6306 : Training: loss:  0.0035721557\n",
      "6307 : Training: loss:  0.0043027904\n",
      "6308 : Training: loss:  0.0033162537\n",
      "6309 : Training: loss:  0.0028665727\n",
      "6310 : Training: loss:  0.0019075766\n",
      "6311 : Training: loss:  0.0044341316\n",
      "6312 : Training: loss:  0.0026202763\n",
      "6313 : Training: loss:  0.003530446\n",
      "6314 : Training: loss:  0.002946392\n",
      "6315 : Training: loss:  0.0024421634\n",
      "6316 : Training: loss:  0.0033608759\n",
      "6317 : Training: loss:  0.0041638\n",
      "6318 : Training: loss:  0.0015264148\n",
      "6319 : Training: loss:  0.0008798854\n",
      "6320 : Training: loss:  0.0018132306\n",
      "Validation: Loss:  0.017113794  Accuracy:  0.96153843\n",
      "6321 : Training: loss:  0.0029166578\n",
      "6322 : Training: loss:  0.003760201\n",
      "6323 : Training: loss:  0.0033667833\n",
      "6324 : Training: loss:  0.0017871754\n",
      "6325 : Training: loss:  0.0015849979\n",
      "6326 : Training: loss:  0.00359014\n",
      "6327 : Training: loss:  0.0054401467\n",
      "6328 : Training: loss:  0.0025907964\n",
      "6329 : Training: loss:  0.003521047\n",
      "6330 : Training: loss:  0.0024859447\n",
      "6331 : Training: loss:  0.0025411632\n",
      "6332 : Training: loss:  0.003737345\n",
      "6333 : Training: loss:  0.0025977313\n",
      "6334 : Training: loss:  0.002890399\n",
      "6335 : Training: loss:  0.0038385226\n",
      "6336 : Training: loss:  0.0018501674\n",
      "6337 : Training: loss:  0.0014844695\n",
      "6338 : Training: loss:  0.0041639768\n",
      "6339 : Training: loss:  0.003585144\n",
      "6340 : Training: loss:  0.0025319103\n",
      "Validation: Loss:  0.017050005  Accuracy:  0.96153843\n",
      "6341 : Training: loss:  0.0028094163\n",
      "6342 : Training: loss:  0.0028813076\n",
      "6343 : Training: loss:  0.00391622\n",
      "6344 : Training: loss:  0.002409035\n",
      "6345 : Training: loss:  0.0020932567\n",
      "6346 : Training: loss:  0.0026477997\n",
      "6347 : Training: loss:  0.0016313993\n",
      "6348 : Training: loss:  0.003035291\n",
      "6349 : Training: loss:  0.001931894\n",
      "6350 : Training: loss:  0.002500308\n",
      "6351 : Training: loss:  0.0051850523\n",
      "6352 : Training: loss:  0.004046169\n",
      "6353 : Training: loss:  0.0025835803\n",
      "6354 : Training: loss:  0.002404569\n",
      "6355 : Training: loss:  0.0011138095\n",
      "6356 : Training: loss:  0.0037124369\n",
      "6357 : Training: loss:  0.0036246448\n",
      "6358 : Training: loss:  0.0013216317\n",
      "6359 : Training: loss:  0.0043606195\n",
      "6360 : Training: loss:  0.0023551062\n",
      "Validation: Loss:  0.017018804  Accuracy:  0.96153843\n",
      "6361 : Training: loss:  0.004158927\n",
      "6362 : Training: loss:  0.0016670346\n",
      "6363 : Training: loss:  0.0040532844\n",
      "6364 : Training: loss:  0.0053354385\n",
      "6365 : Training: loss:  0.00422041\n",
      "6366 : Training: loss:  0.0014046361\n",
      "6367 : Training: loss:  0.0025166962\n",
      "6368 : Training: loss:  0.0011000225\n",
      "6369 : Training: loss:  0.0025607946\n",
      "6370 : Training: loss:  0.0014162896\n",
      "6371 : Training: loss:  0.0029577205\n",
      "6372 : Training: loss:  0.002956562\n",
      "6373 : Training: loss:  0.0021546767\n",
      "6374 : Training: loss:  0.0026773755\n",
      "6375 : Training: loss:  0.0023236463\n",
      "6376 : Training: loss:  0.00333318\n",
      "6377 : Training: loss:  0.0029104142\n",
      "6378 : Training: loss:  0.001542433\n",
      "6379 : Training: loss:  0.0017588751\n",
      "6380 : Training: loss:  0.0041614417\n",
      "Validation: Loss:  0.016939446  Accuracy:  0.96153843\n",
      "6381 : Training: loss:  0.0021416855\n",
      "6382 : Training: loss:  0.0024188247\n",
      "6383 : Training: loss:  0.0029010521\n",
      "6384 : Training: loss:  0.0031484005\n",
      "6385 : Training: loss:  0.0027912462\n",
      "6386 : Training: loss:  0.002790749\n",
      "6387 : Training: loss:  0.00284241\n",
      "6388 : Training: loss:  0.005039025\n",
      "6389 : Training: loss:  0.0038676152\n",
      "6390 : Training: loss:  0.004659472\n",
      "6391 : Training: loss:  0.0032732638\n",
      "6392 : Training: loss:  0.0053648464\n",
      "6393 : Training: loss:  0.0068112467\n",
      "6394 : Training: loss:  0.0034422502\n",
      "6395 : Training: loss:  0.0059147323\n",
      "6396 : Training: loss:  0.0022567648\n",
      "6397 : Training: loss:  0.0031085399\n",
      "6398 : Training: loss:  0.0026696888\n",
      "6399 : Training: loss:  0.0017765007\n",
      "6400 : Training: loss:  0.0018643046\n",
      "Validation: Loss:  0.016687267  Accuracy:  0.96153843\n",
      "6401 : Training: loss:  0.0025316668\n",
      "6402 : Training: loss:  0.0032563976\n",
      "6403 : Training: loss:  0.0069974298\n",
      "6404 : Training: loss:  0.0032397213\n",
      "6405 : Training: loss:  0.0033035504\n",
      "6406 : Training: loss:  0.0028569477\n",
      "6407 : Training: loss:  0.001912796\n",
      "6408 : Training: loss:  0.003614579\n",
      "6409 : Training: loss:  0.002701333\n",
      "6410 : Training: loss:  0.0020833614\n",
      "6411 : Training: loss:  0.003369817\n",
      "6412 : Training: loss:  0.004399077\n",
      "6413 : Training: loss:  0.0027901817\n",
      "6414 : Training: loss:  0.0048864787\n",
      "6415 : Training: loss:  0.0054140035\n",
      "6416 : Training: loss:  0.0047485093\n",
      "6417 : Training: loss:  0.001940886\n",
      "6418 : Training: loss:  0.0024765772\n",
      "6419 : Training: loss:  0.0026032045\n",
      "6420 : Training: loss:  0.0027924078\n",
      "Validation: Loss:  0.016637152  Accuracy:  0.96153843\n",
      "6421 : Training: loss:  0.0035171788\n",
      "6422 : Training: loss:  0.003348432\n",
      "6423 : Training: loss:  0.0036040435\n",
      "6424 : Training: loss:  0.0029886868\n",
      "6425 : Training: loss:  0.0022231126\n",
      "6426 : Training: loss:  0.0033946962\n",
      "6427 : Training: loss:  0.0024682118\n",
      "6428 : Training: loss:  0.0027878971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6429 : Training: loss:  0.005590565\n",
      "6430 : Training: loss:  0.002133094\n",
      "6431 : Training: loss:  0.0033630738\n",
      "6432 : Training: loss:  0.0024215146\n",
      "6433 : Training: loss:  0.0028128063\n",
      "6434 : Training: loss:  0.004047002\n",
      "6435 : Training: loss:  0.0010716179\n",
      "6436 : Training: loss:  0.0036853892\n",
      "6437 : Training: loss:  0.0032903184\n",
      "6438 : Training: loss:  0.0019035547\n",
      "6439 : Training: loss:  0.0023989615\n",
      "6440 : Training: loss:  0.002598079\n",
      "Validation: Loss:  0.016647361  Accuracy:  0.96153843\n",
      "6441 : Training: loss:  0.0018641176\n",
      "6442 : Training: loss:  0.0025751498\n",
      "6443 : Training: loss:  0.0031284152\n",
      "6444 : Training: loss:  0.003939528\n",
      "6445 : Training: loss:  0.003706762\n",
      "6446 : Training: loss:  0.002495316\n",
      "6447 : Training: loss:  0.0035606762\n",
      "6448 : Training: loss:  0.0047725346\n",
      "6449 : Training: loss:  0.0044649816\n",
      "6450 : Training: loss:  0.0039004092\n",
      "6451 : Training: loss:  0.0033304335\n",
      "6452 : Training: loss:  0.0030817715\n",
      "6453 : Training: loss:  0.002612642\n",
      "6454 : Training: loss:  0.0025756513\n",
      "6455 : Training: loss:  0.00447953\n",
      "6456 : Training: loss:  0.0029497733\n",
      "6457 : Training: loss:  0.0024517465\n",
      "6458 : Training: loss:  0.0017733044\n",
      "6459 : Training: loss:  0.0032637643\n",
      "6460 : Training: loss:  0.0028285726\n",
      "Validation: Loss:  0.016754981  Accuracy:  0.96153843\n",
      "6461 : Training: loss:  0.0026576044\n",
      "6462 : Training: loss:  0.003005138\n",
      "6463 : Training: loss:  0.0036605692\n",
      "6464 : Training: loss:  0.0038480016\n",
      "6465 : Training: loss:  0.001937507\n",
      "6466 : Training: loss:  0.003764585\n",
      "6467 : Training: loss:  0.0024503795\n",
      "6468 : Training: loss:  0.002346557\n",
      "6469 : Training: loss:  0.0024746435\n",
      "6470 : Training: loss:  0.0046660965\n",
      "6471 : Training: loss:  0.0028856625\n",
      "6472 : Training: loss:  0.0029727882\n",
      "6473 : Training: loss:  0.0026763454\n",
      "6474 : Training: loss:  0.0029675965\n",
      "6475 : Training: loss:  0.0036448631\n",
      "6476 : Training: loss:  0.001115468\n",
      "6477 : Training: loss:  0.001288575\n",
      "6478 : Training: loss:  0.004551269\n",
      "6479 : Training: loss:  0.003279842\n",
      "6480 : Training: loss:  0.004577059\n",
      "Validation: Loss:  0.016755875  Accuracy:  0.96153843\n",
      "6481 : Training: loss:  0.0032351825\n",
      "6482 : Training: loss:  0.0026670706\n",
      "6483 : Training: loss:  0.0032252967\n",
      "6484 : Training: loss:  0.0040504523\n",
      "6485 : Training: loss:  0.0023159238\n",
      "6486 : Training: loss:  0.0033310438\n",
      "6487 : Training: loss:  0.0021184937\n",
      "6488 : Training: loss:  0.0010032458\n",
      "6489 : Training: loss:  0.0020606215\n",
      "6490 : Training: loss:  0.003926889\n",
      "6491 : Training: loss:  0.0028643515\n",
      "6492 : Training: loss:  0.0022183016\n",
      "6493 : Training: loss:  0.0023790034\n",
      "6494 : Training: loss:  0.0023235807\n",
      "6495 : Training: loss:  0.0034907137\n",
      "6496 : Training: loss:  0.0031802417\n",
      "6497 : Training: loss:  0.0033204046\n",
      "6498 : Training: loss:  0.002265057\n",
      "6499 : Training: loss:  0.004878756\n",
      "6500 : Training: loss:  0.005420611\n",
      "Validation: Loss:  0.016650205  Accuracy:  0.96153843\n",
      "6501 : Training: loss:  0.0040780413\n",
      "6502 : Training: loss:  0.0010525825\n",
      "6503 : Training: loss:  0.0023560172\n",
      "6504 : Training: loss:  0.003117448\n",
      "6505 : Training: loss:  0.0023802056\n",
      "6506 : Training: loss:  0.0036348945\n",
      "6507 : Training: loss:  0.0021686046\n",
      "6508 : Training: loss:  0.0012671223\n",
      "6509 : Training: loss:  0.0026814847\n",
      "6510 : Training: loss:  0.0019700322\n",
      "6511 : Training: loss:  0.0020695608\n",
      "6512 : Training: loss:  0.0041984227\n",
      "6513 : Training: loss:  0.0017494964\n",
      "6514 : Training: loss:  0.003945541\n",
      "6515 : Training: loss:  0.0036236225\n",
      "6516 : Training: loss:  0.0030931933\n",
      "6517 : Training: loss:  0.0026503543\n",
      "6518 : Training: loss:  0.0017082379\n",
      "6519 : Training: loss:  0.0022490325\n",
      "6520 : Training: loss:  0.0028447034\n",
      "Validation: Loss:  0.016598945  Accuracy:  0.96153843\n",
      "6521 : Training: loss:  0.0036609697\n",
      "6522 : Training: loss:  0.003841378\n",
      "6523 : Training: loss:  0.0021383192\n",
      "6524 : Training: loss:  0.0044682752\n",
      "6525 : Training: loss:  0.003967386\n",
      "6526 : Training: loss:  0.00093264476\n",
      "6527 : Training: loss:  0.0031128782\n",
      "6528 : Training: loss:  0.002878108\n",
      "6529 : Training: loss:  0.002318046\n",
      "6530 : Training: loss:  0.003302625\n",
      "6531 : Training: loss:  0.0019555062\n",
      "6532 : Training: loss:  0.0034540836\n",
      "6533 : Training: loss:  0.0028165341\n",
      "6534 : Training: loss:  0.004328651\n",
      "6535 : Training: loss:  0.0028792527\n",
      "6536 : Training: loss:  0.0019506795\n",
      "6537 : Training: loss:  0.0015603758\n",
      "6538 : Training: loss:  0.0021891464\n",
      "6539 : Training: loss:  0.0028278606\n",
      "6540 : Training: loss:  0.0028106782\n",
      "Validation: Loss:  0.016563226  Accuracy:  0.96153843\n",
      "6541 : Training: loss:  0.0028000614\n",
      "6542 : Training: loss:  0.0023472717\n",
      "6543 : Training: loss:  0.002881832\n",
      "6544 : Training: loss:  0.003336048\n",
      "6545 : Training: loss:  0.0035699483\n",
      "6546 : Training: loss:  0.0028940197\n",
      "6547 : Training: loss:  0.002799729\n",
      "6548 : Training: loss:  0.0038687894\n",
      "6549 : Training: loss:  0.0036070654\n",
      "6550 : Training: loss:  0.0017834702\n",
      "6551 : Training: loss:  0.0025349227\n",
      "6552 : Training: loss:  0.0042231004\n",
      "6553 : Training: loss:  0.0038048965\n",
      "6554 : Training: loss:  0.0014680952\n",
      "6555 : Training: loss:  0.0026741794\n",
      "6556 : Training: loss:  0.0033919225\n",
      "6557 : Training: loss:  0.0027125182\n",
      "6558 : Training: loss:  0.0018680176\n",
      "6559 : Training: loss:  0.00250394\n",
      "6560 : Training: loss:  0.002288169\n",
      "Validation: Loss:  0.01646876  Accuracy:  0.96153843\n",
      "6561 : Training: loss:  0.0023984087\n",
      "6562 : Training: loss:  0.0032436987\n",
      "6563 : Training: loss:  0.0013057726\n",
      "6564 : Training: loss:  0.002371272\n",
      "6565 : Training: loss:  0.0034338818\n",
      "6566 : Training: loss:  0.002063253\n",
      "6567 : Training: loss:  0.0033162897\n",
      "6568 : Training: loss:  0.0021540693\n",
      "6569 : Training: loss:  0.0033962643\n",
      "6570 : Training: loss:  0.0017856625\n",
      "6571 : Training: loss:  0.0018037952\n",
      "6572 : Training: loss:  0.0036496082\n",
      "6573 : Training: loss:  0.0071472167\n",
      "6574 : Training: loss:  0.0018628605\n",
      "6575 : Training: loss:  0.0030637158\n",
      "6576 : Training: loss:  0.002381812\n",
      "6577 : Training: loss:  0.0023550005\n",
      "6578 : Training: loss:  0.0018469268\n",
      "6579 : Training: loss:  0.0035784175\n",
      "6580 : Training: loss:  0.0012052312\n",
      "Validation: Loss:  0.016370907  Accuracy:  0.96153843\n",
      "6581 : Training: loss:  0.0019687307\n",
      "6582 : Training: loss:  0.0026497676\n",
      "6583 : Training: loss:  0.0021671383\n",
      "6584 : Training: loss:  0.0031340437\n",
      "6585 : Training: loss:  0.0021935503\n",
      "6586 : Training: loss:  0.0035412118\n",
      "6587 : Training: loss:  0.0044518956\n",
      "6588 : Training: loss:  0.0025596481\n",
      "6589 : Training: loss:  0.002118455\n",
      "6590 : Training: loss:  0.0028292548\n",
      "6591 : Training: loss:  0.0022136387\n",
      "6592 : Training: loss:  0.003106596\n",
      "6593 : Training: loss:  0.0026619157\n",
      "6594 : Training: loss:  0.0025418026\n",
      "6595 : Training: loss:  0.002544428\n",
      "6596 : Training: loss:  0.003007253\n",
      "6597 : Training: loss:  0.0019483833\n",
      "6598 : Training: loss:  0.0030746998\n",
      "6599 : Training: loss:  0.0017808984\n",
      "6600 : Training: loss:  0.0018903048\n",
      "Validation: Loss:  0.016334489  Accuracy:  0.96153843\n",
      "6601 : Training: loss:  0.0035472675\n",
      "6602 : Training: loss:  0.0031822133\n",
      "6603 : Training: loss:  0.002530001\n",
      "6604 : Training: loss:  0.0017056244\n",
      "6605 : Training: loss:  0.0027768167\n",
      "6606 : Training: loss:  0.003052865\n",
      "6607 : Training: loss:  0.0014368613\n",
      "6608 : Training: loss:  0.004388474\n",
      "6609 : Training: loss:  0.0023027544\n",
      "6610 : Training: loss:  0.0034086148\n",
      "6611 : Training: loss:  0.002307961\n",
      "6612 : Training: loss:  0.003906114\n",
      "6613 : Training: loss:  0.003230537\n",
      "6614 : Training: loss:  0.0032033753\n",
      "6615 : Training: loss:  0.0022675341\n",
      "6616 : Training: loss:  0.0022387414\n",
      "6617 : Training: loss:  0.0033828297\n",
      "6618 : Training: loss:  0.003468446\n",
      "6619 : Training: loss:  0.0039245603\n",
      "6620 : Training: loss:  0.0023244042\n",
      "Validation: Loss:  0.016170537  Accuracy:  0.96153843\n",
      "6621 : Training: loss:  0.0018054569\n",
      "6622 : Training: loss:  0.0035288532\n",
      "6623 : Training: loss:  0.0020454254\n",
      "6624 : Training: loss:  0.002554513\n",
      "6625 : Training: loss:  0.0031070632\n",
      "6626 : Training: loss:  0.002302286\n",
      "6627 : Training: loss:  0.0023761222\n",
      "6628 : Training: loss:  0.0027981247\n",
      "6629 : Training: loss:  0.0024379264\n",
      "6630 : Training: loss:  0.002298409\n",
      "6631 : Training: loss:  0.002549767\n",
      "6632 : Training: loss:  0.0017621714\n",
      "6633 : Training: loss:  0.0034756805\n",
      "6634 : Training: loss:  0.0022259718\n",
      "6635 : Training: loss:  0.0027665338\n",
      "6636 : Training: loss:  0.0030843155\n",
      "6637 : Training: loss:  0.0020453588\n",
      "6638 : Training: loss:  0.006073292\n",
      "6639 : Training: loss:  0.0029134797\n",
      "6640 : Training: loss:  0.0031399785\n",
      "Validation: Loss:  0.016047772  Accuracy:  0.96153843\n",
      "6641 : Training: loss:  0.0027037764\n",
      "6642 : Training: loss:  0.0023997503\n",
      "6643 : Training: loss:  0.0030451869\n",
      "6644 : Training: loss:  0.002919251\n",
      "6645 : Training: loss:  0.001604938\n",
      "6646 : Training: loss:  0.002679826\n",
      "6647 : Training: loss:  0.0028603356\n",
      "6648 : Training: loss:  0.00192512\n",
      "6649 : Training: loss:  0.002822348\n",
      "6650 : Training: loss:  0.0010565962\n",
      "6651 : Training: loss:  0.0020608162\n",
      "6652 : Training: loss:  0.0036690845\n",
      "6653 : Training: loss:  0.0025820201\n",
      "6654 : Training: loss:  0.001986562\n",
      "6655 : Training: loss:  0.0017688399\n",
      "6656 : Training: loss:  0.0016263212\n",
      "6657 : Training: loss:  0.0025099183\n",
      "6658 : Training: loss:  0.0022863531\n",
      "6659 : Training: loss:  0.0033370426\n",
      "6660 : Training: loss:  0.0019087924\n",
      "Validation: Loss:  0.016046552  Accuracy:  0.96153843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6661 : Training: loss:  0.0022877073\n",
      "6662 : Training: loss:  0.0016229023\n",
      "6663 : Training: loss:  0.0044008093\n",
      "6664 : Training: loss:  0.001713038\n",
      "6665 : Training: loss:  0.0026517923\n",
      "6666 : Training: loss:  0.0014478067\n",
      "6667 : Training: loss:  0.002947481\n",
      "6668 : Training: loss:  0.0037246824\n",
      "6669 : Training: loss:  0.0036088566\n",
      "6670 : Training: loss:  0.0025840502\n",
      "6671 : Training: loss:  0.0026431407\n",
      "6672 : Training: loss:  0.0028542876\n",
      "6673 : Training: loss:  0.0028571421\n",
      "6674 : Training: loss:  0.004745638\n",
      "6675 : Training: loss:  0.0025873366\n",
      "6676 : Training: loss:  0.0025333462\n",
      "6677 : Training: loss:  0.0016306521\n",
      "6678 : Training: loss:  0.00231677\n",
      "6679 : Training: loss:  0.0016631442\n",
      "6680 : Training: loss:  0.003262661\n",
      "Validation: Loss:  0.016022453  Accuracy:  0.96153843\n",
      "6681 : Training: loss:  0.0024175385\n",
      "6682 : Training: loss:  0.0022055933\n",
      "6683 : Training: loss:  0.0021398992\n",
      "6684 : Training: loss:  0.0031218887\n",
      "6685 : Training: loss:  0.0036549135\n",
      "6686 : Training: loss:  0.0028741409\n",
      "6687 : Training: loss:  0.0037284973\n",
      "6688 : Training: loss:  0.00229048\n",
      "6689 : Training: loss:  0.0027394444\n",
      "6690 : Training: loss:  0.001742486\n",
      "6691 : Training: loss:  0.0022821764\n",
      "6692 : Training: loss:  0.003153625\n",
      "6693 : Training: loss:  0.0033314477\n",
      "6694 : Training: loss:  0.0048860502\n",
      "6695 : Training: loss:  0.0013734867\n",
      "6696 : Training: loss:  0.0046961405\n",
      "6697 : Training: loss:  0.0034264985\n",
      "6698 : Training: loss:  0.0013765557\n",
      "6699 : Training: loss:  0.003139345\n",
      "6700 : Training: loss:  0.004749126\n",
      "Validation: Loss:  0.015979595  Accuracy:  0.96153843\n",
      "6701 : Training: loss:  0.0021665306\n",
      "6702 : Training: loss:  0.0020468289\n",
      "6703 : Training: loss:  0.0029674335\n",
      "6704 : Training: loss:  0.0023919363\n",
      "6705 : Training: loss:  0.0012963499\n",
      "6706 : Training: loss:  0.004709613\n",
      "6707 : Training: loss:  0.0025176418\n",
      "6708 : Training: loss:  0.0037032524\n",
      "6709 : Training: loss:  0.0018854925\n",
      "6710 : Training: loss:  0.0030297565\n",
      "6711 : Training: loss:  0.0019175536\n",
      "6712 : Training: loss:  0.0027177688\n",
      "6713 : Training: loss:  0.0016315894\n",
      "6714 : Training: loss:  0.0016398354\n",
      "6715 : Training: loss:  0.0028718647\n",
      "6716 : Training: loss:  0.0016970698\n",
      "6717 : Training: loss:  0.0010675724\n",
      "6718 : Training: loss:  0.0015043996\n",
      "6719 : Training: loss:  0.0030715396\n",
      "6720 : Training: loss:  0.0031512738\n",
      "Validation: Loss:  0.016003491  Accuracy:  0.96153843\n",
      "6721 : Training: loss:  0.0030058713\n",
      "6722 : Training: loss:  0.0038008129\n",
      "6723 : Training: loss:  0.0016757435\n",
      "6724 : Training: loss:  0.0011006219\n",
      "6725 : Training: loss:  0.0019850004\n",
      "6726 : Training: loss:  0.0025185938\n",
      "6727 : Training: loss:  0.0036866728\n",
      "6728 : Training: loss:  0.002464351\n",
      "6729 : Training: loss:  0.004128151\n",
      "6730 : Training: loss:  0.0033007697\n",
      "6731 : Training: loss:  0.0024961373\n",
      "6732 : Training: loss:  0.0018364101\n",
      "6733 : Training: loss:  0.0020024474\n",
      "6734 : Training: loss:  0.002856135\n",
      "6735 : Training: loss:  0.0030533439\n",
      "6736 : Training: loss:  0.0017428203\n",
      "6737 : Training: loss:  0.0012041344\n",
      "6738 : Training: loss:  0.0018963153\n",
      "6739 : Training: loss:  0.0027786016\n",
      "6740 : Training: loss:  0.0027033826\n",
      "Validation: Loss:  0.01595693  Accuracy:  0.96153843\n",
      "6741 : Training: loss:  0.0014800059\n",
      "6742 : Training: loss:  0.0022628093\n",
      "6743 : Training: loss:  0.0017525757\n",
      "6744 : Training: loss:  0.0010779506\n",
      "6745 : Training: loss:  0.0022176001\n",
      "6746 : Training: loss:  0.0024419846\n",
      "6747 : Training: loss:  0.0033778204\n",
      "6748 : Training: loss:  0.0015630907\n",
      "6749 : Training: loss:  0.0019748972\n",
      "6750 : Training: loss:  0.0022896354\n",
      "6751 : Training: loss:  0.0018024902\n",
      "6752 : Training: loss:  0.0045761466\n",
      "6753 : Training: loss:  0.002254377\n",
      "6754 : Training: loss:  0.0023201064\n",
      "6755 : Training: loss:  0.0037866165\n",
      "6756 : Training: loss:  0.0029335544\n",
      "6757 : Training: loss:  0.0020382302\n",
      "6758 : Training: loss:  0.0017059413\n",
      "6759 : Training: loss:  0.0026929553\n",
      "6760 : Training: loss:  0.002174027\n",
      "Validation: Loss:  0.015870057  Accuracy:  0.96153843\n",
      "6761 : Training: loss:  0.002593952\n",
      "6762 : Training: loss:  0.0036673222\n",
      "6763 : Training: loss:  0.001660605\n",
      "6764 : Training: loss:  0.0038369198\n",
      "6765 : Training: loss:  0.0026648564\n",
      "6766 : Training: loss:  0.003118603\n",
      "6767 : Training: loss:  0.0036073073\n",
      "6768 : Training: loss:  0.002371892\n",
      "6769 : Training: loss:  0.0033650065\n",
      "6770 : Training: loss:  0.0022860428\n",
      "6771 : Training: loss:  0.0020067869\n",
      "6772 : Training: loss:  0.0031781814\n",
      "6773 : Training: loss:  0.0030856107\n",
      "6774 : Training: loss:  0.0033424485\n",
      "6775 : Training: loss:  0.0017186524\n",
      "6776 : Training: loss:  0.0028032435\n",
      "6777 : Training: loss:  0.0023653968\n",
      "6778 : Training: loss:  0.001426145\n",
      "6779 : Training: loss:  0.0026192826\n",
      "6780 : Training: loss:  0.0021331864\n",
      "Validation: Loss:  0.015845528  Accuracy:  0.96153843\n",
      "6781 : Training: loss:  0.0026348114\n",
      "6782 : Training: loss:  0.0031566608\n",
      "6783 : Training: loss:  0.0027226256\n",
      "6784 : Training: loss:  0.0020028\n",
      "6785 : Training: loss:  0.0015443428\n",
      "6786 : Training: loss:  0.0020892932\n",
      "6787 : Training: loss:  0.002726213\n",
      "6788 : Training: loss:  0.0018372118\n",
      "6789 : Training: loss:  0.0026083717\n",
      "6790 : Training: loss:  0.0036893776\n",
      "6791 : Training: loss:  0.0036935096\n",
      "6792 : Training: loss:  0.0029892486\n",
      "6793 : Training: loss:  0.0025240371\n",
      "6794 : Training: loss:  0.003212749\n",
      "6795 : Training: loss:  0.0018545218\n",
      "6796 : Training: loss:  0.0030650278\n",
      "6797 : Training: loss:  0.0025568253\n",
      "6798 : Training: loss:  0.0016353533\n",
      "6799 : Training: loss:  0.0030371356\n",
      "6800 : Training: loss:  0.0017139491\n",
      "Validation: Loss:  0.015823863  Accuracy:  0.96153843\n",
      "6801 : Training: loss:  0.0021699788\n",
      "6802 : Training: loss:  0.0020730856\n",
      "6803 : Training: loss:  0.002327026\n",
      "6804 : Training: loss:  0.0015786124\n",
      "6805 : Training: loss:  0.0019719077\n",
      "6806 : Training: loss:  0.0036076705\n",
      "6807 : Training: loss:  0.0021652298\n",
      "6808 : Training: loss:  0.0019378477\n",
      "6809 : Training: loss:  0.0032113073\n",
      "6810 : Training: loss:  0.0018812956\n",
      "6811 : Training: loss:  0.0025903082\n",
      "6812 : Training: loss:  0.002365867\n",
      "6813 : Training: loss:  0.0014975177\n",
      "6814 : Training: loss:  0.0034016178\n",
      "6815 : Training: loss:  0.0026224053\n",
      "6816 : Training: loss:  0.0017636209\n",
      "6817 : Training: loss:  0.003676823\n",
      "6818 : Training: loss:  0.0031811225\n",
      "6819 : Training: loss:  0.00501396\n",
      "6820 : Training: loss:  0.0028200557\n",
      "Validation: Loss:  0.015822614  Accuracy:  0.96153843\n",
      "6821 : Training: loss:  0.0013326549\n",
      "6822 : Training: loss:  0.0020461814\n",
      "6823 : Training: loss:  0.0022676715\n",
      "6824 : Training: loss:  0.0020602283\n",
      "6825 : Training: loss:  0.0037350783\n",
      "6826 : Training: loss:  0.0028266504\n",
      "6827 : Training: loss:  0.0017854483\n",
      "6828 : Training: loss:  0.0020358884\n",
      "6829 : Training: loss:  0.0028499756\n",
      "6830 : Training: loss:  0.0033528677\n",
      "6831 : Training: loss:  0.002389734\n",
      "6832 : Training: loss:  0.0019834952\n",
      "6833 : Training: loss:  0.0031906504\n",
      "6834 : Training: loss:  0.0019677132\n",
      "6835 : Training: loss:  0.0035357284\n",
      "6836 : Training: loss:  0.001800618\n",
      "6837 : Training: loss:  0.004679565\n",
      "6838 : Training: loss:  0.0012433483\n",
      "6839 : Training: loss:  0.0020712619\n",
      "6840 : Training: loss:  0.0022111386\n",
      "Validation: Loss:  0.015807346  Accuracy:  0.96153843\n",
      "6841 : Training: loss:  0.00118826\n",
      "6842 : Training: loss:  0.002952698\n",
      "6843 : Training: loss:  0.0033095465\n",
      "6844 : Training: loss:  0.0017668175\n",
      "6845 : Training: loss:  0.0029987285\n",
      "6846 : Training: loss:  0.0031473052\n",
      "6847 : Training: loss:  0.001956947\n",
      "6848 : Training: loss:  0.0018342609\n",
      "6849 : Training: loss:  0.0037923376\n",
      "6850 : Training: loss:  0.003019384\n",
      "6851 : Training: loss:  0.0019390269\n",
      "6852 : Training: loss:  0.0013301355\n",
      "6853 : Training: loss:  0.003066406\n",
      "6854 : Training: loss:  0.0024908765\n",
      "6855 : Training: loss:  0.0010953337\n",
      "6856 : Training: loss:  0.0033211885\n",
      "6857 : Training: loss:  0.0018880086\n",
      "6858 : Training: loss:  0.0012873141\n",
      "6859 : Training: loss:  0.0030482854\n",
      "6860 : Training: loss:  0.0029829915\n",
      "Validation: Loss:  0.015734565  Accuracy:  0.96153843\n",
      "6861 : Training: loss:  0.0021348156\n",
      "6862 : Training: loss:  0.0029019862\n",
      "6863 : Training: loss:  0.004483526\n",
      "6864 : Training: loss:  0.005328403\n",
      "6865 : Training: loss:  0.0012585177\n",
      "6866 : Training: loss:  0.0018494079\n",
      "6867 : Training: loss:  0.0020900217\n",
      "6868 : Training: loss:  0.002905305\n",
      "6869 : Training: loss:  0.0029796192\n",
      "6870 : Training: loss:  0.0035628346\n",
      "6871 : Training: loss:  0.0022020657\n",
      "6872 : Training: loss:  0.0020633806\n",
      "6873 : Training: loss:  0.0040603615\n",
      "6874 : Training: loss:  0.0018920517\n",
      "6875 : Training: loss:  0.0015876661\n",
      "6876 : Training: loss:  0.0014922039\n",
      "6877 : Training: loss:  0.0017886395\n",
      "6878 : Training: loss:  0.0018219235\n",
      "6879 : Training: loss:  0.0020760652\n",
      "6880 : Training: loss:  0.0021874337\n",
      "Validation: Loss:  0.015627222  Accuracy:  0.96153843\n",
      "6881 : Training: loss:  0.0025706291\n",
      "6882 : Training: loss:  0.001975469\n",
      "6883 : Training: loss:  0.0036556278\n",
      "6884 : Training: loss:  0.0022430741\n",
      "6885 : Training: loss:  0.001839997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6886 : Training: loss:  0.002125117\n",
      "6887 : Training: loss:  0.0023784456\n",
      "6888 : Training: loss:  0.0026368564\n",
      "6889 : Training: loss:  0.002158927\n",
      "6890 : Training: loss:  0.0018317857\n",
      "6891 : Training: loss:  0.0018975176\n",
      "6892 : Training: loss:  0.0014969059\n",
      "6893 : Training: loss:  0.0019746395\n",
      "6894 : Training: loss:  0.0019832207\n",
      "6895 : Training: loss:  0.00087910594\n",
      "6896 : Training: loss:  0.00216758\n",
      "6897 : Training: loss:  0.002026675\n",
      "6898 : Training: loss:  0.0018138583\n",
      "6899 : Training: loss:  0.0033144683\n",
      "6900 : Training: loss:  0.0019571085\n",
      "Validation: Loss:  0.015563142  Accuracy:  0.96153843\n",
      "6901 : Training: loss:  0.0014307161\n",
      "6902 : Training: loss:  0.0022099295\n",
      "6903 : Training: loss:  0.004303919\n",
      "6904 : Training: loss:  0.0032351036\n",
      "6905 : Training: loss:  0.001348783\n",
      "6906 : Training: loss:  0.001976124\n",
      "6907 : Training: loss:  0.0037686005\n",
      "6908 : Training: loss:  0.0027600806\n",
      "6909 : Training: loss:  0.0017613815\n",
      "6910 : Training: loss:  0.0040077902\n",
      "6911 : Training: loss:  0.001762367\n",
      "6912 : Training: loss:  0.0017750426\n",
      "6913 : Training: loss:  0.0021600623\n",
      "6914 : Training: loss:  0.0016157648\n",
      "6915 : Training: loss:  0.0028075513\n",
      "6916 : Training: loss:  0.0026631192\n",
      "6917 : Training: loss:  0.0022656925\n",
      "6918 : Training: loss:  0.0022483475\n",
      "6919 : Training: loss:  0.0027974462\n",
      "6920 : Training: loss:  0.0011210621\n",
      "Validation: Loss:  0.015608856  Accuracy:  0.96153843\n",
      "6921 : Training: loss:  0.0027339072\n",
      "6922 : Training: loss:  0.002222904\n",
      "6923 : Training: loss:  0.0036616425\n",
      "6924 : Training: loss:  0.0021248646\n",
      "6925 : Training: loss:  0.0034360674\n",
      "6926 : Training: loss:  0.0025323583\n",
      "6927 : Training: loss:  0.002037595\n",
      "6928 : Training: loss:  0.0028865403\n",
      "6929 : Training: loss:  0.0013635432\n",
      "6930 : Training: loss:  0.0024791453\n",
      "6931 : Training: loss:  0.0018557375\n",
      "6932 : Training: loss:  0.0023649244\n",
      "6933 : Training: loss:  0.0017509789\n",
      "6934 : Training: loss:  0.0019658285\n",
      "6935 : Training: loss:  0.0026799676\n",
      "6936 : Training: loss:  0.0041063223\n",
      "6937 : Training: loss:  0.002147408\n",
      "6938 : Training: loss:  0.0018735339\n",
      "6939 : Training: loss:  0.0027509527\n",
      "6940 : Training: loss:  0.0010731181\n",
      "Validation: Loss:  0.015568334  Accuracy:  0.96153843\n",
      "6941 : Training: loss:  0.0019127497\n",
      "6942 : Training: loss:  0.0025696517\n",
      "6943 : Training: loss:  0.0013035315\n",
      "6944 : Training: loss:  0.00096983387\n",
      "6945 : Training: loss:  0.0019264583\n",
      "6946 : Training: loss:  0.0017939411\n",
      "6947 : Training: loss:  0.0028765171\n",
      "6948 : Training: loss:  0.0023934434\n",
      "6949 : Training: loss:  0.0022369842\n",
      "6950 : Training: loss:  0.0038312017\n",
      "6951 : Training: loss:  0.0018872656\n",
      "6952 : Training: loss:  0.0017191338\n",
      "6953 : Training: loss:  0.0018189074\n",
      "6954 : Training: loss:  0.0018672919\n",
      "6955 : Training: loss:  0.0024539274\n",
      "6956 : Training: loss:  0.0025354803\n",
      "6957 : Training: loss:  0.0040533487\n",
      "6958 : Training: loss:  0.0024812873\n",
      "6959 : Training: loss:  0.0026167363\n",
      "6960 : Training: loss:  0.0014982598\n",
      "Validation: Loss:  0.015475268  Accuracy:  0.96153843\n",
      "6961 : Training: loss:  0.0029423896\n",
      "6962 : Training: loss:  0.002833972\n",
      "6963 : Training: loss:  0.0032526085\n",
      "6964 : Training: loss:  0.0026898307\n",
      "6965 : Training: loss:  0.0014210886\n",
      "6966 : Training: loss:  0.002661758\n",
      "6967 : Training: loss:  0.0021310237\n",
      "6968 : Training: loss:  0.0023962886\n",
      "6969 : Training: loss:  0.001999084\n",
      "6970 : Training: loss:  0.0012703328\n",
      "6971 : Training: loss:  0.0021853216\n",
      "6972 : Training: loss:  0.0028561612\n",
      "6973 : Training: loss:  0.0023440581\n",
      "6974 : Training: loss:  0.002884862\n",
      "6975 : Training: loss:  0.0017119811\n",
      "6976 : Training: loss:  0.002790165\n",
      "6977 : Training: loss:  0.0012269481\n",
      "6978 : Training: loss:  0.0021962167\n",
      "6979 : Training: loss:  0.0019464149\n",
      "6980 : Training: loss:  0.0022399342\n",
      "Validation: Loss:  0.015400335  Accuracy:  0.96153843\n",
      "6981 : Training: loss:  0.0015852883\n",
      "6982 : Training: loss:  0.002214131\n",
      "6983 : Training: loss:  0.0011080481\n",
      "6984 : Training: loss:  0.0021932295\n",
      "6985 : Training: loss:  0.0040291823\n",
      "6986 : Training: loss:  0.0018534379\n",
      "6987 : Training: loss:  0.0017874317\n",
      "6988 : Training: loss:  0.0015634003\n",
      "6989 : Training: loss:  0.0010235107\n",
      "6990 : Training: loss:  0.0025472115\n",
      "6991 : Training: loss:  0.0018978642\n",
      "6992 : Training: loss:  0.001704605\n",
      "6993 : Training: loss:  0.0015585002\n",
      "6994 : Training: loss:  0.0034123985\n",
      "6995 : Training: loss:  0.0029192239\n",
      "6996 : Training: loss:  0.0022252258\n",
      "6997 : Training: loss:  0.0017159142\n",
      "6998 : Training: loss:  0.0019294317\n",
      "6999 : Training: loss:  0.003750536\n",
      "7000 : Training: loss:  0.0018061055\n",
      "Validation: Loss:  0.015394439  Accuracy:  0.96153843\n",
      "7001 : Training: loss:  0.003324418\n",
      "7002 : Training: loss:  0.0033355781\n",
      "7003 : Training: loss:  0.0034149748\n",
      "7004 : Training: loss:  0.0026719747\n",
      "7005 : Training: loss:  0.001946655\n",
      "7006 : Training: loss:  0.0027284427\n",
      "7007 : Training: loss:  0.001968476\n",
      "7008 : Training: loss:  0.0015920837\n",
      "7009 : Training: loss:  0.002418233\n",
      "7010 : Training: loss:  0.0014899732\n",
      "7011 : Training: loss:  0.0015480133\n",
      "7012 : Training: loss:  0.0017917176\n",
      "7013 : Training: loss:  0.0016500264\n",
      "7014 : Training: loss:  0.0019756178\n",
      "7015 : Training: loss:  0.0015547781\n",
      "7016 : Training: loss:  0.0021463425\n",
      "7017 : Training: loss:  0.0029038938\n",
      "7018 : Training: loss:  0.0018590232\n",
      "7019 : Training: loss:  0.0025614088\n",
      "7020 : Training: loss:  0.0036659073\n",
      "Validation: Loss:  0.015410334  Accuracy:  0.96153843\n",
      "7021 : Training: loss:  0.0017369898\n",
      "7022 : Training: loss:  0.0022172802\n",
      "7023 : Training: loss:  0.0025701108\n",
      "7024 : Training: loss:  0.0013012447\n",
      "7025 : Training: loss:  0.0028315915\n",
      "7026 : Training: loss:  0.0031350276\n",
      "7027 : Training: loss:  0.0038963119\n",
      "7028 : Training: loss:  0.0016738261\n",
      "7029 : Training: loss:  0.0028408512\n",
      "7030 : Training: loss:  0.0019409343\n",
      "7031 : Training: loss:  0.0032804683\n",
      "7032 : Training: loss:  0.0023394243\n",
      "7033 : Training: loss:  0.00281452\n",
      "7034 : Training: loss:  0.0024986097\n",
      "7035 : Training: loss:  0.0022216293\n",
      "7036 : Training: loss:  0.002398828\n",
      "7037 : Training: loss:  0.0012646711\n",
      "7038 : Training: loss:  0.0011400399\n",
      "7039 : Training: loss:  0.0020198536\n",
      "7040 : Training: loss:  0.0026105582\n",
      "Validation: Loss:  0.015374295  Accuracy:  0.96153843\n",
      "7041 : Training: loss:  0.0024369268\n",
      "7042 : Training: loss:  0.0032733534\n",
      "7043 : Training: loss:  0.0018845076\n",
      "7044 : Training: loss:  0.0015442324\n",
      "7045 : Training: loss:  0.0020192212\n",
      "7046 : Training: loss:  0.0020553886\n",
      "7047 : Training: loss:  0.0016939446\n",
      "7048 : Training: loss:  0.0019062805\n",
      "7049 : Training: loss:  0.0019023335\n",
      "7050 : Training: loss:  0.001967979\n",
      "7051 : Training: loss:  0.0031051843\n",
      "7052 : Training: loss:  0.0032946095\n",
      "7053 : Training: loss:  0.0034645556\n",
      "7054 : Training: loss:  0.0014393731\n",
      "7055 : Training: loss:  0.0026435251\n",
      "7056 : Training: loss:  0.0020758333\n",
      "7057 : Training: loss:  0.0031455224\n",
      "7058 : Training: loss:  0.0014500554\n",
      "7059 : Training: loss:  0.003995464\n",
      "7060 : Training: loss:  0.0039770775\n",
      "Validation: Loss:  0.015398752  Accuracy:  0.96153843\n",
      "7061 : Training: loss:  0.002437354\n",
      "7062 : Training: loss:  0.0020463185\n",
      "7063 : Training: loss:  0.0011528903\n",
      "7064 : Training: loss:  0.0011949901\n",
      "7065 : Training: loss:  0.001314602\n",
      "7066 : Training: loss:  0.0020602755\n",
      "7067 : Training: loss:  0.0020306003\n",
      "7068 : Training: loss:  0.0034729706\n",
      "7069 : Training: loss:  0.0025846765\n",
      "7070 : Training: loss:  0.0016025753\n",
      "7071 : Training: loss:  0.0012492477\n",
      "7072 : Training: loss:  0.0036209677\n",
      "7073 : Training: loss:  0.0019160191\n",
      "7074 : Training: loss:  0.001977901\n",
      "7075 : Training: loss:  0.0024937093\n",
      "7076 : Training: loss:  0.0017149756\n",
      "7077 : Training: loss:  0.0014156546\n",
      "7078 : Training: loss:  0.0020961086\n",
      "7079 : Training: loss:  0.0022938715\n",
      "7080 : Training: loss:  0.0027708826\n",
      "Validation: Loss:  0.015500134  Accuracy:  0.96153843\n",
      "7081 : Training: loss:  0.0014497931\n",
      "7082 : Training: loss:  0.0018356264\n",
      "7083 : Training: loss:  0.0014108459\n",
      "7084 : Training: loss:  0.002138056\n",
      "7085 : Training: loss:  0.0013156703\n",
      "7086 : Training: loss:  0.0030023335\n",
      "7087 : Training: loss:  0.0010223188\n",
      "7088 : Training: loss:  0.0020801541\n",
      "7089 : Training: loss:  0.0014108701\n",
      "7090 : Training: loss:  0.000922191\n",
      "7091 : Training: loss:  0.0028921587\n",
      "7092 : Training: loss:  0.0010877025\n",
      "7093 : Training: loss:  0.001816922\n",
      "7094 : Training: loss:  0.0021194867\n",
      "7095 : Training: loss:  0.0014349079\n",
      "7096 : Training: loss:  0.0013003978\n",
      "7097 : Training: loss:  0.0012605852\n",
      "7098 : Training: loss:  0.0019819455\n",
      "7099 : Training: loss:  0.0031307125\n",
      "7100 : Training: loss:  0.0014570453\n",
      "Validation: Loss:  0.015416919  Accuracy:  0.96153843\n",
      "7101 : Training: loss:  0.002640934\n",
      "7102 : Training: loss:  0.0021756983\n",
      "7103 : Training: loss:  0.0022652668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7104 : Training: loss:  0.0016630623\n",
      "7105 : Training: loss:  0.0031212673\n",
      "7106 : Training: loss:  0.00086019305\n",
      "7107 : Training: loss:  0.0018289029\n",
      "7108 : Training: loss:  0.0021078056\n",
      "7109 : Training: loss:  0.0019316084\n",
      "7110 : Training: loss:  0.0022123994\n",
      "7111 : Training: loss:  0.0030355249\n",
      "7112 : Training: loss:  0.0015568652\n",
      "7113 : Training: loss:  0.0015654622\n",
      "7114 : Training: loss:  0.0018796166\n",
      "7115 : Training: loss:  0.0016911129\n",
      "7116 : Training: loss:  0.0019088686\n",
      "7117 : Training: loss:  0.0010710446\n",
      "7118 : Training: loss:  0.0017338834\n",
      "7119 : Training: loss:  0.0035158058\n",
      "7120 : Training: loss:  0.001948688\n",
      "Validation: Loss:  0.015283857  Accuracy:  0.96153843\n",
      "7121 : Training: loss:  0.0012657754\n",
      "7122 : Training: loss:  0.0028458356\n",
      "7123 : Training: loss:  0.0012576497\n",
      "7124 : Training: loss:  0.0016545376\n",
      "7125 : Training: loss:  0.0015887353\n",
      "7126 : Training: loss:  0.0027224554\n",
      "7127 : Training: loss:  0.0012237603\n",
      "7128 : Training: loss:  0.0027184756\n",
      "7129 : Training: loss:  0.002727927\n",
      "7130 : Training: loss:  0.0016130499\n",
      "7131 : Training: loss:  0.0022589113\n",
      "7132 : Training: loss:  0.0013940794\n",
      "7133 : Training: loss:  0.0016371628\n",
      "7134 : Training: loss:  0.0010647308\n",
      "7135 : Training: loss:  0.0011908348\n",
      "7136 : Training: loss:  0.0021018544\n",
      "7137 : Training: loss:  0.002219843\n",
      "7138 : Training: loss:  0.00096754066\n",
      "7139 : Training: loss:  0.0019085848\n",
      "7140 : Training: loss:  0.0016337365\n",
      "Validation: Loss:  0.015179111  Accuracy:  0.96153843\n",
      "7141 : Training: loss:  0.0017558557\n",
      "7142 : Training: loss:  0.00088715134\n",
      "7143 : Training: loss:  0.0019213797\n",
      "7144 : Training: loss:  0.0029148962\n",
      "7145 : Training: loss:  0.0017236372\n",
      "7146 : Training: loss:  0.0016692413\n",
      "7147 : Training: loss:  0.0030606012\n",
      "7148 : Training: loss:  0.001205139\n",
      "7149 : Training: loss:  0.0019343572\n",
      "7150 : Training: loss:  0.0026219485\n",
      "7151 : Training: loss:  0.00088289764\n",
      "7152 : Training: loss:  0.0016272938\n",
      "7153 : Training: loss:  0.002105425\n",
      "7154 : Training: loss:  0.0029321273\n",
      "7155 : Training: loss:  0.0017394566\n",
      "7156 : Training: loss:  0.002341205\n",
      "7157 : Training: loss:  0.002622783\n",
      "7158 : Training: loss:  0.0012757183\n",
      "7159 : Training: loss:  0.0017887374\n",
      "7160 : Training: loss:  0.0012990308\n",
      "Validation: Loss:  0.015080358  Accuracy:  0.96153843\n",
      "7161 : Training: loss:  0.0017218424\n",
      "7162 : Training: loss:  0.0023316469\n",
      "7163 : Training: loss:  0.0011268957\n",
      "7164 : Training: loss:  0.0022213445\n",
      "7165 : Training: loss:  0.00273226\n",
      "7166 : Training: loss:  0.002508242\n",
      "7167 : Training: loss:  0.0017076844\n",
      "7168 : Training: loss:  0.0016260319\n",
      "7169 : Training: loss:  0.0031814848\n",
      "7170 : Training: loss:  0.0019362578\n",
      "7171 : Training: loss:  0.0019519643\n",
      "7172 : Training: loss:  0.0009866048\n",
      "7173 : Training: loss:  0.0018040963\n",
      "7174 : Training: loss:  0.0013702236\n",
      "7175 : Training: loss:  0.0009998662\n",
      "7176 : Training: loss:  0.0012893549\n",
      "7177 : Training: loss:  0.001666821\n",
      "7178 : Training: loss:  0.0030621036\n",
      "7179 : Training: loss:  0.0016407432\n",
      "7180 : Training: loss:  0.0012698163\n",
      "Validation: Loss:  0.015022278  Accuracy:  0.96153843\n",
      "7181 : Training: loss:  0.0022583285\n",
      "7182 : Training: loss:  0.0022775985\n",
      "7183 : Training: loss:  0.0033473014\n",
      "7184 : Training: loss:  0.0021123495\n",
      "7185 : Training: loss:  0.0021981518\n",
      "7186 : Training: loss:  0.0023201776\n",
      "7187 : Training: loss:  0.002424653\n",
      "7188 : Training: loss:  0.0021656994\n",
      "7189 : Training: loss:  0.0023716053\n",
      "7190 : Training: loss:  0.0016859694\n",
      "7191 : Training: loss:  0.0027690316\n",
      "7192 : Training: loss:  0.0017560868\n",
      "7193 : Training: loss:  0.0008276575\n",
      "7194 : Training: loss:  0.0018334544\n",
      "7195 : Training: loss:  0.0015440936\n",
      "7196 : Training: loss:  0.0023369836\n",
      "7197 : Training: loss:  0.0015511789\n",
      "7198 : Training: loss:  0.0012715654\n",
      "7199 : Training: loss:  0.0021059026\n",
      "7200 : Training: loss:  0.0016105431\n",
      "Validation: Loss:  0.014994429  Accuracy:  0.96153843\n",
      "7201 : Training: loss:  0.0016052978\n",
      "7202 : Training: loss:  0.0017298484\n",
      "7203 : Training: loss:  0.0029977644\n",
      "7204 : Training: loss:  0.0025877843\n",
      "7205 : Training: loss:  0.003114759\n",
      "7206 : Training: loss:  0.0018757203\n",
      "7207 : Training: loss:  0.0020446351\n",
      "7208 : Training: loss:  0.001207333\n",
      "7209 : Training: loss:  0.0018840377\n",
      "7210 : Training: loss:  0.0021920681\n",
      "7211 : Training: loss:  0.0020251556\n",
      "7212 : Training: loss:  0.0027523655\n",
      "7213 : Training: loss:  0.0023270266\n",
      "7214 : Training: loss:  0.0017590857\n",
      "7215 : Training: loss:  0.00092691527\n",
      "7216 : Training: loss:  0.0023217266\n",
      "7217 : Training: loss:  0.0019438715\n",
      "7218 : Training: loss:  0.0009932432\n",
      "7219 : Training: loss:  0.0020196552\n",
      "7220 : Training: loss:  0.001981985\n",
      "Validation: Loss:  0.015018412  Accuracy:  0.96153843\n",
      "7221 : Training: loss:  0.001562637\n",
      "7222 : Training: loss:  0.0016775467\n",
      "7223 : Training: loss:  0.0012191647\n",
      "7224 : Training: loss:  0.0030168365\n",
      "7225 : Training: loss:  0.00097126566\n",
      "7226 : Training: loss:  0.0037678496\n",
      "7227 : Training: loss:  0.00209809\n",
      "7228 : Training: loss:  0.0010832513\n",
      "7229 : Training: loss:  0.0030654622\n",
      "7230 : Training: loss:  0.0012007093\n",
      "7231 : Training: loss:  0.0025385262\n",
      "7232 : Training: loss:  0.0022956808\n",
      "7233 : Training: loss:  0.0015438199\n",
      "7234 : Training: loss:  0.0012048578\n",
      "7235 : Training: loss:  0.0021832949\n",
      "7236 : Training: loss:  0.0017595984\n",
      "7237 : Training: loss:  0.0021161197\n",
      "7238 : Training: loss:  0.0013409525\n",
      "7239 : Training: loss:  0.0029428673\n",
      "7240 : Training: loss:  0.0018218393\n",
      "Validation: Loss:  0.015017303  Accuracy:  0.96153843\n",
      "7241 : Training: loss:  0.0019781122\n",
      "7242 : Training: loss:  0.0015183432\n",
      "7243 : Training: loss:  0.0022192264\n",
      "7244 : Training: loss:  0.0015386572\n",
      "7245 : Training: loss:  0.0025284728\n",
      "7246 : Training: loss:  0.0018071546\n",
      "7247 : Training: loss:  0.0018015929\n",
      "7248 : Training: loss:  0.0017038762\n",
      "7249 : Training: loss:  0.0012350972\n",
      "7250 : Training: loss:  0.0012268566\n",
      "7251 : Training: loss:  0.0022601364\n",
      "7252 : Training: loss:  0.0022024636\n",
      "7253 : Training: loss:  0.0017436013\n",
      "7254 : Training: loss:  0.0029736592\n",
      "7255 : Training: loss:  0.0014491758\n",
      "7256 : Training: loss:  0.0012559942\n",
      "7257 : Training: loss:  0.001291124\n",
      "7258 : Training: loss:  0.00279392\n",
      "7259 : Training: loss:  0.0022044482\n",
      "7260 : Training: loss:  0.002116678\n",
      "Validation: Loss:  0.015042036  Accuracy:  0.96153843\n",
      "7261 : Training: loss:  0.0029113588\n",
      "7262 : Training: loss:  0.0018367998\n",
      "7263 : Training: loss:  0.002346832\n",
      "7264 : Training: loss:  0.0011685104\n",
      "7265 : Training: loss:  0.002595222\n",
      "7266 : Training: loss:  0.0028473404\n",
      "7267 : Training: loss:  0.0020645906\n",
      "7268 : Training: loss:  0.0016839312\n",
      "7269 : Training: loss:  0.0013164819\n",
      "7270 : Training: loss:  0.00092285837\n",
      "7271 : Training: loss:  0.0018753731\n",
      "7272 : Training: loss:  0.001267103\n",
      "7273 : Training: loss:  0.0025362736\n",
      "7274 : Training: loss:  0.0012157859\n",
      "7275 : Training: loss:  0.001505741\n",
      "7276 : Training: loss:  0.0014366081\n",
      "7277 : Training: loss:  0.00078589603\n",
      "7278 : Training: loss:  0.002124798\n",
      "7279 : Training: loss:  0.0020144999\n",
      "7280 : Training: loss:  0.0019169023\n",
      "Validation: Loss:  0.015076936  Accuracy:  0.96153843\n",
      "7281 : Training: loss:  0.002031573\n",
      "7282 : Training: loss:  0.0015221915\n",
      "7283 : Training: loss:  0.0017689941\n",
      "7284 : Training: loss:  0.0012484952\n",
      "7285 : Training: loss:  0.0016270258\n",
      "7286 : Training: loss:  0.0020858685\n",
      "7287 : Training: loss:  0.002173516\n",
      "7288 : Training: loss:  0.0013853176\n",
      "7289 : Training: loss:  0.0035743553\n",
      "7290 : Training: loss:  0.0018106651\n",
      "7291 : Training: loss:  0.0015453538\n",
      "7292 : Training: loss:  0.0018909179\n",
      "7293 : Training: loss:  0.0025582733\n",
      "7294 : Training: loss:  0.0020450426\n",
      "7295 : Training: loss:  0.0011473122\n",
      "7296 : Training: loss:  0.0013061402\n",
      "7297 : Training: loss:  0.0022798125\n",
      "7298 : Training: loss:  0.0013515397\n",
      "7299 : Training: loss:  0.0028249207\n",
      "7300 : Training: loss:  0.0015665529\n",
      "Validation: Loss:  0.015028402  Accuracy:  0.96153843\n",
      "7301 : Training: loss:  0.0023383608\n",
      "7302 : Training: loss:  0.0019628522\n",
      "7303 : Training: loss:  0.0016875608\n",
      "7304 : Training: loss:  0.0020372062\n",
      "7305 : Training: loss:  0.0012672851\n",
      "7306 : Training: loss:  0.0020172007\n",
      "7307 : Training: loss:  0.0017654747\n",
      "7308 : Training: loss:  0.0013993046\n",
      "7309 : Training: loss:  0.0017901296\n",
      "7310 : Training: loss:  0.002153535\n",
      "7311 : Training: loss:  0.0018251727\n",
      "7312 : Training: loss:  0.0018409243\n",
      "7313 : Training: loss:  0.0010975376\n",
      "7314 : Training: loss:  0.0018970749\n",
      "7315 : Training: loss:  0.0017873065\n",
      "7316 : Training: loss:  0.0006630787\n",
      "7317 : Training: loss:  0.0033034424\n",
      "7318 : Training: loss:  0.0026921737\n",
      "7319 : Training: loss:  0.002034297\n",
      "7320 : Training: loss:  0.0018643057\n",
      "Validation: Loss:  0.0150031  Accuracy:  0.96153843\n",
      "7321 : Training: loss:  0.0045775017\n",
      "7322 : Training: loss:  0.0016173715\n",
      "7323 : Training: loss:  0.0023438262\n",
      "7324 : Training: loss:  0.001956782\n",
      "7325 : Training: loss:  0.0022838975\n",
      "7326 : Training: loss:  0.0007005364\n",
      "7327 : Training: loss:  0.0025696633\n",
      "7328 : Training: loss:  0.0023315414\n",
      "7329 : Training: loss:  0.0007463352\n",
      "7330 : Training: loss:  0.0020603708\n",
      "7331 : Training: loss:  0.0023846102\n",
      "7332 : Training: loss:  0.0015523973\n",
      "7333 : Training: loss:  0.0016268166\n",
      "7334 : Training: loss:  0.0020327074\n",
      "7335 : Training: loss:  0.0016537189\n",
      "7336 : Training: loss:  0.002076686\n",
      "7337 : Training: loss:  0.0015686457\n",
      "7338 : Training: loss:  0.0021146303\n",
      "7339 : Training: loss:  0.0018958916\n",
      "7340 : Training: loss:  0.0018086386\n",
      "Validation: Loss:  0.014913046  Accuracy:  0.96153843\n",
      "7341 : Training: loss:  0.0023437196\n",
      "7342 : Training: loss:  0.002080397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7343 : Training: loss:  0.0022175396\n",
      "7344 : Training: loss:  0.0009502173\n",
      "7345 : Training: loss:  0.0019940208\n",
      "7346 : Training: loss:  0.0006130217\n",
      "7347 : Training: loss:  0.0011054338\n",
      "7348 : Training: loss:  0.0016586682\n",
      "7349 : Training: loss:  0.0035647207\n",
      "7350 : Training: loss:  0.0016824408\n",
      "7351 : Training: loss:  0.0014565208\n",
      "7352 : Training: loss:  0.0016651707\n",
      "7353 : Training: loss:  0.001352646\n",
      "7354 : Training: loss:  0.0020142896\n",
      "7355 : Training: loss:  0.0026791054\n",
      "7356 : Training: loss:  0.0025996747\n",
      "7357 : Training: loss:  0.0017457514\n",
      "7358 : Training: loss:  0.0012095316\n",
      "7359 : Training: loss:  0.0027945598\n",
      "7360 : Training: loss:  0.0023522004\n",
      "Validation: Loss:  0.014828542  Accuracy:  0.96153843\n",
      "7361 : Training: loss:  0.0020720377\n",
      "7362 : Training: loss:  0.0025912991\n",
      "7363 : Training: loss:  0.0012797643\n",
      "7364 : Training: loss:  0.000959415\n",
      "7365 : Training: loss:  0.0017293632\n",
      "7366 : Training: loss:  0.0014387242\n",
      "7367 : Training: loss:  0.0019037436\n",
      "7368 : Training: loss:  0.0022558721\n",
      "7369 : Training: loss:  0.0008312272\n",
      "7370 : Training: loss:  0.0022845406\n",
      "7371 : Training: loss:  0.002312064\n",
      "7372 : Training: loss:  0.0012769112\n",
      "7373 : Training: loss:  0.0013992579\n",
      "7374 : Training: loss:  0.0019800311\n",
      "7375 : Training: loss:  0.0015920809\n",
      "7376 : Training: loss:  0.0010538474\n",
      "7377 : Training: loss:  0.0035662022\n",
      "7378 : Training: loss:  0.002125113\n",
      "7379 : Training: loss:  0.0009469735\n",
      "7380 : Training: loss:  0.0030024145\n",
      "Validation: Loss:  0.014715449  Accuracy:  0.96153843\n",
      "7381 : Training: loss:  0.0013940014\n",
      "7382 : Training: loss:  0.0016270848\n",
      "7383 : Training: loss:  0.0010448268\n",
      "7384 : Training: loss:  0.0021272716\n",
      "7385 : Training: loss:  0.0015978498\n",
      "7386 : Training: loss:  0.0014271701\n",
      "7387 : Training: loss:  0.001793207\n",
      "7388 : Training: loss:  0.0009710714\n",
      "7389 : Training: loss:  0.0018676306\n",
      "7390 : Training: loss:  0.0015483616\n",
      "7391 : Training: loss:  0.0019193763\n",
      "7392 : Training: loss:  0.001791618\n",
      "7393 : Training: loss:  0.0011848479\n",
      "7394 : Training: loss:  0.0012923053\n",
      "7395 : Training: loss:  0.0020143446\n",
      "7396 : Training: loss:  0.0011904262\n",
      "7397 : Training: loss:  0.0017953417\n",
      "7398 : Training: loss:  0.0021991548\n",
      "7399 : Training: loss:  0.0018018261\n",
      "7400 : Training: loss:  0.0014311357\n",
      "Validation: Loss:  0.0147381  Accuracy:  0.96153843\n",
      "7401 : Training: loss:  0.0019058773\n",
      "7402 : Training: loss:  0.002261645\n",
      "7403 : Training: loss:  0.0021881687\n",
      "7404 : Training: loss:  0.0016198022\n",
      "7405 : Training: loss:  0.0017348991\n",
      "7406 : Training: loss:  0.0022991803\n",
      "7407 : Training: loss:  0.0018176043\n",
      "7408 : Training: loss:  0.0019620273\n",
      "7409 : Training: loss:  0.0010582126\n",
      "7410 : Training: loss:  0.0013290086\n",
      "7411 : Training: loss:  0.0013854665\n",
      "7412 : Training: loss:  0.0018136426\n",
      "7413 : Training: loss:  0.0017977342\n",
      "7414 : Training: loss:  0.0016363469\n",
      "7415 : Training: loss:  0.0024916115\n",
      "7416 : Training: loss:  0.0017409754\n",
      "7417 : Training: loss:  0.0020680965\n",
      "7418 : Training: loss:  0.0012744417\n",
      "7419 : Training: loss:  0.0014389622\n",
      "7420 : Training: loss:  0.002279075\n",
      "Validation: Loss:  0.014778146  Accuracy:  0.96153843\n",
      "7421 : Training: loss:  0.0014568988\n",
      "7422 : Training: loss:  0.0029848008\n",
      "7423 : Training: loss:  0.001964447\n",
      "7424 : Training: loss:  0.002074267\n",
      "7425 : Training: loss:  0.0005163324\n",
      "7426 : Training: loss:  0.0007084996\n",
      "7427 : Training: loss:  0.0016460951\n",
      "7428 : Training: loss:  0.0017224635\n",
      "7429 : Training: loss:  0.0017058379\n",
      "7430 : Training: loss:  0.0007593169\n",
      "7431 : Training: loss:  0.001962792\n",
      "7432 : Training: loss:  0.0020633715\n",
      "7433 : Training: loss:  0.0020515996\n",
      "7434 : Training: loss:  0.0015016876\n",
      "7435 : Training: loss:  0.0019771038\n",
      "7436 : Training: loss:  0.0014906441\n",
      "7437 : Training: loss:  0.0018271628\n",
      "7438 : Training: loss:  0.0019665104\n",
      "7439 : Training: loss:  0.0015438237\n",
      "7440 : Training: loss:  0.0013976279\n",
      "Validation: Loss:  0.014793987  Accuracy:  0.96153843\n",
      "7441 : Training: loss:  0.001772478\n",
      "7442 : Training: loss:  0.0022469936\n",
      "7443 : Training: loss:  0.0013416973\n",
      "7444 : Training: loss:  0.0014476696\n",
      "7445 : Training: loss:  0.0028363636\n",
      "7446 : Training: loss:  0.0014498292\n",
      "7447 : Training: loss:  0.0028246078\n",
      "7448 : Training: loss:  0.0023915498\n",
      "7449 : Training: loss:  0.0015132853\n",
      "7450 : Training: loss:  0.0010359625\n",
      "7451 : Training: loss:  0.002362962\n",
      "7452 : Training: loss:  0.0013715689\n",
      "7453 : Training: loss:  0.0014980595\n",
      "7454 : Training: loss:  0.0012766772\n",
      "7455 : Training: loss:  0.0016716489\n",
      "7456 : Training: loss:  0.0016765671\n",
      "7457 : Training: loss:  0.001088897\n",
      "7458 : Training: loss:  0.0013390966\n",
      "7459 : Training: loss:  0.0012699255\n",
      "7460 : Training: loss:  0.0013979601\n",
      "Validation: Loss:  0.014780734  Accuracy:  0.96153843\n",
      "7461 : Training: loss:  0.001512676\n",
      "7462 : Training: loss:  0.0017460645\n",
      "7463 : Training: loss:  0.0014298665\n",
      "7464 : Training: loss:  0.0019423543\n",
      "7465 : Training: loss:  0.002590537\n",
      "7466 : Training: loss:  0.0016096956\n",
      "7467 : Training: loss:  0.0013433322\n",
      "7468 : Training: loss:  0.0008860891\n",
      "7469 : Training: loss:  0.0011243144\n",
      "7470 : Training: loss:  0.0011636089\n",
      "7471 : Training: loss:  0.0021777763\n",
      "7472 : Training: loss:  0.0019732923\n",
      "7473 : Training: loss:  0.0007705972\n",
      "7474 : Training: loss:  0.0014534453\n",
      "7475 : Training: loss:  0.0034280275\n",
      "7476 : Training: loss:  0.0017347704\n",
      "7477 : Training: loss:  0.0012047549\n",
      "7478 : Training: loss:  0.0011500806\n",
      "7479 : Training: loss:  0.0013938184\n",
      "7480 : Training: loss:  0.0011753129\n",
      "Validation: Loss:  0.014712414  Accuracy:  0.96153843\n",
      "7481 : Training: loss:  0.0014994778\n",
      "7482 : Training: loss:  0.0014418917\n",
      "7483 : Training: loss:  0.0017408422\n",
      "7484 : Training: loss:  0.001564491\n",
      "7485 : Training: loss:  0.0017895363\n",
      "7486 : Training: loss:  0.0015704253\n",
      "7487 : Training: loss:  0.0014303342\n",
      "7488 : Training: loss:  0.0047297818\n",
      "7489 : Training: loss:  0.0016697898\n",
      "7490 : Training: loss:  0.0016569667\n",
      "7491 : Training: loss:  0.0023894396\n",
      "7492 : Training: loss:  0.0015249854\n",
      "7493 : Training: loss:  0.0024969953\n",
      "7494 : Training: loss:  0.0014506726\n",
      "7495 : Training: loss:  0.00251438\n",
      "7496 : Training: loss:  0.0017420509\n",
      "7497 : Training: loss:  0.0014464373\n",
      "7498 : Training: loss:  0.0012969516\n",
      "7499 : Training: loss:  0.0029484003\n",
      "7500 : Training: loss:  0.0012508376\n",
      "Validation: Loss:  0.014626997  Accuracy:  0.96153843\n",
      "7501 : Training: loss:  0.0012067226\n",
      "7502 : Training: loss:  0.0011362048\n",
      "7503 : Training: loss:  0.0014332116\n",
      "7504 : Training: loss:  0.001505754\n",
      "7505 : Training: loss:  0.0012609442\n",
      "7506 : Training: loss:  0.0016671005\n",
      "7507 : Training: loss:  0.0015546008\n",
      "7508 : Training: loss:  0.0019728413\n",
      "7509 : Training: loss:  0.002372278\n",
      "7510 : Training: loss:  0.0016673865\n",
      "7511 : Training: loss:  0.0014871471\n",
      "7512 : Training: loss:  0.002486772\n",
      "7513 : Training: loss:  0.002835868\n",
      "7514 : Training: loss:  0.0019109446\n",
      "7515 : Training: loss:  0.0011114897\n",
      "7516 : Training: loss:  0.0016759554\n",
      "7517 : Training: loss:  0.0016809026\n",
      "7518 : Training: loss:  0.0014164314\n",
      "7519 : Training: loss:  0.0019053601\n",
      "7520 : Training: loss:  0.0012438344\n",
      "Validation: Loss:  0.01459467  Accuracy:  0.96153843\n",
      "7521 : Training: loss:  0.0013458197\n",
      "7522 : Training: loss:  0.001248522\n",
      "7523 : Training: loss:  0.0012146519\n",
      "7524 : Training: loss:  0.0013928829\n",
      "7525 : Training: loss:  0.0018546217\n",
      "7526 : Training: loss:  0.0011885508\n",
      "7527 : Training: loss:  0.0017568312\n",
      "7528 : Training: loss:  0.0012238311\n",
      "7529 : Training: loss:  0.0017560401\n",
      "7530 : Training: loss:  0.001007269\n",
      "7531 : Training: loss:  0.0019374357\n",
      "7532 : Training: loss:  0.0015278598\n",
      "7533 : Training: loss:  0.0016128524\n",
      "7534 : Training: loss:  0.0014925556\n",
      "7535 : Training: loss:  0.002483364\n",
      "7536 : Training: loss:  0.0010559417\n",
      "7537 : Training: loss:  0.0015692887\n",
      "7538 : Training: loss:  0.0022968191\n",
      "7539 : Training: loss:  0.0020120859\n",
      "7540 : Training: loss:  0.0019693342\n",
      "Validation: Loss:  0.014570684  Accuracy:  0.96153843\n",
      "7541 : Training: loss:  0.0011332576\n",
      "7542 : Training: loss:  0.0020085806\n",
      "7543 : Training: loss:  0.00097568653\n",
      "7544 : Training: loss:  0.00080618594\n",
      "7545 : Training: loss:  0.0010385112\n",
      "7546 : Training: loss:  0.0008670551\n",
      "7547 : Training: loss:  0.001352117\n",
      "7548 : Training: loss:  0.001547882\n",
      "7549 : Training: loss:  0.0021316828\n",
      "7550 : Training: loss:  0.0016104555\n",
      "7551 : Training: loss:  0.0014709069\n",
      "7552 : Training: loss:  0.0022105025\n",
      "7553 : Training: loss:  0.0009591979\n",
      "7554 : Training: loss:  0.0021132494\n",
      "7555 : Training: loss:  0.001469165\n",
      "7556 : Training: loss:  0.001233414\n",
      "7557 : Training: loss:  0.00093076885\n",
      "7558 : Training: loss:  0.00048700103\n",
      "7559 : Training: loss:  0.0016231219\n",
      "7560 : Training: loss:  0.0014224552\n",
      "Validation: Loss:  0.01450312  Accuracy:  0.96153843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7561 : Training: loss:  0.0013995541\n",
      "7562 : Training: loss:  0.0014343954\n",
      "7563 : Training: loss:  0.0010574786\n",
      "7564 : Training: loss:  0.0016746017\n",
      "7565 : Training: loss:  0.0018066476\n",
      "7566 : Training: loss:  0.0010717687\n",
      "7567 : Training: loss:  0.0014860396\n",
      "7568 : Training: loss:  0.001622677\n",
      "7569 : Training: loss:  0.0015001084\n",
      "7570 : Training: loss:  0.0020652171\n",
      "7571 : Training: loss:  0.0017931312\n",
      "7572 : Training: loss:  0.0026249322\n",
      "7573 : Training: loss:  0.002104452\n",
      "7574 : Training: loss:  0.0019624976\n",
      "7575 : Training: loss:  0.0014250823\n",
      "7576 : Training: loss:  0.00094891735\n",
      "7577 : Training: loss:  0.0011708365\n",
      "7578 : Training: loss:  0.0019591395\n",
      "7579 : Training: loss:  0.0012736312\n",
      "7580 : Training: loss:  0.0023393715\n",
      "Validation: Loss:  0.014472917  Accuracy:  0.96153843\n",
      "7581 : Training: loss:  0.0014532661\n",
      "7582 : Training: loss:  0.0017156\n",
      "7583 : Training: loss:  0.0024402845\n",
      "7584 : Training: loss:  0.001241268\n",
      "7585 : Training: loss:  0.000635182\n",
      "7586 : Training: loss:  0.0011882745\n",
      "7587 : Training: loss:  0.0011871948\n",
      "7588 : Training: loss:  0.0017547385\n",
      "7589 : Training: loss:  0.0008819184\n",
      "7590 : Training: loss:  0.0011377765\n",
      "7591 : Training: loss:  0.001878768\n",
      "7592 : Training: loss:  0.00072135986\n",
      "7593 : Training: loss:  0.001921861\n",
      "7594 : Training: loss:  0.0011520599\n",
      "7595 : Training: loss:  0.0012878969\n",
      "7596 : Training: loss:  0.0025457398\n",
      "7597 : Training: loss:  0.001223613\n",
      "7598 : Training: loss:  0.0032751753\n",
      "7599 : Training: loss:  0.000968436\n",
      "7600 : Training: loss:  0.0018432952\n",
      "Validation: Loss:  0.014494601  Accuracy:  0.96153843\n",
      "7601 : Training: loss:  0.0038002892\n",
      "7602 : Training: loss:  0.0012149742\n",
      "7603 : Training: loss:  0.0009118962\n",
      "7604 : Training: loss:  0.0010702285\n",
      "7605 : Training: loss:  0.0019728434\n",
      "7606 : Training: loss:  0.0015448302\n",
      "7607 : Training: loss:  0.0013168079\n",
      "7608 : Training: loss:  0.0015211838\n",
      "7609 : Training: loss:  0.0013738301\n",
      "7610 : Training: loss:  0.0022510334\n",
      "7611 : Training: loss:  0.0010745251\n",
      "7612 : Training: loss:  0.0020208072\n",
      "7613 : Training: loss:  0.001145514\n",
      "7614 : Training: loss:  0.0019741298\n",
      "7615 : Training: loss:  0.0015133087\n",
      "7616 : Training: loss:  0.001853182\n",
      "7617 : Training: loss:  0.0016104641\n",
      "7618 : Training: loss:  0.0015915311\n",
      "7619 : Training: loss:  0.0013203954\n",
      "7620 : Training: loss:  0.0013338876\n",
      "Validation: Loss:  0.014492393  Accuracy:  0.96153843\n",
      "7621 : Training: loss:  0.0016031851\n",
      "7622 : Training: loss:  0.0014335618\n",
      "7623 : Training: loss:  0.00096361316\n",
      "7624 : Training: loss:  0.0014151043\n",
      "7625 : Training: loss:  0.001587939\n",
      "7626 : Training: loss:  0.0016941777\n",
      "7627 : Training: loss:  0.0016870712\n",
      "7628 : Training: loss:  0.0033613397\n",
      "7629 : Training: loss:  0.0010621514\n",
      "7630 : Training: loss:  0.001298285\n",
      "7631 : Training: loss:  0.0008037266\n",
      "7632 : Training: loss:  0.001182285\n",
      "7633 : Training: loss:  0.0013439608\n",
      "7634 : Training: loss:  0.0014408225\n",
      "7635 : Training: loss:  0.0022337104\n",
      "7636 : Training: loss:  0.001128876\n",
      "7637 : Training: loss:  0.0015841627\n",
      "7638 : Training: loss:  0.0018791606\n",
      "7639 : Training: loss:  0.00064484245\n",
      "7640 : Training: loss:  0.0020463215\n",
      "Validation: Loss:  0.014550587  Accuracy:  0.96153843\n",
      "7641 : Training: loss:  0.0015235889\n",
      "7642 : Training: loss:  0.001590103\n",
      "7643 : Training: loss:  0.0009261788\n",
      "7644 : Training: loss:  0.0011647808\n",
      "7645 : Training: loss:  0.0012848075\n",
      "7646 : Training: loss:  0.0016083681\n",
      "7647 : Training: loss:  0.0012514394\n",
      "7648 : Training: loss:  0.0011083244\n",
      "7649 : Training: loss:  0.0017257774\n",
      "7650 : Training: loss:  0.001553235\n",
      "7651 : Training: loss:  0.0014074163\n",
      "7652 : Training: loss:  0.0022426825\n",
      "7653 : Training: loss:  0.0013931075\n",
      "7654 : Training: loss:  0.0016873351\n",
      "7655 : Training: loss:  0.0011388697\n",
      "7656 : Training: loss:  0.0012786252\n",
      "7657 : Training: loss:  0.001211428\n",
      "7658 : Training: loss:  0.001487009\n",
      "7659 : Training: loss:  0.0012814109\n",
      "7660 : Training: loss:  0.0017492006\n",
      "Validation: Loss:  0.014449417  Accuracy:  0.96153843\n",
      "7661 : Training: loss:  0.0010939449\n",
      "7662 : Training: loss:  0.0019449246\n",
      "7663 : Training: loss:  0.001027088\n",
      "7664 : Training: loss:  0.0015613245\n",
      "7665 : Training: loss:  0.000810972\n",
      "7666 : Training: loss:  0.0016549582\n",
      "7667 : Training: loss:  0.0017122234\n",
      "7668 : Training: loss:  0.0012684539\n",
      "7669 : Training: loss:  0.0011304404\n",
      "7670 : Training: loss:  0.0018157148\n",
      "7671 : Training: loss:  0.0015875777\n",
      "7672 : Training: loss:  0.0015678701\n",
      "7673 : Training: loss:  0.0011802108\n",
      "7674 : Training: loss:  0.0018999004\n",
      "7675 : Training: loss:  0.0011958776\n",
      "7676 : Training: loss:  0.001061644\n",
      "7677 : Training: loss:  0.0012308323\n",
      "7678 : Training: loss:  0.0014639284\n",
      "7679 : Training: loss:  0.0011414604\n",
      "7680 : Training: loss:  0.0014571359\n",
      "Validation: Loss:  0.0143198585  Accuracy:  0.96153843\n",
      "7681 : Training: loss:  0.0010025903\n",
      "7682 : Training: loss:  0.0006693251\n",
      "7683 : Training: loss:  0.0018470883\n",
      "7684 : Training: loss:  0.001434604\n",
      "7685 : Training: loss:  0.0015600851\n",
      "7686 : Training: loss:  0.0017810473\n",
      "7687 : Training: loss:  0.0030423957\n",
      "7688 : Training: loss:  0.0013563096\n",
      "7689 : Training: loss:  0.0014190858\n",
      "7690 : Training: loss:  0.0023821553\n",
      "7691 : Training: loss:  0.0017944946\n",
      "7692 : Training: loss:  0.00059809914\n",
      "7693 : Training: loss:  0.00087545416\n",
      "7694 : Training: loss:  0.0008338372\n",
      "7695 : Training: loss:  0.0018527021\n",
      "7696 : Training: loss:  0.0009589824\n",
      "7697 : Training: loss:  0.0011711715\n",
      "7698 : Training: loss:  0.0010953255\n",
      "7699 : Training: loss:  0.0007712592\n",
      "7700 : Training: loss:  0.0016546593\n",
      "Validation: Loss:  0.014272933  Accuracy:  0.96153843\n",
      "7701 : Training: loss:  0.0014197527\n",
      "7702 : Training: loss:  0.0015771306\n",
      "7703 : Training: loss:  0.0012770675\n",
      "7704 : Training: loss:  0.002362435\n",
      "7705 : Training: loss:  0.001350802\n",
      "7706 : Training: loss:  0.0010694106\n",
      "7707 : Training: loss:  0.0018374348\n",
      "7708 : Training: loss:  0.0029100543\n",
      "7709 : Training: loss:  0.0013809153\n",
      "7710 : Training: loss:  0.0020710113\n",
      "7711 : Training: loss:  0.0012370265\n",
      "7712 : Training: loss:  0.0016709155\n",
      "7713 : Training: loss:  0.0018762136\n",
      "7714 : Training: loss:  0.0012602052\n",
      "7715 : Training: loss:  0.0011378507\n",
      "7716 : Training: loss:  0.0011273515\n",
      "7717 : Training: loss:  0.0015901512\n",
      "7718 : Training: loss:  0.0017476518\n",
      "7719 : Training: loss:  0.0013442948\n",
      "7720 : Training: loss:  0.0024679343\n",
      "Validation: Loss:  0.01426316  Accuracy:  0.96153843\n",
      "7721 : Training: loss:  0.0019530001\n",
      "7722 : Training: loss:  0.0012291702\n",
      "7723 : Training: loss:  0.00084807276\n",
      "7724 : Training: loss:  0.0013987159\n",
      "7725 : Training: loss:  0.0010485953\n",
      "7726 : Training: loss:  0.0015070551\n",
      "7727 : Training: loss:  0.0010512674\n",
      "7728 : Training: loss:  0.0015295087\n",
      "7729 : Training: loss:  0.0013014058\n",
      "7730 : Training: loss:  0.0013507856\n",
      "7731 : Training: loss:  0.0018778214\n",
      "7732 : Training: loss:  0.0012997641\n",
      "7733 : Training: loss:  0.0011700129\n",
      "7734 : Training: loss:  0.00066110236\n",
      "7735 : Training: loss:  0.00091091054\n",
      "7736 : Training: loss:  0.0013806747\n",
      "7737 : Training: loss:  0.0015166574\n",
      "7738 : Training: loss:  0.001276958\n",
      "7739 : Training: loss:  0.0009941148\n",
      "7740 : Training: loss:  0.0034045537\n",
      "Validation: Loss:  0.014344683  Accuracy:  0.96153843\n",
      "7741 : Training: loss:  0.00087339716\n",
      "7742 : Training: loss:  0.0013361757\n",
      "7743 : Training: loss:  0.0006808817\n",
      "7744 : Training: loss:  0.0014148636\n",
      "7745 : Training: loss:  0.0010379481\n",
      "7746 : Training: loss:  0.0018020292\n",
      "7747 : Training: loss:  0.002216861\n",
      "7748 : Training: loss:  0.0012968737\n",
      "7749 : Training: loss:  0.0025776061\n",
      "7750 : Training: loss:  0.0007841715\n",
      "7751 : Training: loss:  0.0021996966\n",
      "7752 : Training: loss:  0.0013394995\n",
      "7753 : Training: loss:  0.0021658584\n",
      "7754 : Training: loss:  0.0009865832\n",
      "7755 : Training: loss:  0.001512355\n",
      "7756 : Training: loss:  0.0016328734\n",
      "7757 : Training: loss:  0.0021430815\n",
      "7758 : Training: loss:  0.0016073615\n",
      "7759 : Training: loss:  0.0009844367\n",
      "7760 : Training: loss:  0.0014790832\n",
      "Validation: Loss:  0.014436575  Accuracy:  0.96153843\n",
      "7761 : Training: loss:  0.001485152\n",
      "7762 : Training: loss:  0.0011591649\n",
      "7763 : Training: loss:  0.0018320383\n",
      "7764 : Training: loss:  0.0010571985\n",
      "7765 : Training: loss:  0.001963387\n",
      "7766 : Training: loss:  0.0016175275\n",
      "7767 : Training: loss:  0.0014358277\n",
      "7768 : Training: loss:  0.0011655308\n",
      "7769 : Training: loss:  0.0019843448\n",
      "7770 : Training: loss:  0.0014359349\n",
      "7771 : Training: loss:  0.0012097171\n",
      "7772 : Training: loss:  0.0012270054\n",
      "7773 : Training: loss:  0.0024795795\n",
      "7774 : Training: loss:  0.0009569205\n",
      "7775 : Training: loss:  0.00072972267\n",
      "7776 : Training: loss:  0.001308415\n",
      "7777 : Training: loss:  0.001710506\n",
      "7778 : Training: loss:  0.0010510379\n",
      "7779 : Training: loss:  0.0007727396\n",
      "7780 : Training: loss:  0.0016228905\n",
      "Validation: Loss:  0.014493675  Accuracy:  0.96153843\n",
      "7781 : Training: loss:  0.0013453962\n",
      "7782 : Training: loss:  0.0012726866\n",
      "7783 : Training: loss:  0.0012631529\n",
      "7784 : Training: loss:  0.0015336737\n",
      "7785 : Training: loss:  0.0012907202\n",
      "7786 : Training: loss:  0.0009348028\n",
      "7787 : Training: loss:  0.0010955343\n",
      "7788 : Training: loss:  0.0025423681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7789 : Training: loss:  0.0016232652\n",
      "7790 : Training: loss:  0.0013992807\n",
      "7791 : Training: loss:  0.0027033205\n",
      "7792 : Training: loss:  0.00080939155\n",
      "7793 : Training: loss:  0.0015939118\n",
      "7794 : Training: loss:  0.0010042588\n",
      "7795 : Training: loss:  0.00077174447\n",
      "7796 : Training: loss:  0.0016739789\n",
      "7797 : Training: loss:  0.0008067112\n",
      "7798 : Training: loss:  0.0023634909\n",
      "7799 : Training: loss:  0.00203022\n",
      "7800 : Training: loss:  0.00066473865\n",
      "Validation: Loss:  0.014498788  Accuracy:  0.96153843\n",
      "7801 : Training: loss:  0.00058100553\n",
      "7802 : Training: loss:  0.0015505509\n",
      "7803 : Training: loss:  0.0017307238\n",
      "7804 : Training: loss:  0.0021104775\n",
      "7805 : Training: loss:  0.0017222672\n",
      "7806 : Training: loss:  0.0015570836\n",
      "7807 : Training: loss:  0.0005871832\n",
      "7808 : Training: loss:  0.0017199181\n",
      "7809 : Training: loss:  0.0013116732\n",
      "7810 : Training: loss:  0.0011214477\n",
      "7811 : Training: loss:  0.0011343914\n",
      "7812 : Training: loss:  0.0011148317\n",
      "7813 : Training: loss:  0.0016594976\n",
      "7814 : Training: loss:  0.0013669957\n",
      "7815 : Training: loss:  0.0017802649\n",
      "7816 : Training: loss:  0.001850974\n",
      "7817 : Training: loss:  0.00086496124\n",
      "7818 : Training: loss:  0.0016626619\n",
      "7819 : Training: loss:  0.0009863847\n",
      "7820 : Training: loss:  0.0016104112\n",
      "Validation: Loss:  0.014459084  Accuracy:  0.96153843\n",
      "7821 : Training: loss:  0.0019135855\n",
      "7822 : Training: loss:  0.0015891966\n",
      "7823 : Training: loss:  0.0012091704\n",
      "7824 : Training: loss:  0.0005667988\n",
      "7825 : Training: loss:  0.0011863463\n",
      "7826 : Training: loss:  0.0014243618\n",
      "7827 : Training: loss:  0.0019412218\n",
      "7828 : Training: loss:  0.0012258618\n",
      "7829 : Training: loss:  0.0010296801\n",
      "7830 : Training: loss:  0.0012010629\n",
      "7831 : Training: loss:  0.001051605\n",
      "7832 : Training: loss:  0.0017636166\n",
      "7833 : Training: loss:  0.00073006627\n",
      "7834 : Training: loss:  0.0016830593\n",
      "7835 : Training: loss:  0.0027082444\n",
      "7836 : Training: loss:  0.0011018068\n",
      "7837 : Training: loss:  0.0009245558\n",
      "7838 : Training: loss:  0.0013818154\n",
      "7839 : Training: loss:  0.0010565857\n",
      "7840 : Training: loss:  0.0011486419\n",
      "Validation: Loss:  0.0144701395  Accuracy:  0.96153843\n",
      "7841 : Training: loss:  0.001085101\n",
      "7842 : Training: loss:  0.0010114447\n",
      "7843 : Training: loss:  0.0011348345\n",
      "7844 : Training: loss:  0.0020070195\n",
      "7845 : Training: loss:  0.002289407\n",
      "7846 : Training: loss:  0.0006853427\n",
      "7847 : Training: loss:  0.00083817006\n",
      "7848 : Training: loss:  0.001155244\n",
      "7849 : Training: loss:  0.0014777641\n",
      "7850 : Training: loss:  0.0013862357\n",
      "7851 : Training: loss:  0.00097506354\n",
      "7852 : Training: loss:  0.0014431996\n",
      "7853 : Training: loss:  0.0013205583\n",
      "7854 : Training: loss:  0.0005297992\n",
      "7855 : Training: loss:  0.0005259741\n",
      "7856 : Training: loss:  0.001052322\n",
      "7857 : Training: loss:  0.0015697646\n",
      "7858 : Training: loss:  0.0018683359\n",
      "7859 : Training: loss:  0.0013029696\n",
      "7860 : Training: loss:  0.0015139697\n",
      "Validation: Loss:  0.014491398  Accuracy:  0.96153843\n",
      "7861 : Training: loss:  0.0011471971\n",
      "7862 : Training: loss:  0.0010930818\n",
      "7863 : Training: loss:  0.00137587\n",
      "7864 : Training: loss:  0.0015756758\n",
      "7865 : Training: loss:  0.0008096458\n",
      "7866 : Training: loss:  0.0014883141\n",
      "7867 : Training: loss:  0.0012145505\n",
      "7868 : Training: loss:  0.001368986\n",
      "7869 : Training: loss:  0.0014024412\n",
      "7870 : Training: loss:  0.0010361952\n",
      "7871 : Training: loss:  0.002584395\n",
      "7872 : Training: loss:  0.0014723873\n",
      "7873 : Training: loss:  0.0011394155\n",
      "7874 : Training: loss:  0.0013530842\n",
      "7875 : Training: loss:  0.0011234657\n",
      "7876 : Training: loss:  0.0016693393\n",
      "7877 : Training: loss:  0.0012789669\n",
      "7878 : Training: loss:  0.0014990851\n",
      "7879 : Training: loss:  0.0005193005\n",
      "7880 : Training: loss:  0.001306825\n",
      "Validation: Loss:  0.01447856  Accuracy:  0.96153843\n",
      "7881 : Training: loss:  0.0013095154\n",
      "7882 : Training: loss:  0.0019521975\n",
      "7883 : Training: loss:  0.0016468181\n",
      "7884 : Training: loss:  0.0011672536\n",
      "7885 : Training: loss:  0.0020219674\n",
      "7886 : Training: loss:  0.0022686853\n",
      "7887 : Training: loss:  0.0013899733\n",
      "7888 : Training: loss:  0.0011484531\n",
      "7889 : Training: loss:  0.0016111954\n",
      "7890 : Training: loss:  0.0013425079\n",
      "7891 : Training: loss:  0.0010703737\n",
      "7892 : Training: loss:  0.0011448759\n",
      "7893 : Training: loss:  0.0012076815\n",
      "7894 : Training: loss:  0.0014601775\n",
      "7895 : Training: loss:  0.0010186469\n",
      "7896 : Training: loss:  0.001705631\n",
      "7897 : Training: loss:  0.0011667007\n",
      "7898 : Training: loss:  0.0011976474\n",
      "7899 : Training: loss:  0.0019018376\n",
      "7900 : Training: loss:  0.0017119524\n",
      "Validation: Loss:  0.014376722  Accuracy:  0.96153843\n",
      "7901 : Training: loss:  0.0015401271\n",
      "7902 : Training: loss:  0.0019729312\n",
      "7903 : Training: loss:  0.0009282128\n",
      "7904 : Training: loss:  0.0015892726\n",
      "7905 : Training: loss:  0.0010057113\n",
      "7906 : Training: loss:  0.00076051056\n",
      "7907 : Training: loss:  0.00093539146\n",
      "7908 : Training: loss:  0.00088778546\n",
      "7909 : Training: loss:  0.0014880141\n",
      "7910 : Training: loss:  0.0011932477\n",
      "7911 : Training: loss:  0.0010084858\n",
      "7912 : Training: loss:  0.0016018924\n",
      "7913 : Training: loss:  0.001217074\n",
      "7914 : Training: loss:  0.0018612328\n",
      "7915 : Training: loss:  0.0011650388\n",
      "7916 : Training: loss:  0.0017150099\n",
      "7917 : Training: loss:  0.001081494\n",
      "7918 : Training: loss:  0.0009311353\n",
      "7919 : Training: loss:  0.0016839182\n",
      "7920 : Training: loss:  0.0010902145\n",
      "Validation: Loss:  0.014379561  Accuracy:  0.96153843\n",
      "7921 : Training: loss:  0.0010933336\n",
      "7922 : Training: loss:  0.0009629554\n",
      "7923 : Training: loss:  0.00086240855\n",
      "7924 : Training: loss:  0.0015720336\n",
      "7925 : Training: loss:  0.0011456787\n",
      "7926 : Training: loss:  0.0011283281\n",
      "7927 : Training: loss:  0.0020607605\n",
      "7928 : Training: loss:  0.0021275033\n",
      "7929 : Training: loss:  0.0013572576\n",
      "7930 : Training: loss:  0.0008946195\n",
      "7931 : Training: loss:  0.00089677196\n",
      "7932 : Training: loss:  0.0014290019\n",
      "7933 : Training: loss:  0.0011020754\n",
      "7934 : Training: loss:  0.00065604364\n",
      "7935 : Training: loss:  0.0017582311\n",
      "7936 : Training: loss:  0.0020874534\n",
      "7937 : Training: loss:  0.0017332205\n",
      "7938 : Training: loss:  0.0012977506\n",
      "7939 : Training: loss:  0.0013070329\n",
      "7940 : Training: loss:  0.002140572\n",
      "Validation: Loss:  0.014469747  Accuracy:  0.96153843\n",
      "7941 : Training: loss:  0.0017775069\n",
      "7942 : Training: loss:  0.0014988002\n",
      "7943 : Training: loss:  0.0013014282\n",
      "7944 : Training: loss:  0.001775326\n",
      "7945 : Training: loss:  0.0012805824\n",
      "7946 : Training: loss:  0.001092176\n",
      "7947 : Training: loss:  0.0009785343\n",
      "7948 : Training: loss:  0.0010665102\n",
      "7949 : Training: loss:  0.0017545162\n",
      "7950 : Training: loss:  0.0012108599\n",
      "7951 : Training: loss:  0.0011053904\n",
      "7952 : Training: loss:  0.0023175022\n",
      "7953 : Training: loss:  0.0009063949\n",
      "7954 : Training: loss:  0.0016783609\n",
      "7955 : Training: loss:  0.0013141274\n",
      "7956 : Training: loss:  0.0015370282\n",
      "7957 : Training: loss:  0.0014139084\n",
      "7958 : Training: loss:  0.0009195926\n",
      "7959 : Training: loss:  0.0009182085\n",
      "7960 : Training: loss:  0.0008204528\n",
      "Validation: Loss:  0.0144595215  Accuracy:  0.96153843\n",
      "7961 : Training: loss:  0.0010335009\n",
      "7962 : Training: loss:  0.0011942127\n",
      "7963 : Training: loss:  0.0012175128\n",
      "7964 : Training: loss:  0.0011088117\n",
      "7965 : Training: loss:  0.0015067367\n",
      "7966 : Training: loss:  0.002154082\n",
      "7967 : Training: loss:  0.0019067281\n",
      "7968 : Training: loss:  0.0010162372\n",
      "7969 : Training: loss:  0.0009512809\n",
      "7970 : Training: loss:  0.0018714826\n",
      "7971 : Training: loss:  0.0005243335\n",
      "7972 : Training: loss:  0.0011106307\n",
      "7973 : Training: loss:  0.0010910735\n",
      "7974 : Training: loss:  0.00092700287\n",
      "7975 : Training: loss:  0.0017139041\n",
      "7976 : Training: loss:  0.0010230972\n",
      "7977 : Training: loss:  0.0019231088\n",
      "7978 : Training: loss:  0.0008484193\n",
      "7979 : Training: loss:  0.0017476459\n",
      "7980 : Training: loss:  0.00063737517\n",
      "Validation: Loss:  0.014396621  Accuracy:  0.96153843\n",
      "7981 : Training: loss:  0.0013385477\n",
      "7982 : Training: loss:  0.0025050947\n",
      "7983 : Training: loss:  0.0004804881\n",
      "7984 : Training: loss:  0.0011247998\n",
      "7985 : Training: loss:  0.0014283501\n",
      "7986 : Training: loss:  0.00064826437\n",
      "7987 : Training: loss:  0.0017294767\n",
      "7988 : Training: loss:  0.0015200443\n",
      "7989 : Training: loss:  0.000581615\n",
      "7990 : Training: loss:  0.0016319655\n",
      "7991 : Training: loss:  0.00070330326\n",
      "7992 : Training: loss:  0.001903694\n",
      "7993 : Training: loss:  0.0019820188\n",
      "7994 : Training: loss:  0.0012312209\n",
      "7995 : Training: loss:  0.0010542359\n",
      "7996 : Training: loss:  0.0007127513\n",
      "7997 : Training: loss:  0.001468423\n",
      "7998 : Training: loss:  0.0010307322\n",
      "7999 : Training: loss:  0.0014176148\n",
      "8000 : Training: loss:  0.0014682844\n",
      "Validation: Loss:  0.014355964  Accuracy:  0.96153843\n",
      "8001 : Training: loss:  0.0019566114\n",
      "8002 : Training: loss:  0.0010295295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8003 : Training: loss:  0.0008716788\n",
      "8004 : Training: loss:  0.0014203971\n",
      "8005 : Training: loss:  0.0009223374\n",
      "8006 : Training: loss:  0.00074604515\n",
      "8007 : Training: loss:  0.0009881303\n",
      "8008 : Training: loss:  0.0007563926\n",
      "8009 : Training: loss:  0.0013446509\n",
      "8010 : Training: loss:  0.001546605\n",
      "8011 : Training: loss:  0.0012484648\n",
      "8012 : Training: loss:  0.0012993808\n",
      "8013 : Training: loss:  0.00087650644\n",
      "8014 : Training: loss:  0.0010241091\n",
      "8015 : Training: loss:  0.0008269655\n",
      "8016 : Training: loss:  0.00069118396\n",
      "8017 : Training: loss:  0.0016613617\n",
      "8018 : Training: loss:  0.002706271\n",
      "8019 : Training: loss:  0.0006907386\n",
      "8020 : Training: loss:  0.001095776\n",
      "Validation: Loss:  0.014282206  Accuracy:  0.96153843\n",
      "8021 : Training: loss:  0.0020388258\n",
      "8022 : Training: loss:  0.0027042476\n",
      "8023 : Training: loss:  0.00086370966\n",
      "8024 : Training: loss:  0.0008648138\n",
      "8025 : Training: loss:  0.0007039927\n",
      "8026 : Training: loss:  0.0010204301\n",
      "8027 : Training: loss:  0.00087107596\n",
      "8028 : Training: loss:  0.0029433388\n",
      "8029 : Training: loss:  0.001833347\n",
      "8030 : Training: loss:  0.0015048904\n",
      "8031 : Training: loss:  0.0016334119\n",
      "8032 : Training: loss:  0.0010749729\n",
      "8033 : Training: loss:  0.0017254705\n",
      "8034 : Training: loss:  0.0017051767\n",
      "8035 : Training: loss:  0.0009826906\n",
      "8036 : Training: loss:  0.00068125996\n",
      "8037 : Training: loss:  0.00079525023\n",
      "8038 : Training: loss:  0.0016048817\n",
      "8039 : Training: loss:  0.0013562383\n",
      "8040 : Training: loss:  0.000760472\n",
      "Validation: Loss:  0.014194142  Accuracy:  0.96153843\n",
      "8041 : Training: loss:  0.0006832708\n",
      "8042 : Training: loss:  0.00085261685\n",
      "8043 : Training: loss:  0.0015940167\n",
      "8044 : Training: loss:  0.0010727531\n",
      "8045 : Training: loss:  0.0010647186\n",
      "8046 : Training: loss:  0.00084966543\n",
      "8047 : Training: loss:  0.0012835496\n",
      "8048 : Training: loss:  0.0008097565\n",
      "8049 : Training: loss:  0.0010760461\n",
      "8050 : Training: loss:  0.0011465355\n",
      "8051 : Training: loss:  0.0013485993\n",
      "8052 : Training: loss:  0.0023623998\n",
      "8053 : Training: loss:  0.00067547127\n",
      "8054 : Training: loss:  0.0010380835\n",
      "8055 : Training: loss:  0.00069090753\n",
      "8056 : Training: loss:  0.0013535874\n",
      "8057 : Training: loss:  0.0010474431\n",
      "8058 : Training: loss:  0.0007817927\n",
      "8059 : Training: loss:  0.0006887107\n",
      "8060 : Training: loss:  0.0016988735\n",
      "Validation: Loss:  0.014154798  Accuracy:  0.96153843\n",
      "8061 : Training: loss:  0.0007165913\n",
      "8062 : Training: loss:  0.00089077844\n",
      "8063 : Training: loss:  0.0015651934\n",
      "8064 : Training: loss:  0.0012590078\n",
      "8065 : Training: loss:  0.0016375311\n",
      "8066 : Training: loss:  0.0011506459\n",
      "8067 : Training: loss:  0.00175789\n",
      "8068 : Training: loss:  0.0012577613\n",
      "8069 : Training: loss:  0.0018084735\n",
      "8070 : Training: loss:  0.0007069239\n",
      "8071 : Training: loss:  0.0011143701\n",
      "8072 : Training: loss:  0.0007958734\n",
      "8073 : Training: loss:  0.0005762775\n",
      "8074 : Training: loss:  0.00089238\n",
      "8075 : Training: loss:  0.0014676965\n",
      "8076 : Training: loss:  0.0016553976\n",
      "8077 : Training: loss:  0.0011659899\n",
      "8078 : Training: loss:  0.0019086514\n",
      "8079 : Training: loss:  0.002079595\n",
      "8080 : Training: loss:  0.0011660998\n",
      "Validation: Loss:  0.014102433  Accuracy:  0.96153843\n",
      "8081 : Training: loss:  0.0013131269\n",
      "8082 : Training: loss:  0.0010223615\n",
      "8083 : Training: loss:  0.0011456197\n",
      "8084 : Training: loss:  0.00089692674\n",
      "8085 : Training: loss:  0.001199879\n",
      "8086 : Training: loss:  0.00094694336\n",
      "8087 : Training: loss:  0.0015753034\n",
      "8088 : Training: loss:  0.0017293617\n",
      "8089 : Training: loss:  0.0008820098\n",
      "8090 : Training: loss:  0.00096671603\n",
      "8091 : Training: loss:  0.0010383035\n",
      "8092 : Training: loss:  0.0012570146\n",
      "8093 : Training: loss:  0.0015996343\n",
      "8094 : Training: loss:  0.0010013898\n",
      "8095 : Training: loss:  0.0014691683\n",
      "8096 : Training: loss:  0.001415568\n",
      "8097 : Training: loss:  0.0011058514\n",
      "8098 : Training: loss:  0.0010712686\n",
      "8099 : Training: loss:  0.001601664\n",
      "8100 : Training: loss:  0.00088563235\n",
      "Validation: Loss:  0.014151399  Accuracy:  0.96153843\n",
      "8101 : Training: loss:  0.0010575557\n",
      "8102 : Training: loss:  0.0016739583\n",
      "8103 : Training: loss:  0.00090669334\n",
      "8104 : Training: loss:  0.0013838682\n",
      "8105 : Training: loss:  0.001015803\n",
      "8106 : Training: loss:  0.0028380055\n",
      "8107 : Training: loss:  0.0009645496\n",
      "8108 : Training: loss:  0.0016630149\n",
      "8109 : Training: loss:  0.0014516404\n",
      "8110 : Training: loss:  0.0010683516\n",
      "8111 : Training: loss:  0.00059411983\n",
      "8112 : Training: loss:  0.0013097269\n",
      "8113 : Training: loss:  0.000954621\n",
      "8114 : Training: loss:  0.0006856284\n",
      "8115 : Training: loss:  0.0012225732\n",
      "8116 : Training: loss:  0.0016278677\n",
      "8117 : Training: loss:  0.0013890696\n",
      "8118 : Training: loss:  0.001034067\n",
      "8119 : Training: loss:  0.0022959039\n",
      "8120 : Training: loss:  0.001130569\n",
      "Validation: Loss:  0.014275195  Accuracy:  0.96153843\n",
      "8121 : Training: loss:  0.00079691847\n",
      "8122 : Training: loss:  0.00084299897\n",
      "8123 : Training: loss:  0.0015792566\n",
      "8124 : Training: loss:  0.00080480985\n",
      "8125 : Training: loss:  0.0014682604\n",
      "8126 : Training: loss:  0.0017700578\n",
      "8127 : Training: loss:  0.0018540544\n",
      "8128 : Training: loss:  0.0009821431\n",
      "8129 : Training: loss:  0.0018750271\n",
      "8130 : Training: loss:  0.0012965496\n",
      "8131 : Training: loss:  0.0013138943\n",
      "8132 : Training: loss:  0.0011186802\n",
      "8133 : Training: loss:  0.001773101\n",
      "8134 : Training: loss:  0.0010196158\n",
      "8135 : Training: loss:  0.0009106034\n",
      "8136 : Training: loss:  0.0015049548\n",
      "8137 : Training: loss:  0.0008221282\n",
      "8138 : Training: loss:  0.0009150409\n",
      "8139 : Training: loss:  0.0006667702\n",
      "8140 : Training: loss:  0.0008902672\n",
      "Validation: Loss:  0.014329861  Accuracy:  0.96153843\n",
      "8141 : Training: loss:  0.0009896909\n",
      "8142 : Training: loss:  0.0009316571\n",
      "8143 : Training: loss:  0.0006771665\n",
      "8144 : Training: loss:  0.00090215175\n",
      "8145 : Training: loss:  0.0013338586\n",
      "8146 : Training: loss:  0.002257989\n",
      "8147 : Training: loss:  0.0014031882\n",
      "8148 : Training: loss:  0.0015485012\n",
      "8149 : Training: loss:  0.0008731141\n",
      "8150 : Training: loss:  0.00063148\n",
      "8151 : Training: loss:  0.0011298112\n",
      "8152 : Training: loss:  0.0010880657\n",
      "8153 : Training: loss:  0.0016182957\n",
      "8154 : Training: loss:  0.0009089013\n",
      "8155 : Training: loss:  0.00094614265\n",
      "8156 : Training: loss:  0.0019622238\n",
      "8157 : Training: loss:  0.0011456104\n",
      "8158 : Training: loss:  0.0010483378\n",
      "8159 : Training: loss:  0.0012253216\n",
      "8160 : Training: loss:  0.0009492947\n",
      "Validation: Loss:  0.0142741995  Accuracy:  0.96153843\n",
      "8161 : Training: loss:  0.0012008195\n",
      "8162 : Training: loss:  0.0008308612\n",
      "8163 : Training: loss:  0.0012433261\n",
      "8164 : Training: loss:  0.00067292666\n",
      "8165 : Training: loss:  0.0016301186\n",
      "8166 : Training: loss:  0.0016069995\n",
      "8167 : Training: loss:  0.0014650659\n",
      "8168 : Training: loss:  0.00070890883\n",
      "8169 : Training: loss:  0.0016168345\n",
      "8170 : Training: loss:  0.001801292\n",
      "8171 : Training: loss:  0.000984318\n",
      "8172 : Training: loss:  0.00061409065\n",
      "8173 : Training: loss:  0.0011898545\n",
      "8174 : Training: loss:  0.0009582314\n",
      "8175 : Training: loss:  0.0011179027\n",
      "8176 : Training: loss:  0.0017378937\n",
      "8177 : Training: loss:  0.0014838991\n",
      "8178 : Training: loss:  0.0010825066\n",
      "8179 : Training: loss:  0.0010694311\n",
      "8180 : Training: loss:  0.00063667685\n",
      "Validation: Loss:  0.014166823  Accuracy:  0.96153843\n",
      "8181 : Training: loss:  0.00086880487\n",
      "8182 : Training: loss:  0.00066814484\n",
      "8183 : Training: loss:  0.001191695\n",
      "8184 : Training: loss:  0.00096212176\n",
      "8185 : Training: loss:  0.0010854267\n",
      "8186 : Training: loss:  0.0010542587\n",
      "8187 : Training: loss:  0.000963622\n",
      "8188 : Training: loss:  0.0011687541\n",
      "8189 : Training: loss:  0.001607615\n",
      "8190 : Training: loss:  0.0010796972\n",
      "8191 : Training: loss:  0.0008706894\n",
      "8192 : Training: loss:  0.0011465959\n",
      "8193 : Training: loss:  0.0018000486\n",
      "8194 : Training: loss:  0.0010499486\n",
      "8195 : Training: loss:  0.0013397747\n",
      "8196 : Training: loss:  0.00054054905\n",
      "8197 : Training: loss:  0.0007141172\n",
      "8198 : Training: loss:  0.0009531808\n",
      "8199 : Training: loss:  0.0011672952\n",
      "8200 : Training: loss:  0.0007365617\n",
      "Validation: Loss:  0.014068612  Accuracy:  0.96153843\n",
      "8201 : Training: loss:  0.0017068115\n",
      "8202 : Training: loss:  0.0012569199\n",
      "8203 : Training: loss:  0.001072236\n",
      "8204 : Training: loss:  0.000966106\n",
      "8205 : Training: loss:  0.002059123\n",
      "8206 : Training: loss:  0.00074241206\n",
      "8207 : Training: loss:  0.0012689811\n",
      "8208 : Training: loss:  0.00086372794\n",
      "8209 : Training: loss:  0.00051526685\n",
      "8210 : Training: loss:  0.0009528935\n",
      "8211 : Training: loss:  0.001115246\n",
      "8212 : Training: loss:  0.0012332463\n",
      "8213 : Training: loss:  0.0005609628\n",
      "8214 : Training: loss:  0.0009884699\n",
      "8215 : Training: loss:  0.00056101783\n",
      "8216 : Training: loss:  0.00044534454\n",
      "8217 : Training: loss:  0.0013579428\n",
      "8218 : Training: loss:  0.001200449\n",
      "8219 : Training: loss:  0.00082649814\n",
      "8220 : Training: loss:  0.0007188777\n",
      "Validation: Loss:  0.013921792  Accuracy:  0.96153843\n",
      "8221 : Training: loss:  0.0013070068\n",
      "8222 : Training: loss:  0.0004270473\n",
      "8223 : Training: loss:  0.0005009247\n",
      "8224 : Training: loss:  0.000823116\n",
      "8225 : Training: loss:  0.0006702184\n",
      "8226 : Training: loss:  0.0012958144\n",
      "8227 : Training: loss:  0.0014273447\n",
      "8228 : Training: loss:  0.0009837167\n",
      "8229 : Training: loss:  0.0008779858\n",
      "8230 : Training: loss:  0.0012515681\n",
      "8231 : Training: loss:  0.0010033781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8232 : Training: loss:  0.0011698009\n",
      "8233 : Training: loss:  0.0011647743\n",
      "8234 : Training: loss:  0.0009429906\n",
      "8235 : Training: loss:  0.00081017264\n",
      "8236 : Training: loss:  0.00074264355\n",
      "8237 : Training: loss:  0.0014154835\n",
      "8238 : Training: loss:  0.0015780062\n",
      "8239 : Training: loss:  0.0009342956\n",
      "8240 : Training: loss:  0.0017781021\n",
      "Validation: Loss:  0.01384428  Accuracy:  0.96153843\n",
      "8241 : Training: loss:  0.0013369748\n",
      "8242 : Training: loss:  0.0014782377\n",
      "8243 : Training: loss:  0.0008650229\n",
      "8244 : Training: loss:  0.000972613\n",
      "8245 : Training: loss:  0.0008612017\n",
      "8246 : Training: loss:  0.00092411763\n",
      "8247 : Training: loss:  0.0010589773\n",
      "8248 : Training: loss:  0.00064012123\n",
      "8249 : Training: loss:  0.0012305746\n",
      "8250 : Training: loss:  0.00032241826\n",
      "8251 : Training: loss:  0.001025528\n",
      "8252 : Training: loss:  0.0007105745\n",
      "8253 : Training: loss:  0.0014877254\n",
      "8254 : Training: loss:  0.00076464977\n",
      "8255 : Training: loss:  0.000638112\n",
      "8256 : Training: loss:  0.0008604786\n",
      "8257 : Training: loss:  0.00070549746\n",
      "8258 : Training: loss:  0.001200826\n",
      "8259 : Training: loss:  0.00084086694\n",
      "8260 : Training: loss:  0.0006843765\n",
      "Validation: Loss:  0.0137443375  Accuracy:  0.96153843\n",
      "8261 : Training: loss:  0.0007903778\n",
      "8262 : Training: loss:  0.0011249899\n",
      "8263 : Training: loss:  0.00054480036\n",
      "8264 : Training: loss:  0.00089099194\n",
      "8265 : Training: loss:  0.00059876096\n",
      "8266 : Training: loss:  0.0014411946\n",
      "8267 : Training: loss:  0.00152603\n",
      "8268 : Training: loss:  0.00086755183\n",
      "8269 : Training: loss:  0.0011441659\n",
      "8270 : Training: loss:  0.0009672314\n",
      "8271 : Training: loss:  0.0006173276\n",
      "8272 : Training: loss:  0.0021716603\n",
      "8273 : Training: loss:  0.0017768872\n",
      "8274 : Training: loss:  0.0009799061\n",
      "8275 : Training: loss:  0.0006955582\n",
      "8276 : Training: loss:  0.00081682124\n",
      "8277 : Training: loss:  0.0009927301\n",
      "8278 : Training: loss:  0.0006610681\n",
      "8279 : Training: loss:  0.00089737127\n",
      "8280 : Training: loss:  0.0010499873\n",
      "Validation: Loss:  0.013645746  Accuracy:  0.96153843\n",
      "8281 : Training: loss:  0.0015598243\n",
      "8282 : Training: loss:  0.0006939192\n",
      "8283 : Training: loss:  0.0005791988\n",
      "8284 : Training: loss:  0.00070226827\n",
      "8285 : Training: loss:  0.0011682208\n",
      "8286 : Training: loss:  0.0014999852\n",
      "8287 : Training: loss:  0.00093188137\n",
      "8288 : Training: loss:  0.0008757705\n",
      "8289 : Training: loss:  0.0006314668\n",
      "8290 : Training: loss:  0.0016916163\n",
      "8291 : Training: loss:  0.00079093734\n",
      "8292 : Training: loss:  0.0010116402\n",
      "8293 : Training: loss:  0.0022481908\n",
      "8294 : Training: loss:  0.0007891106\n",
      "8295 : Training: loss:  0.0008079491\n",
      "8296 : Training: loss:  0.0015120368\n",
      "8297 : Training: loss:  0.00076825335\n",
      "8298 : Training: loss:  0.0008571494\n",
      "8299 : Training: loss:  0.0023767087\n",
      "8300 : Training: loss:  0.00091605686\n",
      "Validation: Loss:  0.013738599  Accuracy:  0.96153843\n",
      "8301 : Training: loss:  0.0020586927\n",
      "8302 : Training: loss:  0.00080772984\n",
      "8303 : Training: loss:  0.00071099953\n",
      "8304 : Training: loss:  0.00097030035\n",
      "8305 : Training: loss:  0.0014446704\n",
      "8306 : Training: loss:  0.00096606807\n",
      "8307 : Training: loss:  0.0019303444\n",
      "8308 : Training: loss:  0.0015703454\n",
      "8309 : Training: loss:  0.0010485337\n",
      "8310 : Training: loss:  0.00083607505\n",
      "8311 : Training: loss:  0.0011390854\n",
      "8312 : Training: loss:  0.00094700046\n",
      "8313 : Training: loss:  0.0007427597\n",
      "8314 : Training: loss:  0.00093301956\n",
      "8315 : Training: loss:  0.0008891043\n",
      "8316 : Training: loss:  0.0008631401\n",
      "8317 : Training: loss:  0.001220511\n",
      "8318 : Training: loss:  0.0017819005\n",
      "8319 : Training: loss:  0.0018353685\n",
      "8320 : Training: loss:  0.00077670364\n",
      "Validation: Loss:  0.013844603  Accuracy:  0.96153843\n",
      "8321 : Training: loss:  0.0012349305\n",
      "8322 : Training: loss:  0.0016189815\n",
      "8323 : Training: loss:  0.0007087894\n",
      "8324 : Training: loss:  0.0012477322\n",
      "8325 : Training: loss:  0.00074129674\n",
      "8326 : Training: loss:  0.0013326694\n",
      "8327 : Training: loss:  0.00079405255\n",
      "8328 : Training: loss:  0.0012367263\n",
      "8329 : Training: loss:  0.0010109151\n",
      "8330 : Training: loss:  0.00075808115\n",
      "8331 : Training: loss:  0.0013818615\n",
      "8332 : Training: loss:  0.0009350756\n",
      "8333 : Training: loss:  0.0010127716\n",
      "8334 : Training: loss:  0.0012169563\n",
      "8335 : Training: loss:  0.0012436743\n",
      "8336 : Training: loss:  0.00081300014\n",
      "8337 : Training: loss:  0.00038206673\n",
      "8338 : Training: loss:  0.0012558654\n",
      "8339 : Training: loss:  0.0014668616\n",
      "8340 : Training: loss:  0.0006289854\n",
      "Validation: Loss:  0.013750743  Accuracy:  0.96153843\n",
      "8341 : Training: loss:  0.00081449444\n",
      "8342 : Training: loss:  0.0008109658\n",
      "8343 : Training: loss:  0.0014588124\n",
      "8344 : Training: loss:  0.0010834719\n",
      "8345 : Training: loss:  0.00080338714\n",
      "8346 : Training: loss:  0.0009766732\n",
      "8347 : Training: loss:  0.0017812266\n",
      "8348 : Training: loss:  0.000726113\n",
      "8349 : Training: loss:  0.00079575717\n",
      "8350 : Training: loss:  0.00087959686\n",
      "8351 : Training: loss:  0.000628161\n",
      "8352 : Training: loss:  0.0005845628\n",
      "8353 : Training: loss:  0.0014341143\n",
      "8354 : Training: loss:  0.00046406296\n",
      "8355 : Training: loss:  0.0010352717\n",
      "8356 : Training: loss:  0.0011716768\n",
      "8357 : Training: loss:  0.0009703166\n",
      "8358 : Training: loss:  0.0010110685\n",
      "8359 : Training: loss:  0.0006235224\n",
      "8360 : Training: loss:  0.0009381889\n",
      "Validation: Loss:  0.013789416  Accuracy:  0.96153843\n",
      "8361 : Training: loss:  0.0008618108\n",
      "8362 : Training: loss:  0.0010235292\n",
      "8363 : Training: loss:  0.0009957118\n",
      "8364 : Training: loss:  0.00085189793\n",
      "8365 : Training: loss:  0.0015123577\n",
      "8366 : Training: loss:  0.00079741987\n",
      "8367 : Training: loss:  0.00082641066\n",
      "8368 : Training: loss:  0.0008254282\n",
      "8369 : Training: loss:  0.0007064256\n",
      "8370 : Training: loss:  0.001781509\n",
      "8371 : Training: loss:  0.0018691095\n",
      "8372 : Training: loss:  0.000679788\n",
      "8373 : Training: loss:  0.00095416344\n",
      "8374 : Training: loss:  0.0008971782\n",
      "8375 : Training: loss:  0.0010279887\n",
      "8376 : Training: loss:  0.0010716245\n",
      "8377 : Training: loss:  0.0008701693\n",
      "8378 : Training: loss:  0.0011395277\n",
      "8379 : Training: loss:  0.0010471095\n",
      "8380 : Training: loss:  0.00073998113\n",
      "Validation: Loss:  0.013806432  Accuracy:  0.96153843\n",
      "8381 : Training: loss:  0.0011427306\n",
      "8382 : Training: loss:  0.001831724\n",
      "8383 : Training: loss:  0.00075844384\n",
      "8384 : Training: loss:  0.00093956484\n",
      "8385 : Training: loss:  0.0008770387\n",
      "8386 : Training: loss:  0.00072279957\n",
      "8387 : Training: loss:  0.0011898475\n",
      "8388 : Training: loss:  0.0008996878\n",
      "8389 : Training: loss:  0.0013860479\n",
      "8390 : Training: loss:  0.0007559229\n",
      "8391 : Training: loss:  0.0012292194\n",
      "8392 : Training: loss:  0.00082767894\n",
      "8393 : Training: loss:  0.0011258057\n",
      "8394 : Training: loss:  0.0011281526\n",
      "8395 : Training: loss:  0.0010063099\n",
      "8396 : Training: loss:  0.0011402334\n",
      "8397 : Training: loss:  0.0009285842\n",
      "8398 : Training: loss:  0.0012130797\n",
      "8399 : Training: loss:  0.00070256996\n",
      "8400 : Training: loss:  0.001218307\n",
      "Validation: Loss:  0.013805348  Accuracy:  0.96153843\n",
      "8401 : Training: loss:  0.0015860997\n",
      "8402 : Training: loss:  0.0018272281\n",
      "8403 : Training: loss:  0.0018733726\n",
      "8404 : Training: loss:  0.0006633815\n",
      "8405 : Training: loss:  0.0017597899\n",
      "8406 : Training: loss:  0.0013131835\n",
      "8407 : Training: loss:  0.0009777328\n",
      "8408 : Training: loss:  0.0012766264\n",
      "8409 : Training: loss:  0.0011290229\n",
      "8410 : Training: loss:  0.0010121061\n",
      "8411 : Training: loss:  0.0008914683\n",
      "8412 : Training: loss:  0.000857225\n",
      "8413 : Training: loss:  0.0009366148\n",
      "8414 : Training: loss:  0.0008876457\n",
      "8415 : Training: loss:  0.00065003434\n",
      "8416 : Training: loss:  0.0017024316\n",
      "8417 : Training: loss:  0.0006756545\n",
      "8418 : Training: loss:  0.0010549874\n",
      "8419 : Training: loss:  0.0012931265\n",
      "8420 : Training: loss:  0.0009814097\n",
      "Validation: Loss:  0.013831933  Accuracy:  0.96153843\n",
      "8421 : Training: loss:  0.0010382624\n",
      "8422 : Training: loss:  0.0011283724\n",
      "8423 : Training: loss:  0.0013238272\n",
      "8424 : Training: loss:  0.0008997004\n",
      "8425 : Training: loss:  0.0012940624\n",
      "8426 : Training: loss:  0.0005216995\n",
      "8427 : Training: loss:  0.0010519621\n",
      "8428 : Training: loss:  0.0008300741\n",
      "8429 : Training: loss:  0.0012774701\n",
      "8430 : Training: loss:  0.00062240387\n",
      "8431 : Training: loss:  0.000911673\n",
      "8432 : Training: loss:  0.0008506459\n",
      "8433 : Training: loss:  0.0014296831\n",
      "8434 : Training: loss:  0.0010019117\n",
      "8435 : Training: loss:  0.00096106145\n",
      "8436 : Training: loss:  0.0010319288\n",
      "8437 : Training: loss:  0.0010717588\n",
      "8438 : Training: loss:  0.0011941598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8439 : Training: loss:  0.0011328226\n",
      "8440 : Training: loss:  0.00072158047\n",
      "Validation: Loss:  0.013859397  Accuracy:  0.96153843\n",
      "8441 : Training: loss:  0.00065909734\n",
      "8442 : Training: loss:  0.0016878856\n",
      "8443 : Training: loss:  0.0014263983\n",
      "8444 : Training: loss:  0.0013173185\n",
      "8445 : Training: loss:  0.00080543564\n",
      "8446 : Training: loss:  0.0009716082\n",
      "8447 : Training: loss:  0.00084834517\n",
      "8448 : Training: loss:  0.0012225917\n",
      "8449 : Training: loss:  0.00047702715\n",
      "8450 : Training: loss:  0.0013618594\n",
      "8451 : Training: loss:  0.0011188904\n",
      "8452 : Training: loss:  0.001357166\n",
      "8453 : Training: loss:  0.0008304319\n",
      "8454 : Training: loss:  0.0015451169\n",
      "8455 : Training: loss:  0.00097647053\n",
      "8456 : Training: loss:  0.0017234561\n",
      "8457 : Training: loss:  0.0017965472\n",
      "8458 : Training: loss:  0.0010572734\n",
      "8459 : Training: loss:  0.0007555922\n",
      "8460 : Training: loss:  0.0010394859\n",
      "Validation: Loss:  0.013899117  Accuracy:  0.96153843\n",
      "8461 : Training: loss:  0.0010692676\n",
      "8462 : Training: loss:  0.0004747691\n",
      "8463 : Training: loss:  0.0007746081\n",
      "8464 : Training: loss:  0.0009985705\n",
      "8465 : Training: loss:  0.0011483087\n",
      "8466 : Training: loss:  0.0015943052\n",
      "8467 : Training: loss:  0.0010520184\n",
      "8468 : Training: loss:  0.0012609924\n",
      "8469 : Training: loss:  0.0013671874\n",
      "8470 : Training: loss:  0.001150442\n",
      "8471 : Training: loss:  0.0009541329\n",
      "8472 : Training: loss:  0.0013798197\n",
      "8473 : Training: loss:  0.00090551557\n",
      "8474 : Training: loss:  0.0015470114\n",
      "8475 : Training: loss:  0.0013287329\n",
      "8476 : Training: loss:  0.00073966786\n",
      "8477 : Training: loss:  0.00066629663\n",
      "8478 : Training: loss:  0.0013417033\n",
      "8479 : Training: loss:  0.0016298835\n",
      "8480 : Training: loss:  0.0009215456\n",
      "Validation: Loss:  0.013897155  Accuracy:  0.96153843\n",
      "8481 : Training: loss:  0.0011964827\n",
      "8482 : Training: loss:  0.0010641649\n",
      "8483 : Training: loss:  0.00072077825\n",
      "8484 : Training: loss:  0.00046733808\n",
      "8485 : Training: loss:  0.0011086417\n",
      "8486 : Training: loss:  0.0007635355\n",
      "8487 : Training: loss:  0.0011615887\n",
      "8488 : Training: loss:  0.00097155455\n",
      "8489 : Training: loss:  0.0007109069\n",
      "8490 : Training: loss:  0.0010600616\n",
      "8491 : Training: loss:  0.0011858882\n",
      "8492 : Training: loss:  0.000994056\n",
      "8493 : Training: loss:  0.000935356\n",
      "8494 : Training: loss:  0.0006160454\n",
      "8495 : Training: loss:  0.0004834721\n",
      "8496 : Training: loss:  0.000913657\n",
      "8497 : Training: loss:  0.00040763704\n",
      "8498 : Training: loss:  0.0010060687\n",
      "8499 : Training: loss:  0.00084130454\n",
      "8500 : Training: loss:  0.0012201914\n",
      "Validation: Loss:  0.013914292  Accuracy:  0.96153843\n",
      "8501 : Training: loss:  0.0010730153\n",
      "8502 : Training: loss:  0.0011509404\n",
      "8503 : Training: loss:  0.00059091963\n",
      "8504 : Training: loss:  0.0007290417\n",
      "8505 : Training: loss:  0.0010145379\n",
      "8506 : Training: loss:  0.001158365\n",
      "8507 : Training: loss:  0.0011412341\n",
      "8508 : Training: loss:  0.0013898961\n",
      "8509 : Training: loss:  0.000997153\n",
      "8510 : Training: loss:  0.00049729296\n",
      "8511 : Training: loss:  0.0010692889\n",
      "8512 : Training: loss:  0.001050561\n",
      "8513 : Training: loss:  0.0006414867\n",
      "8514 : Training: loss:  0.0016493374\n",
      "8515 : Training: loss:  0.0009327428\n",
      "8516 : Training: loss:  0.0009451409\n",
      "8517 : Training: loss:  0.0009183178\n",
      "8518 : Training: loss:  0.0012930658\n",
      "8519 : Training: loss:  0.0011698884\n",
      "8520 : Training: loss:  0.001133764\n",
      "Validation: Loss:  0.013941664  Accuracy:  0.96153843\n",
      "8521 : Training: loss:  0.0008451532\n",
      "8522 : Training: loss:  0.0009335129\n",
      "8523 : Training: loss:  0.0009330433\n",
      "8524 : Training: loss:  0.0010857217\n",
      "8525 : Training: loss:  0.0007229338\n",
      "8526 : Training: loss:  0.00096043316\n",
      "8527 : Training: loss:  0.0010138332\n",
      "8528 : Training: loss:  0.0014656087\n",
      "8529 : Training: loss:  0.00045509456\n",
      "8530 : Training: loss:  0.00070108665\n",
      "8531 : Training: loss:  0.0007855571\n",
      "8532 : Training: loss:  0.0013560953\n",
      "8533 : Training: loss:  0.0008580589\n",
      "8534 : Training: loss:  0.00095731753\n",
      "8535 : Training: loss:  0.0012595952\n",
      "8536 : Training: loss:  0.0009460456\n",
      "8537 : Training: loss:  0.0018618841\n",
      "8538 : Training: loss:  0.00103572\n",
      "8539 : Training: loss:  0.0013350094\n",
      "8540 : Training: loss:  0.0013074911\n",
      "Validation: Loss:  0.013949101  Accuracy:  0.96153843\n",
      "8541 : Training: loss:  0.00074381125\n",
      "8542 : Training: loss:  0.0007784571\n",
      "8543 : Training: loss:  0.00051244715\n",
      "8544 : Training: loss:  0.0007121702\n",
      "8545 : Training: loss:  0.0013799479\n",
      "8546 : Training: loss:  0.0008723843\n",
      "8547 : Training: loss:  0.00048314736\n",
      "8548 : Training: loss:  0.0008868987\n",
      "8549 : Training: loss:  0.000688447\n",
      "8550 : Training: loss:  0.00065205834\n",
      "8551 : Training: loss:  0.0015731284\n",
      "8552 : Training: loss:  0.00062532315\n",
      "8553 : Training: loss:  0.0014496354\n",
      "8554 : Training: loss:  0.00060777285\n",
      "8555 : Training: loss:  0.001001341\n",
      "8556 : Training: loss:  0.001616363\n",
      "8557 : Training: loss:  0.0015129603\n",
      "8558 : Training: loss:  0.00079972943\n",
      "8559 : Training: loss:  0.0011020207\n",
      "8560 : Training: loss:  0.00077304087\n",
      "Validation: Loss:  0.013883134  Accuracy:  0.96153843\n",
      "8561 : Training: loss:  0.0011547758\n",
      "8562 : Training: loss:  0.0009533618\n",
      "8563 : Training: loss:  0.0009667584\n",
      "8564 : Training: loss:  0.0011376413\n",
      "8565 : Training: loss:  0.002990316\n",
      "8566 : Training: loss:  0.00049535785\n",
      "8567 : Training: loss:  0.0009089979\n",
      "8568 : Training: loss:  0.0007652464\n",
      "8569 : Training: loss:  0.0011141541\n",
      "8570 : Training: loss:  0.00079431135\n",
      "8571 : Training: loss:  0.0011968582\n",
      "8572 : Training: loss:  0.0010283523\n",
      "8573 : Training: loss:  0.002318891\n",
      "8574 : Training: loss:  0.00072118075\n",
      "8575 : Training: loss:  0.0013196619\n",
      "8576 : Training: loss:  0.0014720483\n",
      "8577 : Training: loss:  0.0007253514\n",
      "8578 : Training: loss:  0.00093397027\n",
      "8579 : Training: loss:  0.0011746649\n",
      "8580 : Training: loss:  0.0009463015\n",
      "Validation: Loss:  0.013842795  Accuracy:  0.96153843\n",
      "8581 : Training: loss:  0.001161091\n",
      "8582 : Training: loss:  0.00038426556\n",
      "8583 : Training: loss:  0.00094259897\n",
      "8584 : Training: loss:  0.00072495785\n",
      "8585 : Training: loss:  0.0009675703\n",
      "8586 : Training: loss:  0.0010295522\n",
      "8587 : Training: loss:  0.0006244522\n",
      "8588 : Training: loss:  0.00083659764\n",
      "8589 : Training: loss:  0.00096601294\n",
      "8590 : Training: loss:  0.001086672\n",
      "8591 : Training: loss:  0.0009919509\n",
      "8592 : Training: loss:  0.0011802741\n",
      "8593 : Training: loss:  0.00086605403\n",
      "8594 : Training: loss:  0.001418318\n",
      "8595 : Training: loss:  0.0008768902\n",
      "8596 : Training: loss:  0.0006555752\n",
      "8597 : Training: loss:  0.0011949019\n",
      "8598 : Training: loss:  0.0007690606\n",
      "8599 : Training: loss:  0.00086427364\n",
      "8600 : Training: loss:  0.00048799688\n",
      "Validation: Loss:  0.013779065  Accuracy:  0.96153843\n",
      "8601 : Training: loss:  0.00032325913\n",
      "8602 : Training: loss:  0.00040598647\n",
      "8603 : Training: loss:  0.0013382626\n",
      "8604 : Training: loss:  0.0024089774\n",
      "8605 : Training: loss:  0.001738421\n",
      "8606 : Training: loss:  0.0014094241\n",
      "8607 : Training: loss:  0.0012037397\n",
      "8608 : Training: loss:  0.0008193796\n",
      "8609 : Training: loss:  0.0016121202\n",
      "8610 : Training: loss:  0.00073050364\n",
      "8611 : Training: loss:  0.0008949031\n",
      "8612 : Training: loss:  0.0015418648\n",
      "8613 : Training: loss:  0.0012310798\n",
      "8614 : Training: loss:  0.0007710679\n",
      "8615 : Training: loss:  0.00085036096\n",
      "8616 : Training: loss:  0.0012267462\n",
      "8617 : Training: loss:  0.00077089225\n",
      "8618 : Training: loss:  0.0013867556\n",
      "8619 : Training: loss:  0.0009753987\n",
      "8620 : Training: loss:  0.0006029281\n",
      "Validation: Loss:  0.013728251  Accuracy:  0.96153843\n",
      "8621 : Training: loss:  0.0011464395\n",
      "8622 : Training: loss:  0.0010671434\n",
      "8623 : Training: loss:  0.0006825733\n",
      "8624 : Training: loss:  0.0016238927\n",
      "8625 : Training: loss:  0.0014099014\n",
      "8626 : Training: loss:  0.00051397225\n",
      "8627 : Training: loss:  0.00045718928\n",
      "8628 : Training: loss:  0.0013625978\n",
      "8629 : Training: loss:  0.0010541261\n",
      "8630 : Training: loss:  0.0007520661\n",
      "8631 : Training: loss:  0.0009798006\n",
      "8632 : Training: loss:  0.0008848566\n",
      "8633 : Training: loss:  0.0011762427\n",
      "8634 : Training: loss:  0.0013186188\n",
      "8635 : Training: loss:  0.0007725111\n",
      "8636 : Training: loss:  0.0011152326\n",
      "8637 : Training: loss:  0.0009600571\n",
      "8638 : Training: loss:  0.0006979503\n",
      "8639 : Training: loss:  0.0011883235\n",
      "8640 : Training: loss:  0.0008916023\n",
      "Validation: Loss:  0.013635456  Accuracy:  0.96153843\n",
      "8641 : Training: loss:  0.0015215902\n",
      "8642 : Training: loss:  0.0010619039\n",
      "8643 : Training: loss:  0.0013364612\n",
      "8644 : Training: loss:  0.0006585319\n",
      "8645 : Training: loss:  0.0009973584\n",
      "8646 : Training: loss:  0.0012067943\n",
      "8647 : Training: loss:  0.00060440326\n",
      "8648 : Training: loss:  0.0014525143\n",
      "8649 : Training: loss:  0.00081832067\n",
      "8650 : Training: loss:  0.00078135735\n",
      "8651 : Training: loss:  0.000932611\n",
      "8652 : Training: loss:  0.0010073343\n",
      "8653 : Training: loss:  0.0007936715\n",
      "8654 : Training: loss:  0.00086314604\n",
      "8655 : Training: loss:  0.0014835356\n",
      "8656 : Training: loss:  0.00060551503\n",
      "8657 : Training: loss:  0.00074258284\n",
      "8658 : Training: loss:  0.0014174333\n",
      "8659 : Training: loss:  0.001070988\n",
      "8660 : Training: loss:  0.0007054746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss:  0.0136051  Accuracy:  0.96153843\n",
      "8661 : Training: loss:  0.0006287709\n",
      "8662 : Training: loss:  0.0011300324\n",
      "8663 : Training: loss:  0.00079586846\n",
      "8664 : Training: loss:  0.0005296792\n",
      "8665 : Training: loss:  0.00072034815\n",
      "8666 : Training: loss:  0.00082004216\n",
      "8667 : Training: loss:  0.0009053423\n",
      "8668 : Training: loss:  0.0008004592\n",
      "8669 : Training: loss:  0.00063106307\n",
      "8670 : Training: loss:  0.0008005499\n",
      "8671 : Training: loss:  0.00085983466\n",
      "8672 : Training: loss:  0.0012034429\n",
      "8673 : Training: loss:  0.00035072616\n",
      "8674 : Training: loss:  0.0013888635\n",
      "8675 : Training: loss:  0.0012588346\n",
      "8676 : Training: loss:  0.0006187404\n",
      "8677 : Training: loss:  0.0009640187\n",
      "8678 : Training: loss:  0.00062741817\n",
      "8679 : Training: loss:  0.000829618\n",
      "8680 : Training: loss:  0.0011648729\n",
      "Validation: Loss:  0.013659205  Accuracy:  0.96153843\n",
      "8681 : Training: loss:  0.00058683916\n",
      "8682 : Training: loss:  0.0008940749\n",
      "8683 : Training: loss:  0.0010981504\n",
      "8684 : Training: loss:  0.00076471973\n",
      "8685 : Training: loss:  0.00069775287\n",
      "8686 : Training: loss:  0.0006409812\n",
      "8687 : Training: loss:  0.0010298516\n",
      "8688 : Training: loss:  0.000989905\n",
      "8689 : Training: loss:  0.00036237284\n",
      "8690 : Training: loss:  0.00061780034\n",
      "8691 : Training: loss:  0.0013367882\n",
      "8692 : Training: loss:  0.0009199116\n",
      "8693 : Training: loss:  0.0011168369\n",
      "8694 : Training: loss:  0.00085549906\n",
      "8695 : Training: loss:  0.001681515\n",
      "8696 : Training: loss:  0.0005422417\n",
      "8697 : Training: loss:  0.0011289935\n",
      "8698 : Training: loss:  0.00096187694\n",
      "8699 : Training: loss:  0.0010348095\n",
      "8700 : Training: loss:  0.0008927365\n",
      "Validation: Loss:  0.013664255  Accuracy:  0.96153843\n",
      "8701 : Training: loss:  0.00029328206\n",
      "8702 : Training: loss:  0.0006437847\n",
      "8703 : Training: loss:  0.0005127917\n",
      "8704 : Training: loss:  0.00051618996\n",
      "8705 : Training: loss:  0.0014229171\n",
      "8706 : Training: loss:  0.00069589925\n",
      "8707 : Training: loss:  0.00103857\n",
      "8708 : Training: loss:  0.00086242566\n",
      "8709 : Training: loss:  0.0011698903\n",
      "8710 : Training: loss:  0.0012180039\n",
      "8711 : Training: loss:  0.00080453913\n",
      "8712 : Training: loss:  0.001159809\n",
      "8713 : Training: loss:  0.0014410729\n",
      "8714 : Training: loss:  0.00055032\n",
      "8715 : Training: loss:  0.001003197\n",
      "8716 : Training: loss:  0.0007815449\n",
      "8717 : Training: loss:  0.0011826591\n",
      "8718 : Training: loss:  0.000764656\n",
      "8719 : Training: loss:  0.00085937494\n",
      "8720 : Training: loss:  0.00022862313\n",
      "Validation: Loss:  0.013607468  Accuracy:  0.96153843\n",
      "8721 : Training: loss:  0.0006042444\n",
      "8722 : Training: loss:  0.0011122391\n",
      "8723 : Training: loss:  0.0009037105\n",
      "8724 : Training: loss:  0.0010578176\n",
      "8725 : Training: loss:  0.0006902954\n",
      "8726 : Training: loss:  0.0008082997\n",
      "8727 : Training: loss:  0.0011737077\n",
      "8728 : Training: loss:  0.0006848\n",
      "8729 : Training: loss:  0.00046063255\n",
      "8730 : Training: loss:  0.00063333125\n",
      "8731 : Training: loss:  0.0009262784\n",
      "8732 : Training: loss:  0.0011739251\n",
      "8733 : Training: loss:  0.0006894982\n",
      "8734 : Training: loss:  0.00077112066\n",
      "8735 : Training: loss:  0.0007519305\n",
      "8736 : Training: loss:  0.0011167639\n",
      "8737 : Training: loss:  0.00073207874\n",
      "8738 : Training: loss:  0.00062607\n",
      "8739 : Training: loss:  0.00086587254\n",
      "8740 : Training: loss:  0.0014921087\n",
      "Validation: Loss:  0.013644778  Accuracy:  0.96153843\n",
      "8741 : Training: loss:  0.0009767084\n",
      "8742 : Training: loss:  0.001261942\n",
      "8743 : Training: loss:  0.001397515\n",
      "8744 : Training: loss:  0.00048000764\n",
      "8745 : Training: loss:  0.0006109907\n",
      "8746 : Training: loss:  0.0013014459\n",
      "8747 : Training: loss:  0.00056674733\n",
      "8748 : Training: loss:  0.0013622813\n",
      "8749 : Training: loss:  0.0010926101\n",
      "8750 : Training: loss:  0.0010103494\n",
      "8751 : Training: loss:  0.0006174565\n",
      "8752 : Training: loss:  0.001625392\n",
      "8753 : Training: loss:  0.0006786921\n",
      "8754 : Training: loss:  0.00068040367\n",
      "8755 : Training: loss:  0.0005306402\n",
      "8756 : Training: loss:  0.0011744447\n",
      "8757 : Training: loss:  0.0004909817\n",
      "8758 : Training: loss:  0.0009370445\n",
      "8759 : Training: loss:  0.0008358799\n",
      "8760 : Training: loss:  0.00076606835\n",
      "Validation: Loss:  0.013696309  Accuracy:  0.96153843\n",
      "8761 : Training: loss:  0.0011196532\n",
      "8762 : Training: loss:  0.0010315332\n",
      "8763 : Training: loss:  0.0007997507\n",
      "8764 : Training: loss:  0.00078159303\n",
      "8765 : Training: loss:  0.00056190765\n",
      "8766 : Training: loss:  0.0004900027\n",
      "8767 : Training: loss:  0.00059331\n",
      "8768 : Training: loss:  0.0011598272\n",
      "8769 : Training: loss:  0.0006614465\n",
      "8770 : Training: loss:  0.0010789508\n",
      "8771 : Training: loss:  0.00064440444\n",
      "8772 : Training: loss:  0.00082193984\n",
      "8773 : Training: loss:  0.000438547\n",
      "8774 : Training: loss:  0.0007243957\n",
      "8775 : Training: loss:  0.0008497119\n",
      "8776 : Training: loss:  0.0010464155\n",
      "8777 : Training: loss:  0.00090822886\n",
      "8778 : Training: loss:  0.0011791774\n",
      "8779 : Training: loss:  0.00082125835\n",
      "8780 : Training: loss:  0.0006253392\n",
      "Validation: Loss:  0.013656611  Accuracy:  0.96153843\n",
      "8781 : Training: loss:  0.0006607612\n",
      "8782 : Training: loss:  0.0011317094\n",
      "8783 : Training: loss:  0.00094392576\n",
      "8784 : Training: loss:  0.0006047475\n",
      "8785 : Training: loss:  0.0012061964\n",
      "8786 : Training: loss:  0.00084290723\n",
      "8787 : Training: loss:  0.0008922913\n",
      "8788 : Training: loss:  0.0011937842\n",
      "8789 : Training: loss:  0.00095175\n",
      "8790 : Training: loss:  0.00096250075\n",
      "8791 : Training: loss:  0.00084650976\n",
      "8792 : Training: loss:  0.0007701355\n",
      "8793 : Training: loss:  0.0011005272\n",
      "8794 : Training: loss:  0.00087364163\n",
      "8795 : Training: loss:  0.00065353117\n",
      "8796 : Training: loss:  0.0011825413\n",
      "8797 : Training: loss:  0.0012201176\n",
      "8798 : Training: loss:  0.0009864527\n",
      "8799 : Training: loss:  0.0008143053\n",
      "8800 : Training: loss:  0.0008343008\n",
      "Validation: Loss:  0.013618517  Accuracy:  0.96153843\n",
      "8801 : Training: loss:  0.00040823274\n",
      "8802 : Training: loss:  0.00037394755\n",
      "8803 : Training: loss:  0.00039951995\n",
      "8804 : Training: loss:  0.0009182774\n",
      "8805 : Training: loss:  0.0008149053\n",
      "8806 : Training: loss:  0.00043632183\n",
      "8807 : Training: loss:  0.0014429869\n",
      "8808 : Training: loss:  0.0012415648\n",
      "8809 : Training: loss:  0.0009495157\n",
      "8810 : Training: loss:  0.0006273501\n",
      "8811 : Training: loss:  0.00047770157\n",
      "8812 : Training: loss:  0.0007053681\n",
      "8813 : Training: loss:  0.0011359963\n",
      "8814 : Training: loss:  0.00031167336\n",
      "8815 : Training: loss:  0.00053745793\n",
      "8816 : Training: loss:  0.00047574207\n",
      "8817 : Training: loss:  0.00062113226\n",
      "8818 : Training: loss:  0.00085694296\n",
      "8819 : Training: loss:  0.0009735013\n",
      "8820 : Training: loss:  0.0007140155\n",
      "Validation: Loss:  0.013606858  Accuracy:  0.96153843\n",
      "8821 : Training: loss:  0.0010021501\n",
      "8822 : Training: loss:  0.00020165814\n",
      "8823 : Training: loss:  0.0007515319\n",
      "8824 : Training: loss:  0.000870337\n",
      "8825 : Training: loss:  0.00071486604\n",
      "8826 : Training: loss:  0.0005720367\n",
      "8827 : Training: loss:  0.0010127311\n",
      "8828 : Training: loss:  0.00060778117\n",
      "8829 : Training: loss:  0.00048736087\n",
      "8830 : Training: loss:  0.0007691558\n",
      "8831 : Training: loss:  0.0006515312\n",
      "8832 : Training: loss:  0.0005498897\n",
      "8833 : Training: loss:  0.0011319587\n",
      "8834 : Training: loss:  0.00092532625\n",
      "8835 : Training: loss:  0.00066902087\n",
      "8836 : Training: loss:  0.00053321465\n",
      "8837 : Training: loss:  0.0007834844\n",
      "8838 : Training: loss:  0.0006433744\n",
      "8839 : Training: loss:  0.00066172134\n",
      "8840 : Training: loss:  0.0009896124\n",
      "Validation: Loss:  0.013635007  Accuracy:  0.96153843\n",
      "8841 : Training: loss:  0.00045009577\n",
      "8842 : Training: loss:  0.0010226422\n",
      "8843 : Training: loss:  0.00082832883\n",
      "8844 : Training: loss:  0.0005097267\n",
      "8845 : Training: loss:  0.0011312751\n",
      "8846 : Training: loss:  0.0004772192\n",
      "8847 : Training: loss:  0.0012075937\n",
      "8848 : Training: loss:  0.0017473692\n",
      "8849 : Training: loss:  0.0004211373\n",
      "8850 : Training: loss:  0.0010346228\n",
      "8851 : Training: loss:  0.0010511259\n",
      "8852 : Training: loss:  0.0011399651\n",
      "8853 : Training: loss:  0.00081334304\n",
      "8854 : Training: loss:  0.00056210876\n",
      "8855 : Training: loss:  0.0010294428\n",
      "8856 : Training: loss:  0.00050306105\n",
      "8857 : Training: loss:  0.0009138318\n",
      "8858 : Training: loss:  0.0011341901\n",
      "8859 : Training: loss:  0.00093658484\n",
      "8860 : Training: loss:  0.00043082915\n",
      "Validation: Loss:  0.013599139  Accuracy:  0.96153843\n",
      "8861 : Training: loss:  0.00182423\n",
      "8862 : Training: loss:  0.0006338876\n",
      "8863 : Training: loss:  0.0008304584\n",
      "8864 : Training: loss:  0.00071754877\n",
      "8865 : Training: loss:  0.0014582435\n",
      "8866 : Training: loss:  0.0009648518\n",
      "8867 : Training: loss:  0.00042975997\n",
      "8868 : Training: loss:  0.0010074178\n",
      "8869 : Training: loss:  0.0007188869\n",
      "8870 : Training: loss:  0.0010734001\n",
      "8871 : Training: loss:  0.0005171264\n",
      "8872 : Training: loss:  0.0007434243\n",
      "8873 : Training: loss:  0.00081750593\n",
      "8874 : Training: loss:  0.0013703124\n",
      "8875 : Training: loss:  0.0005748228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8876 : Training: loss:  0.0007789423\n",
      "8877 : Training: loss:  0.00036328976\n",
      "8878 : Training: loss:  0.00082836836\n",
      "8879 : Training: loss:  0.001267474\n",
      "8880 : Training: loss:  0.0006312113\n",
      "Validation: Loss:  0.013606851  Accuracy:  0.96153843\n",
      "8881 : Training: loss:  0.00052743545\n",
      "8882 : Training: loss:  0.00074886665\n",
      "8883 : Training: loss:  0.000517678\n",
      "8884 : Training: loss:  0.0005803064\n",
      "8885 : Training: loss:  0.0010541314\n",
      "8886 : Training: loss:  0.0008842118\n",
      "8887 : Training: loss:  0.00093181804\n",
      "8888 : Training: loss:  0.00045973624\n",
      "8889 : Training: loss:  0.0005105228\n",
      "8890 : Training: loss:  0.0004023215\n",
      "8891 : Training: loss:  0.00069748185\n",
      "8892 : Training: loss:  0.0011400498\n",
      "8893 : Training: loss:  0.0010303927\n",
      "8894 : Training: loss:  0.00043253216\n",
      "8895 : Training: loss:  0.00083407067\n",
      "8896 : Training: loss:  0.0007465173\n",
      "8897 : Training: loss:  0.00040364408\n",
      "8898 : Training: loss:  0.001315543\n",
      "8899 : Training: loss:  0.0004459412\n",
      "8900 : Training: loss:  0.0014755835\n",
      "Validation: Loss:  0.013580932  Accuracy:  0.96153843\n",
      "8901 : Training: loss:  0.0009369632\n",
      "8902 : Training: loss:  0.0005890233\n",
      "8903 : Training: loss:  0.0005651033\n",
      "8904 : Training: loss:  0.0006883595\n",
      "8905 : Training: loss:  0.0005305668\n",
      "8906 : Training: loss:  0.00039819028\n",
      "8907 : Training: loss:  0.001104469\n",
      "8908 : Training: loss:  0.0008197346\n",
      "8909 : Training: loss:  0.00049734366\n",
      "8910 : Training: loss:  0.00068591867\n",
      "8911 : Training: loss:  0.001417916\n",
      "8912 : Training: loss:  0.00070773275\n",
      "8913 : Training: loss:  0.00028640902\n",
      "8914 : Training: loss:  0.0006907557\n",
      "8915 : Training: loss:  0.00047011164\n",
      "8916 : Training: loss:  0.00063866563\n",
      "8917 : Training: loss:  0.0007812078\n",
      "8918 : Training: loss:  0.00118391\n",
      "8919 : Training: loss:  0.00048358797\n",
      "8920 : Training: loss:  0.00045814307\n",
      "Validation: Loss:  0.013618661  Accuracy:  0.96153843\n",
      "8921 : Training: loss:  0.0005408219\n",
      "8922 : Training: loss:  0.00034044893\n",
      "8923 : Training: loss:  0.0009867913\n",
      "8924 : Training: loss:  0.0010035546\n",
      "8925 : Training: loss:  0.00079890114\n",
      "8926 : Training: loss:  0.0011832446\n",
      "8927 : Training: loss:  0.0004758504\n",
      "8928 : Training: loss:  0.00064225093\n",
      "8929 : Training: loss:  0.0005279994\n",
      "8930 : Training: loss:  0.0005955176\n",
      "8931 : Training: loss:  0.0011960923\n",
      "8932 : Training: loss:  0.0008940148\n",
      "8933 : Training: loss:  0.0010586893\n",
      "8934 : Training: loss:  0.00082712254\n",
      "8935 : Training: loss:  0.0008253627\n",
      "8936 : Training: loss:  0.0011021764\n",
      "8937 : Training: loss:  0.00038387938\n",
      "8938 : Training: loss:  0.0011279322\n",
      "8939 : Training: loss:  0.0006360906\n",
      "8940 : Training: loss:  0.00037087224\n",
      "Validation: Loss:  0.013527437  Accuracy:  0.96153843\n",
      "8941 : Training: loss:  0.0006719562\n",
      "8942 : Training: loss:  0.0006321858\n",
      "8943 : Training: loss:  0.000825435\n",
      "8944 : Training: loss:  0.0012307344\n",
      "8945 : Training: loss:  0.0006938753\n",
      "8946 : Training: loss:  0.001021806\n",
      "8947 : Training: loss:  0.0005903744\n",
      "8948 : Training: loss:  0.0010198515\n",
      "8949 : Training: loss:  0.0008210462\n",
      "8950 : Training: loss:  0.0005892241\n",
      "8951 : Training: loss:  0.000870118\n",
      "8952 : Training: loss:  0.0008104335\n",
      "8953 : Training: loss:  0.0006628734\n",
      "8954 : Training: loss:  0.0004122795\n",
      "8955 : Training: loss:  0.00022021557\n",
      "8956 : Training: loss:  0.00073132914\n",
      "8957 : Training: loss:  0.0010519123\n",
      "8958 : Training: loss:  0.0008235226\n",
      "8959 : Training: loss:  0.0015192957\n",
      "8960 : Training: loss:  0.0009076312\n",
      "Validation: Loss:  0.013519415  Accuracy:  0.96153843\n",
      "8961 : Training: loss:  0.0011754851\n",
      "8962 : Training: loss:  0.00071517593\n",
      "8963 : Training: loss:  0.00047299077\n",
      "8964 : Training: loss:  0.0006611996\n",
      "8965 : Training: loss:  0.0010780668\n",
      "8966 : Training: loss:  0.0008116982\n",
      "8967 : Training: loss:  0.001053091\n",
      "8968 : Training: loss:  0.0010101848\n",
      "8969 : Training: loss:  0.0005397271\n",
      "8970 : Training: loss:  0.0013189019\n",
      "8971 : Training: loss:  0.0006906149\n",
      "8972 : Training: loss:  0.0012281873\n",
      "8973 : Training: loss:  0.00050090253\n",
      "8974 : Training: loss:  0.00081687333\n",
      "8975 : Training: loss:  0.00089404307\n",
      "8976 : Training: loss:  0.00028226717\n",
      "8977 : Training: loss:  0.000947383\n",
      "8978 : Training: loss:  0.0003646078\n",
      "8979 : Training: loss:  0.0008995094\n",
      "8980 : Training: loss:  0.0012975476\n",
      "Validation: Loss:  0.013463231  Accuracy:  0.96153843\n",
      "8981 : Training: loss:  0.00068490324\n",
      "8982 : Training: loss:  0.0012448947\n",
      "8983 : Training: loss:  0.0004826667\n",
      "8984 : Training: loss:  0.0005627752\n",
      "8985 : Training: loss:  0.00074114953\n",
      "8986 : Training: loss:  0.0008853707\n",
      "8987 : Training: loss:  0.0010078617\n",
      "8988 : Training: loss:  0.00041104612\n",
      "8989 : Training: loss:  0.0006364403\n",
      "8990 : Training: loss:  0.0010682746\n",
      "8991 : Training: loss:  0.0014660574\n",
      "8992 : Training: loss:  0.0016577797\n",
      "8993 : Training: loss:  0.00058521895\n",
      "8994 : Training: loss:  0.0005400166\n",
      "8995 : Training: loss:  0.0007109461\n",
      "8996 : Training: loss:  0.0007082115\n",
      "8997 : Training: loss:  0.0006951283\n",
      "8998 : Training: loss:  0.0007509792\n",
      "8999 : Training: loss:  0.0007964903\n",
      "9000 : Training: loss:  0.0011157052\n",
      "Validation: Loss:  0.01340486  Accuracy:  0.96153843\n",
      "9001 : Training: loss:  0.0011032592\n",
      "9002 : Training: loss:  0.0011258582\n",
      "9003 : Training: loss:  0.0004080429\n",
      "9004 : Training: loss:  0.00029053955\n",
      "9005 : Training: loss:  0.00090225006\n",
      "9006 : Training: loss:  0.001057809\n",
      "9007 : Training: loss:  0.00074998854\n",
      "9008 : Training: loss:  0.0010631918\n",
      "9009 : Training: loss:  0.00048754638\n",
      "9010 : Training: loss:  0.0011101669\n",
      "9011 : Training: loss:  0.000822059\n",
      "9012 : Training: loss:  0.0010205589\n",
      "9013 : Training: loss:  0.0009634084\n",
      "9014 : Training: loss:  0.0007909277\n",
      "9015 : Training: loss:  0.00062765897\n",
      "9016 : Training: loss:  0.000922856\n",
      "9017 : Training: loss:  0.0009063312\n",
      "9018 : Training: loss:  0.0004634394\n",
      "9019 : Training: loss:  0.0008185773\n",
      "9020 : Training: loss:  0.0005060197\n",
      "Validation: Loss:  0.013396868  Accuracy:  0.96153843\n",
      "9021 : Training: loss:  0.0005187713\n",
      "9022 : Training: loss:  0.0010353645\n",
      "9023 : Training: loss:  0.0005909249\n",
      "9024 : Training: loss:  0.00057659886\n",
      "9025 : Training: loss:  0.00057214167\n",
      "9026 : Training: loss:  0.00072271534\n",
      "9027 : Training: loss:  0.00037281678\n",
      "9028 : Training: loss:  0.00045432537\n",
      "9029 : Training: loss:  0.00061827945\n",
      "9030 : Training: loss:  0.0006445761\n",
      "9031 : Training: loss:  0.00061066\n",
      "9032 : Training: loss:  0.0008041075\n",
      "9033 : Training: loss:  0.00056999497\n",
      "9034 : Training: loss:  0.0010219967\n",
      "9035 : Training: loss:  0.0007472732\n",
      "9036 : Training: loss:  0.001054425\n",
      "9037 : Training: loss:  0.00048725263\n",
      "9038 : Training: loss:  0.00070605084\n",
      "9039 : Training: loss:  0.000808005\n",
      "9040 : Training: loss:  0.00047035347\n",
      "Validation: Loss:  0.013406294  Accuracy:  0.96153843\n",
      "9041 : Training: loss:  0.0007632706\n",
      "9042 : Training: loss:  0.00091330806\n",
      "9043 : Training: loss:  0.00087344076\n",
      "9044 : Training: loss:  0.00034120865\n",
      "9045 : Training: loss:  0.0006946691\n",
      "9046 : Training: loss:  0.0005660691\n",
      "9047 : Training: loss:  0.0010140176\n",
      "9048 : Training: loss:  0.0003701477\n",
      "9049 : Training: loss:  0.00043211115\n",
      "9050 : Training: loss:  0.000985345\n",
      "9051 : Training: loss:  0.0005503254\n",
      "9052 : Training: loss:  0.0005131633\n",
      "9053 : Training: loss:  0.000679917\n",
      "9054 : Training: loss:  0.0004496476\n",
      "9055 : Training: loss:  0.0009819159\n",
      "9056 : Training: loss:  0.00053217146\n",
      "9057 : Training: loss:  0.00080132636\n",
      "9058 : Training: loss:  0.00054021244\n",
      "9059 : Training: loss:  0.0010847873\n",
      "9060 : Training: loss:  0.0005058003\n",
      "Validation: Loss:  0.013385574  Accuracy:  0.96153843\n",
      "9061 : Training: loss:  0.0011829255\n",
      "9062 : Training: loss:  0.0006047527\n",
      "9063 : Training: loss:  0.00037758\n",
      "9064 : Training: loss:  0.0007904448\n",
      "9065 : Training: loss:  0.0010258193\n",
      "9066 : Training: loss:  0.00061164936\n",
      "9067 : Training: loss:  0.0007313516\n",
      "9068 : Training: loss:  0.0006294957\n",
      "9069 : Training: loss:  0.0005303874\n",
      "9070 : Training: loss:  0.0008781834\n",
      "9071 : Training: loss:  0.0008653059\n",
      "9072 : Training: loss:  0.0011615403\n",
      "9073 : Training: loss:  0.0003917941\n",
      "9074 : Training: loss:  0.0010022199\n",
      "9075 : Training: loss:  0.00053097185\n",
      "9076 : Training: loss:  0.0010332209\n",
      "9077 : Training: loss:  0.00094416295\n",
      "9078 : Training: loss:  0.0012263326\n",
      "9079 : Training: loss:  0.00027069447\n",
      "9080 : Training: loss:  0.00070088287\n",
      "Validation: Loss:  0.013334826  Accuracy:  0.96153843\n",
      "9081 : Training: loss:  0.0004375994\n",
      "9082 : Training: loss:  0.0007567924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9083 : Training: loss:  0.00065285666\n",
      "9084 : Training: loss:  0.00046735964\n",
      "9085 : Training: loss:  0.00056090497\n",
      "9086 : Training: loss:  0.00065864273\n",
      "9087 : Training: loss:  0.00040485166\n",
      "9088 : Training: loss:  0.0004966656\n",
      "9089 : Training: loss:  0.00067908276\n",
      "9090 : Training: loss:  0.0006641641\n",
      "9091 : Training: loss:  0.0011042786\n",
      "9092 : Training: loss:  0.0005969415\n",
      "9093 : Training: loss:  0.0008021421\n",
      "9094 : Training: loss:  0.0009821608\n",
      "9095 : Training: loss:  0.00067277\n",
      "9096 : Training: loss:  0.0010328724\n",
      "9097 : Training: loss:  0.0012764265\n",
      "9098 : Training: loss:  0.00040941595\n",
      "9099 : Training: loss:  0.0007555705\n",
      "9100 : Training: loss:  0.0010166743\n",
      "Validation: Loss:  0.013273802  Accuracy:  0.96153843\n",
      "9101 : Training: loss:  0.00069863093\n",
      "9102 : Training: loss:  0.0008100732\n",
      "9103 : Training: loss:  0.0009521167\n",
      "9104 : Training: loss:  0.0007890429\n",
      "9105 : Training: loss:  0.0010006668\n",
      "9106 : Training: loss:  0.0007850662\n",
      "9107 : Training: loss:  0.00073388615\n",
      "9108 : Training: loss:  0.00042676154\n",
      "9109 : Training: loss:  0.0006371937\n",
      "9110 : Training: loss:  0.00097216695\n",
      "9111 : Training: loss:  0.0007260054\n",
      "9112 : Training: loss:  0.0006253992\n",
      "9113 : Training: loss:  0.00060383865\n",
      "9114 : Training: loss:  0.0005772258\n",
      "9115 : Training: loss:  0.0004162617\n",
      "9116 : Training: loss:  0.00028150604\n",
      "9117 : Training: loss:  0.00040161167\n",
      "9118 : Training: loss:  0.0010609644\n",
      "9119 : Training: loss:  0.00058290525\n",
      "9120 : Training: loss:  0.0010607307\n",
      "Validation: Loss:  0.013287278  Accuracy:  0.96153843\n",
      "9121 : Training: loss:  0.0010393903\n",
      "9122 : Training: loss:  0.0008110647\n",
      "9123 : Training: loss:  0.0008544373\n",
      "9124 : Training: loss:  0.00065947627\n",
      "9125 : Training: loss:  0.0012360541\n",
      "9126 : Training: loss:  0.00030051023\n",
      "9127 : Training: loss:  0.0006992165\n",
      "9128 : Training: loss:  0.0005524474\n",
      "9129 : Training: loss:  0.0006687729\n",
      "9130 : Training: loss:  0.0005905828\n",
      "9131 : Training: loss:  0.00047855673\n",
      "9132 : Training: loss:  0.00048881146\n",
      "9133 : Training: loss:  0.0011165768\n",
      "9134 : Training: loss:  0.00084686803\n",
      "9135 : Training: loss:  0.00048510023\n",
      "9136 : Training: loss:  0.00039554227\n",
      "9137 : Training: loss:  0.000824897\n",
      "9138 : Training: loss:  0.00062157883\n",
      "9139 : Training: loss:  0.0005726299\n",
      "9140 : Training: loss:  0.0004702567\n",
      "Validation: Loss:  0.013320449  Accuracy:  0.96153843\n",
      "9141 : Training: loss:  0.00049167726\n",
      "9142 : Training: loss:  0.0005594724\n",
      "9143 : Training: loss:  0.0007544241\n",
      "9144 : Training: loss:  0.0010566561\n",
      "9145 : Training: loss:  0.0011944015\n",
      "9146 : Training: loss:  0.0010401248\n",
      "9147 : Training: loss:  0.00073981285\n",
      "9148 : Training: loss:  0.0008760423\n",
      "9149 : Training: loss:  0.00083795836\n",
      "9150 : Training: loss:  0.00076909177\n",
      "9151 : Training: loss:  0.0005644877\n",
      "9152 : Training: loss:  0.0005994747\n",
      "9153 : Training: loss:  0.00047502056\n",
      "9154 : Training: loss:  0.0006127703\n",
      "9155 : Training: loss:  0.0009864861\n",
      "9156 : Training: loss:  0.0007408871\n",
      "9157 : Training: loss:  0.0009601059\n",
      "9158 : Training: loss:  0.00026913168\n",
      "9159 : Training: loss:  0.00034224862\n",
      "9160 : Training: loss:  0.00041686255\n",
      "Validation: Loss:  0.013305811  Accuracy:  0.96153843\n",
      "9161 : Training: loss:  0.0012106923\n",
      "9162 : Training: loss:  0.0009044726\n",
      "9163 : Training: loss:  0.00038806334\n",
      "9164 : Training: loss:  0.00062822504\n",
      "9165 : Training: loss:  0.0009985504\n",
      "9166 : Training: loss:  0.0018166511\n",
      "9167 : Training: loss:  0.0032345562\n",
      "9168 : Training: loss:  0.0006832028\n",
      "9169 : Training: loss:  0.0012049515\n",
      "9170 : Training: loss:  0.0005903784\n",
      "9171 : Training: loss:  0.0005948049\n",
      "9172 : Training: loss:  0.00027890503\n",
      "9173 : Training: loss:  0.00034237554\n",
      "9174 : Training: loss:  0.0004018036\n",
      "9175 : Training: loss:  0.0006613076\n",
      "9176 : Training: loss:  0.0007688611\n",
      "9177 : Training: loss:  0.0006377109\n",
      "9178 : Training: loss:  0.0005406137\n",
      "9179 : Training: loss:  0.0005525856\n",
      "9180 : Training: loss:  0.0010003541\n",
      "Validation: Loss:  0.01330575  Accuracy:  0.96153843\n",
      "9181 : Training: loss:  0.00050902937\n",
      "9182 : Training: loss:  0.00095572276\n",
      "9183 : Training: loss:  0.0008539594\n",
      "9184 : Training: loss:  0.000458801\n",
      "9185 : Training: loss:  0.0005033148\n",
      "9186 : Training: loss:  0.00043077415\n",
      "9187 : Training: loss:  0.00018165402\n",
      "9188 : Training: loss:  0.0009949784\n",
      "9189 : Training: loss:  0.00045619343\n",
      "9190 : Training: loss:  0.0009089371\n",
      "9191 : Training: loss:  0.0005866734\n",
      "9192 : Training: loss:  0.000514182\n",
      "9193 : Training: loss:  0.00088932214\n",
      "9194 : Training: loss:  0.0011477715\n",
      "9195 : Training: loss:  0.0005019785\n",
      "9196 : Training: loss:  0.0009245454\n",
      "9197 : Training: loss:  0.00052047643\n",
      "9198 : Training: loss:  0.00048999826\n",
      "9199 : Training: loss:  0.0008527376\n",
      "9200 : Training: loss:  0.00094515434\n",
      "Validation: Loss:  0.013313568  Accuracy:  0.96153843\n",
      "9201 : Training: loss:  0.0010224818\n",
      "9202 : Training: loss:  0.0005375866\n",
      "9203 : Training: loss:  0.0011436811\n",
      "9204 : Training: loss:  0.00044591664\n",
      "9205 : Training: loss:  0.00092488946\n",
      "9206 : Training: loss:  0.0008958511\n",
      "9207 : Training: loss:  0.0012238934\n",
      "9208 : Training: loss:  0.0007073809\n",
      "9209 : Training: loss:  0.0008121122\n",
      "9210 : Training: loss:  0.00077444414\n",
      "9211 : Training: loss:  0.00062086625\n",
      "9212 : Training: loss:  0.0011515411\n",
      "9213 : Training: loss:  0.00035322196\n",
      "9214 : Training: loss:  0.00088498107\n",
      "9215 : Training: loss:  0.0005906031\n",
      "9216 : Training: loss:  0.00050741254\n",
      "9217 : Training: loss:  0.0007586605\n",
      "9218 : Training: loss:  0.0009430953\n",
      "9219 : Training: loss:  0.0011350097\n",
      "9220 : Training: loss:  0.0011646702\n",
      "Validation: Loss:  0.01335697  Accuracy:  0.96153843\n",
      "9221 : Training: loss:  0.00060329074\n",
      "9222 : Training: loss:  0.0006693552\n",
      "9223 : Training: loss:  0.0005663694\n",
      "9224 : Training: loss:  0.00027741515\n",
      "9225 : Training: loss:  0.0010374017\n",
      "9226 : Training: loss:  0.0015284942\n",
      "9227 : Training: loss:  0.0007881424\n",
      "9228 : Training: loss:  0.00033970576\n",
      "9229 : Training: loss:  0.00072905264\n",
      "9230 : Training: loss:  0.0010037652\n",
      "9231 : Training: loss:  0.00069900905\n",
      "9232 : Training: loss:  0.0010969331\n",
      "9233 : Training: loss:  0.0005634722\n",
      "9234 : Training: loss:  0.00040754018\n",
      "9235 : Training: loss:  0.00087914616\n",
      "9236 : Training: loss:  0.00080752716\n",
      "9237 : Training: loss:  0.0010571228\n",
      "9238 : Training: loss:  0.00051337027\n",
      "9239 : Training: loss:  0.0012212493\n",
      "9240 : Training: loss:  0.000602275\n",
      "Validation: Loss:  0.013285341  Accuracy:  0.96153843\n",
      "9241 : Training: loss:  0.0005425334\n",
      "9242 : Training: loss:  0.00070400606\n",
      "9243 : Training: loss:  0.0008481886\n",
      "9244 : Training: loss:  0.00030492985\n",
      "9245 : Training: loss:  0.00068268593\n",
      "9246 : Training: loss:  0.0006606214\n",
      "9247 : Training: loss:  0.00080351694\n",
      "9248 : Training: loss:  0.00041015123\n",
      "9249 : Training: loss:  0.0003714157\n",
      "9250 : Training: loss:  0.0005869872\n",
      "9251 : Training: loss:  0.0005171202\n",
      "9252 : Training: loss:  0.00048431772\n",
      "9253 : Training: loss:  0.00076508446\n",
      "9254 : Training: loss:  0.001165088\n",
      "9255 : Training: loss:  0.0008288451\n",
      "9256 : Training: loss:  0.0009413206\n",
      "9257 : Training: loss:  0.00056211697\n",
      "9258 : Training: loss:  0.00076327886\n",
      "9259 : Training: loss:  0.0007027221\n",
      "9260 : Training: loss:  0.00094077934\n",
      "Validation: Loss:  0.013135018  Accuracy:  0.96153843\n",
      "9261 : Training: loss:  0.0005871974\n",
      "9262 : Training: loss:  0.00067317556\n",
      "9263 : Training: loss:  0.001046284\n",
      "9264 : Training: loss:  0.00073336303\n",
      "9265 : Training: loss:  0.00095285714\n",
      "9266 : Training: loss:  0.00065475743\n",
      "9267 : Training: loss:  0.00053031254\n",
      "9268 : Training: loss:  0.0005071677\n",
      "9269 : Training: loss:  0.0009847437\n",
      "9270 : Training: loss:  0.0008299158\n",
      "9271 : Training: loss:  0.00030830424\n",
      "9272 : Training: loss:  0.00084048917\n",
      "9273 : Training: loss:  0.0010629499\n",
      "9274 : Training: loss:  0.0007758614\n",
      "9275 : Training: loss:  0.000674245\n",
      "9276 : Training: loss:  0.00037500358\n",
      "9277 : Training: loss:  0.0007684571\n",
      "9278 : Training: loss:  0.0004488839\n",
      "9279 : Training: loss:  0.00059895625\n",
      "9280 : Training: loss:  0.0006272831\n",
      "Validation: Loss:  0.0131104  Accuracy:  0.96153843\n",
      "9281 : Training: loss:  0.0006257162\n",
      "9282 : Training: loss:  0.00044472632\n",
      "9283 : Training: loss:  0.00041182202\n",
      "9284 : Training: loss:  0.0010053457\n",
      "9285 : Training: loss:  0.0007168759\n",
      "9286 : Training: loss:  0.00065537094\n",
      "9287 : Training: loss:  0.0011275713\n",
      "9288 : Training: loss:  0.00063988427\n",
      "9289 : Training: loss:  0.00037917352\n",
      "9290 : Training: loss:  0.0007432728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9291 : Training: loss:  0.0005637885\n",
      "9292 : Training: loss:  0.0008965988\n",
      "9293 : Training: loss:  0.0008141794\n",
      "9294 : Training: loss:  0.0007358089\n",
      "9295 : Training: loss:  0.0007641714\n",
      "9296 : Training: loss:  0.00036222444\n",
      "9297 : Training: loss:  0.00041861137\n",
      "9298 : Training: loss:  0.0007734618\n",
      "9299 : Training: loss:  0.00018446779\n",
      "9300 : Training: loss:  0.00043971167\n",
      "Validation: Loss:  0.013176899  Accuracy:  0.96153843\n",
      "9301 : Training: loss:  0.000968956\n",
      "9302 : Training: loss:  0.0002751936\n",
      "9303 : Training: loss:  0.00065355556\n",
      "9304 : Training: loss:  0.0006759814\n",
      "9305 : Training: loss:  0.00046206234\n",
      "9306 : Training: loss:  0.0007508018\n",
      "9307 : Training: loss:  0.0008199024\n",
      "9308 : Training: loss:  0.00083567103\n",
      "9309 : Training: loss:  0.00034112117\n",
      "9310 : Training: loss:  0.0010520722\n",
      "9311 : Training: loss:  0.00048411713\n",
      "9312 : Training: loss:  0.0008546946\n",
      "9313 : Training: loss:  0.0006772433\n",
      "9314 : Training: loss:  0.0007330075\n",
      "9315 : Training: loss:  0.00077451096\n",
      "9316 : Training: loss:  0.0005078616\n",
      "9317 : Training: loss:  0.00061934476\n",
      "9318 : Training: loss:  0.00031972353\n",
      "9319 : Training: loss:  0.0007239646\n",
      "9320 : Training: loss:  0.0005007895\n",
      "Validation: Loss:  0.013154053  Accuracy:  0.96153843\n",
      "9321 : Training: loss:  0.0008054609\n",
      "9322 : Training: loss:  0.00048426221\n",
      "9323 : Training: loss:  0.00043334396\n",
      "9324 : Training: loss:  0.0006165255\n",
      "9325 : Training: loss:  0.00058000546\n",
      "9326 : Training: loss:  0.00040520306\n",
      "9327 : Training: loss:  0.0003840929\n",
      "9328 : Training: loss:  0.00074301957\n",
      "9329 : Training: loss:  0.00081803673\n",
      "9330 : Training: loss:  0.00039030053\n",
      "9331 : Training: loss:  0.0005376037\n",
      "9332 : Training: loss:  0.0006076633\n",
      "9333 : Training: loss:  0.0005130875\n",
      "9334 : Training: loss:  0.00046577796\n",
      "9335 : Training: loss:  0.0004822848\n",
      "9336 : Training: loss:  0.0012086375\n",
      "9337 : Training: loss:  0.0005537554\n",
      "9338 : Training: loss:  0.0008151278\n",
      "9339 : Training: loss:  0.0008970227\n",
      "9340 : Training: loss:  0.0007666025\n",
      "Validation: Loss:  0.0131181255  Accuracy:  0.96153843\n",
      "9341 : Training: loss:  0.0006467737\n",
      "9342 : Training: loss:  0.0007335332\n",
      "9343 : Training: loss:  0.00044787838\n",
      "9344 : Training: loss:  0.0005048458\n",
      "9345 : Training: loss:  0.0007151696\n",
      "9346 : Training: loss:  0.00020199793\n",
      "9347 : Training: loss:  0.00086897874\n",
      "9348 : Training: loss:  0.00071824255\n",
      "9349 : Training: loss:  0.0008965555\n",
      "9350 : Training: loss:  0.00086263195\n",
      "9351 : Training: loss:  0.0004935606\n",
      "9352 : Training: loss:  0.0007323338\n",
      "9353 : Training: loss:  0.0007967297\n",
      "9354 : Training: loss:  0.0006150241\n",
      "9355 : Training: loss:  0.00078316405\n",
      "9356 : Training: loss:  0.00055648544\n",
      "9357 : Training: loss:  0.00041625317\n",
      "9358 : Training: loss:  0.0010588306\n",
      "9359 : Training: loss:  0.00046803264\n",
      "9360 : Training: loss:  0.0005241063\n",
      "Validation: Loss:  0.013103716  Accuracy:  0.96153843\n",
      "9361 : Training: loss:  0.00071203755\n",
      "9362 : Training: loss:  0.0008472756\n",
      "9363 : Training: loss:  0.0007558935\n",
      "9364 : Training: loss:  0.0003659236\n",
      "9365 : Training: loss:  0.0008677417\n",
      "9366 : Training: loss:  0.00068113353\n",
      "9367 : Training: loss:  0.00052212103\n",
      "9368 : Training: loss:  0.00075974973\n",
      "9369 : Training: loss:  0.00043261927\n",
      "9370 : Training: loss:  0.0008349795\n",
      "9371 : Training: loss:  0.00073643954\n",
      "9372 : Training: loss:  0.000893322\n",
      "9373 : Training: loss:  0.00075253495\n",
      "9374 : Training: loss:  0.00055096985\n",
      "9375 : Training: loss:  0.0004530803\n",
      "9376 : Training: loss:  0.00062781584\n",
      "9377 : Training: loss:  0.000962103\n",
      "9378 : Training: loss:  0.00030604663\n",
      "9379 : Training: loss:  0.0008788512\n",
      "9380 : Training: loss:  0.00057632464\n",
      "Validation: Loss:  0.013113162  Accuracy:  0.96153843\n",
      "9381 : Training: loss:  0.0008412551\n",
      "9382 : Training: loss:  0.0008116051\n",
      "9383 : Training: loss:  0.0009975345\n",
      "9384 : Training: loss:  0.0009450714\n",
      "9385 : Training: loss:  0.0007934369\n",
      "9386 : Training: loss:  0.0005933654\n",
      "9387 : Training: loss:  0.0005692578\n",
      "9388 : Training: loss:  0.00089093007\n",
      "9389 : Training: loss:  0.0005886919\n",
      "9390 : Training: loss:  0.00030576493\n",
      "9391 : Training: loss:  0.001251816\n",
      "9392 : Training: loss:  0.00062640745\n",
      "9393 : Training: loss:  0.0010259247\n",
      "9394 : Training: loss:  0.00036359858\n",
      "9395 : Training: loss:  0.00048749495\n",
      "9396 : Training: loss:  0.00077906414\n",
      "9397 : Training: loss:  0.0005809794\n",
      "9398 : Training: loss:  0.00056005706\n",
      "9399 : Training: loss:  0.00049392413\n",
      "9400 : Training: loss:  0.0008672157\n",
      "Validation: Loss:  0.013144178  Accuracy:  0.96153843\n",
      "9401 : Training: loss:  0.0003842413\n",
      "9402 : Training: loss:  0.00042477754\n",
      "9403 : Training: loss:  0.00041205404\n",
      "9404 : Training: loss:  0.0010299073\n",
      "9405 : Training: loss:  0.00068150676\n",
      "9406 : Training: loss:  0.0009161439\n",
      "9407 : Training: loss:  0.00050321966\n",
      "9408 : Training: loss:  0.00068646757\n",
      "9409 : Training: loss:  0.001119404\n",
      "9410 : Training: loss:  0.0006599751\n",
      "9411 : Training: loss:  0.0006230741\n",
      "9412 : Training: loss:  0.00080806535\n",
      "9413 : Training: loss:  0.00076761347\n",
      "9414 : Training: loss:  0.0012623911\n",
      "9415 : Training: loss:  0.0006243671\n",
      "9416 : Training: loss:  0.00050445885\n",
      "9417 : Training: loss:  0.00095730426\n",
      "9418 : Training: loss:  0.00039034226\n",
      "9419 : Training: loss:  0.00038665996\n",
      "9420 : Training: loss:  0.0006439553\n",
      "Validation: Loss:  0.013163611  Accuracy:  0.96153843\n",
      "9421 : Training: loss:  0.00083230896\n",
      "9422 : Training: loss:  0.0009033715\n",
      "9423 : Training: loss:  0.00073150505\n",
      "9424 : Training: loss:  0.0008685093\n",
      "9425 : Training: loss:  0.0005368741\n",
      "9426 : Training: loss:  0.0004614332\n",
      "9427 : Training: loss:  0.00039767404\n",
      "9428 : Training: loss:  0.0008219902\n",
      "9429 : Training: loss:  0.0011433652\n",
      "9430 : Training: loss:  0.0012788952\n",
      "9431 : Training: loss:  0.00092271937\n",
      "9432 : Training: loss:  0.00039009453\n",
      "9433 : Training: loss:  0.000401596\n",
      "9434 : Training: loss:  0.00087832095\n",
      "9435 : Training: loss:  0.00036964187\n",
      "9436 : Training: loss:  0.0005167546\n",
      "9437 : Training: loss:  0.0005979336\n",
      "9438 : Training: loss:  0.0006477227\n",
      "9439 : Training: loss:  0.0004148692\n",
      "9440 : Training: loss:  0.0006097241\n",
      "Validation: Loss:  0.013230702  Accuracy:  0.96153843\n",
      "9441 : Training: loss:  0.00093800604\n",
      "9442 : Training: loss:  0.00034374875\n",
      "9443 : Training: loss:  0.00074950955\n",
      "9444 : Training: loss:  0.000937562\n",
      "9445 : Training: loss:  0.0015016517\n",
      "9446 : Training: loss:  0.0002974747\n",
      "9447 : Training: loss:  0.0004702784\n",
      "9448 : Training: loss:  0.00072851055\n",
      "9449 : Training: loss:  0.00038102397\n",
      "9450 : Training: loss:  0.0007746887\n",
      "9451 : Training: loss:  0.0006324069\n",
      "9452 : Training: loss:  0.00060262077\n",
      "9453 : Training: loss:  0.00031768403\n",
      "9454 : Training: loss:  0.00061429356\n",
      "9455 : Training: loss:  0.0007324842\n",
      "9456 : Training: loss:  0.0009203795\n",
      "9457 : Training: loss:  0.00048644052\n",
      "9458 : Training: loss:  0.00059778814\n",
      "9459 : Training: loss:  0.00063946063\n",
      "9460 : Training: loss:  0.0004460043\n",
      "Validation: Loss:  0.013317162  Accuracy:  0.96153843\n",
      "9461 : Training: loss:  0.0006101045\n",
      "9462 : Training: loss:  0.0010098785\n",
      "9463 : Training: loss:  0.00049318344\n",
      "9464 : Training: loss:  0.0005063738\n",
      "9465 : Training: loss:  0.0007552373\n",
      "9466 : Training: loss:  0.00060613314\n",
      "9467 : Training: loss:  0.0005475094\n",
      "9468 : Training: loss:  0.0005677789\n",
      "9469 : Training: loss:  0.0006752162\n",
      "9470 : Training: loss:  0.00039656975\n",
      "9471 : Training: loss:  0.0015858613\n",
      "9472 : Training: loss:  0.0007336007\n",
      "9473 : Training: loss:  0.00035745802\n",
      "9474 : Training: loss:  0.000743249\n",
      "9475 : Training: loss:  0.00047748815\n",
      "9476 : Training: loss:  0.00042162652\n",
      "9477 : Training: loss:  0.0009378624\n",
      "9478 : Training: loss:  0.0005123877\n",
      "9479 : Training: loss:  0.0005132139\n",
      "9480 : Training: loss:  0.0008566541\n",
      "Validation: Loss:  0.013331488  Accuracy:  0.96153843\n",
      "9481 : Training: loss:  0.00078387\n",
      "9482 : Training: loss:  0.0008600021\n",
      "9483 : Training: loss:  0.00071012456\n",
      "9484 : Training: loss:  0.00040553432\n",
      "9485 : Training: loss:  0.00061800925\n",
      "9486 : Training: loss:  0.0004699006\n",
      "9487 : Training: loss:  0.0005371885\n",
      "9488 : Training: loss:  0.00042157806\n",
      "9489 : Training: loss:  0.00053565163\n",
      "9490 : Training: loss:  0.00078513223\n",
      "9491 : Training: loss:  0.00078228477\n",
      "9492 : Training: loss:  0.00031641783\n",
      "9493 : Training: loss:  0.0007891528\n",
      "9494 : Training: loss:  0.0005538251\n",
      "9495 : Training: loss:  0.00045609626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9496 : Training: loss:  0.00024364625\n",
      "9497 : Training: loss:  0.0010014316\n",
      "9498 : Training: loss:  0.0010623976\n",
      "9499 : Training: loss:  0.00035032327\n",
      "9500 : Training: loss:  0.0005712149\n",
      "Validation: Loss:  0.0133366585  Accuracy:  0.96153843\n",
      "9501 : Training: loss:  0.0003055455\n",
      "9502 : Training: loss:  0.0005248549\n",
      "9503 : Training: loss:  0.0005135079\n",
      "9504 : Training: loss:  0.0004212875\n",
      "9505 : Training: loss:  0.0006411575\n",
      "9506 : Training: loss:  0.00071010046\n",
      "9507 : Training: loss:  0.00034119928\n",
      "9508 : Training: loss:  0.0007512123\n",
      "9509 : Training: loss:  0.00080910785\n",
      "9510 : Training: loss:  0.00062920776\n",
      "9511 : Training: loss:  0.00061181025\n",
      "9512 : Training: loss:  0.00042044293\n",
      "9513 : Training: loss:  0.00068498694\n",
      "9514 : Training: loss:  0.00057855423\n",
      "9515 : Training: loss:  0.00046314913\n",
      "9516 : Training: loss:  0.00039306053\n",
      "9517 : Training: loss:  0.000535092\n",
      "9518 : Training: loss:  0.00068726065\n",
      "9519 : Training: loss:  0.0011953199\n",
      "9520 : Training: loss:  0.0012571624\n",
      "Validation: Loss:  0.013177148  Accuracy:  0.96153843\n",
      "9521 : Training: loss:  0.0006160193\n",
      "9522 : Training: loss:  0.00064229994\n",
      "9523 : Training: loss:  0.000655242\n",
      "9524 : Training: loss:  0.00085789\n",
      "9525 : Training: loss:  0.00073252217\n",
      "9526 : Training: loss:  0.00046100494\n",
      "9527 : Training: loss:  0.000352893\n",
      "9528 : Training: loss:  0.00089156214\n",
      "9529 : Training: loss:  0.0007409512\n",
      "9530 : Training: loss:  0.00045098484\n",
      "9531 : Training: loss:  0.00072027335\n",
      "9532 : Training: loss:  0.00047962385\n",
      "9533 : Training: loss:  0.00058808475\n",
      "9534 : Training: loss:  0.00046383642\n",
      "9535 : Training: loss:  0.0007834009\n",
      "9536 : Training: loss:  0.0006940105\n",
      "9537 : Training: loss:  0.0008820135\n",
      "9538 : Training: loss:  0.0010301763\n",
      "9539 : Training: loss:  0.00055017124\n",
      "9540 : Training: loss:  0.0004202913\n",
      "Validation: Loss:  0.0131516745  Accuracy:  0.96153843\n",
      "9541 : Training: loss:  0.0006951607\n",
      "9542 : Training: loss:  0.0005750376\n",
      "9543 : Training: loss:  0.0007043153\n",
      "9544 : Training: loss:  0.00059383974\n",
      "9545 : Training: loss:  0.0005640835\n",
      "9546 : Training: loss:  0.00026676574\n",
      "9547 : Training: loss:  0.00078718545\n",
      "9548 : Training: loss:  0.0009501847\n",
      "9549 : Training: loss:  0.0006540229\n",
      "9550 : Training: loss:  0.0007194062\n",
      "9551 : Training: loss:  0.00075452565\n",
      "9552 : Training: loss:  0.0009407811\n",
      "9553 : Training: loss:  0.000736989\n",
      "9554 : Training: loss:  0.0006682944\n",
      "9555 : Training: loss:  0.0006687664\n",
      "9556 : Training: loss:  0.0005447795\n",
      "9557 : Training: loss:  0.00044440603\n",
      "9558 : Training: loss:  0.0007560257\n",
      "9559 : Training: loss:  0.0005472899\n",
      "9560 : Training: loss:  0.00077666267\n",
      "Validation: Loss:  0.013118776  Accuracy:  0.96153843\n",
      "9561 : Training: loss:  0.0007128616\n",
      "9562 : Training: loss:  0.0004729523\n",
      "9563 : Training: loss:  0.00064590643\n",
      "9564 : Training: loss:  0.00040336445\n",
      "9565 : Training: loss:  0.0007147918\n",
      "9566 : Training: loss:  0.00078391295\n",
      "9567 : Training: loss:  0.00062594726\n",
      "9568 : Training: loss:  0.00038393078\n",
      "9569 : Training: loss:  0.00061187847\n",
      "9570 : Training: loss:  0.00048007094\n",
      "9571 : Training: loss:  0.00060930685\n",
      "9572 : Training: loss:  0.0006112477\n",
      "9573 : Training: loss:  0.00022779804\n",
      "9574 : Training: loss:  0.000388135\n",
      "9575 : Training: loss:  0.0009373865\n",
      "9576 : Training: loss:  0.0005440878\n",
      "9577 : Training: loss:  0.0007495713\n",
      "9578 : Training: loss:  0.0008865588\n",
      "9579 : Training: loss:  0.00034621067\n",
      "9580 : Training: loss:  0.000312829\n",
      "Validation: Loss:  0.013032847  Accuracy:  0.96153843\n",
      "9581 : Training: loss:  0.00040632268\n",
      "9582 : Training: loss:  0.00047422762\n",
      "9583 : Training: loss:  0.0004724956\n",
      "9584 : Training: loss:  0.0004152232\n",
      "9585 : Training: loss:  0.00040129162\n",
      "9586 : Training: loss:  0.00043283147\n",
      "9587 : Training: loss:  0.00059419416\n",
      "9588 : Training: loss:  0.0005501931\n",
      "9589 : Training: loss:  0.0006410758\n",
      "9590 : Training: loss:  0.0004443353\n",
      "9591 : Training: loss:  0.0006211173\n",
      "9592 : Training: loss:  0.0006234604\n",
      "9593 : Training: loss:  0.0005219602\n",
      "9594 : Training: loss:  0.00042621765\n",
      "9595 : Training: loss:  0.00058496825\n",
      "9596 : Training: loss:  0.00034119398\n",
      "9597 : Training: loss:  0.0006625537\n",
      "9598 : Training: loss:  0.000693097\n",
      "9599 : Training: loss:  0.00047512518\n",
      "9600 : Training: loss:  0.00061009894\n",
      "Validation: Loss:  0.01308762  Accuracy:  0.96153843\n",
      "9601 : Training: loss:  0.0007636797\n",
      "9602 : Training: loss:  0.00075870147\n",
      "9603 : Training: loss:  0.0004574351\n",
      "9604 : Training: loss:  0.00040861234\n",
      "9605 : Training: loss:  0.0004732404\n",
      "9606 : Training: loss:  0.0004078865\n",
      "9607 : Training: loss:  0.0004849536\n",
      "9608 : Training: loss:  0.0003136402\n",
      "9609 : Training: loss:  0.00055664673\n",
      "9610 : Training: loss:  0.0006993633\n",
      "9611 : Training: loss:  0.0006565756\n",
      "9612 : Training: loss:  0.00049291505\n",
      "9613 : Training: loss:  0.00035356998\n",
      "9614 : Training: loss:  0.00050158094\n",
      "9615 : Training: loss:  0.00041886666\n",
      "9616 : Training: loss:  0.00044281562\n",
      "9617 : Training: loss:  0.00046846402\n",
      "9618 : Training: loss:  0.00074949965\n",
      "9619 : Training: loss:  0.00028360382\n",
      "9620 : Training: loss:  0.00095940806\n",
      "Validation: Loss:  0.013103808  Accuracy:  0.96153843\n",
      "9621 : Training: loss:  0.00017574409\n",
      "9622 : Training: loss:  0.0006472851\n",
      "9623 : Training: loss:  0.00058232556\n",
      "9624 : Training: loss:  0.0007352837\n",
      "9625 : Training: loss:  0.00036095802\n",
      "9626 : Training: loss:  0.00030843366\n",
      "9627 : Training: loss:  0.00043180882\n",
      "9628 : Training: loss:  0.000490556\n",
      "9629 : Training: loss:  0.00048220964\n",
      "9630 : Training: loss:  0.0014052347\n",
      "9631 : Training: loss:  0.0004940479\n",
      "9632 : Training: loss:  0.0008534787\n",
      "9633 : Training: loss:  0.0004686157\n",
      "9634 : Training: loss:  0.00028153803\n",
      "9635 : Training: loss:  0.000584491\n",
      "9636 : Training: loss:  0.00046143975\n",
      "9637 : Training: loss:  0.0005590283\n",
      "9638 : Training: loss:  0.00049767096\n",
      "9639 : Training: loss:  0.000528104\n",
      "9640 : Training: loss:  0.00054599834\n",
      "Validation: Loss:  0.013121907  Accuracy:  0.96153843\n",
      "9641 : Training: loss:  0.00041670928\n",
      "9642 : Training: loss:  0.00084149954\n",
      "9643 : Training: loss:  0.0004904173\n",
      "9644 : Training: loss:  0.00074404036\n",
      "9645 : Training: loss:  0.0006751138\n",
      "9646 : Training: loss:  0.00039721472\n",
      "9647 : Training: loss:  0.00070032396\n",
      "9648 : Training: loss:  0.00088611175\n",
      "9649 : Training: loss:  0.000398377\n",
      "9650 : Training: loss:  0.00050615444\n",
      "9651 : Training: loss:  0.0006621974\n",
      "9652 : Training: loss:  0.00057775906\n",
      "9653 : Training: loss:  0.00031263957\n",
      "9654 : Training: loss:  0.0009434349\n",
      "9655 : Training: loss:  0.0004959313\n",
      "9656 : Training: loss:  0.0007372268\n",
      "9657 : Training: loss:  0.00034287744\n",
      "9658 : Training: loss:  0.00066719326\n",
      "9659 : Training: loss:  0.0007018345\n",
      "9660 : Training: loss:  0.00031931984\n",
      "Validation: Loss:  0.013086648  Accuracy:  0.96153843\n",
      "9661 : Training: loss:  0.00050765456\n",
      "9662 : Training: loss:  0.0004557187\n",
      "9663 : Training: loss:  0.000724323\n",
      "9664 : Training: loss:  0.00057899184\n",
      "9665 : Training: loss:  0.00032591747\n",
      "9666 : Training: loss:  0.00062411703\n",
      "9667 : Training: loss:  0.0005256625\n",
      "9668 : Training: loss:  0.0009393522\n",
      "9669 : Training: loss:  0.00040921272\n",
      "9670 : Training: loss:  0.00044085973\n",
      "9671 : Training: loss:  0.00040724248\n",
      "9672 : Training: loss:  0.0006862368\n",
      "9673 : Training: loss:  0.00043168836\n",
      "9674 : Training: loss:  0.001062729\n",
      "9675 : Training: loss:  0.00061117124\n",
      "9676 : Training: loss:  0.000739361\n",
      "9677 : Training: loss:  0.0004999501\n",
      "9678 : Training: loss:  0.0007873222\n",
      "9679 : Training: loss:  0.00031624656\n",
      "9680 : Training: loss:  0.0009155703\n",
      "Validation: Loss:  0.013113988  Accuracy:  0.96153843\n",
      "9681 : Training: loss:  0.00032683832\n",
      "9682 : Training: loss:  0.00024241609\n",
      "9683 : Training: loss:  0.00033198964\n",
      "9684 : Training: loss:  0.00047653768\n",
      "9685 : Training: loss:  0.000306013\n",
      "9686 : Training: loss:  0.0006722026\n",
      "9687 : Training: loss:  0.0004955777\n",
      "9688 : Training: loss:  0.0005890666\n",
      "9689 : Training: loss:  0.00032172014\n",
      "9690 : Training: loss:  0.00072903506\n",
      "9691 : Training: loss:  0.00089500507\n",
      "9692 : Training: loss:  0.0006371569\n",
      "9693 : Training: loss:  0.00047610383\n",
      "9694 : Training: loss:  0.00051089545\n",
      "9695 : Training: loss:  0.00016490826\n",
      "9696 : Training: loss:  0.00059433345\n",
      "9697 : Training: loss:  0.00062683923\n",
      "9698 : Training: loss:  0.0003434338\n",
      "9699 : Training: loss:  0.00056306785\n",
      "9700 : Training: loss:  0.0005997126\n",
      "Validation: Loss:  0.013061234  Accuracy:  0.96153843\n",
      "9701 : Training: loss:  0.00088639767\n",
      "9702 : Training: loss:  0.0003699953\n",
      "9703 : Training: loss:  0.00050569\n",
      "9704 : Training: loss:  0.0002860483\n",
      "9705 : Training: loss:  0.00055223075\n",
      "9706 : Training: loss:  0.000822386\n",
      "9707 : Training: loss:  0.00070077484\n",
      "9708 : Training: loss:  0.00032290106\n",
      "9709 : Training: loss:  0.0010164396\n",
      "9710 : Training: loss:  0.00033202337\n",
      "9711 : Training: loss:  0.00030813625\n",
      "9712 : Training: loss:  0.00032580082\n",
      "9713 : Training: loss:  0.0006606822\n",
      "9714 : Training: loss:  0.00046813034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9715 : Training: loss:  0.00031357442\n",
      "9716 : Training: loss:  0.00052314746\n",
      "9717 : Training: loss:  0.0005826819\n",
      "9718 : Training: loss:  0.00019955156\n",
      "9719 : Training: loss:  0.000434708\n",
      "9720 : Training: loss:  0.0005666898\n",
      "Validation: Loss:  0.013074771  Accuracy:  0.96153843\n",
      "9721 : Training: loss:  0.0004912681\n",
      "9722 : Training: loss:  0.00072312844\n",
      "9723 : Training: loss:  0.0004764534\n",
      "9724 : Training: loss:  0.00038034373\n",
      "9725 : Training: loss:  0.00016240178\n",
      "9726 : Training: loss:  0.0007331654\n",
      "9727 : Training: loss:  0.00047920787\n",
      "9728 : Training: loss:  0.00030836556\n",
      "9729 : Training: loss:  0.00030882296\n",
      "9730 : Training: loss:  0.0006383408\n",
      "9731 : Training: loss:  0.00034093417\n",
      "9732 : Training: loss:  0.0003096634\n",
      "9733 : Training: loss:  0.00054379937\n",
      "9734 : Training: loss:  0.000630052\n",
      "9735 : Training: loss:  0.000737692\n",
      "9736 : Training: loss:  0.00038724448\n",
      "9737 : Training: loss:  0.00063507404\n",
      "9738 : Training: loss:  0.00053338864\n",
      "9739 : Training: loss:  0.00045931432\n",
      "9740 : Training: loss:  0.00025427504\n",
      "Validation: Loss:  0.013055296  Accuracy:  0.96153843\n",
      "9741 : Training: loss:  0.0005841407\n",
      "9742 : Training: loss:  0.0005341748\n",
      "9743 : Training: loss:  0.00033209624\n",
      "9744 : Training: loss:  0.00025097257\n",
      "9745 : Training: loss:  0.0002459447\n",
      "9746 : Training: loss:  0.0006267547\n",
      "9747 : Training: loss:  0.0007087435\n",
      "9748 : Training: loss:  0.00063099334\n",
      "9749 : Training: loss:  0.0004878719\n",
      "9750 : Training: loss:  0.00060150854\n",
      "9751 : Training: loss:  0.00033365295\n",
      "9752 : Training: loss:  0.0012496072\n",
      "9753 : Training: loss:  0.00045939747\n",
      "9754 : Training: loss:  0.00061494677\n",
      "9755 : Training: loss:  0.00030557826\n",
      "9756 : Training: loss:  0.00051162863\n",
      "9757 : Training: loss:  0.00036148672\n",
      "9758 : Training: loss:  0.00054452464\n",
      "9759 : Training: loss:  0.0007310654\n",
      "9760 : Training: loss:  0.00033429192\n",
      "Validation: Loss:  0.012984229  Accuracy:  0.96153843\n",
      "9761 : Training: loss:  0.0005302128\n",
      "9762 : Training: loss:  0.0005168525\n",
      "9763 : Training: loss:  0.00032108978\n",
      "9764 : Training: loss:  0.0009365087\n",
      "9765 : Training: loss:  0.00022041745\n",
      "9766 : Training: loss:  0.0007352543\n",
      "9767 : Training: loss:  0.00071129523\n",
      "9768 : Training: loss:  0.0004286658\n",
      "9769 : Training: loss:  0.00068506115\n",
      "9770 : Training: loss:  0.001029983\n",
      "9771 : Training: loss:  0.00075239915\n",
      "9772 : Training: loss:  0.000608956\n",
      "9773 : Training: loss:  0.0005331111\n",
      "9774 : Training: loss:  0.00038945422\n",
      "9775 : Training: loss:  0.00039680928\n",
      "9776 : Training: loss:  0.0012825541\n",
      "9777 : Training: loss:  0.00053560396\n",
      "9778 : Training: loss:  0.0005847174\n",
      "9779 : Training: loss:  0.00039779604\n",
      "9780 : Training: loss:  0.00040447465\n",
      "Validation: Loss:  0.012971229  Accuracy:  0.96153843\n",
      "9781 : Training: loss:  0.000704674\n",
      "9782 : Training: loss:  0.00038320143\n",
      "9783 : Training: loss:  0.0003644441\n",
      "9784 : Training: loss:  0.00076059846\n",
      "9785 : Training: loss:  0.00038838555\n",
      "9786 : Training: loss:  0.00051273225\n",
      "9787 : Training: loss:  0.000407501\n",
      "9788 : Training: loss:  0.00042443463\n",
      "9789 : Training: loss:  0.00057421695\n",
      "9790 : Training: loss:  0.0005863079\n",
      "9791 : Training: loss:  0.0004139617\n",
      "9792 : Training: loss:  0.00035236872\n",
      "9793 : Training: loss:  0.0015893816\n",
      "9794 : Training: loss:  0.0004436068\n",
      "9795 : Training: loss:  0.00060692313\n",
      "9796 : Training: loss:  0.00070646044\n",
      "9797 : Training: loss:  0.0005858854\n",
      "9798 : Training: loss:  0.00041981172\n",
      "9799 : Training: loss:  0.0007371401\n",
      "9800 : Training: loss:  0.0005911546\n",
      "Validation: Loss:  0.012958154  Accuracy:  0.96153843\n",
      "9801 : Training: loss:  0.00042668715\n",
      "9802 : Training: loss:  0.0006943078\n",
      "9803 : Training: loss:  0.0005842202\n",
      "9804 : Training: loss:  0.0003472945\n",
      "9805 : Training: loss:  0.00068435934\n",
      "9806 : Training: loss:  0.0005473608\n",
      "9807 : Training: loss:  0.00047417707\n",
      "9808 : Training: loss:  0.0004211919\n",
      "9809 : Training: loss:  0.0005704754\n",
      "9810 : Training: loss:  0.00045409688\n",
      "9811 : Training: loss:  0.00043165783\n",
      "9812 : Training: loss:  0.000828388\n",
      "9813 : Training: loss:  0.00031052876\n",
      "9814 : Training: loss:  0.0003621925\n",
      "9815 : Training: loss:  0.0004487052\n",
      "9816 : Training: loss:  0.0005275517\n",
      "9817 : Training: loss:  0.0004641379\n",
      "9818 : Training: loss:  0.00021004866\n",
      "9819 : Training: loss:  0.00041601283\n",
      "9820 : Training: loss:  0.00071995927\n",
      "Validation: Loss:  0.0130117405  Accuracy:  0.96153843\n",
      "9821 : Training: loss:  0.00034919678\n",
      "9822 : Training: loss:  0.0005067958\n",
      "9823 : Training: loss:  0.0007139919\n",
      "9824 : Training: loss:  0.00056368747\n",
      "9825 : Training: loss:  0.00052450784\n",
      "9826 : Training: loss:  0.00043542427\n",
      "9827 : Training: loss:  0.00060096063\n",
      "9828 : Training: loss:  0.0003055909\n",
      "9829 : Training: loss:  0.0005070536\n",
      "9830 : Training: loss:  0.00042402075\n",
      "9831 : Training: loss:  0.00041745268\n",
      "9832 : Training: loss:  0.00022438938\n",
      "9833 : Training: loss:  0.0008395893\n",
      "9834 : Training: loss:  0.00044325064\n",
      "9835 : Training: loss:  0.00034549876\n",
      "9836 : Training: loss:  0.00054420327\n",
      "9837 : Training: loss:  0.00033734457\n",
      "9838 : Training: loss:  0.0005390578\n",
      "9839 : Training: loss:  0.0008351522\n",
      "9840 : Training: loss:  0.0003538328\n",
      "Validation: Loss:  0.013069724  Accuracy:  0.96153843\n",
      "9841 : Training: loss:  0.00032031574\n",
      "9842 : Training: loss:  0.00055849296\n",
      "9843 : Training: loss:  0.000517488\n",
      "9844 : Training: loss:  0.00057132146\n",
      "9845 : Training: loss:  0.0006201979\n",
      "9846 : Training: loss:  0.0004936081\n",
      "9847 : Training: loss:  0.00035743997\n",
      "9848 : Training: loss:  0.00015931533\n",
      "9849 : Training: loss:  0.0006030652\n",
      "9850 : Training: loss:  0.00027512625\n",
      "9851 : Training: loss:  0.00052425946\n",
      "9852 : Training: loss:  0.0005590274\n",
      "9853 : Training: loss:  0.00054235244\n",
      "9854 : Training: loss:  0.00068521017\n",
      "9855 : Training: loss:  0.0002156604\n",
      "9856 : Training: loss:  0.000878214\n",
      "9857 : Training: loss:  0.0004577987\n",
      "9858 : Training: loss:  0.0005910945\n",
      "9859 : Training: loss:  0.00045096915\n",
      "9860 : Training: loss:  0.0006654471\n",
      "Validation: Loss:  0.013140296  Accuracy:  0.96153843\n",
      "9861 : Training: loss:  0.00075390335\n",
      "9862 : Training: loss:  0.00049346266\n",
      "9863 : Training: loss:  0.00045331047\n",
      "9864 : Training: loss:  0.00046867036\n",
      "9865 : Training: loss:  0.00046069833\n",
      "9866 : Training: loss:  0.0004262533\n",
      "9867 : Training: loss:  0.0005326498\n",
      "9868 : Training: loss:  0.0008973119\n",
      "9869 : Training: loss:  0.00053835456\n",
      "9870 : Training: loss:  0.00045822765\n",
      "9871 : Training: loss:  0.0006536492\n",
      "9872 : Training: loss:  0.00026566186\n",
      "9873 : Training: loss:  0.0003416587\n",
      "9874 : Training: loss:  0.000335134\n",
      "9875 : Training: loss:  0.0006815507\n",
      "9876 : Training: loss:  0.0005275919\n",
      "9877 : Training: loss:  0.0005325028\n",
      "9878 : Training: loss:  0.00047185857\n",
      "9879 : Training: loss:  0.00047392046\n",
      "9880 : Training: loss:  0.00043467432\n",
      "Validation: Loss:  0.013099419  Accuracy:  0.96153843\n",
      "9881 : Training: loss:  0.0004014099\n",
      "9882 : Training: loss:  0.0007565844\n",
      "9883 : Training: loss:  0.00019555913\n",
      "9884 : Training: loss:  0.00048850727\n",
      "9885 : Training: loss:  0.00028517263\n",
      "9886 : Training: loss:  0.00037345503\n",
      "9887 : Training: loss:  0.00045609716\n",
      "9888 : Training: loss:  0.0005473484\n",
      "9889 : Training: loss:  0.00041189266\n",
      "9890 : Training: loss:  0.0005580656\n",
      "9891 : Training: loss:  0.00053697394\n",
      "9892 : Training: loss:  0.00024688017\n",
      "9893 : Training: loss:  0.00027687944\n",
      "9894 : Training: loss:  0.0004003204\n",
      "9895 : Training: loss:  0.00049633725\n",
      "9896 : Training: loss:  0.00037691524\n",
      "9897 : Training: loss:  0.0006411281\n",
      "9898 : Training: loss:  0.00031793083\n",
      "9899 : Training: loss:  0.00068051973\n",
      "9900 : Training: loss:  0.00039966337\n",
      "Validation: Loss:  0.013097222  Accuracy:  0.96153843\n",
      "9901 : Training: loss:  0.0005441276\n",
      "9902 : Training: loss:  0.0007487811\n",
      "9903 : Training: loss:  0.0007321193\n",
      "9904 : Training: loss:  0.00046540017\n",
      "9905 : Training: loss:  0.00042433236\n",
      "9906 : Training: loss:  0.00066192815\n",
      "9907 : Training: loss:  0.0006536476\n",
      "9908 : Training: loss:  0.00051557156\n",
      "9909 : Training: loss:  0.00032366734\n",
      "9910 : Training: loss:  0.00021623702\n",
      "9911 : Training: loss:  0.0007225123\n",
      "9912 : Training: loss:  0.00041020065\n",
      "9913 : Training: loss:  0.0005746149\n",
      "9914 : Training: loss:  0.00039389444\n",
      "9915 : Training: loss:  0.0002934798\n",
      "9916 : Training: loss:  0.00044802786\n",
      "9917 : Training: loss:  0.00069615955\n",
      "9918 : Training: loss:  0.00077251083\n",
      "9919 : Training: loss:  0.0005023003\n",
      "9920 : Training: loss:  0.00055299635\n",
      "Validation: Loss:  0.013061708  Accuracy:  0.96153843\n",
      "9921 : Training: loss:  0.0004722316\n",
      "9922 : Training: loss:  0.00026689476\n",
      "9923 : Training: loss:  0.0008600259\n",
      "9924 : Training: loss:  0.0005230364\n",
      "9925 : Training: loss:  0.00037118417\n",
      "9926 : Training: loss:  0.00047876002\n",
      "9927 : Training: loss:  0.00044927953\n",
      "9928 : Training: loss:  0.00070067873\n",
      "9929 : Training: loss:  0.0007319561\n",
      "9930 : Training: loss:  0.00031278026\n",
      "9931 : Training: loss:  0.0005153816\n",
      "9932 : Training: loss:  0.0008311796\n",
      "9933 : Training: loss:  0.0006979178\n",
      "9934 : Training: loss:  0.00025977773\n",
      "9935 : Training: loss:  0.00055776304\n",
      "9936 : Training: loss:  0.00055334764\n",
      "9937 : Training: loss:  0.00029507006\n",
      "9938 : Training: loss:  0.00013813461\n",
      "9939 : Training: loss:  0.00058721355\n",
      "9940 : Training: loss:  0.000379041\n",
      "Validation: Loss:  0.012973493  Accuracy:  0.96153843\n",
      "9941 : Training: loss:  0.000192738\n",
      "9942 : Training: loss:  0.00032625574\n",
      "9943 : Training: loss:  0.00034245558\n",
      "9944 : Training: loss:  0.0005850861\n",
      "9945 : Training: loss:  0.0011721349\n",
      "9946 : Training: loss:  0.0007359284\n",
      "9947 : Training: loss:  0.00037269294\n",
      "9948 : Training: loss:  0.0002882378\n",
      "9949 : Training: loss:  0.0006060041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9950 : Training: loss:  0.0007993594\n",
      "9951 : Training: loss:  0.00045946144\n",
      "9952 : Training: loss:  0.0009021682\n",
      "9953 : Training: loss:  0.00051269244\n",
      "9954 : Training: loss:  0.00033543387\n",
      "9955 : Training: loss:  0.0009625834\n",
      "9956 : Training: loss:  0.0007636533\n",
      "9957 : Training: loss:  0.00024166724\n",
      "9958 : Training: loss:  0.0008089751\n",
      "9959 : Training: loss:  0.00090869557\n",
      "9960 : Training: loss:  0.0006118233\n",
      "Validation: Loss:  0.013004846  Accuracy:  0.96153843\n",
      "9961 : Training: loss:  0.0007884718\n",
      "9962 : Training: loss:  0.00040734612\n",
      "9963 : Training: loss:  0.00020885163\n",
      "9964 : Training: loss:  0.0006844687\n",
      "9965 : Training: loss:  0.00032189718\n",
      "9966 : Training: loss:  0.00041740277\n",
      "9967 : Training: loss:  0.00034270328\n",
      "9968 : Training: loss:  0.0005299951\n",
      "9969 : Training: loss:  0.0006350178\n",
      "9970 : Training: loss:  0.0006947012\n",
      "9971 : Training: loss:  0.00036039544\n",
      "9972 : Training: loss:  0.00032521805\n",
      "9973 : Training: loss:  0.00052940485\n",
      "9974 : Training: loss:  0.00017937292\n",
      "9975 : Training: loss:  0.00036941576\n",
      "9976 : Training: loss:  0.00025808293\n",
      "9977 : Training: loss:  0.00044708903\n",
      "9978 : Training: loss:  0.00045272114\n",
      "9979 : Training: loss:  0.00069335353\n",
      "9980 : Training: loss:  0.00024692755\n",
      "Validation: Loss:  0.012996276  Accuracy:  0.96153843\n",
      "9981 : Training: loss:  0.00053495885\n",
      "9982 : Training: loss:  0.0006127559\n",
      "9983 : Training: loss:  0.00049403484\n",
      "9984 : Training: loss:  0.00064708374\n",
      "9985 : Training: loss:  0.00081820757\n",
      "9986 : Training: loss:  0.000656816\n",
      "9987 : Training: loss:  0.00044185444\n",
      "9988 : Training: loss:  0.00061840186\n",
      "9989 : Training: loss:  0.00021941426\n",
      "9990 : Training: loss:  0.00035921356\n",
      "9991 : Training: loss:  0.00067596685\n",
      "9992 : Training: loss:  0.00044560587\n",
      "9993 : Training: loss:  0.00030923312\n",
      "9994 : Training: loss:  0.00073059346\n",
      "9995 : Training: loss:  0.0005366376\n",
      "9996 : Training: loss:  0.00038799588\n",
      "9997 : Training: loss:  0.00033290236\n",
      "9998 : Training: loss:  0.0007395372\n",
      "9999 : Training: loss:  0.00035384632\n",
      "10000 : Training: loss:  0.000359832\n",
      "Validation: Loss:  0.0130152665  Accuracy:  0.96153843\n",
      "10001 : Training: loss:  0.00067600084\n",
      "10002 : Training: loss:  0.00021020528\n",
      "10003 : Training: loss:  0.0003035307\n",
      "10004 : Training: loss:  0.00049049023\n",
      "10005 : Training: loss:  0.00078447163\n",
      "10006 : Training: loss:  0.0005130064\n",
      "10007 : Training: loss:  0.0006541948\n",
      "10008 : Training: loss:  0.0004974602\n",
      "10009 : Training: loss:  0.0003951646\n",
      "10010 : Training: loss:  0.00043504895\n",
      "10011 : Training: loss:  0.00052254414\n",
      "10012 : Training: loss:  0.0008967664\n",
      "10013 : Training: loss:  0.00030321596\n",
      "10014 : Training: loss:  0.0003894138\n",
      "10015 : Training: loss:  0.00021825229\n",
      "10016 : Training: loss:  0.0003079178\n",
      "10017 : Training: loss:  0.00046639828\n",
      "10018 : Training: loss:  0.00034894177\n",
      "10019 : Training: loss:  0.00026260514\n",
      "10020 : Training: loss:  0.0005212087\n",
      "Validation: Loss:  0.012957309  Accuracy:  0.96153843\n",
      "10021 : Training: loss:  0.00014971635\n",
      "10022 : Training: loss:  0.00027654355\n",
      "10023 : Training: loss:  0.00073715363\n",
      "10024 : Training: loss:  0.0010136341\n",
      "10025 : Training: loss:  0.00032315808\n",
      "10026 : Training: loss:  0.00041308918\n",
      "10027 : Training: loss:  0.0003795922\n",
      "10028 : Training: loss:  0.00071780593\n",
      "10029 : Training: loss:  0.0006241772\n",
      "10030 : Training: loss:  0.00083892094\n",
      "10031 : Training: loss:  0.00083051174\n",
      "10032 : Training: loss:  0.0006839882\n",
      "10033 : Training: loss:  0.0005855545\n",
      "10034 : Training: loss:  0.0003195588\n",
      "10035 : Training: loss:  0.00021223628\n",
      "10036 : Training: loss:  0.0005199396\n",
      "10037 : Training: loss:  0.00026712535\n",
      "10038 : Training: loss:  0.00035264675\n",
      "10039 : Training: loss:  0.00071391714\n",
      "10040 : Training: loss:  0.0003328554\n",
      "Validation: Loss:  0.012911811  Accuracy:  0.96153843\n",
      "10041 : Training: loss:  0.0006041405\n",
      "10042 : Training: loss:  0.00047665337\n",
      "10043 : Training: loss:  0.0004079245\n",
      "10044 : Training: loss:  0.00067078235\n",
      "10045 : Training: loss:  0.00056439126\n",
      "10046 : Training: loss:  0.0005744687\n",
      "10047 : Training: loss:  0.00073191314\n",
      "10048 : Training: loss:  0.0002938436\n",
      "10049 : Training: loss:  0.0004878496\n",
      "10050 : Training: loss:  0.0003063315\n",
      "10051 : Training: loss:  0.0008298487\n",
      "10052 : Training: loss:  0.00028944836\n",
      "10053 : Training: loss:  0.0004508613\n",
      "10054 : Training: loss:  0.00047823295\n",
      "10055 : Training: loss:  0.00046916745\n",
      "10056 : Training: loss:  0.0004549269\n",
      "10057 : Training: loss:  0.00067543605\n",
      "10058 : Training: loss:  0.0005310237\n",
      "10059 : Training: loss:  0.0005472902\n",
      "10060 : Training: loss:  0.00045740043\n",
      "Validation: Loss:  0.012961984  Accuracy:  0.96153843\n",
      "10061 : Training: loss:  9.7488424e-05\n",
      "10062 : Training: loss:  0.00066723453\n",
      "10063 : Training: loss:  0.0002660534\n",
      "10064 : Training: loss:  0.0005096052\n",
      "10065 : Training: loss:  0.0004960923\n",
      "10066 : Training: loss:  0.00038901425\n",
      "10067 : Training: loss:  0.00054278586\n",
      "10068 : Training: loss:  0.00069978414\n",
      "10069 : Training: loss:  0.0007348934\n",
      "10070 : Training: loss:  0.00074262655\n",
      "10071 : Training: loss:  0.00033524077\n",
      "10072 : Training: loss:  0.0003207527\n",
      "10073 : Training: loss:  0.0004804315\n",
      "10074 : Training: loss:  0.00052933197\n",
      "10075 : Training: loss:  0.0006162072\n",
      "10076 : Training: loss:  0.00038128538\n",
      "10077 : Training: loss:  0.0006330633\n",
      "10078 : Training: loss:  0.0008182224\n",
      "10079 : Training: loss:  0.00084594777\n",
      "10080 : Training: loss:  0.000420813\n",
      "Validation: Loss:  0.0129071735  Accuracy:  0.96153843\n",
      "10081 : Training: loss:  0.00037330404\n",
      "10082 : Training: loss:  0.00043299329\n",
      "10083 : Training: loss:  0.00025511815\n",
      "10084 : Training: loss:  0.00034911768\n",
      "10085 : Training: loss:  0.0002482493\n",
      "10086 : Training: loss:  0.00042806094\n",
      "10087 : Training: loss:  0.00046331747\n",
      "10088 : Training: loss:  0.0007596104\n",
      "10089 : Training: loss:  0.0006797615\n",
      "10090 : Training: loss:  0.00019160801\n",
      "10091 : Training: loss:  0.00070058304\n",
      "10092 : Training: loss:  0.00040864144\n",
      "10093 : Training: loss:  0.00038493008\n",
      "10094 : Training: loss:  0.000402299\n",
      "10095 : Training: loss:  0.00017355231\n",
      "10096 : Training: loss:  0.00049899734\n",
      "10097 : Training: loss:  0.0006074176\n",
      "10098 : Training: loss:  0.00040113754\n",
      "10099 : Training: loss:  0.00054103363\n",
      "10100 : Training: loss:  0.00036644808\n",
      "Validation: Loss:  0.01288691  Accuracy:  0.96153843\n",
      "10101 : Training: loss:  0.00039331338\n",
      "10102 : Training: loss:  0.0004751028\n",
      "10103 : Training: loss:  0.00048611072\n",
      "10104 : Training: loss:  0.0002777576\n",
      "10105 : Training: loss:  0.00038578335\n",
      "10106 : Training: loss:  0.00033739436\n",
      "10107 : Training: loss:  0.00035969887\n",
      "10108 : Training: loss:  0.0007004133\n",
      "10109 : Training: loss:  0.0008020241\n",
      "10110 : Training: loss:  0.0004886125\n",
      "10111 : Training: loss:  0.00061561423\n",
      "10112 : Training: loss:  0.0004226593\n",
      "10113 : Training: loss:  0.00043082837\n",
      "10114 : Training: loss:  0.0002905338\n",
      "10115 : Training: loss:  0.0006690743\n",
      "10116 : Training: loss:  0.0004268815\n",
      "10117 : Training: loss:  0.00042847614\n",
      "10118 : Training: loss:  0.00035503187\n",
      "10119 : Training: loss:  0.0005601757\n",
      "10120 : Training: loss:  0.00025799184\n",
      "Validation: Loss:  0.01292405  Accuracy:  0.96153843\n",
      "10121 : Training: loss:  0.0006481726\n",
      "10122 : Training: loss:  0.00046775222\n",
      "10123 : Training: loss:  0.00052979734\n",
      "10124 : Training: loss:  0.00071616983\n",
      "10125 : Training: loss:  0.00043293904\n",
      "10126 : Training: loss:  0.0003912134\n",
      "10127 : Training: loss:  0.0003419968\n",
      "10128 : Training: loss:  0.0005883833\n",
      "10129 : Training: loss:  0.00028216443\n",
      "10130 : Training: loss:  0.00043237748\n",
      "10131 : Training: loss:  0.00025299838\n",
      "10132 : Training: loss:  0.0005483163\n",
      "10133 : Training: loss:  0.0004382824\n",
      "10134 : Training: loss:  0.00031590785\n",
      "10135 : Training: loss:  0.0003621982\n",
      "10136 : Training: loss:  0.0005896174\n",
      "10137 : Training: loss:  0.00043830674\n",
      "10138 : Training: loss:  0.000603512\n",
      "10139 : Training: loss:  0.0006298232\n",
      "10140 : Training: loss:  0.000542938\n",
      "Validation: Loss:  0.012830492  Accuracy:  0.96153843\n",
      "10141 : Training: loss:  0.00037472133\n",
      "10142 : Training: loss:  0.00048514752\n",
      "10143 : Training: loss:  0.0004995252\n",
      "10144 : Training: loss:  0.00052611565\n",
      "10145 : Training: loss:  0.00027928053\n",
      "10146 : Training: loss:  0.00046166114\n",
      "10147 : Training: loss:  0.00046158844\n",
      "10148 : Training: loss:  0.000488587\n",
      "10149 : Training: loss:  0.000610284\n",
      "10150 : Training: loss:  0.0006401663\n",
      "10151 : Training: loss:  0.00037237263\n",
      "10152 : Training: loss:  0.0005242553\n",
      "10153 : Training: loss:  0.00025296543\n",
      "10154 : Training: loss:  0.00064820494\n",
      "10155 : Training: loss:  0.00063629437\n",
      "10156 : Training: loss:  0.000665269\n",
      "10157 : Training: loss:  0.00032616552\n",
      "10158 : Training: loss:  0.00040113903\n",
      "10159 : Training: loss:  0.00019293382\n",
      "10160 : Training: loss:  0.00047779927\n",
      "Validation: Loss:  0.012837108  Accuracy:  0.96153843\n",
      "10161 : Training: loss:  0.0003417934\n",
      "10162 : Training: loss:  0.0002723008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10163 : Training: loss:  0.0004570045\n",
      "10164 : Training: loss:  0.0004466438\n",
      "10165 : Training: loss:  0.000406021\n",
      "10166 : Training: loss:  0.0005107419\n",
      "10167 : Training: loss:  0.00039198235\n",
      "10168 : Training: loss:  0.00085391593\n",
      "10169 : Training: loss:  0.00061146717\n",
      "10170 : Training: loss:  0.0007212698\n",
      "10171 : Training: loss:  0.0005451356\n",
      "10172 : Training: loss:  0.00063727313\n",
      "10173 : Training: loss:  0.00033324552\n",
      "10174 : Training: loss:  0.0005803202\n",
      "10175 : Training: loss:  0.00035864345\n",
      "10176 : Training: loss:  0.0002986884\n",
      "10177 : Training: loss:  0.00048431836\n",
      "10178 : Training: loss:  0.0003742166\n",
      "10179 : Training: loss:  0.00036309205\n",
      "10180 : Training: loss:  0.0007373118\n",
      "Validation: Loss:  0.012887958  Accuracy:  0.96153843\n",
      "10181 : Training: loss:  0.00072201795\n",
      "10182 : Training: loss:  0.0008260305\n",
      "10183 : Training: loss:  0.00030881327\n",
      "10184 : Training: loss:  0.00044168593\n",
      "10185 : Training: loss:  0.00056697364\n",
      "10186 : Training: loss:  0.0004924774\n",
      "10187 : Training: loss:  0.0005305522\n",
      "10188 : Training: loss:  0.0004939293\n",
      "10189 : Training: loss:  0.00038859114\n",
      "10190 : Training: loss:  0.0004525399\n",
      "10191 : Training: loss:  0.00046028514\n",
      "10192 : Training: loss:  0.0003627062\n",
      "10193 : Training: loss:  0.00020164174\n",
      "10194 : Training: loss:  0.00024987452\n",
      "10195 : Training: loss:  0.00035965536\n",
      "10196 : Training: loss:  0.00045980312\n",
      "10197 : Training: loss:  0.0005475205\n",
      "10198 : Training: loss:  0.00050441443\n",
      "10199 : Training: loss:  0.0003036961\n",
      "10200 : Training: loss:  0.00065030507\n",
      "Validation: Loss:  0.012928338  Accuracy:  0.96153843\n",
      "10201 : Training: loss:  0.00045638924\n",
      "10202 : Training: loss:  0.00039194847\n",
      "10203 : Training: loss:  0.0004641911\n",
      "10204 : Training: loss:  0.0005463796\n",
      "10205 : Training: loss:  0.0004081406\n",
      "10206 : Training: loss:  0.00016044806\n",
      "10207 : Training: loss:  0.00018033687\n",
      "10208 : Training: loss:  0.00048769248\n",
      "10209 : Training: loss:  0.00041308266\n",
      "10210 : Training: loss:  0.00052734656\n",
      "10211 : Training: loss:  0.00036721188\n",
      "10212 : Training: loss:  0.0002756533\n",
      "10213 : Training: loss:  0.0005017764\n",
      "10214 : Training: loss:  0.00039512757\n",
      "10215 : Training: loss:  0.00030795202\n",
      "10216 : Training: loss:  0.0003912683\n",
      "10217 : Training: loss:  0.0004336007\n",
      "10218 : Training: loss:  0.00044395498\n",
      "10219 : Training: loss:  0.0005525415\n",
      "10220 : Training: loss:  0.000423766\n",
      "Validation: Loss:  0.012996298  Accuracy:  0.96153843\n",
      "10221 : Training: loss:  0.0004164269\n",
      "10222 : Training: loss:  0.0003999898\n",
      "10223 : Training: loss:  0.0008703463\n",
      "10224 : Training: loss:  0.0004060017\n",
      "10225 : Training: loss:  0.00050523534\n",
      "10226 : Training: loss:  0.00047427323\n",
      "10227 : Training: loss:  0.00021998733\n",
      "10228 : Training: loss:  0.0002527532\n",
      "10229 : Training: loss:  0.00076238596\n",
      "10230 : Training: loss:  0.00041693752\n",
      "10231 : Training: loss:  0.00036521896\n",
      "10232 : Training: loss:  0.00051746855\n",
      "10233 : Training: loss:  0.0002902137\n",
      "10234 : Training: loss:  0.00026849943\n",
      "10235 : Training: loss:  0.00035908434\n",
      "10236 : Training: loss:  0.00063213473\n",
      "10237 : Training: loss:  0.00032462494\n",
      "10238 : Training: loss:  0.00032810096\n",
      "10239 : Training: loss:  0.0004908229\n",
      "10240 : Training: loss:  0.00037564896\n",
      "Validation: Loss:  0.012937759  Accuracy:  0.96153843\n",
      "10241 : Training: loss:  0.00057144865\n",
      "10242 : Training: loss:  0.0004656589\n",
      "10243 : Training: loss:  0.0006268168\n",
      "10244 : Training: loss:  0.0006142459\n",
      "10245 : Training: loss:  0.00030941248\n",
      "10246 : Training: loss:  0.00044509713\n",
      "10247 : Training: loss:  0.0006692325\n",
      "10248 : Training: loss:  0.0004910376\n",
      "10249 : Training: loss:  0.00057457935\n",
      "10250 : Training: loss:  0.00035776317\n",
      "10251 : Training: loss:  0.00034804197\n",
      "10252 : Training: loss:  0.000649011\n",
      "10253 : Training: loss:  0.00021110618\n",
      "10254 : Training: loss:  0.00045682595\n",
      "10255 : Training: loss:  0.00055894285\n",
      "10256 : Training: loss:  0.0005413473\n",
      "10257 : Training: loss:  0.00035078754\n",
      "10258 : Training: loss:  0.00035155803\n",
      "10259 : Training: loss:  0.00048724643\n",
      "10260 : Training: loss:  0.00056991423\n",
      "Validation: Loss:  0.012915662  Accuracy:  0.96153843\n",
      "10261 : Training: loss:  0.00076787337\n",
      "10262 : Training: loss:  0.00040405616\n",
      "10263 : Training: loss:  0.0005840752\n",
      "10264 : Training: loss:  0.0008175709\n",
      "10265 : Training: loss:  0.00021271851\n",
      "10266 : Training: loss:  0.00044464282\n",
      "10267 : Training: loss:  0.00049751473\n",
      "10268 : Training: loss:  0.00036738723\n",
      "10269 : Training: loss:  0.00035168082\n",
      "10270 : Training: loss:  0.0003118378\n",
      "10271 : Training: loss:  0.00058778585\n",
      "10272 : Training: loss:  0.0005436954\n",
      "10273 : Training: loss:  0.0004390514\n",
      "10274 : Training: loss:  0.00090070633\n",
      "10275 : Training: loss:  0.00033021628\n",
      "10276 : Training: loss:  0.00033934868\n",
      "10277 : Training: loss:  0.0005608942\n",
      "10278 : Training: loss:  0.0006316996\n",
      "10279 : Training: loss:  0.00060000265\n",
      "10280 : Training: loss:  0.0005568366\n",
      "Validation: Loss:  0.012846622  Accuracy:  0.96153843\n",
      "10281 : Training: loss:  0.00017936016\n",
      "10282 : Training: loss:  0.00026871008\n",
      "10283 : Training: loss:  0.00029395227\n",
      "10284 : Training: loss:  0.0003177594\n",
      "10285 : Training: loss:  0.00029608546\n",
      "10286 : Training: loss:  0.00043642768\n",
      "10287 : Training: loss:  0.0003777709\n",
      "10288 : Training: loss:  0.00038928638\n",
      "10289 : Training: loss:  0.00024135585\n",
      "10290 : Training: loss:  0.00040503472\n",
      "10291 : Training: loss:  0.0004451888\n",
      "10292 : Training: loss:  0.0003964999\n",
      "10293 : Training: loss:  0.0006080389\n",
      "10294 : Training: loss:  0.00042238433\n",
      "10295 : Training: loss:  0.0004458407\n",
      "10296 : Training: loss:  0.00011937185\n",
      "10297 : Training: loss:  0.00053082913\n",
      "10298 : Training: loss:  0.00035111365\n",
      "10299 : Training: loss:  0.00023899114\n",
      "10300 : Training: loss:  0.0004892354\n",
      "Validation: Loss:  0.012794996  Accuracy:  0.96153843\n",
      "10301 : Training: loss:  0.00037047514\n",
      "10302 : Training: loss:  0.0003335795\n",
      "10303 : Training: loss:  0.00034858726\n",
      "10304 : Training: loss:  0.00022007193\n",
      "10305 : Training: loss:  0.00025198975\n",
      "10306 : Training: loss:  0.0004240353\n",
      "10307 : Training: loss:  0.00036737346\n",
      "10308 : Training: loss:  0.0003012396\n",
      "10309 : Training: loss:  0.0004241642\n",
      "10310 : Training: loss:  0.00044577106\n",
      "10311 : Training: loss:  0.00042624402\n",
      "10312 : Training: loss:  0.00017843425\n",
      "10313 : Training: loss:  0.000293692\n",
      "10314 : Training: loss:  0.000629242\n",
      "10315 : Training: loss:  0.00021024646\n",
      "10316 : Training: loss:  0.0002972534\n",
      "10317 : Training: loss:  0.00036817792\n",
      "10318 : Training: loss:  0.00082537613\n",
      "10319 : Training: loss:  0.0003844425\n",
      "10320 : Training: loss:  0.0009454835\n",
      "Validation: Loss:  0.012782018  Accuracy:  0.96153843\n",
      "10321 : Training: loss:  0.00028251938\n",
      "10322 : Training: loss:  0.0003805881\n",
      "10323 : Training: loss:  0.00047794543\n",
      "10324 : Training: loss:  0.0002826913\n",
      "10325 : Training: loss:  0.00030181522\n",
      "10326 : Training: loss:  0.00048435456\n",
      "10327 : Training: loss:  0.00016138454\n",
      "10328 : Training: loss:  0.00034642083\n",
      "10329 : Training: loss:  0.00035241473\n",
      "10330 : Training: loss:  0.0006913543\n",
      "10331 : Training: loss:  0.00048738124\n",
      "10332 : Training: loss:  0.00019970874\n",
      "10333 : Training: loss:  0.00053563766\n",
      "10334 : Training: loss:  0.00046361\n",
      "10335 : Training: loss:  0.00043063672\n",
      "10336 : Training: loss:  0.0006007668\n",
      "10337 : Training: loss:  0.00049657084\n",
      "10338 : Training: loss:  0.00042230313\n",
      "10339 : Training: loss:  0.00045420844\n",
      "10340 : Training: loss:  0.00041010906\n",
      "Validation: Loss:  0.01277428  Accuracy:  0.96153843\n",
      "10341 : Training: loss:  0.00016360568\n",
      "10342 : Training: loss:  0.00039920508\n",
      "10343 : Training: loss:  0.00049789203\n",
      "10344 : Training: loss:  0.0005720376\n",
      "10345 : Training: loss:  0.0004696335\n",
      "10346 : Training: loss:  0.00046199636\n",
      "10347 : Training: loss:  0.00024761134\n",
      "10348 : Training: loss:  0.00040049566\n",
      "10349 : Training: loss:  0.0006244703\n",
      "10350 : Training: loss:  0.00047690677\n",
      "10351 : Training: loss:  0.0003441356\n",
      "10352 : Training: loss:  0.0004009306\n",
      "10353 : Training: loss:  0.00045657405\n",
      "10354 : Training: loss:  0.00056533277\n",
      "10355 : Training: loss:  0.00043407202\n",
      "10356 : Training: loss:  0.0004148278\n",
      "10357 : Training: loss:  0.00045301273\n",
      "10358 : Training: loss:  0.00047317968\n",
      "10359 : Training: loss:  0.000447325\n",
      "10360 : Training: loss:  0.0004673124\n",
      "Validation: Loss:  0.012737032  Accuracy:  0.96153843\n",
      "10361 : Training: loss:  0.00030684954\n",
      "10362 : Training: loss:  0.0003925334\n",
      "10363 : Training: loss:  0.00080284517\n",
      "10364 : Training: loss:  0.0002771935\n",
      "10365 : Training: loss:  0.00046092368\n",
      "10366 : Training: loss:  0.0003370347\n",
      "10367 : Training: loss:  0.00033470415\n",
      "10368 : Training: loss:  0.00050708983\n",
      "10369 : Training: loss:  0.00024516348\n",
      "10370 : Training: loss:  0.0006001096\n",
      "10371 : Training: loss:  0.0007469867\n",
      "10372 : Training: loss:  0.00043459682\n",
      "10373 : Training: loss:  0.000557745\n",
      "10374 : Training: loss:  0.00039309883\n",
      "10375 : Training: loss:  0.00043115983\n",
      "10376 : Training: loss:  0.00052643596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10377 : Training: loss:  0.00037088266\n",
      "10378 : Training: loss:  0.0004947957\n",
      "10379 : Training: loss:  0.00023839038\n",
      "10380 : Training: loss:  0.000459527\n",
      "Validation: Loss:  0.012691145  Accuracy:  0.96153843\n",
      "10381 : Training: loss:  0.00043466594\n",
      "10382 : Training: loss:  0.00021068416\n",
      "10383 : Training: loss:  0.00035936676\n",
      "10384 : Training: loss:  0.00053618546\n",
      "10385 : Training: loss:  0.0004019242\n",
      "10386 : Training: loss:  0.00034854814\n",
      "10387 : Training: loss:  0.00034807363\n",
      "10388 : Training: loss:  0.00042706297\n",
      "10389 : Training: loss:  0.00032191552\n",
      "10390 : Training: loss:  0.00034501235\n",
      "10391 : Training: loss:  0.0005082282\n",
      "10392 : Training: loss:  0.0004436143\n",
      "10393 : Training: loss:  0.0003319291\n",
      "10394 : Training: loss:  0.00034876546\n",
      "10395 : Training: loss:  0.0005152933\n",
      "10396 : Training: loss:  0.00042646207\n",
      "10397 : Training: loss:  0.00032082293\n",
      "10398 : Training: loss:  0.0002177032\n",
      "10399 : Training: loss:  0.0006231646\n",
      "10400 : Training: loss:  0.0002825475\n",
      "Validation: Loss:  0.0126780635  Accuracy:  0.96153843\n",
      "10401 : Training: loss:  0.00053120183\n",
      "10402 : Training: loss:  0.0006720021\n",
      "10403 : Training: loss:  0.000385594\n",
      "10404 : Training: loss:  0.00040175908\n",
      "10405 : Training: loss:  0.00020702649\n",
      "10406 : Training: loss:  0.00024883682\n",
      "10407 : Training: loss:  0.0004460867\n",
      "10408 : Training: loss:  0.00030521004\n",
      "10409 : Training: loss:  0.0003600947\n",
      "10410 : Training: loss:  0.00034253785\n",
      "10411 : Training: loss:  0.00013412943\n",
      "10412 : Training: loss:  0.0003609598\n",
      "10413 : Training: loss:  0.00042603255\n",
      "10414 : Training: loss:  0.00057494687\n",
      "10415 : Training: loss:  0.0004493984\n",
      "10416 : Training: loss:  0.0003736502\n",
      "10417 : Training: loss:  0.0005818941\n",
      "10418 : Training: loss:  0.0006005379\n",
      "10419 : Training: loss:  0.00052758516\n",
      "10420 : Training: loss:  0.0002625437\n",
      "Validation: Loss:  0.012701674  Accuracy:  0.96153843\n",
      "10421 : Training: loss:  0.00030586106\n",
      "10422 : Training: loss:  0.00056543876\n",
      "10423 : Training: loss:  0.00045359103\n",
      "10424 : Training: loss:  0.00028170197\n",
      "10425 : Training: loss:  0.0004079849\n",
      "10426 : Training: loss:  0.00041811934\n",
      "10427 : Training: loss:  0.00016265541\n",
      "10428 : Training: loss:  0.0004777223\n",
      "10429 : Training: loss:  0.00029678622\n",
      "10430 : Training: loss:  0.00043187948\n",
      "10431 : Training: loss:  0.0004931522\n",
      "10432 : Training: loss:  0.00027732807\n",
      "10433 : Training: loss:  0.0006126938\n",
      "10434 : Training: loss:  0.00032734664\n",
      "10435 : Training: loss:  0.00031547633\n",
      "10436 : Training: loss:  0.00046387775\n",
      "10437 : Training: loss:  0.00045626512\n",
      "10438 : Training: loss:  0.0002991859\n",
      "10439 : Training: loss:  0.00044465554\n",
      "10440 : Training: loss:  0.000341895\n",
      "Validation: Loss:  0.012752079  Accuracy:  0.96153843\n",
      "10441 : Training: loss:  0.0004975115\n",
      "10442 : Training: loss:  0.0006241298\n",
      "10443 : Training: loss:  0.0006587855\n",
      "10444 : Training: loss:  0.00050637475\n",
      "10445 : Training: loss:  0.00030688202\n",
      "10446 : Training: loss:  0.0005198741\n",
      "10447 : Training: loss:  0.00049177255\n",
      "10448 : Training: loss:  0.00037887506\n",
      "10449 : Training: loss:  0.00046747472\n",
      "10450 : Training: loss:  0.00071590417\n",
      "10451 : Training: loss:  0.00046131422\n",
      "10452 : Training: loss:  0.00034917708\n",
      "10453 : Training: loss:  0.0006214319\n",
      "10454 : Training: loss:  0.0004883293\n",
      "10455 : Training: loss:  0.00042094442\n",
      "10456 : Training: loss:  0.0003532728\n",
      "10457 : Training: loss:  0.00037127125\n",
      "10458 : Training: loss:  0.0006242604\n",
      "10459 : Training: loss:  0.00037141828\n",
      "10460 : Training: loss:  0.00092396804\n",
      "Validation: Loss:  0.01276705  Accuracy:  0.96153843\n",
      "10461 : Training: loss:  0.00067426777\n",
      "10462 : Training: loss:  0.00023620296\n",
      "10463 : Training: loss:  0.0006615065\n",
      "10464 : Training: loss:  0.00034552178\n",
      "10465 : Training: loss:  0.00023035005\n",
      "10466 : Training: loss:  0.00035482453\n",
      "10467 : Training: loss:  0.0003662069\n",
      "10468 : Training: loss:  0.0003525791\n",
      "10469 : Training: loss:  0.00046668894\n",
      "10470 : Training: loss:  0.00015444947\n",
      "10471 : Training: loss:  0.00031766598\n",
      "10472 : Training: loss:  0.0004800099\n",
      "10473 : Training: loss:  0.00032194264\n",
      "10474 : Training: loss:  0.0004760167\n",
      "10475 : Training: loss:  0.0002912318\n",
      "10476 : Training: loss:  0.00044231737\n",
      "10477 : Training: loss:  0.00028234752\n",
      "10478 : Training: loss:  0.00027763142\n",
      "10479 : Training: loss:  0.00047994475\n",
      "10480 : Training: loss:  0.00030176216\n",
      "Validation: Loss:  0.012840235  Accuracy:  0.96153843\n",
      "10481 : Training: loss:  0.00046681392\n",
      "10482 : Training: loss:  0.00038594354\n",
      "10483 : Training: loss:  0.00027431623\n",
      "10484 : Training: loss:  0.0004408134\n",
      "10485 : Training: loss:  0.00030236604\n",
      "10486 : Training: loss:  0.00029428446\n",
      "10487 : Training: loss:  0.0003027006\n",
      "10488 : Training: loss:  0.00044590526\n",
      "10489 : Training: loss:  0.00032291596\n",
      "10490 : Training: loss:  0.0003502845\n",
      "10491 : Training: loss:  0.00031812658\n",
      "10492 : Training: loss:  0.0004302573\n",
      "10493 : Training: loss:  0.0002892145\n",
      "10494 : Training: loss:  0.00020757622\n",
      "10495 : Training: loss:  0.00026467565\n",
      "10496 : Training: loss:  0.0003523688\n",
      "10497 : Training: loss:  0.0001608377\n",
      "10498 : Training: loss:  0.00033510508\n",
      "10499 : Training: loss:  0.00013034308\n",
      "10500 : Training: loss:  0.0005480396\n",
      "Validation: Loss:  0.012793278  Accuracy:  0.96153843\n",
      "10501 : Training: loss:  0.00036953815\n",
      "10502 : Training: loss:  0.0003999621\n",
      "10503 : Training: loss:  0.0001654077\n",
      "10504 : Training: loss:  0.00020014208\n",
      "10505 : Training: loss:  0.00038747236\n",
      "10506 : Training: loss:  0.00043695982\n",
      "10507 : Training: loss:  0.00015123011\n",
      "10508 : Training: loss:  0.00028555846\n",
      "10509 : Training: loss:  0.00041491629\n",
      "10510 : Training: loss:  0.00060496846\n",
      "10511 : Training: loss:  0.00032574733\n",
      "10512 : Training: loss:  0.0005752206\n",
      "10513 : Training: loss:  0.0006156143\n",
      "10514 : Training: loss:  0.00020164627\n",
      "10515 : Training: loss:  0.00029743635\n",
      "10516 : Training: loss:  0.00048069737\n",
      "10517 : Training: loss:  0.00038967113\n",
      "10518 : Training: loss:  0.00040427953\n",
      "10519 : Training: loss:  0.00044410504\n",
      "10520 : Training: loss:  0.0005335759\n",
      "Validation: Loss:  0.012778733  Accuracy:  0.96153843\n",
      "10521 : Training: loss:  0.0004620233\n",
      "10522 : Training: loss:  0.0003403786\n",
      "10523 : Training: loss:  0.00034678157\n",
      "10524 : Training: loss:  0.00030739803\n",
      "10525 : Training: loss:  0.00028282337\n",
      "10526 : Training: loss:  0.00050862925\n",
      "10527 : Training: loss:  0.0003062166\n",
      "10528 : Training: loss:  0.00056334416\n",
      "10529 : Training: loss:  0.0005053669\n",
      "10530 : Training: loss:  0.00036922484\n",
      "10531 : Training: loss:  0.00075099035\n",
      "10532 : Training: loss:  0.00036392614\n",
      "10533 : Training: loss:  0.00036572706\n",
      "10534 : Training: loss:  0.0002897052\n",
      "10535 : Training: loss:  0.0006774666\n",
      "10536 : Training: loss:  0.00020501908\n",
      "10537 : Training: loss:  0.0002658572\n",
      "10538 : Training: loss:  0.0003171974\n",
      "10539 : Training: loss:  0.00018798796\n",
      "10540 : Training: loss:  0.0004600392\n",
      "Validation: Loss:  0.012763784  Accuracy:  0.96153843\n",
      "10541 : Training: loss:  0.0003549677\n",
      "10542 : Training: loss:  0.00028358586\n",
      "10543 : Training: loss:  0.00021310303\n",
      "10544 : Training: loss:  0.00020955708\n",
      "10545 : Training: loss:  0.00014289565\n",
      "10546 : Training: loss:  0.00027731262\n",
      "10547 : Training: loss:  0.0003498221\n",
      "10548 : Training: loss:  0.00043607035\n",
      "10549 : Training: loss:  0.0001563023\n",
      "10550 : Training: loss:  0.00018964896\n",
      "10551 : Training: loss:  0.00038254945\n",
      "10552 : Training: loss:  0.00027685554\n",
      "10553 : Training: loss:  0.00027663517\n",
      "10554 : Training: loss:  0.0009102393\n",
      "10555 : Training: loss:  0.0003267011\n",
      "10556 : Training: loss:  0.00048023593\n",
      "10557 : Training: loss:  0.0005265503\n",
      "10558 : Training: loss:  0.00013838266\n",
      "10559 : Training: loss:  0.00041619383\n",
      "10560 : Training: loss:  0.00082466303\n",
      "Validation: Loss:  0.0127939405  Accuracy:  0.96153843\n",
      "10561 : Training: loss:  0.00029853632\n",
      "10562 : Training: loss:  0.0005220345\n",
      "10563 : Training: loss:  0.00021558916\n",
      "10564 : Training: loss:  0.00055082986\n",
      "10565 : Training: loss:  0.0004485607\n",
      "10566 : Training: loss:  0.00030962998\n",
      "10567 : Training: loss:  0.00017696392\n",
      "10568 : Training: loss:  0.000286611\n",
      "10569 : Training: loss:  0.0005571935\n",
      "10570 : Training: loss:  0.00023507343\n",
      "10571 : Training: loss:  0.00022055191\n",
      "10572 : Training: loss:  0.00025824807\n",
      "10573 : Training: loss:  0.0002728046\n",
      "10574 : Training: loss:  0.00023537374\n",
      "10575 : Training: loss:  0.00015324105\n",
      "10576 : Training: loss:  0.00023295166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10577 : Training: loss:  0.00021211611\n",
      "10578 : Training: loss:  0.0003351741\n",
      "10579 : Training: loss:  0.00039499873\n",
      "10580 : Training: loss:  0.00017619805\n",
      "Validation: Loss:  0.012789875  Accuracy:  0.96153843\n",
      "10581 : Training: loss:  0.0003595509\n",
      "10582 : Training: loss:  0.0004412661\n",
      "10583 : Training: loss:  0.00035388736\n",
      "10584 : Training: loss:  0.0003348299\n",
      "10585 : Training: loss:  0.00026439087\n",
      "10586 : Training: loss:  0.0006858635\n",
      "10587 : Training: loss:  0.00033458637\n",
      "10588 : Training: loss:  0.0004381079\n",
      "10589 : Training: loss:  0.00042577952\n",
      "10590 : Training: loss:  0.0002682935\n",
      "10591 : Training: loss:  0.00062260294\n",
      "10592 : Training: loss:  0.0003961503\n",
      "10593 : Training: loss:  0.0003127158\n",
      "10594 : Training: loss:  0.00024548362\n",
      "10595 : Training: loss:  0.00041834378\n",
      "10596 : Training: loss:  0.00046895884\n",
      "10597 : Training: loss:  0.00022920185\n",
      "10598 : Training: loss:  0.0005972077\n",
      "10599 : Training: loss:  0.00027193152\n",
      "10600 : Training: loss:  0.00033245116\n",
      "Validation: Loss:  0.012820362  Accuracy:  0.96153843\n",
      "10601 : Training: loss:  0.00040816615\n",
      "10602 : Training: loss:  0.0004996308\n",
      "10603 : Training: loss:  0.00038810633\n",
      "10604 : Training: loss:  0.00038071498\n",
      "10605 : Training: loss:  0.0003702817\n",
      "10606 : Training: loss:  0.00035760383\n",
      "10607 : Training: loss:  0.00043746593\n",
      "10608 : Training: loss:  0.0003225069\n",
      "10609 : Training: loss:  0.00053897075\n",
      "10610 : Training: loss:  0.00044004695\n",
      "10611 : Training: loss:  0.0006484944\n",
      "10612 : Training: loss:  0.0002073571\n",
      "10613 : Training: loss:  0.00030394294\n",
      "10614 : Training: loss:  0.0010591086\n",
      "10615 : Training: loss:  0.00031471186\n",
      "10616 : Training: loss:  0.00021499785\n",
      "10617 : Training: loss:  0.00040629433\n",
      "10618 : Training: loss:  0.00019174039\n",
      "10619 : Training: loss:  0.00016517041\n",
      "10620 : Training: loss:  0.00026019657\n",
      "Validation: Loss:  0.012796532  Accuracy:  0.96153843\n",
      "10621 : Training: loss:  0.00027409647\n",
      "10622 : Training: loss:  0.00043986333\n",
      "10623 : Training: loss:  0.0004116121\n",
      "10624 : Training: loss:  0.0002938315\n",
      "10625 : Training: loss:  0.0003131924\n",
      "10626 : Training: loss:  0.0005824461\n",
      "10627 : Training: loss:  0.00051197666\n",
      "10628 : Training: loss:  0.0004527712\n",
      "10629 : Training: loss:  0.00029981768\n",
      "10630 : Training: loss:  0.00025395662\n",
      "10631 : Training: loss:  0.00041463695\n",
      "10632 : Training: loss:  0.0005770919\n",
      "10633 : Training: loss:  0.00015066296\n",
      "10634 : Training: loss:  0.0002880951\n",
      "10635 : Training: loss:  0.0003763905\n",
      "10636 : Training: loss:  0.0003150696\n",
      "10637 : Training: loss:  0.00040394088\n",
      "10638 : Training: loss:  0.00015876007\n",
      "10639 : Training: loss:  0.00036710122\n",
      "10640 : Training: loss:  0.00031661108\n",
      "Validation: Loss:  0.0127704665  Accuracy:  0.96153843\n",
      "10641 : Training: loss:  0.00047647438\n",
      "10642 : Training: loss:  0.00042420437\n",
      "10643 : Training: loss:  0.00030318115\n",
      "10644 : Training: loss:  0.00015644806\n",
      "10645 : Training: loss:  0.00027462127\n",
      "10646 : Training: loss:  0.00039726522\n",
      "10647 : Training: loss:  0.00047040504\n",
      "10648 : Training: loss:  0.00042637883\n",
      "10649 : Training: loss:  0.00034897108\n",
      "10650 : Training: loss:  0.000538985\n",
      "10651 : Training: loss:  0.00040131822\n",
      "10652 : Training: loss:  0.00042896363\n",
      "10653 : Training: loss:  0.00039171017\n",
      "10654 : Training: loss:  0.00029146674\n",
      "10655 : Training: loss:  0.00043691605\n",
      "10656 : Training: loss:  0.00024861985\n",
      "10657 : Training: loss:  0.0004403978\n",
      "10658 : Training: loss:  0.00020190579\n",
      "10659 : Training: loss:  0.0005346796\n",
      "10660 : Training: loss:  0.00022524892\n",
      "Validation: Loss:  0.012715921  Accuracy:  0.96153843\n",
      "10661 : Training: loss:  0.00044107283\n",
      "10662 : Training: loss:  0.00031680346\n",
      "10663 : Training: loss:  0.0004977155\n",
      "10664 : Training: loss:  0.0003895233\n",
      "10665 : Training: loss:  0.00047993998\n",
      "10666 : Training: loss:  0.0003362285\n",
      "10667 : Training: loss:  0.00022054212\n",
      "10668 : Training: loss:  0.00035875518\n",
      "10669 : Training: loss:  0.0002259823\n",
      "10670 : Training: loss:  0.00036850173\n",
      "10671 : Training: loss:  0.00034078644\n",
      "10672 : Training: loss:  0.00031072102\n",
      "10673 : Training: loss:  0.00013533361\n",
      "10674 : Training: loss:  0.00026751804\n",
      "10675 : Training: loss:  0.000498012\n",
      "10676 : Training: loss:  0.0004384709\n",
      "10677 : Training: loss:  0.00049411244\n",
      "10678 : Training: loss:  0.00038053005\n",
      "10679 : Training: loss:  0.00025828596\n",
      "10680 : Training: loss:  0.00043078174\n",
      "Validation: Loss:  0.012787949  Accuracy:  0.96153843\n",
      "10681 : Training: loss:  0.00023575322\n",
      "10682 : Training: loss:  0.00018404324\n",
      "10683 : Training: loss:  0.0005181837\n",
      "10684 : Training: loss:  0.00020367558\n",
      "10685 : Training: loss:  0.00029407497\n",
      "10686 : Training: loss:  0.000583898\n",
      "10687 : Training: loss:  0.00062444474\n",
      "10688 : Training: loss:  0.000426239\n",
      "10689 : Training: loss:  0.00041505668\n",
      "10690 : Training: loss:  0.00027972186\n",
      "10691 : Training: loss:  0.00030340528\n",
      "10692 : Training: loss:  0.00060561416\n",
      "10693 : Training: loss:  0.00045244073\n",
      "10694 : Training: loss:  0.0002532047\n",
      "10695 : Training: loss:  0.0003385506\n",
      "10696 : Training: loss:  0.00036235157\n",
      "10697 : Training: loss:  0.0002982009\n",
      "10698 : Training: loss:  0.000482496\n",
      "10699 : Training: loss:  0.00015455141\n",
      "10700 : Training: loss:  0.00034905874\n",
      "Validation: Loss:  0.01279033  Accuracy:  0.96153843\n",
      "10701 : Training: loss:  0.00034041255\n",
      "10702 : Training: loss:  0.00031364884\n",
      "10703 : Training: loss:  0.00038272425\n",
      "10704 : Training: loss:  0.00030369137\n",
      "10705 : Training: loss:  0.00036537208\n",
      "10706 : Training: loss:  0.00019384778\n",
      "10707 : Training: loss:  0.00020112106\n",
      "10708 : Training: loss:  0.00021622433\n",
      "10709 : Training: loss:  0.00031328987\n",
      "10710 : Training: loss:  0.00023740127\n",
      "10711 : Training: loss:  0.00031639784\n",
      "10712 : Training: loss:  0.0002565458\n",
      "10713 : Training: loss:  0.000244957\n",
      "10714 : Training: loss:  0.0002374298\n",
      "10715 : Training: loss:  0.00015610643\n",
      "10716 : Training: loss:  0.0003344178\n",
      "10717 : Training: loss:  0.00043820514\n",
      "10718 : Training: loss:  0.00019540414\n",
      "10719 : Training: loss:  0.00031619388\n",
      "10720 : Training: loss:  0.0004099733\n",
      "Validation: Loss:  0.012753172  Accuracy:  0.96153843\n",
      "10721 : Training: loss:  0.00042788827\n",
      "10722 : Training: loss:  0.00032062584\n",
      "10723 : Training: loss:  0.00047844215\n",
      "10724 : Training: loss:  0.00019419238\n",
      "10725 : Training: loss:  0.00042132792\n",
      "10726 : Training: loss:  0.0002385597\n",
      "10727 : Training: loss:  0.00030979465\n",
      "10728 : Training: loss:  0.00025966612\n",
      "10729 : Training: loss:  0.0002648437\n",
      "10730 : Training: loss:  0.00036725035\n",
      "10731 : Training: loss:  0.00024581028\n",
      "10732 : Training: loss:  0.00027179852\n",
      "10733 : Training: loss:  0.00023579696\n",
      "10734 : Training: loss:  0.0002518521\n",
      "10735 : Training: loss:  0.0002030782\n",
      "10736 : Training: loss:  0.00036816334\n",
      "10737 : Training: loss:  0.00030590149\n",
      "10738 : Training: loss:  0.00022940153\n",
      "10739 : Training: loss:  0.00036296344\n",
      "10740 : Training: loss:  0.00024952594\n",
      "Validation: Loss:  0.012738042  Accuracy:  0.96153843\n",
      "10741 : Training: loss:  0.00019751489\n",
      "10742 : Training: loss:  0.0007053331\n",
      "10743 : Training: loss:  0.00025025758\n",
      "10744 : Training: loss:  0.00041733196\n",
      "10745 : Training: loss:  0.00039628203\n",
      "10746 : Training: loss:  0.00019604828\n",
      "10747 : Training: loss:  0.00042914457\n",
      "10748 : Training: loss:  0.00022856088\n",
      "10749 : Training: loss:  0.000247964\n",
      "10750 : Training: loss:  0.00045342447\n",
      "10751 : Training: loss:  0.00022477737\n",
      "10752 : Training: loss:  0.00025090115\n",
      "10753 : Training: loss:  0.00032104764\n",
      "10754 : Training: loss:  0.00026134576\n",
      "10755 : Training: loss:  0.00013214696\n",
      "10756 : Training: loss:  0.00032383928\n",
      "10757 : Training: loss:  0.00031629516\n",
      "10758 : Training: loss:  0.00019354954\n",
      "10759 : Training: loss:  0.0002787758\n",
      "10760 : Training: loss:  0.00025516574\n",
      "Validation: Loss:  0.012722105  Accuracy:  0.96153843\n",
      "10761 : Training: loss:  0.0004112257\n",
      "10762 : Training: loss:  0.00033555218\n",
      "10763 : Training: loss:  0.00032162957\n",
      "10764 : Training: loss:  0.00029989032\n",
      "10765 : Training: loss:  0.00029118123\n",
      "10766 : Training: loss:  0.0004291318\n",
      "10767 : Training: loss:  0.00033197217\n",
      "10768 : Training: loss:  0.00025766922\n",
      "10769 : Training: loss:  9.1373455e-05\n",
      "10770 : Training: loss:  0.00041089425\n",
      "10771 : Training: loss:  0.000337161\n",
      "10772 : Training: loss:  0.00026487044\n",
      "10773 : Training: loss:  0.00046442635\n",
      "10774 : Training: loss:  0.0004262609\n",
      "10775 : Training: loss:  0.0003687504\n",
      "10776 : Training: loss:  0.0004883091\n",
      "10777 : Training: loss:  0.00020369639\n",
      "10778 : Training: loss:  0.00030289922\n",
      "10779 : Training: loss:  0.00025086396\n",
      "10780 : Training: loss:  0.00022748146\n",
      "Validation: Loss:  0.012636845  Accuracy:  0.96153843\n",
      "10781 : Training: loss:  0.00033854466\n",
      "10782 : Training: loss:  0.00022579526\n",
      "10783 : Training: loss:  0.00039106462\n",
      "10784 : Training: loss:  0.00017630793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10785 : Training: loss:  0.00015259536\n",
      "10786 : Training: loss:  9.939688e-05\n",
      "10787 : Training: loss:  0.00024326592\n",
      "10788 : Training: loss:  0.00024837142\n",
      "10789 : Training: loss:  0.00014303777\n",
      "10790 : Training: loss:  0.0002143984\n",
      "10791 : Training: loss:  0.00044326117\n",
      "10792 : Training: loss:  0.0002619913\n",
      "10793 : Training: loss:  0.0002533549\n",
      "10794 : Training: loss:  0.0004037399\n",
      "10795 : Training: loss:  0.00024499238\n",
      "10796 : Training: loss:  0.00021393025\n",
      "10797 : Training: loss:  0.00016836484\n",
      "10798 : Training: loss:  0.0003453597\n",
      "10799 : Training: loss:  0.00017333502\n",
      "10800 : Training: loss:  0.00018227017\n",
      "Validation: Loss:  0.012638835  Accuracy:  0.96153843\n",
      "10801 : Training: loss:  0.00071569055\n",
      "10802 : Training: loss:  0.00017835718\n",
      "10803 : Training: loss:  0.00048286858\n",
      "10804 : Training: loss:  0.00027111362\n",
      "10805 : Training: loss:  0.0003360275\n",
      "10806 : Training: loss:  0.0003867051\n",
      "10807 : Training: loss:  0.00026191588\n",
      "10808 : Training: loss:  0.000199284\n",
      "10809 : Training: loss:  0.00029193165\n",
      "10810 : Training: loss:  0.00019130419\n",
      "10811 : Training: loss:  0.00021559109\n",
      "10812 : Training: loss:  0.00032261683\n",
      "10813 : Training: loss:  0.00032251948\n",
      "10814 : Training: loss:  0.0005445224\n",
      "10815 : Training: loss:  0.00033414538\n",
      "10816 : Training: loss:  8.1053186e-05\n",
      "10817 : Training: loss:  0.00024364736\n",
      "10818 : Training: loss:  0.00022211784\n",
      "10819 : Training: loss:  0.00034422099\n",
      "10820 : Training: loss:  0.00043191778\n",
      "Validation: Loss:  0.01268938  Accuracy:  0.96153843\n",
      "10821 : Training: loss:  0.00023463098\n",
      "10822 : Training: loss:  0.00021239171\n",
      "10823 : Training: loss:  0.00020183988\n",
      "10824 : Training: loss:  0.00014659722\n",
      "10825 : Training: loss:  0.00045252233\n",
      "10826 : Training: loss:  0.00038057452\n",
      "10827 : Training: loss:  0.00023172695\n",
      "10828 : Training: loss:  0.00018931368\n",
      "10829 : Training: loss:  0.0004735216\n",
      "10830 : Training: loss:  0.00034780617\n",
      "10831 : Training: loss:  0.000222319\n",
      "10832 : Training: loss:  0.00027919436\n",
      "10833 : Training: loss:  0.00043885942\n",
      "10834 : Training: loss:  0.00031301746\n",
      "10835 : Training: loss:  0.00022221536\n",
      "10836 : Training: loss:  0.0002017643\n",
      "10837 : Training: loss:  0.00048464874\n",
      "10838 : Training: loss:  0.0001608129\n",
      "10839 : Training: loss:  0.00041170212\n",
      "10840 : Training: loss:  0.00032642882\n",
      "Validation: Loss:  0.012635889  Accuracy:  0.96153843\n",
      "10841 : Training: loss:  0.00024772403\n",
      "10842 : Training: loss:  0.00022244145\n",
      "10843 : Training: loss:  0.00022530436\n",
      "10844 : Training: loss:  0.0002007071\n",
      "10845 : Training: loss:  0.00027007592\n",
      "10846 : Training: loss:  0.00052719546\n",
      "10847 : Training: loss:  0.0003567997\n",
      "10848 : Training: loss:  0.0006850722\n",
      "10849 : Training: loss:  0.00031633215\n",
      "10850 : Training: loss:  0.00035033954\n",
      "10851 : Training: loss:  0.00022883234\n",
      "10852 : Training: loss:  0.0003499495\n",
      "10853 : Training: loss:  0.00018554858\n",
      "10854 : Training: loss:  0.0004926792\n",
      "10855 : Training: loss:  0.00029548648\n",
      "10856 : Training: loss:  0.00030485634\n",
      "10857 : Training: loss:  0.00027976322\n",
      "10858 : Training: loss:  0.00035514345\n",
      "10859 : Training: loss:  0.00032695677\n",
      "10860 : Training: loss:  0.00048601994\n",
      "Validation: Loss:  0.012618974  Accuracy:  0.96153843\n",
      "10861 : Training: loss:  0.00018029468\n",
      "10862 : Training: loss:  0.0002605359\n",
      "10863 : Training: loss:  0.0002744834\n",
      "10864 : Training: loss:  0.00020312946\n",
      "10865 : Training: loss:  0.00036409075\n",
      "10866 : Training: loss:  0.0003360301\n",
      "10867 : Training: loss:  0.00024868583\n",
      "10868 : Training: loss:  0.00025728656\n",
      "10869 : Training: loss:  0.00021475789\n",
      "10870 : Training: loss:  0.00027848483\n",
      "10871 : Training: loss:  0.00033149554\n",
      "10872 : Training: loss:  0.00023744014\n",
      "10873 : Training: loss:  0.00015130689\n",
      "10874 : Training: loss:  0.00022910508\n",
      "10875 : Training: loss:  0.00045421778\n",
      "10876 : Training: loss:  0.00042074375\n",
      "10877 : Training: loss:  0.00023053667\n",
      "10878 : Training: loss:  0.00038861876\n",
      "10879 : Training: loss:  0.0003817205\n",
      "10880 : Training: loss:  0.00031987566\n",
      "Validation: Loss:  0.012610169  Accuracy:  0.96153843\n",
      "10881 : Training: loss:  0.00033538532\n",
      "10882 : Training: loss:  0.00047533118\n",
      "10883 : Training: loss:  0.000191624\n",
      "10884 : Training: loss:  0.00048420505\n",
      "10885 : Training: loss:  0.00026410478\n",
      "10886 : Training: loss:  0.0003088601\n",
      "10887 : Training: loss:  0.0004060568\n",
      "10888 : Training: loss:  0.0004074047\n",
      "10889 : Training: loss:  0.00026503662\n",
      "10890 : Training: loss:  0.00049079506\n",
      "10891 : Training: loss:  0.00031700893\n",
      "10892 : Training: loss:  0.00028877897\n",
      "10893 : Training: loss:  0.00036967438\n",
      "10894 : Training: loss:  0.00010761745\n",
      "10895 : Training: loss:  0.00032131377\n",
      "10896 : Training: loss:  0.0003219461\n",
      "10897 : Training: loss:  0.00032409432\n",
      "10898 : Training: loss:  0.00063180295\n",
      "10899 : Training: loss:  0.0004007301\n",
      "10900 : Training: loss:  0.00028043144\n",
      "Validation: Loss:  0.01263793  Accuracy:  0.96153843\n",
      "10901 : Training: loss:  0.00037549276\n",
      "10902 : Training: loss:  0.00032754632\n",
      "10903 : Training: loss:  0.00030408037\n",
      "10904 : Training: loss:  0.00016681934\n",
      "10905 : Training: loss:  0.00032130376\n",
      "10906 : Training: loss:  0.0002718823\n",
      "10907 : Training: loss:  0.0001466369\n",
      "10908 : Training: loss:  0.00022099193\n",
      "10909 : Training: loss:  0.00020870769\n",
      "10910 : Training: loss:  0.0001969066\n",
      "10911 : Training: loss:  0.00041895077\n",
      "10912 : Training: loss:  0.00015697314\n",
      "10913 : Training: loss:  0.00036819163\n",
      "10914 : Training: loss:  0.0003216094\n",
      "10915 : Training: loss:  0.00032070023\n",
      "10916 : Training: loss:  0.00022771688\n",
      "10917 : Training: loss:  0.00041073482\n",
      "10918 : Training: loss:  0.00039485763\n",
      "10919 : Training: loss:  0.00022348824\n",
      "10920 : Training: loss:  0.00023534453\n",
      "Validation: Loss:  0.012688118  Accuracy:  0.96153843\n",
      "10921 : Training: loss:  8.1626895e-05\n",
      "10922 : Training: loss:  0.00043402673\n",
      "10923 : Training: loss:  0.00034622432\n",
      "10924 : Training: loss:  0.00015066362\n",
      "10925 : Training: loss:  0.00024679\n",
      "10926 : Training: loss:  0.00030769425\n",
      "10927 : Training: loss:  0.0002852066\n",
      "10928 : Training: loss:  0.00024749365\n",
      "10929 : Training: loss:  0.00049494643\n",
      "10930 : Training: loss:  0.0005242682\n",
      "10931 : Training: loss:  0.00034765806\n",
      "10932 : Training: loss:  0.000299604\n",
      "10933 : Training: loss:  0.00013223862\n",
      "10934 : Training: loss:  0.00033421096\n",
      "10935 : Training: loss:  0.00021979815\n",
      "10936 : Training: loss:  0.0004841078\n",
      "10937 : Training: loss:  0.00020128771\n",
      "10938 : Training: loss:  0.00048355275\n",
      "10939 : Training: loss:  0.00018621504\n",
      "10940 : Training: loss:  0.0002694812\n",
      "Validation: Loss:  0.0126883015  Accuracy:  0.96153843\n",
      "10941 : Training: loss:  0.00030396355\n",
      "10942 : Training: loss:  0.00031359532\n",
      "10943 : Training: loss:  0.00017373975\n",
      "10944 : Training: loss:  0.0005364225\n",
      "10945 : Training: loss:  0.0002968062\n",
      "10946 : Training: loss:  0.0003481368\n",
      "10947 : Training: loss:  0.0003336565\n",
      "10948 : Training: loss:  0.00020282356\n",
      "10949 : Training: loss:  0.00023872888\n",
      "10950 : Training: loss:  0.0002483228\n",
      "10951 : Training: loss:  0.00034667453\n",
      "10952 : Training: loss:  0.00017589402\n",
      "10953 : Training: loss:  0.0004263315\n",
      "10954 : Training: loss:  0.00035668054\n",
      "10955 : Training: loss:  0.00012012329\n",
      "10956 : Training: loss:  0.00023299351\n",
      "10957 : Training: loss:  0.00028794355\n",
      "10958 : Training: loss:  0.00023570484\n",
      "10959 : Training: loss:  0.00044339616\n",
      "10960 : Training: loss:  0.00012882797\n",
      "Validation: Loss:  0.012742492  Accuracy:  0.96153843\n",
      "10961 : Training: loss:  0.0003111433\n",
      "10962 : Training: loss:  0.00046904164\n",
      "10963 : Training: loss:  0.0002885139\n",
      "10964 : Training: loss:  0.00035726558\n",
      "10965 : Training: loss:  0.00030814816\n",
      "10966 : Training: loss:  0.00012593725\n",
      "10967 : Training: loss:  0.00037363594\n",
      "10968 : Training: loss:  0.0001549245\n",
      "10969 : Training: loss:  0.00028789803\n",
      "10970 : Training: loss:  0.00032939602\n",
      "10971 : Training: loss:  0.00030519627\n",
      "10972 : Training: loss:  0.0001839784\n",
      "10973 : Training: loss:  0.00017368999\n",
      "10974 : Training: loss:  0.00014826283\n",
      "10975 : Training: loss:  0.00033660943\n",
      "10976 : Training: loss:  0.00020271816\n",
      "10977 : Training: loss:  0.00044313972\n",
      "10978 : Training: loss:  0.00018164601\n",
      "10979 : Training: loss:  0.0004600774\n",
      "10980 : Training: loss:  0.0004001955\n",
      "Validation: Loss:  0.01274392  Accuracy:  0.96153843\n",
      "10981 : Training: loss:  0.00031036307\n",
      "10982 : Training: loss:  0.00019017058\n",
      "10983 : Training: loss:  0.00027890794\n",
      "10984 : Training: loss:  0.00035153548\n",
      "10985 : Training: loss:  0.0004083095\n",
      "10986 : Training: loss:  0.0003872441\n",
      "10987 : Training: loss:  0.0004318775\n",
      "10988 : Training: loss:  0.00034302103\n",
      "10989 : Training: loss:  0.00025516146\n",
      "10990 : Training: loss:  0.00038421273\n",
      "10991 : Training: loss:  0.00033756034\n",
      "10992 : Training: loss:  0.00012078355\n",
      "10993 : Training: loss:  0.00019777853\n",
      "10994 : Training: loss:  0.00021620018\n",
      "10995 : Training: loss:  0.00035712754\n",
      "10996 : Training: loss:  0.00022306376\n",
      "10997 : Training: loss:  0.0003765546\n",
      "10998 : Training: loss:  0.00018458089\n",
      "10999 : Training: loss:  0.00030566237\n",
      "11000 : Training: loss:  0.00021389688\n",
      "Validation: Loss:  0.012693968  Accuracy:  0.96153843\n",
      "11001 : Training: loss:  0.00050424045\n",
      "11002 : Training: loss:  0.000303062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11003 : Training: loss:  0.0003728689\n",
      "11004 : Training: loss:  0.00029340194\n",
      "11005 : Training: loss:  0.0004330055\n",
      "11006 : Training: loss:  0.00026074148\n",
      "11007 : Training: loss:  0.00020831994\n",
      "11008 : Training: loss:  0.00024487465\n",
      "11009 : Training: loss:  0.00021442828\n",
      "11010 : Training: loss:  0.00032209692\n",
      "11011 : Training: loss:  0.00035547983\n",
      "11012 : Training: loss:  0.00042913703\n",
      "11013 : Training: loss:  0.00024870594\n",
      "11014 : Training: loss:  0.00018773676\n",
      "11015 : Training: loss:  0.0004521355\n",
      "11016 : Training: loss:  0.00028951658\n",
      "11017 : Training: loss:  0.00039092064\n",
      "11018 : Training: loss:  0.00014356163\n",
      "11019 : Training: loss:  0.00018039782\n",
      "11020 : Training: loss:  0.0007454934\n",
      "Validation: Loss:  0.012766448  Accuracy:  0.96153843\n",
      "11021 : Training: loss:  0.00044676574\n",
      "11022 : Training: loss:  0.00028728446\n",
      "11023 : Training: loss:  0.0003054997\n",
      "11024 : Training: loss:  0.0002968958\n",
      "11025 : Training: loss:  0.00036409142\n",
      "11026 : Training: loss:  0.00027601724\n",
      "11027 : Training: loss:  0.00033740606\n",
      "11028 : Training: loss:  0.00030690388\n",
      "11029 : Training: loss:  0.00035501827\n",
      "11030 : Training: loss:  0.00026665602\n",
      "11031 : Training: loss:  0.000129777\n",
      "11032 : Training: loss:  0.00032464805\n",
      "11033 : Training: loss:  0.0002783165\n",
      "11034 : Training: loss:  0.00016867882\n",
      "11035 : Training: loss:  0.00041888485\n",
      "11036 : Training: loss:  0.00024981715\n",
      "11037 : Training: loss:  0.00028812073\n",
      "11038 : Training: loss:  0.00012271974\n",
      "11039 : Training: loss:  0.00036261827\n",
      "11040 : Training: loss:  0.0005646308\n",
      "Validation: Loss:  0.012803936  Accuracy:  0.96153843\n",
      "11041 : Training: loss:  0.00026660602\n",
      "11042 : Training: loss:  0.00018079927\n",
      "11043 : Training: loss:  0.00017425085\n",
      "11044 : Training: loss:  0.00023426137\n",
      "11045 : Training: loss:  0.0004109046\n",
      "11046 : Training: loss:  0.00027881507\n",
      "11047 : Training: loss:  0.00015701761\n",
      "11048 : Training: loss:  0.0003491272\n",
      "11049 : Training: loss:  0.0003859023\n",
      "11050 : Training: loss:  0.00022544731\n",
      "11051 : Training: loss:  0.00027607774\n",
      "11052 : Training: loss:  0.0003316241\n",
      "11053 : Training: loss:  0.00026265974\n",
      "11054 : Training: loss:  0.00020183323\n",
      "11055 : Training: loss:  0.00032141947\n",
      "11056 : Training: loss:  0.00016371867\n",
      "11057 : Training: loss:  0.00034956296\n",
      "11058 : Training: loss:  0.00033365175\n",
      "11059 : Training: loss:  0.00019947525\n",
      "11060 : Training: loss:  0.00037673887\n",
      "Validation: Loss:  0.012841329  Accuracy:  0.96153843\n",
      "11061 : Training: loss:  0.00020152543\n",
      "11062 : Training: loss:  0.00026020335\n",
      "11063 : Training: loss:  0.00029945915\n",
      "11064 : Training: loss:  0.00048005913\n",
      "11065 : Training: loss:  0.00023921413\n",
      "11066 : Training: loss:  0.00031827035\n",
      "11067 : Training: loss:  0.00020012399\n",
      "11068 : Training: loss:  0.0002741668\n",
      "11069 : Training: loss:  0.00018378776\n",
      "11070 : Training: loss:  0.00015017804\n",
      "11071 : Training: loss:  0.00035036227\n",
      "11072 : Training: loss:  0.00025550878\n",
      "11073 : Training: loss:  0.00032381248\n",
      "11074 : Training: loss:  0.00052514527\n",
      "11075 : Training: loss:  0.00029353204\n",
      "11076 : Training: loss:  0.0003387853\n",
      "11077 : Training: loss:  0.000204714\n",
      "11078 : Training: loss:  0.00032492293\n",
      "11079 : Training: loss:  0.00017886423\n",
      "11080 : Training: loss:  0.00028212514\n",
      "Validation: Loss:  0.012784955  Accuracy:  0.96153843\n",
      "11081 : Training: loss:  9.4686126e-05\n",
      "11082 : Training: loss:  0.00028348019\n",
      "11083 : Training: loss:  0.00031918645\n",
      "11084 : Training: loss:  0.00023275934\n",
      "11085 : Training: loss:  0.00047914457\n",
      "11086 : Training: loss:  0.00054351764\n",
      "11087 : Training: loss:  0.0002788664\n",
      "11088 : Training: loss:  0.00028751796\n",
      "11089 : Training: loss:  0.00026988165\n",
      "11090 : Training: loss:  0.00026422698\n",
      "11091 : Training: loss:  0.0001172821\n",
      "11092 : Training: loss:  0.00015485013\n",
      "11093 : Training: loss:  0.00033346415\n",
      "11094 : Training: loss:  0.00035623796\n",
      "11095 : Training: loss:  0.00024907573\n",
      "11096 : Training: loss:  0.00044159696\n",
      "11097 : Training: loss:  0.00030681584\n",
      "11098 : Training: loss:  0.00029679877\n",
      "11099 : Training: loss:  0.0002595324\n",
      "11100 : Training: loss:  0.00012013742\n",
      "Validation: Loss:  0.012723792  Accuracy:  0.96153843\n",
      "11101 : Training: loss:  0.00030721497\n",
      "11102 : Training: loss:  0.00040468515\n",
      "11103 : Training: loss:  0.0003714051\n",
      "11104 : Training: loss:  0.00025473625\n",
      "11105 : Training: loss:  0.00051804644\n",
      "11106 : Training: loss:  0.00021614063\n",
      "11107 : Training: loss:  0.00014297788\n",
      "11108 : Training: loss:  0.00040837828\n",
      "11109 : Training: loss:  0.00025405563\n",
      "11110 : Training: loss:  0.0003024036\n",
      "11111 : Training: loss:  0.0002485201\n",
      "11112 : Training: loss:  0.0003784472\n",
      "11113 : Training: loss:  0.00017195112\n",
      "11114 : Training: loss:  0.00015656857\n",
      "11115 : Training: loss:  0.00040785933\n",
      "11116 : Training: loss:  0.00033111952\n",
      "11117 : Training: loss:  0.00025210457\n",
      "11118 : Training: loss:  0.00037621704\n",
      "11119 : Training: loss:  0.00031100467\n",
      "11120 : Training: loss:  0.00021949939\n",
      "Validation: Loss:  0.012772773  Accuracy:  0.96153843\n",
      "11121 : Training: loss:  0.00024193988\n",
      "11122 : Training: loss:  0.00026409986\n",
      "11123 : Training: loss:  0.00022509534\n",
      "11124 : Training: loss:  0.0004892654\n",
      "11125 : Training: loss:  0.00041701685\n",
      "11126 : Training: loss:  0.0003557786\n",
      "11127 : Training: loss:  0.0002187172\n",
      "11128 : Training: loss:  0.0004277408\n",
      "11129 : Training: loss:  0.00023303517\n",
      "11130 : Training: loss:  0.0003997699\n",
      "11131 : Training: loss:  0.00014303226\n",
      "11132 : Training: loss:  0.00025515395\n",
      "11133 : Training: loss:  0.0003330635\n",
      "11134 : Training: loss:  0.00022742209\n",
      "11135 : Training: loss:  0.000119024604\n",
      "11136 : Training: loss:  0.0002986529\n",
      "11137 : Training: loss:  0.00033432458\n",
      "11138 : Training: loss:  0.0003233658\n",
      "11139 : Training: loss:  0.00028200707\n",
      "11140 : Training: loss:  9.8306184e-05\n",
      "Validation: Loss:  0.012700541  Accuracy:  0.96153843\n",
      "11141 : Training: loss:  0.00016770045\n",
      "11142 : Training: loss:  0.0002498759\n",
      "11143 : Training: loss:  0.00022790403\n",
      "11144 : Training: loss:  0.0002971305\n",
      "11145 : Training: loss:  0.00038681715\n",
      "11146 : Training: loss:  0.00022794276\n",
      "11147 : Training: loss:  0.0001703057\n",
      "11148 : Training: loss:  0.00039849788\n",
      "11149 : Training: loss:  0.0003195762\n",
      "11150 : Training: loss:  0.00019285959\n",
      "11151 : Training: loss:  0.0003573812\n",
      "11152 : Training: loss:  0.00025210832\n",
      "11153 : Training: loss:  0.00035347298\n",
      "11154 : Training: loss:  0.00031238142\n",
      "11155 : Training: loss:  0.00024979454\n",
      "11156 : Training: loss:  0.0002955002\n",
      "11157 : Training: loss:  0.00013755621\n",
      "11158 : Training: loss:  0.00014375664\n",
      "11159 : Training: loss:  0.00030614648\n",
      "11160 : Training: loss:  0.00034319726\n",
      "Validation: Loss:  0.012737864  Accuracy:  0.96153843\n",
      "11161 : Training: loss:  0.00022595507\n",
      "11162 : Training: loss:  0.0003094453\n",
      "11163 : Training: loss:  0.0004656192\n",
      "11164 : Training: loss:  9.9441364e-05\n",
      "11165 : Training: loss:  0.00029231183\n",
      "11166 : Training: loss:  0.0006517795\n",
      "11167 : Training: loss:  0.00017967202\n",
      "11168 : Training: loss:  0.0004097121\n",
      "11169 : Training: loss:  0.00025515744\n",
      "11170 : Training: loss:  0.00017724054\n",
      "11171 : Training: loss:  0.00030473495\n",
      "11172 : Training: loss:  0.00012614534\n",
      "11173 : Training: loss:  0.00010917331\n",
      "11174 : Training: loss:  0.00016108248\n",
      "11175 : Training: loss:  0.00028547496\n",
      "11176 : Training: loss:  0.0001392745\n",
      "11177 : Training: loss:  0.00017822142\n",
      "11178 : Training: loss:  0.00018042758\n",
      "11179 : Training: loss:  0.00023414809\n",
      "11180 : Training: loss:  9.3469265e-05\n",
      "Validation: Loss:  0.01282849  Accuracy:  0.96153843\n",
      "11181 : Training: loss:  0.00055586797\n",
      "11182 : Training: loss:  0.00037903705\n",
      "11183 : Training: loss:  0.0003307138\n",
      "11184 : Training: loss:  0.00038466635\n",
      "11185 : Training: loss:  0.00028157723\n",
      "11186 : Training: loss:  0.00043263639\n",
      "11187 : Training: loss:  0.00015550043\n",
      "11188 : Training: loss:  0.00037671722\n",
      "11189 : Training: loss:  0.00017036301\n",
      "11190 : Training: loss:  0.00039900758\n",
      "11191 : Training: loss:  0.00022734326\n",
      "11192 : Training: loss:  0.00024657804\n",
      "11193 : Training: loss:  0.00029370928\n",
      "11194 : Training: loss:  0.0005756724\n",
      "11195 : Training: loss:  0.00035953108\n",
      "11196 : Training: loss:  0.00018913423\n",
      "11197 : Training: loss:  0.0002619358\n",
      "11198 : Training: loss:  0.00031694124\n",
      "11199 : Training: loss:  0.00018971584\n",
      "11200 : Training: loss:  0.0004851859\n",
      "Validation: Loss:  0.012768792  Accuracy:  0.96153843\n",
      "11201 : Training: loss:  0.0004075892\n",
      "11202 : Training: loss:  0.00025902313\n",
      "11203 : Training: loss:  0.00029780422\n",
      "11204 : Training: loss:  0.00026387585\n",
      "11205 : Training: loss:  0.00031181096\n",
      "11206 : Training: loss:  0.00024378859\n",
      "11207 : Training: loss:  0.00030562378\n",
      "11208 : Training: loss:  0.00024301412\n",
      "11209 : Training: loss:  0.0003486224\n",
      "11210 : Training: loss:  0.00016207137\n",
      "11211 : Training: loss:  0.00026440952\n",
      "11212 : Training: loss:  0.00028892775\n",
      "11213 : Training: loss:  0.00028578215\n",
      "11214 : Training: loss:  0.00015501922\n",
      "11215 : Training: loss:  0.00037396012\n",
      "11216 : Training: loss:  0.00027521703\n",
      "11217 : Training: loss:  0.00012880733\n",
      "11218 : Training: loss:  0.00033998158\n",
      "11219 : Training: loss:  0.00030135192\n",
      "11220 : Training: loss:  0.00019700138\n",
      "Validation: Loss:  0.012651083  Accuracy:  0.96153843\n",
      "11221 : Training: loss:  0.00031426028\n",
      "11222 : Training: loss:  0.0003150765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11223 : Training: loss:  0.00032902186\n",
      "11224 : Training: loss:  0.00027929596\n",
      "11225 : Training: loss:  0.00021164316\n",
      "11226 : Training: loss:  0.00051468797\n",
      "11227 : Training: loss:  0.00040628997\n",
      "11228 : Training: loss:  0.0003328049\n",
      "11229 : Training: loss:  0.00030960015\n",
      "11230 : Training: loss:  0.00028274854\n",
      "11231 : Training: loss:  0.0003010003\n",
      "11232 : Training: loss:  0.0002998447\n",
      "11233 : Training: loss:  0.00020076966\n",
      "11234 : Training: loss:  0.00026994\n",
      "11235 : Training: loss:  0.00040745878\n",
      "11236 : Training: loss:  0.00024402847\n",
      "11237 : Training: loss:  0.0003825044\n",
      "11238 : Training: loss:  0.00014415111\n",
      "11239 : Training: loss:  0.00019819933\n",
      "11240 : Training: loss:  0.00019037808\n",
      "Validation: Loss:  0.012598564  Accuracy:  0.96153843\n",
      "11241 : Training: loss:  0.00038864336\n",
      "11242 : Training: loss:  0.0002485583\n",
      "11243 : Training: loss:  0.00023606785\n",
      "11244 : Training: loss:  0.00027262757\n",
      "11245 : Training: loss:  0.00026738725\n",
      "11246 : Training: loss:  0.000149326\n",
      "11247 : Training: loss:  0.00027887366\n",
      "11248 : Training: loss:  0.00029452538\n",
      "11249 : Training: loss:  0.00016135024\n",
      "11250 : Training: loss:  0.00037452238\n",
      "11251 : Training: loss:  0.0002655004\n",
      "11252 : Training: loss:  0.00019186472\n",
      "11253 : Training: loss:  0.00014782928\n",
      "11254 : Training: loss:  0.0003398349\n",
      "11255 : Training: loss:  0.00026285366\n",
      "11256 : Training: loss:  0.00023436917\n",
      "11257 : Training: loss:  0.00018743797\n",
      "11258 : Training: loss:  0.00031156678\n",
      "11259 : Training: loss:  0.00021847639\n",
      "11260 : Training: loss:  0.00022921065\n",
      "Validation: Loss:  0.012578407  Accuracy:  0.96153843\n",
      "11261 : Training: loss:  0.00021162993\n",
      "11262 : Training: loss:  0.00031975616\n",
      "11263 : Training: loss:  0.00023214782\n",
      "11264 : Training: loss:  0.00019557615\n",
      "11265 : Training: loss:  0.00041715425\n",
      "11266 : Training: loss:  0.00029492754\n",
      "11267 : Training: loss:  6.325461e-05\n",
      "11268 : Training: loss:  0.00040366288\n",
      "11269 : Training: loss:  0.00015719503\n",
      "11270 : Training: loss:  0.00027736695\n",
      "11271 : Training: loss:  0.00019338811\n",
      "11272 : Training: loss:  0.00050004345\n",
      "11273 : Training: loss:  0.00019795707\n",
      "11274 : Training: loss:  0.00037131464\n",
      "11275 : Training: loss:  0.0003900867\n",
      "11276 : Training: loss:  0.00032742193\n",
      "11277 : Training: loss:  0.00012381942\n",
      "11278 : Training: loss:  0.0001641432\n",
      "11279 : Training: loss:  0.0003113893\n",
      "11280 : Training: loss:  0.00031284167\n",
      "Validation: Loss:  0.012619483  Accuracy:  0.96153843\n",
      "11281 : Training: loss:  0.0001793149\n",
      "11282 : Training: loss:  0.00032818277\n",
      "11283 : Training: loss:  0.00035949997\n",
      "11284 : Training: loss:  0.00049548835\n",
      "11285 : Training: loss:  0.00015028506\n",
      "11286 : Training: loss:  0.00033353973\n",
      "11287 : Training: loss:  0.00019104539\n",
      "11288 : Training: loss:  0.00036520418\n",
      "11289 : Training: loss:  0.0004510774\n",
      "11290 : Training: loss:  0.00019060717\n",
      "11291 : Training: loss:  0.0001622226\n",
      "11292 : Training: loss:  0.00021323551\n",
      "11293 : Training: loss:  0.00032804837\n",
      "11294 : Training: loss:  0.00027990912\n",
      "11295 : Training: loss:  0.00026744496\n",
      "11296 : Training: loss:  0.00018953903\n",
      "11297 : Training: loss:  0.000274678\n",
      "11298 : Training: loss:  0.00026755861\n",
      "11299 : Training: loss:  0.00017825056\n",
      "11300 : Training: loss:  9.0730944e-05\n",
      "Validation: Loss:  0.012612331  Accuracy:  0.96153843\n",
      "11301 : Training: loss:  0.00026316408\n",
      "11302 : Training: loss:  0.00035930358\n",
      "11303 : Training: loss:  0.00040230673\n",
      "11304 : Training: loss:  0.00025557276\n",
      "11305 : Training: loss:  0.0002651338\n",
      "11306 : Training: loss:  0.00016835694\n",
      "11307 : Training: loss:  0.00016132563\n",
      "11308 : Training: loss:  0.00024544666\n",
      "11309 : Training: loss:  0.00016471224\n",
      "11310 : Training: loss:  0.0003285853\n",
      "11311 : Training: loss:  0.00030672594\n",
      "11312 : Training: loss:  0.0001873151\n",
      "11313 : Training: loss:  0.00012885117\n",
      "11314 : Training: loss:  0.00026239618\n",
      "11315 : Training: loss:  0.00027244515\n",
      "11316 : Training: loss:  0.00028250844\n",
      "11317 : Training: loss:  0.00024933286\n",
      "11318 : Training: loss:  0.00039855888\n",
      "11319 : Training: loss:  0.00030618013\n",
      "11320 : Training: loss:  0.00031370312\n",
      "Validation: Loss:  0.012635234  Accuracy:  0.9807692\n",
      "11321 : Training: loss:  0.0002762561\n",
      "11322 : Training: loss:  0.00024806557\n",
      "11323 : Training: loss:  0.00022835423\n",
      "11324 : Training: loss:  0.00024085137\n",
      "11325 : Training: loss:  0.00023155064\n",
      "11326 : Training: loss:  0.00024209254\n",
      "11327 : Training: loss:  0.00022450098\n",
      "11328 : Training: loss:  0.00018609247\n",
      "11329 : Training: loss:  0.00019563835\n",
      "11330 : Training: loss:  0.00025037222\n",
      "11331 : Training: loss:  0.00019667741\n",
      "11332 : Training: loss:  0.00025252413\n",
      "11333 : Training: loss:  0.00016704436\n",
      "11334 : Training: loss:  0.00028058203\n",
      "11335 : Training: loss:  0.0004162433\n",
      "11336 : Training: loss:  0.00026870478\n",
      "11337 : Training: loss:  0.00019545647\n",
      "11338 : Training: loss:  0.00025483794\n",
      "11339 : Training: loss:  0.00026910374\n",
      "11340 : Training: loss:  0.00028754276\n",
      "Validation: Loss:  0.012624719  Accuracy:  0.9807692\n",
      "11341 : Training: loss:  0.00029474887\n",
      "11342 : Training: loss:  0.00020113628\n",
      "11343 : Training: loss:  0.00029128493\n",
      "11344 : Training: loss:  0.00021773855\n",
      "11345 : Training: loss:  0.00028949586\n",
      "11346 : Training: loss:  0.0002761308\n",
      "11347 : Training: loss:  0.00033161402\n",
      "11348 : Training: loss:  0.000271517\n",
      "11349 : Training: loss:  0.0003162487\n",
      "11350 : Training: loss:  0.00022077427\n",
      "11351 : Training: loss:  0.00022176768\n",
      "11352 : Training: loss:  0.00017161509\n",
      "11353 : Training: loss:  0.00026553054\n",
      "11354 : Training: loss:  0.0003513602\n",
      "11355 : Training: loss:  0.00027026405\n",
      "11356 : Training: loss:  0.0003618976\n",
      "11357 : Training: loss:  0.00031841497\n",
      "11358 : Training: loss:  0.00028201885\n",
      "11359 : Training: loss:  0.0002696057\n",
      "11360 : Training: loss:  0.00021818622\n",
      "Validation: Loss:  0.0126334485  Accuracy:  0.9807692\n",
      "11361 : Training: loss:  0.00032991485\n",
      "11362 : Training: loss:  0.00036706452\n",
      "11363 : Training: loss:  0.00014844618\n",
      "11364 : Training: loss:  0.00024844002\n",
      "11365 : Training: loss:  0.00025890415\n",
      "11366 : Training: loss:  0.00020195871\n",
      "11367 : Training: loss:  0.00023084656\n",
      "11368 : Training: loss:  0.00024170615\n",
      "11369 : Training: loss:  0.00020570272\n",
      "11370 : Training: loss:  0.00031999688\n",
      "11371 : Training: loss:  0.00025549528\n",
      "11372 : Training: loss:  0.0004186713\n",
      "11373 : Training: loss:  0.00011902464\n",
      "11374 : Training: loss:  0.0003718743\n",
      "11375 : Training: loss:  0.00020961477\n",
      "11376 : Training: loss:  0.00015354302\n",
      "11377 : Training: loss:  0.00014896235\n",
      "11378 : Training: loss:  0.00013249699\n",
      "11379 : Training: loss:  0.00026201675\n",
      "11380 : Training: loss:  0.00021348063\n",
      "Validation: Loss:  0.012736868  Accuracy:  0.9807692\n",
      "11381 : Training: loss:  0.00019595168\n",
      "11382 : Training: loss:  0.00026264388\n",
      "11383 : Training: loss:  0.00039443118\n",
      "11384 : Training: loss:  9.864016e-05\n",
      "11385 : Training: loss:  0.00029742325\n",
      "11386 : Training: loss:  0.00025894798\n",
      "11387 : Training: loss:  0.00018302651\n",
      "11388 : Training: loss:  0.00015733857\n",
      "11389 : Training: loss:  0.00014986492\n",
      "11390 : Training: loss:  0.00024820003\n",
      "11391 : Training: loss:  0.00022708953\n",
      "11392 : Training: loss:  0.00011630008\n",
      "11393 : Training: loss:  0.00038666234\n",
      "11394 : Training: loss:  0.00025128678\n",
      "11395 : Training: loss:  0.00016556765\n",
      "11396 : Training: loss:  0.00027712237\n",
      "11397 : Training: loss:  0.00019922435\n",
      "11398 : Training: loss:  0.0001731973\n",
      "11399 : Training: loss:  9.754927e-05\n",
      "11400 : Training: loss:  0.00024439456\n",
      "Validation: Loss:  0.012838148  Accuracy:  0.9807692\n",
      "11401 : Training: loss:  0.00023467476\n",
      "11402 : Training: loss:  0.00019460544\n",
      "11403 : Training: loss:  0.00024099526\n",
      "11404 : Training: loss:  0.00033385333\n",
      "11405 : Training: loss:  0.00025424917\n",
      "11406 : Training: loss:  0.00026406595\n",
      "11407 : Training: loss:  0.00030896452\n",
      "11408 : Training: loss:  0.00020914171\n",
      "11409 : Training: loss:  0.00029555478\n",
      "11410 : Training: loss:  0.00028132214\n",
      "11411 : Training: loss:  0.00026425367\n",
      "11412 : Training: loss:  0.0002070506\n",
      "11413 : Training: loss:  0.00012744709\n",
      "11414 : Training: loss:  0.00026412637\n",
      "11415 : Training: loss:  0.0002055958\n",
      "11416 : Training: loss:  0.00029931124\n",
      "11417 : Training: loss:  0.0003033057\n",
      "11418 : Training: loss:  0.00027342312\n",
      "11419 : Training: loss:  0.0002949286\n",
      "11420 : Training: loss:  0.00019955939\n",
      "Validation: Loss:  0.012836246  Accuracy:  0.9807692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11421 : Training: loss:  0.00021613787\n",
      "11422 : Training: loss:  0.00016635982\n",
      "11423 : Training: loss:  0.0002762349\n",
      "11424 : Training: loss:  0.00016277489\n",
      "11425 : Training: loss:  0.00022049571\n",
      "11426 : Training: loss:  0.00021856575\n",
      "11427 : Training: loss:  0.00020497358\n",
      "11428 : Training: loss:  0.00010329233\n",
      "11429 : Training: loss:  0.00038736954\n",
      "11430 : Training: loss:  0.0002203295\n",
      "11431 : Training: loss:  0.00021848579\n",
      "11432 : Training: loss:  0.00010747173\n",
      "11433 : Training: loss:  0.00025511015\n",
      "11434 : Training: loss:  0.000380809\n",
      "11435 : Training: loss:  0.00019075669\n",
      "11436 : Training: loss:  0.00022559868\n",
      "11437 : Training: loss:  0.00018862206\n",
      "11438 : Training: loss:  0.0005007968\n",
      "11439 : Training: loss:  0.0002668855\n",
      "11440 : Training: loss:  0.00039332014\n",
      "Validation: Loss:  0.012760654  Accuracy:  0.96153843\n",
      "11441 : Training: loss:  0.0003570446\n",
      "11442 : Training: loss:  0.00033576452\n",
      "11443 : Training: loss:  0.0002884805\n",
      "11444 : Training: loss:  0.00021415623\n",
      "11445 : Training: loss:  0.0002699177\n",
      "11446 : Training: loss:  0.00023587488\n",
      "11447 : Training: loss:  0.00024634297\n",
      "11448 : Training: loss:  0.00030838975\n",
      "11449 : Training: loss:  8.5027175e-05\n",
      "11450 : Training: loss:  0.00048008419\n",
      "11451 : Training: loss:  0.00025259983\n",
      "11452 : Training: loss:  0.0002161275\n",
      "11453 : Training: loss:  0.0003858624\n",
      "11454 : Training: loss:  0.00021481287\n",
      "11455 : Training: loss:  0.0002608488\n",
      "11456 : Training: loss:  0.00022392714\n",
      "11457 : Training: loss:  0.00031265256\n",
      "11458 : Training: loss:  0.00016232628\n",
      "11459 : Training: loss:  0.00025140413\n",
      "11460 : Training: loss:  0.00034495463\n",
      "Validation: Loss:  0.012594041  Accuracy:  0.9807692\n",
      "11461 : Training: loss:  0.00010906783\n",
      "11462 : Training: loss:  0.00013796048\n",
      "11463 : Training: loss:  0.00021808\n",
      "11464 : Training: loss:  0.00026853237\n",
      "11465 : Training: loss:  7.6748365e-05\n",
      "11466 : Training: loss:  0.00018301327\n",
      "11467 : Training: loss:  0.00021809258\n",
      "11468 : Training: loss:  0.00048397822\n",
      "11469 : Training: loss:  0.00026801485\n",
      "11470 : Training: loss:  0.00020711022\n",
      "11471 : Training: loss:  0.0001692461\n",
      "11472 : Training: loss:  0.00022811517\n",
      "11473 : Training: loss:  0.00015279076\n",
      "11474 : Training: loss:  0.0003208186\n",
      "11475 : Training: loss:  0.00033099562\n",
      "11476 : Training: loss:  0.00022340126\n",
      "11477 : Training: loss:  0.0002995667\n",
      "11478 : Training: loss:  0.00013729447\n",
      "11479 : Training: loss:  0.00019572738\n",
      "11480 : Training: loss:  0.00021151065\n",
      "Validation: Loss:  0.01253417  Accuracy:  0.9807692\n",
      "11481 : Training: loss:  0.00054171594\n",
      "11482 : Training: loss:  0.00028962325\n",
      "11483 : Training: loss:  0.00011697229\n",
      "11484 : Training: loss:  0.0002682214\n",
      "11485 : Training: loss:  0.00038077333\n",
      "11486 : Training: loss:  0.00026591227\n",
      "11487 : Training: loss:  0.00034808755\n",
      "11488 : Training: loss:  0.0002855591\n",
      "11489 : Training: loss:  0.0002027681\n",
      "11490 : Training: loss:  9.7470096e-05\n",
      "11491 : Training: loss:  0.0002469325\n",
      "11492 : Training: loss:  0.00019586111\n",
      "11493 : Training: loss:  0.00030257716\n",
      "11494 : Training: loss:  0.00023337413\n",
      "11495 : Training: loss:  0.00018397011\n",
      "11496 : Training: loss:  0.00031495688\n",
      "11497 : Training: loss:  0.00036997747\n",
      "11498 : Training: loss:  0.0002623848\n",
      "11499 : Training: loss:  0.0003716258\n",
      "11500 : Training: loss:  0.00023879018\n",
      "Validation: Loss:  0.012433593  Accuracy:  0.9807692\n",
      "11501 : Training: loss:  0.0003919888\n",
      "11502 : Training: loss:  0.00022799282\n",
      "11503 : Training: loss:  0.0003428655\n",
      "11504 : Training: loss:  0.00031833557\n",
      "11505 : Training: loss:  0.00033946746\n",
      "11506 : Training: loss:  0.00021731944\n",
      "11507 : Training: loss:  0.00030980015\n",
      "11508 : Training: loss:  0.00040499435\n",
      "11509 : Training: loss:  0.00029061924\n",
      "11510 : Training: loss:  0.00022610603\n",
      "11511 : Training: loss:  0.0004279203\n",
      "11512 : Training: loss:  0.00019519738\n",
      "11513 : Training: loss:  0.00021381873\n",
      "11514 : Training: loss:  0.0001619045\n",
      "11515 : Training: loss:  0.00020691531\n",
      "11516 : Training: loss:  0.00018175643\n",
      "11517 : Training: loss:  0.00016993911\n",
      "11518 : Training: loss:  0.0002455767\n",
      "11519 : Training: loss:  0.00018266327\n",
      "11520 : Training: loss:  0.00029684938\n",
      "Validation: Loss:  0.012367234  Accuracy:  0.9807692\n",
      "11521 : Training: loss:  0.000251222\n",
      "11522 : Training: loss:  0.0003018281\n",
      "11523 : Training: loss:  0.00014579052\n",
      "11524 : Training: loss:  0.00023860394\n",
      "11525 : Training: loss:  0.00022240459\n",
      "11526 : Training: loss:  0.0002066741\n",
      "11527 : Training: loss:  0.00018126397\n",
      "11528 : Training: loss:  0.000114796305\n",
      "11529 : Training: loss:  0.00036036127\n",
      "11530 : Training: loss:  0.00019697227\n",
      "11531 : Training: loss:  0.0002438795\n",
      "11532 : Training: loss:  0.00022760687\n",
      "11533 : Training: loss:  0.00028408947\n",
      "11534 : Training: loss:  0.00014886967\n",
      "11535 : Training: loss:  0.0001251855\n",
      "11536 : Training: loss:  0.00023184792\n",
      "11537 : Training: loss:  0.00019631803\n",
      "11538 : Training: loss:  9.122928e-05\n",
      "11539 : Training: loss:  0.00018160846\n",
      "11540 : Training: loss:  0.00033827696\n",
      "Validation: Loss:  0.012330742  Accuracy:  0.9807692\n",
      "11541 : Training: loss:  0.00025104656\n",
      "11542 : Training: loss:  0.00017930534\n",
      "11543 : Training: loss:  0.00022081521\n",
      "11544 : Training: loss:  0.00022058871\n",
      "11545 : Training: loss:  0.0002770746\n",
      "11546 : Training: loss:  0.00023751397\n",
      "11547 : Training: loss:  0.0003058424\n",
      "11548 : Training: loss:  0.00024587056\n",
      "11549 : Training: loss:  0.0002908675\n",
      "11550 : Training: loss:  0.0002792158\n",
      "11551 : Training: loss:  0.0002664734\n",
      "11552 : Training: loss:  0.00014856976\n",
      "11553 : Training: loss:  0.00023922486\n",
      "11554 : Training: loss:  0.00019681116\n",
      "11555 : Training: loss:  0.00016500689\n",
      "11556 : Training: loss:  0.00015124153\n",
      "11557 : Training: loss:  0.0002251517\n",
      "11558 : Training: loss:  0.00015326394\n",
      "11559 : Training: loss:  0.00018733022\n",
      "11560 : Training: loss:  0.00018976898\n",
      "Validation: Loss:  0.012314491  Accuracy:  0.9807692\n",
      "11561 : Training: loss:  0.00021100565\n",
      "11562 : Training: loss:  0.0003343886\n",
      "11563 : Training: loss:  0.00021045496\n",
      "11564 : Training: loss:  0.00028286895\n",
      "11565 : Training: loss:  0.00026925377\n",
      "11566 : Training: loss:  0.00026794156\n",
      "11567 : Training: loss:  0.00013721049\n",
      "11568 : Training: loss:  0.00027878708\n",
      "11569 : Training: loss:  0.00019004407\n",
      "11570 : Training: loss:  0.00012864616\n",
      "11571 : Training: loss:  0.0001331598\n",
      "11572 : Training: loss:  0.00032807223\n",
      "11573 : Training: loss:  0.00024402353\n",
      "11574 : Training: loss:  0.00024955755\n",
      "11575 : Training: loss:  0.00016701018\n",
      "11576 : Training: loss:  0.00020802526\n",
      "11577 : Training: loss:  0.00022736753\n",
      "11578 : Training: loss:  0.00029084893\n",
      "11579 : Training: loss:  0.00026502373\n",
      "11580 : Training: loss:  0.00038629744\n",
      "Validation: Loss:  0.012309658  Accuracy:  0.96153843\n",
      "11581 : Training: loss:  0.000276298\n",
      "11582 : Training: loss:  0.00012122131\n",
      "11583 : Training: loss:  0.00020691991\n",
      "11584 : Training: loss:  0.00019961061\n",
      "11585 : Training: loss:  0.00013806668\n",
      "11586 : Training: loss:  0.00030101681\n",
      "11587 : Training: loss:  0.00016244492\n",
      "11588 : Training: loss:  0.00023519281\n",
      "11589 : Training: loss:  0.00023911317\n",
      "11590 : Training: loss:  0.0002228474\n",
      "11591 : Training: loss:  0.000121192694\n",
      "11592 : Training: loss:  0.00026775379\n",
      "11593 : Training: loss:  0.00024509162\n",
      "11594 : Training: loss:  0.00030324017\n",
      "11595 : Training: loss:  0.00019932874\n",
      "11596 : Training: loss:  0.00015361272\n",
      "11597 : Training: loss:  0.0002459302\n",
      "11598 : Training: loss:  0.00019734816\n",
      "11599 : Training: loss:  0.00010520959\n",
      "11600 : Training: loss:  0.0003231649\n",
      "Validation: Loss:  0.012232676  Accuracy:  0.96153843\n",
      "11601 : Training: loss:  0.0002113708\n",
      "11602 : Training: loss:  0.00013904907\n",
      "11603 : Training: loss:  0.00022844774\n",
      "11604 : Training: loss:  0.00016977997\n",
      "11605 : Training: loss:  0.00020781867\n",
      "11606 : Training: loss:  0.00019175313\n",
      "11607 : Training: loss:  0.00026261443\n",
      "11608 : Training: loss:  0.00014814669\n",
      "11609 : Training: loss:  0.00027888748\n",
      "11610 : Training: loss:  0.00019473984\n",
      "11611 : Training: loss:  0.00016522872\n",
      "11612 : Training: loss:  0.00033379637\n",
      "11613 : Training: loss:  0.0002699466\n",
      "11614 : Training: loss:  0.00020004137\n",
      "11615 : Training: loss:  0.00034040082\n",
      "11616 : Training: loss:  0.00019452855\n",
      "11617 : Training: loss:  0.00020919715\n",
      "11618 : Training: loss:  0.00011333389\n",
      "11619 : Training: loss:  0.00017737166\n",
      "11620 : Training: loss:  0.00016117499\n",
      "Validation: Loss:  0.012186473  Accuracy:  0.9807692\n",
      "11621 : Training: loss:  0.00024328838\n",
      "11622 : Training: loss:  0.00031448083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11623 : Training: loss:  0.00016497329\n",
      "11624 : Training: loss:  0.00025081256\n",
      "11625 : Training: loss:  0.00010293853\n",
      "11626 : Training: loss:  0.00025978938\n",
      "11627 : Training: loss:  0.00019783882\n",
      "11628 : Training: loss:  0.00012897185\n",
      "11629 : Training: loss:  0.00020697011\n",
      "11630 : Training: loss:  0.00025504155\n",
      "11631 : Training: loss:  0.00016741801\n",
      "11632 : Training: loss:  0.00020410342\n",
      "11633 : Training: loss:  0.00027210542\n",
      "11634 : Training: loss:  0.0002734748\n",
      "11635 : Training: loss:  0.00041277608\n",
      "11636 : Training: loss:  0.00014146857\n",
      "11637 : Training: loss:  0.00022043737\n",
      "11638 : Training: loss:  0.00018259602\n",
      "11639 : Training: loss:  0.00013710509\n",
      "11640 : Training: loss:  0.0001892343\n",
      "Validation: Loss:  0.012286985  Accuracy:  0.96153843\n",
      "11641 : Training: loss:  0.00016031926\n",
      "11642 : Training: loss:  7.991305e-05\n",
      "11643 : Training: loss:  0.00021554582\n",
      "11644 : Training: loss:  5.1606396e-05\n",
      "11645 : Training: loss:  0.00015252113\n",
      "11646 : Training: loss:  0.00029995912\n",
      "11647 : Training: loss:  0.00025582148\n",
      "11648 : Training: loss:  0.00024302483\n",
      "11649 : Training: loss:  0.00024876985\n",
      "11650 : Training: loss:  0.00018785223\n",
      "11651 : Training: loss:  0.00016842998\n",
      "11652 : Training: loss:  0.00028587625\n",
      "11653 : Training: loss:  9.355083e-05\n",
      "11654 : Training: loss:  0.00015706597\n",
      "11655 : Training: loss:  0.0003473818\n",
      "11656 : Training: loss:  0.00022367353\n",
      "11657 : Training: loss:  0.00019407122\n",
      "11658 : Training: loss:  0.00021666108\n",
      "11659 : Training: loss:  0.000103058555\n",
      "11660 : Training: loss:  0.00026397468\n",
      "Validation: Loss:  0.012341923  Accuracy:  0.96153843\n",
      "11661 : Training: loss:  0.000209964\n",
      "11662 : Training: loss:  0.00018634532\n",
      "11663 : Training: loss:  0.00023303769\n",
      "11664 : Training: loss:  0.00017989386\n",
      "11665 : Training: loss:  0.0002982981\n",
      "11666 : Training: loss:  0.00021262474\n",
      "11667 : Training: loss:  0.00011413289\n",
      "11668 : Training: loss:  0.00025776302\n",
      "11669 : Training: loss:  0.000178515\n",
      "11670 : Training: loss:  0.00031487676\n",
      "11671 : Training: loss:  0.00021465478\n",
      "11672 : Training: loss:  0.00031742416\n",
      "11673 : Training: loss:  0.00016891908\n",
      "11674 : Training: loss:  0.0001738835\n",
      "11675 : Training: loss:  0.00021813993\n",
      "11676 : Training: loss:  0.00020019851\n",
      "11677 : Training: loss:  0.0003358168\n",
      "11678 : Training: loss:  0.00023114403\n",
      "11679 : Training: loss:  0.00026192106\n",
      "11680 : Training: loss:  0.00036109838\n",
      "Validation: Loss:  0.012396603  Accuracy:  0.9807692\n",
      "11681 : Training: loss:  0.0002426097\n",
      "11682 : Training: loss:  0.00028132516\n",
      "11683 : Training: loss:  0.00013550518\n",
      "11684 : Training: loss:  0.00012921385\n",
      "11685 : Training: loss:  0.000114181305\n",
      "11686 : Training: loss:  0.00023718992\n",
      "11687 : Training: loss:  0.00017090303\n",
      "11688 : Training: loss:  0.0001117025\n",
      "11689 : Training: loss:  0.00025906906\n",
      "11690 : Training: loss:  0.0001706208\n",
      "11691 : Training: loss:  0.00019155254\n",
      "11692 : Training: loss:  0.00028874353\n",
      "11693 : Training: loss:  0.00036188855\n",
      "11694 : Training: loss:  0.00020001773\n",
      "11695 : Training: loss:  9.533102e-05\n",
      "11696 : Training: loss:  0.000105902516\n",
      "11697 : Training: loss:  0.00013361874\n",
      "11698 : Training: loss:  0.00036952217\n",
      "11699 : Training: loss:  0.00028713254\n",
      "11700 : Training: loss:  0.00029736117\n",
      "Validation: Loss:  0.012429432  Accuracy:  0.9807692\n",
      "11701 : Training: loss:  0.00024141504\n",
      "11702 : Training: loss:  0.00026170904\n",
      "11703 : Training: loss:  0.0002513345\n",
      "11704 : Training: loss:  0.00010386734\n",
      "11705 : Training: loss:  0.000237712\n",
      "11706 : Training: loss:  0.00013594422\n",
      "11707 : Training: loss:  0.00017594079\n",
      "11708 : Training: loss:  0.00021671184\n",
      "11709 : Training: loss:  0.00036800763\n",
      "11710 : Training: loss:  0.00036733158\n",
      "11711 : Training: loss:  0.00015359766\n",
      "11712 : Training: loss:  0.00021040346\n",
      "11713 : Training: loss:  0.00023119383\n",
      "11714 : Training: loss:  0.00011815163\n",
      "11715 : Training: loss:  0.00029226163\n",
      "11716 : Training: loss:  0.00034145472\n",
      "11717 : Training: loss:  0.00020307727\n",
      "11718 : Training: loss:  0.0002994755\n",
      "11719 : Training: loss:  8.2715866e-05\n",
      "11720 : Training: loss:  0.0001148279\n",
      "Validation: Loss:  0.012405861  Accuracy:  0.9807692\n",
      "11721 : Training: loss:  0.00017502153\n",
      "11722 : Training: loss:  0.00021318601\n",
      "11723 : Training: loss:  0.00022585811\n",
      "11724 : Training: loss:  0.00017393181\n",
      "11725 : Training: loss:  0.00030974508\n",
      "11726 : Training: loss:  0.00018134678\n",
      "11727 : Training: loss:  0.0002338459\n",
      "11728 : Training: loss:  0.00015641417\n",
      "11729 : Training: loss:  0.00010510151\n",
      "11730 : Training: loss:  0.00012228597\n",
      "11731 : Training: loss:  0.00036725716\n",
      "11732 : Training: loss:  0.0002749041\n",
      "11733 : Training: loss:  0.00015951348\n",
      "11734 : Training: loss:  0.00023775772\n",
      "11735 : Training: loss:  0.00016303129\n",
      "11736 : Training: loss:  0.00031422538\n",
      "11737 : Training: loss:  0.00017820243\n",
      "11738 : Training: loss:  0.00021615448\n",
      "11739 : Training: loss:  0.00013714994\n",
      "11740 : Training: loss:  0.000355458\n",
      "Validation: Loss:  0.012429959  Accuracy:  0.9807692\n",
      "11741 : Training: loss:  0.00029748163\n",
      "11742 : Training: loss:  0.00029737726\n",
      "11743 : Training: loss:  0.00015993284\n",
      "11744 : Training: loss:  0.00029805905\n",
      "11745 : Training: loss:  0.00030233114\n",
      "11746 : Training: loss:  0.0001550159\n",
      "11747 : Training: loss:  0.00042166113\n",
      "11748 : Training: loss:  0.00018346218\n",
      "11749 : Training: loss:  0.0003265643\n",
      "11750 : Training: loss:  0.00017222927\n",
      "11751 : Training: loss:  0.00013833545\n",
      "11752 : Training: loss:  0.00018587796\n",
      "11753 : Training: loss:  0.00022172235\n",
      "11754 : Training: loss:  0.00022645488\n",
      "11755 : Training: loss:  0.0003518755\n",
      "11756 : Training: loss:  0.00010976217\n",
      "11757 : Training: loss:  0.00010441244\n",
      "11758 : Training: loss:  0.00014582695\n",
      "11759 : Training: loss:  0.00022994651\n",
      "11760 : Training: loss:  0.00024365507\n",
      "Validation: Loss:  0.012403461  Accuracy:  0.9807692\n",
      "11761 : Training: loss:  0.00017384601\n",
      "11762 : Training: loss:  0.00030719422\n",
      "11763 : Training: loss:  0.00027945184\n",
      "11764 : Training: loss:  0.0001253842\n",
      "11765 : Training: loss:  0.00024819182\n",
      "11766 : Training: loss:  0.00035893742\n",
      "11767 : Training: loss:  0.00030678438\n",
      "11768 : Training: loss:  0.00016586609\n",
      "11769 : Training: loss:  0.00013185637\n",
      "11770 : Training: loss:  0.00018092155\n",
      "11771 : Training: loss:  0.0003746244\n",
      "11772 : Training: loss:  0.0002734707\n",
      "11773 : Training: loss:  0.00023880259\n",
      "11774 : Training: loss:  0.00022056334\n",
      "11775 : Training: loss:  0.00029666157\n",
      "11776 : Training: loss:  0.0002469213\n",
      "11777 : Training: loss:  0.00017404644\n",
      "11778 : Training: loss:  0.00019423402\n",
      "11779 : Training: loss:  0.00026070545\n",
      "11780 : Training: loss:  0.00022448221\n",
      "Validation: Loss:  0.012341857  Accuracy:  0.9807692\n",
      "11781 : Training: loss:  0.00018517402\n",
      "11782 : Training: loss:  0.00023986824\n",
      "11783 : Training: loss:  0.00019788712\n",
      "11784 : Training: loss:  0.00016428826\n",
      "11785 : Training: loss:  0.00012738704\n",
      "11786 : Training: loss:  8.843854e-05\n",
      "11787 : Training: loss:  0.00036631135\n",
      "11788 : Training: loss:  0.00019808617\n",
      "11789 : Training: loss:  0.00024211893\n",
      "11790 : Training: loss:  0.00020488762\n",
      "11791 : Training: loss:  9.006648e-05\n",
      "11792 : Training: loss:  0.0002394495\n",
      "11793 : Training: loss:  0.00017934908\n",
      "11794 : Training: loss:  0.00015151069\n",
      "11795 : Training: loss:  0.00011502121\n",
      "11796 : Training: loss:  0.00018052764\n",
      "11797 : Training: loss:  0.00014392233\n",
      "11798 : Training: loss:  9.436983e-05\n",
      "11799 : Training: loss:  0.00014830998\n",
      "11800 : Training: loss:  0.0001781514\n",
      "Validation: Loss:  0.012325076  Accuracy:  0.9807692\n",
      "11801 : Training: loss:  0.00026227464\n",
      "11802 : Training: loss:  0.00020297318\n",
      "11803 : Training: loss:  0.00023867603\n",
      "11804 : Training: loss:  0.00017590179\n",
      "11805 : Training: loss:  0.00027937358\n",
      "11806 : Training: loss:  0.000222875\n",
      "11807 : Training: loss:  0.00029333046\n",
      "11808 : Training: loss:  0.00023069137\n",
      "11809 : Training: loss:  0.00024458373\n",
      "11810 : Training: loss:  0.00014444273\n",
      "11811 : Training: loss:  0.00013946804\n",
      "11812 : Training: loss:  0.00021264682\n",
      "11813 : Training: loss:  0.00013898869\n",
      "11814 : Training: loss:  0.00016996243\n",
      "11815 : Training: loss:  0.00017970052\n",
      "11816 : Training: loss:  0.00019262562\n",
      "11817 : Training: loss:  0.00011707015\n",
      "11818 : Training: loss:  0.00011914884\n",
      "11819 : Training: loss:  0.00018847095\n",
      "11820 : Training: loss:  8.754353e-05\n",
      "Validation: Loss:  0.012345705  Accuracy:  0.9807692\n",
      "11821 : Training: loss:  0.00029075673\n",
      "11822 : Training: loss:  0.00022633691\n",
      "11823 : Training: loss:  0.00011255617\n",
      "11824 : Training: loss:  0.00019110425\n",
      "11825 : Training: loss:  0.0001761792\n",
      "11826 : Training: loss:  0.00017273314\n",
      "11827 : Training: loss:  0.0001961045\n",
      "11828 : Training: loss:  0.00015606989\n",
      "11829 : Training: loss:  0.00013745086\n",
      "11830 : Training: loss:  0.00018795085\n",
      "11831 : Training: loss:  0.00032684044\n",
      "11832 : Training: loss:  0.00020078494\n",
      "11833 : Training: loss:  0.00020397318\n",
      "11834 : Training: loss:  0.00030820843\n",
      "11835 : Training: loss:  0.00021708806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11836 : Training: loss:  0.00024669667\n",
      "11837 : Training: loss:  0.00018503002\n",
      "11838 : Training: loss:  0.00016223191\n",
      "11839 : Training: loss:  0.00019277858\n",
      "11840 : Training: loss:  0.0002230406\n",
      "Validation: Loss:  0.01236864  Accuracy:  0.9807692\n",
      "11841 : Training: loss:  0.00015234562\n",
      "11842 : Training: loss:  0.0002568887\n",
      "11843 : Training: loss:  0.00016471281\n",
      "11844 : Training: loss:  0.00017161509\n",
      "11845 : Training: loss:  0.00016363034\n",
      "11846 : Training: loss:  0.00023720953\n",
      "11847 : Training: loss:  0.0002263257\n",
      "11848 : Training: loss:  0.0001813358\n",
      "11849 : Training: loss:  0.00017857517\n",
      "11850 : Training: loss:  0.00017523173\n",
      "11851 : Training: loss:  5.5407425e-05\n",
      "11852 : Training: loss:  0.00022120337\n",
      "11853 : Training: loss:  0.00011488406\n",
      "11854 : Training: loss:  0.00014841926\n",
      "11855 : Training: loss:  0.00019140905\n",
      "11856 : Training: loss:  0.0002759958\n",
      "11857 : Training: loss:  0.00017910378\n",
      "11858 : Training: loss:  0.00017057781\n",
      "11859 : Training: loss:  0.000102542916\n",
      "11860 : Training: loss:  0.000115672505\n",
      "Validation: Loss:  0.012375933  Accuracy:  0.9807692\n",
      "11861 : Training: loss:  0.00029462983\n",
      "11862 : Training: loss:  0.00029453172\n",
      "11863 : Training: loss:  0.00017766032\n",
      "11864 : Training: loss:  0.00010479396\n",
      "11865 : Training: loss:  0.00014311122\n",
      "11866 : Training: loss:  0.0001352653\n",
      "11867 : Training: loss:  0.00010009793\n",
      "11868 : Training: loss:  0.000108167915\n",
      "11869 : Training: loss:  0.00010066469\n",
      "11870 : Training: loss:  0.00023935582\n",
      "11871 : Training: loss:  0.00021613314\n",
      "11872 : Training: loss:  0.0001628677\n",
      "11873 : Training: loss:  0.00025291758\n",
      "11874 : Training: loss:  0.00015283783\n",
      "11875 : Training: loss:  0.00014021761\n",
      "11876 : Training: loss:  0.000130035\n",
      "11877 : Training: loss:  0.00021103474\n",
      "11878 : Training: loss:  0.00020546111\n",
      "11879 : Training: loss:  0.0002232956\n",
      "11880 : Training: loss:  0.00010985824\n",
      "Validation: Loss:  0.012327401  Accuracy:  1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class scores</th>\n",
       "      <th>Correct labels</th>\n",
       "      <th>Predicted labels</th>\n",
       "      <th>Trust to textual modality</th>\n",
       "      <th>Trust to visual modality</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3605, 0.0056, 0.0, 0.0, 0.0, 1e-04, 0.0, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5667, 0.8508, 0.0415, 0.8179, 0.5946, 0.032...</td>\n",
       "      <td>[0.4333, 0.1492, 0.9585, 0.1821, 0.4054, 0.967...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.9978, 0.1794, 0.0, 0.0, 0.002, 0.0, 0.0, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.916, 0.9429, 0.1123, 0.7217, 0.9216, 0.2132...</td>\n",
       "      <td>[0.084, 0.0571, 0.8877, 0.2783, 0.0784, 0.7868...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.9986, 0.0037, 0.0, 0.0, 0.0, 0.0006, 0.0, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6383, 0.961, 0.1062, 0.9306, 0.6189, 0.0528...</td>\n",
       "      <td>[0.3617, 0.039, 0.8938, 0.0694, 0.3811, 0.9472...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.8612, 0.0, 0.0, 0.0023, 0.0002, 0.0, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5619, 0.6391, 0.0503, 0.8094, 0.9924, 0.595...</td>\n",
       "      <td>[0.4381, 0.3609, 0.9497, 0.1906, 0.0076, 0.404...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1e-04, 0.8416, 0.0, 0.0, 0.0, 0.0, 1e-04, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1703, 0.6129, 0.0142, 0.4832, 0.8252, 0.055...</td>\n",
       "      <td>[0.8297, 0.3871, 0.9858, 0.5168, 0.1748, 0.944...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0, 1e-04, 0.9962, 0.0, 0.0, 0.0, 1e-04, 0.0...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.4713, 0.1222, 0.0621, 0.3696, 0.4155, 0.979...</td>\n",
       "      <td>[0.5287, 0.8778, 0.9379, 0.6304, 0.5845, 0.020...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0002, 0.0, 0.0, 0.9956, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9934, 0.9964, 0.9655, 0.9968, 0.0585, 0.953...</td>\n",
       "      <td>[0.0066, 0.0036, 0.0345, 0.0032, 0.9415, 0.046...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.9716, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9412, 0.5835, 0.8056, 0.7867, 0.174, 0.9731...</td>\n",
       "      <td>[0.0588, 0.4165, 0.1944, 0.2133, 0.826, 0.0269...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0002, 0.0, 0.0, 0.9945, 1e-04, 0.0, 0.0, 0....</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.9676, 0.9739, 0.9827, 0.5247, 0.2469, 0.939...</td>\n",
       "      <td>[0.0324, 0.0261, 0.0173, 0.4753, 0.7531, 0.060...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0004, 0.9979, 0.0023, 0.0, 0...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.9717, 0.9764, 0.8071, 0.4996, 0.9984, 0.846...</td>\n",
       "      <td>[0.0283, 0.0236, 0.1929, 0.5004, 0.0016, 0.154...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.0027, 0.0222, 0.0, 0.0, 0.9178, 0.0194, 0.0...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.7457, 0.9272, 0.0279, 0.1536, 0.9855, 0.862...</td>\n",
       "      <td>[0.2543, 0.0728, 0.9721, 0.8464, 0.0145, 0.137...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0008, 0.9077, 0.0, 0.0,...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.3021, 0.8989, 0.1799, 0.1935, 0.9913, 0.286...</td>\n",
       "      <td>[0.6979, 0.1011, 0.8201, 0.8065, 0.0087, 0.713...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.8573, 0.0,...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.3685, 0.7817, 0.8698, 0.9423, 0.946, 0.4996...</td>\n",
       "      <td>[0.6315, 0.2183, 0.1302, 0.0577, 0.054, 0.5004...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.0009, 0.0002, 0.0018, 1e-04, 0.0, 1e-04, 1e...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.0359, 0.0005, 0.3702, 0.6302, 0.6404, 0.996...</td>\n",
       "      <td>[0.9641, 0.9995, 0.6298, 0.3698, 0.3596, 0.003...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.0003, 0.0, 0.0, 0.0, 0.0045, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.8749, 0.9678, 0.6114, 0.9746, 0.9351, 0.260...</td>\n",
       "      <td>[0.1251, 0.0322, 0.3886, 0.0254, 0.0649, 0.739...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[1e-04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.1628, 0.9868, 0.5967, 0.4215, 0.9375, 0.013...</td>\n",
       "      <td>[0.8372, 0.0132, 0.4033, 0.5785, 0.0625, 0.986...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0, 0.0, 0.0254, 0.0, 0.0, 0.0, 0.0015, 0.05...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.4516, 0.459, 0.0695, 0.593, 0.2458, 0.9076,...</td>\n",
       "      <td>[0.5484, 0.541, 0.9305, 0.407, 0.7542, 0.0924,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[1e-04, 0.0, 1e-04, 0.0003, 0.0, 0.0, 0.0, 1e-...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.8671, 0.9431, 0.5993, 0.8193, 0.2907, 0.956...</td>\n",
       "      <td>[0.1329, 0.0569, 0.4007, 0.1807, 0.7093, 0.044...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1418, 0.8237, 0.0863, 0.026, 0.9548, 0.411,...</td>\n",
       "      <td>[0.8582, 0.1763, 0.9137, 0.974, 0.0452, 0.589,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1606, 0.9709, 0.1422, 0.0304, 0.9866, 0.098...</td>\n",
       "      <td>[0.8394, 0.0291, 0.8578, 0.9696, 0.0134, 0.901...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[1e-04, 1e-04, 0.0, 1e-04, 0.0, 0.0, 0.0002, 0...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.7436, 0.8825, 0.93, 0.9741, 0.3781, 0.4364,...</td>\n",
       "      <td>[0.2564, 0.1175, 0.07, 0.0259, 0.6219, 0.5636,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-04, 0.0, 0.0...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.6326, 0.2945, 0.7956, 0.964, 0.2669, 0.5534...</td>\n",
       "      <td>[0.3674, 0.7055, 0.2044, 0.036, 0.7331, 0.4466...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.0004, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0008, 0....</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.2633, 0.8123, 0.5041, 0.8821, 0.2611, 0.520...</td>\n",
       "      <td>[0.7367, 0.1877, 0.4959, 0.1179, 0.7389, 0.479...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.0004, 0.0, 0.0, 0.0002, 0.0011, 0.001, 0.0,...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.7941, 0.9717, 0.935, 0.0217, 0.9598, 0.858,...</td>\n",
       "      <td>[0.2059, 0.0283, 0.065, 0.9783, 0.0402, 0.142,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[1e-04, 0.0, 0.0457, 1e-04, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[0.6119, 0.8506, 0.6425, 0.1677, 0.3437, 0.882...</td>\n",
       "      <td>[0.3881, 0.1494, 0.3575, 0.8323, 0.6563, 0.117...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.4201, 0.7958, 0.9684, 0.9226, 0.947, 0.8477...</td>\n",
       "      <td>[0.5799, 0.2042, 0.0316, 0.0774, 0.053, 0.1523...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, 0.0, 1e...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[0.1194, 0.1145, 0.8615, 0.6405, 0.4783, 0.815...</td>\n",
       "      <td>[0.8806, 0.8855, 0.1385, 0.3595, 0.5217, 0.184...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-04...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8154, 0.5781, 0.5128, 0.9418, 0.9729, 0.964...</td>\n",
       "      <td>[0.1846, 0.4219, 0.4872, 0.0582, 0.0271, 0.035...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0, 1e-04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.7106, 0.0171, 0.842, 0.9654, 0.9416, 0.9972...</td>\n",
       "      <td>[0.2894, 0.9829, 0.158, 0.0346, 0.0584, 0.0028...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0.0, 1e-04, 0.0, 0.0, 1e-04, 0.0, 0.0, 0.0, 1...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.8434, 0.0793, 0.8981, 0.9773, 0.9984, 0.981...</td>\n",
       "      <td>[0.1566, 0.9207, 0.1019, 0.0227, 0.0016, 0.018...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[1e-04, 0.0002, 0.0, 0.0, 0.0, 0.0, 1e-04, 1e-...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.2424, 0.0025, 0.5724, 0.0, 1.0, 0.0004, 1.0...</td>\n",
       "      <td>[0.7576, 0.9975, 0.4276, 1.0, 0.0, 0.9996, 0.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9763, 0.8523, 0.9744, 0.9439, 0.474, 0.928,...</td>\n",
       "      <td>[0.0237, 0.1477, 0.0256, 0.0561, 0.526, 0.072,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[0.0, 0.0, 0.0003, 0.0002, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[0.9178, 0.9929, 0.9951, 0.9986, 0.249, 0.9392...</td>\n",
       "      <td>[0.0822, 0.0071, 0.0049, 0.0014, 0.751, 0.0608...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0007, 1e-0...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.2436, 0.8568, 0.1075, 0.5019, 0.2121, 0.446...</td>\n",
       "      <td>[0.7564, 0.1432, 0.8925, 0.4981, 0.7879, 0.553...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[0.0007, 1e-04, 0.0, 0.0, 0.0004, 0.0, 0.0002,...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>[0.9485, 0.9917, 0.0972, 0.5088, 0.9773, 0.036...</td>\n",
       "      <td>[0.0515, 0.0083, 0.9028, 0.4912, 0.0227, 0.963...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.0005, 1e-04, 1e-04, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>[0.3095, 0.6106, 0.7074, 0.8509, 0.2371, 0.760...</td>\n",
       "      <td>[0.6905, 0.3894, 0.2926, 0.1491, 0.7629, 0.239...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1e-04, 1e-04, 0.0, 0...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0247, 0.2915, 0.5515, 0.7486, 0.0853, 0.850...</td>\n",
       "      <td>[0.9753, 0.7085, 0.4485, 0.2514, 0.9147, 0.149...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0007, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>[0.0046, 0.0209, 0.9515, 0.8549, 0.0148, 0.992...</td>\n",
       "      <td>[0.9954, 0.9791, 0.0485, 0.1451, 0.9852, 0.007...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-04, 0.0...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.6995, 0.0089, 0.0044, 0.8795, 0.0817, 0.988...</td>\n",
       "      <td>[0.3005, 0.9911, 0.9956, 0.1205, 0.9183, 0.012...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.7581, 0.0787, 0.0164, 0.7972, 0.2316, 0.921...</td>\n",
       "      <td>[0.2419, 0.9213, 0.9836, 0.2028, 0.7684, 0.078...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.9202, 0.3111, 0.241, 0.9921, 0.6966, 0.7921...</td>\n",
       "      <td>[0.0798, 0.6889, 0.759, 0.0079, 0.3034, 0.2079...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0, 0.0169,...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>[0.1513, 0.0322, 0.0093, 0.3302, 0.1145, 0.956...</td>\n",
       "      <td>[0.8487, 0.9678, 0.9907, 0.6698, 0.8855, 0.043...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.1008, 0.1325, 0.8053, 0.432, 0.0304, 0.5072...</td>\n",
       "      <td>[0.8992, 0.8675, 0.1947, 0.568, 0.9696, 0.4928...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0907, 0.0075, 0.8421, 0.0939, 0.0061, 0.809...</td>\n",
       "      <td>[0.9093, 0.9925, 0.1579, 0.9061, 0.9939, 0.190...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, 0....</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0396, 0.0018, 0.9713, 0.125, 0.0042, 0.9791...</td>\n",
       "      <td>[0.9604, 0.9982, 0.0287, 0.875, 0.9958, 0.0209...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>[0.0937, 0.0649, 0.883, 0.4895, 0.0114, 0.6968...</td>\n",
       "      <td>[0.9063, 0.9351, 0.117, 0.5105, 0.9886, 0.3032...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0, 0.0, 0.0139, 0.0, 0.0, 1e-04, 0.0002, 0....</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.9016, 0.9037, 0.9872, 0.1349, 0.7908, 0.958...</td>\n",
       "      <td>[0.0984, 0.0963, 0.0128, 0.8651, 0.2092, 0.041...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[0.0, 0.0019, 1e-04, 1e-04, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>[0.9709, 0.9607, 0.9771, 0.6037, 0.6722, 0.418...</td>\n",
       "      <td>[0.0291, 0.0393, 0.0229, 0.3963, 0.3278, 0.581...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, 0.0, 0....</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.0609, 0.8228, 0.0055, 0.0807, 0.981, 0.1236...</td>\n",
       "      <td>[0.9391, 0.1772, 0.9945, 0.9193, 0.019, 0.8764...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[0.0002, 0.0, 0.0, 0.0, 1e-04, 0.0033, 0.0, 0....</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>[0.0277, 0.9928, 0.3582, 0.5459, 0.9336, 0.144...</td>\n",
       "      <td>[0.9723, 0.0072, 0.6418, 0.4541, 0.0664, 0.855...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[0.0002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.6377, 0.8493, 0.977, 0.9454, 0.631, 0.0589,...</td>\n",
       "      <td>[0.3623, 0.1507, 0.023, 0.0546, 0.369, 0.9411,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.0, 0.0, 0.0, 1e-04, 0.0005, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>[0.9286, 0.9888, 0.9347, 0.9185, 0.9283, 0.386...</td>\n",
       "      <td>[0.0714, 0.0112, 0.0653, 0.0815, 0.0717, 0.613...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Class scores  Correct labels  \\\n",
       "0   [0.3605, 0.0056, 0.0, 0.0, 0.0, 1e-04, 0.0, 0....               0   \n",
       "1   [0.9978, 0.1794, 0.0, 0.0, 0.002, 0.0, 0.0, 0....               0   \n",
       "2   [0.9986, 0.0037, 0.0, 0.0, 0.0, 0.0006, 0.0, 0...               0   \n",
       "3   [0.0, 0.8612, 0.0, 0.0, 0.0023, 0.0002, 0.0, 0...               1   \n",
       "4   [1e-04, 0.8416, 0.0, 0.0, 0.0, 0.0, 1e-04, 0.0...               1   \n",
       "5   [0.0, 1e-04, 0.9962, 0.0, 0.0, 0.0, 1e-04, 0.0...               2   \n",
       "6   [0.0002, 0.0, 0.0, 0.9956, 0.0, 0.0, 0.0, 0.0,...               3   \n",
       "7   [0.0, 0.0, 0.0, 0.9716, 0.0, 0.0, 0.0, 0.0, 0....               3   \n",
       "8   [0.0002, 0.0, 0.0, 0.9945, 1e-04, 0.0, 0.0, 0....               3   \n",
       "9   [0.0, 0.0, 0.0, 0.0004, 0.9979, 0.0023, 0.0, 0...               4   \n",
       "10  [0.0027, 0.0222, 0.0, 0.0, 0.9178, 0.0194, 0.0...               4   \n",
       "11  [0.0, 0.0, 0.0, 0.0, 0.0008, 0.9077, 0.0, 0.0,...               5   \n",
       "12  [0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.8573, 0.0,...               6   \n",
       "13  [0.0009, 0.0002, 0.0018, 1e-04, 0.0, 1e-04, 1e...               7   \n",
       "14  [0.0003, 0.0, 0.0, 0.0, 0.0045, 0.0, 0.0, 0.0,...               8   \n",
       "15  [1e-04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1...               8   \n",
       "16  [0.0, 0.0, 0.0254, 0.0, 0.0, 0.0, 0.0015, 0.05...               9   \n",
       "17  [1e-04, 0.0, 1e-04, 0.0003, 0.0, 0.0, 0.0, 1e-...               9   \n",
       "18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              10   \n",
       "19  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              10   \n",
       "20  [1e-04, 1e-04, 0.0, 1e-04, 0.0, 0.0, 0.0002, 0...              11   \n",
       "21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-04, 0.0, 0.0...              11   \n",
       "22  [0.0004, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0008, 0....              12   \n",
       "23  [0.0004, 0.0, 0.0, 0.0002, 0.0011, 0.001, 0.0,...              13   \n",
       "24  [1e-04, 0.0, 0.0457, 1e-04, 0.0, 0.0, 0.0, 0.0...              13   \n",
       "25  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              14   \n",
       "26  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, 0.0, 1e...              14   \n",
       "27  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-04...              15   \n",
       "28  [0.0, 1e-04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, ...              15   \n",
       "29  [0.0, 1e-04, 0.0, 0.0, 1e-04, 0.0, 0.0, 0.0, 1...              15   \n",
       "30  [1e-04, 0.0002, 0.0, 0.0, 0.0, 0.0, 1e-04, 1e-...              16   \n",
       "31  [0.0, 0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0, 0....              17   \n",
       "32  [0.0, 0.0, 0.0003, 0.0002, 0.0, 0.0, 0.0, 0.0,...              17   \n",
       "33  [0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0007, 1e-0...              18   \n",
       "34  [0.0007, 1e-04, 0.0, 0.0, 0.0004, 0.0, 0.0002,...              19   \n",
       "35  [0.0005, 1e-04, 1e-04, 0.0, 0.0, 0.0, 0.0, 0.0...              20   \n",
       "36  [0.0, 0.0, 0.0, 0.0, 0.0, 1e-04, 1e-04, 0.0, 0...              21   \n",
       "37  [0.0, 0.0, 0.0, 0.0007, 0.0, 0.0, 0.0, 0.0, 0....              21   \n",
       "38  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-04, 0.0...              22   \n",
       "39  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              22   \n",
       "40  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              22   \n",
       "41  [0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0, 0.0169,...              22   \n",
       "42  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              23   \n",
       "43  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              23   \n",
       "44  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, 0....              23   \n",
       "45  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              23   \n",
       "46  [0.0, 0.0, 0.0139, 0.0, 0.0, 1e-04, 0.0002, 0....              24   \n",
       "47  [0.0, 0.0019, 1e-04, 1e-04, 0.0, 0.0, 0.0, 0.0...              24   \n",
       "48  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, 0.0, 0....              25   \n",
       "49  [0.0002, 0.0, 0.0, 0.0, 1e-04, 0.0033, 0.0, 0....              25   \n",
       "50  [0.0002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....              26   \n",
       "51  [0.0, 0.0, 0.0, 1e-04, 0.0005, 0.0, 0.0, 0.0, ...              26   \n",
       "\n",
       "    Predicted labels                          Trust to textual modality  \\\n",
       "0                  0  [0.5667, 0.8508, 0.0415, 0.8179, 0.5946, 0.032...   \n",
       "1                  0  [0.916, 0.9429, 0.1123, 0.7217, 0.9216, 0.2132...   \n",
       "2                  0  [0.6383, 0.961, 0.1062, 0.9306, 0.6189, 0.0528...   \n",
       "3                  1  [0.5619, 0.6391, 0.0503, 0.8094, 0.9924, 0.595...   \n",
       "4                  1  [0.1703, 0.6129, 0.0142, 0.4832, 0.8252, 0.055...   \n",
       "5                  2  [0.4713, 0.1222, 0.0621, 0.3696, 0.4155, 0.979...   \n",
       "6                  3  [0.9934, 0.9964, 0.9655, 0.9968, 0.0585, 0.953...   \n",
       "7                  3  [0.9412, 0.5835, 0.8056, 0.7867, 0.174, 0.9731...   \n",
       "8                  3  [0.9676, 0.9739, 0.9827, 0.5247, 0.2469, 0.939...   \n",
       "9                  4  [0.9717, 0.9764, 0.8071, 0.4996, 0.9984, 0.846...   \n",
       "10                 4  [0.7457, 0.9272, 0.0279, 0.1536, 0.9855, 0.862...   \n",
       "11                 5  [0.3021, 0.8989, 0.1799, 0.1935, 0.9913, 0.286...   \n",
       "12                 6  [0.3685, 0.7817, 0.8698, 0.9423, 0.946, 0.4996...   \n",
       "13                 7  [0.0359, 0.0005, 0.3702, 0.6302, 0.6404, 0.996...   \n",
       "14                 8  [0.8749, 0.9678, 0.6114, 0.9746, 0.9351, 0.260...   \n",
       "15                 8  [0.1628, 0.9868, 0.5967, 0.4215, 0.9375, 0.013...   \n",
       "16                 9  [0.4516, 0.459, 0.0695, 0.593, 0.2458, 0.9076,...   \n",
       "17                 9  [0.8671, 0.9431, 0.5993, 0.8193, 0.2907, 0.956...   \n",
       "18                10  [0.1418, 0.8237, 0.0863, 0.026, 0.9548, 0.411,...   \n",
       "19                10  [0.1606, 0.9709, 0.1422, 0.0304, 0.9866, 0.098...   \n",
       "20                11  [0.7436, 0.8825, 0.93, 0.9741, 0.3781, 0.4364,...   \n",
       "21                11  [0.6326, 0.2945, 0.7956, 0.964, 0.2669, 0.5534...   \n",
       "22                12  [0.2633, 0.8123, 0.5041, 0.8821, 0.2611, 0.520...   \n",
       "23                13  [0.7941, 0.9717, 0.935, 0.0217, 0.9598, 0.858,...   \n",
       "24                13  [0.6119, 0.8506, 0.6425, 0.1677, 0.3437, 0.882...   \n",
       "25                14  [0.4201, 0.7958, 0.9684, 0.9226, 0.947, 0.8477...   \n",
       "26                14  [0.1194, 0.1145, 0.8615, 0.6405, 0.4783, 0.815...   \n",
       "27                15  [0.8154, 0.5781, 0.5128, 0.9418, 0.9729, 0.964...   \n",
       "28                15  [0.7106, 0.0171, 0.842, 0.9654, 0.9416, 0.9972...   \n",
       "29                15  [0.8434, 0.0793, 0.8981, 0.9773, 0.9984, 0.981...   \n",
       "30                16  [0.2424, 0.0025, 0.5724, 0.0, 1.0, 0.0004, 1.0...   \n",
       "31                17  [0.9763, 0.8523, 0.9744, 0.9439, 0.474, 0.928,...   \n",
       "32                17  [0.9178, 0.9929, 0.9951, 0.9986, 0.249, 0.9392...   \n",
       "33                18  [0.2436, 0.8568, 0.1075, 0.5019, 0.2121, 0.446...   \n",
       "34                19  [0.9485, 0.9917, 0.0972, 0.5088, 0.9773, 0.036...   \n",
       "35                20  [0.3095, 0.6106, 0.7074, 0.8509, 0.2371, 0.760...   \n",
       "36                21  [0.0247, 0.2915, 0.5515, 0.7486, 0.0853, 0.850...   \n",
       "37                21  [0.0046, 0.0209, 0.9515, 0.8549, 0.0148, 0.992...   \n",
       "38                22  [0.6995, 0.0089, 0.0044, 0.8795, 0.0817, 0.988...   \n",
       "39                22  [0.7581, 0.0787, 0.0164, 0.7972, 0.2316, 0.921...   \n",
       "40                22  [0.9202, 0.3111, 0.241, 0.9921, 0.6966, 0.7921...   \n",
       "41                22  [0.1513, 0.0322, 0.0093, 0.3302, 0.1145, 0.956...   \n",
       "42                23  [0.1008, 0.1325, 0.8053, 0.432, 0.0304, 0.5072...   \n",
       "43                23  [0.0907, 0.0075, 0.8421, 0.0939, 0.0061, 0.809...   \n",
       "44                23  [0.0396, 0.0018, 0.9713, 0.125, 0.0042, 0.9791...   \n",
       "45                23  [0.0937, 0.0649, 0.883, 0.4895, 0.0114, 0.6968...   \n",
       "46                24  [0.9016, 0.9037, 0.9872, 0.1349, 0.7908, 0.958...   \n",
       "47                24  [0.9709, 0.9607, 0.9771, 0.6037, 0.6722, 0.418...   \n",
       "48                25  [0.0609, 0.8228, 0.0055, 0.0807, 0.981, 0.1236...   \n",
       "49                25  [0.0277, 0.9928, 0.3582, 0.5459, 0.9336, 0.144...   \n",
       "50                26  [0.6377, 0.8493, 0.977, 0.9454, 0.631, 0.0589,...   \n",
       "51                26  [0.9286, 0.9888, 0.9347, 0.9185, 0.9283, 0.386...   \n",
       "\n",
       "                             Trust to visual modality  Accuracy      Loss  \n",
       "0   [0.4333, 0.1492, 0.9585, 0.1821, 0.4054, 0.967...       1.0  0.012327  \n",
       "1   [0.084, 0.0571, 0.8877, 0.2783, 0.0784, 0.7868...       NaN       NaN  \n",
       "2   [0.3617, 0.039, 0.8938, 0.0694, 0.3811, 0.9472...       NaN       NaN  \n",
       "3   [0.4381, 0.3609, 0.9497, 0.1906, 0.0076, 0.404...       NaN       NaN  \n",
       "4   [0.8297, 0.3871, 0.9858, 0.5168, 0.1748, 0.944...       NaN       NaN  \n",
       "5   [0.5287, 0.8778, 0.9379, 0.6304, 0.5845, 0.020...       NaN       NaN  \n",
       "6   [0.0066, 0.0036, 0.0345, 0.0032, 0.9415, 0.046...       NaN       NaN  \n",
       "7   [0.0588, 0.4165, 0.1944, 0.2133, 0.826, 0.0269...       NaN       NaN  \n",
       "8   [0.0324, 0.0261, 0.0173, 0.4753, 0.7531, 0.060...       NaN       NaN  \n",
       "9   [0.0283, 0.0236, 0.1929, 0.5004, 0.0016, 0.154...       NaN       NaN  \n",
       "10  [0.2543, 0.0728, 0.9721, 0.8464, 0.0145, 0.137...       NaN       NaN  \n",
       "11  [0.6979, 0.1011, 0.8201, 0.8065, 0.0087, 0.713...       NaN       NaN  \n",
       "12  [0.6315, 0.2183, 0.1302, 0.0577, 0.054, 0.5004...       NaN       NaN  \n",
       "13  [0.9641, 0.9995, 0.6298, 0.3698, 0.3596, 0.003...       NaN       NaN  \n",
       "14  [0.1251, 0.0322, 0.3886, 0.0254, 0.0649, 0.739...       NaN       NaN  \n",
       "15  [0.8372, 0.0132, 0.4033, 0.5785, 0.0625, 0.986...       NaN       NaN  \n",
       "16  [0.5484, 0.541, 0.9305, 0.407, 0.7542, 0.0924,...       NaN       NaN  \n",
       "17  [0.1329, 0.0569, 0.4007, 0.1807, 0.7093, 0.044...       NaN       NaN  \n",
       "18  [0.8582, 0.1763, 0.9137, 0.974, 0.0452, 0.589,...       NaN       NaN  \n",
       "19  [0.8394, 0.0291, 0.8578, 0.9696, 0.0134, 0.901...       NaN       NaN  \n",
       "20  [0.2564, 0.1175, 0.07, 0.0259, 0.6219, 0.5636,...       NaN       NaN  \n",
       "21  [0.3674, 0.7055, 0.2044, 0.036, 0.7331, 0.4466...       NaN       NaN  \n",
       "22  [0.7367, 0.1877, 0.4959, 0.1179, 0.7389, 0.479...       NaN       NaN  \n",
       "23  [0.2059, 0.0283, 0.065, 0.9783, 0.0402, 0.142,...       NaN       NaN  \n",
       "24  [0.3881, 0.1494, 0.3575, 0.8323, 0.6563, 0.117...       NaN       NaN  \n",
       "25  [0.5799, 0.2042, 0.0316, 0.0774, 0.053, 0.1523...       NaN       NaN  \n",
       "26  [0.8806, 0.8855, 0.1385, 0.3595, 0.5217, 0.184...       NaN       NaN  \n",
       "27  [0.1846, 0.4219, 0.4872, 0.0582, 0.0271, 0.035...       NaN       NaN  \n",
       "28  [0.2894, 0.9829, 0.158, 0.0346, 0.0584, 0.0028...       NaN       NaN  \n",
       "29  [0.1566, 0.9207, 0.1019, 0.0227, 0.0016, 0.018...       NaN       NaN  \n",
       "30  [0.7576, 0.9975, 0.4276, 1.0, 0.0, 0.9996, 0.0...       NaN       NaN  \n",
       "31  [0.0237, 0.1477, 0.0256, 0.0561, 0.526, 0.072,...       NaN       NaN  \n",
       "32  [0.0822, 0.0071, 0.0049, 0.0014, 0.751, 0.0608...       NaN       NaN  \n",
       "33  [0.7564, 0.1432, 0.8925, 0.4981, 0.7879, 0.553...       NaN       NaN  \n",
       "34  [0.0515, 0.0083, 0.9028, 0.4912, 0.0227, 0.963...       NaN       NaN  \n",
       "35  [0.6905, 0.3894, 0.2926, 0.1491, 0.7629, 0.239...       NaN       NaN  \n",
       "36  [0.9753, 0.7085, 0.4485, 0.2514, 0.9147, 0.149...       NaN       NaN  \n",
       "37  [0.9954, 0.9791, 0.0485, 0.1451, 0.9852, 0.007...       NaN       NaN  \n",
       "38  [0.3005, 0.9911, 0.9956, 0.1205, 0.9183, 0.012...       NaN       NaN  \n",
       "39  [0.2419, 0.9213, 0.9836, 0.2028, 0.7684, 0.078...       NaN       NaN  \n",
       "40  [0.0798, 0.6889, 0.759, 0.0079, 0.3034, 0.2079...       NaN       NaN  \n",
       "41  [0.8487, 0.9678, 0.9907, 0.6698, 0.8855, 0.043...       NaN       NaN  \n",
       "42  [0.8992, 0.8675, 0.1947, 0.568, 0.9696, 0.4928...       NaN       NaN  \n",
       "43  [0.9093, 0.9925, 0.1579, 0.9061, 0.9939, 0.190...       NaN       NaN  \n",
       "44  [0.9604, 0.9982, 0.0287, 0.875, 0.9958, 0.0209...       NaN       NaN  \n",
       "45  [0.9063, 0.9351, 0.117, 0.5105, 0.9886, 0.3032...       NaN       NaN  \n",
       "46  [0.0984, 0.0963, 0.0128, 0.8651, 0.2092, 0.041...       NaN       NaN  \n",
       "47  [0.0291, 0.0393, 0.0229, 0.3963, 0.3278, 0.581...       NaN       NaN  \n",
       "48  [0.9391, 0.1772, 0.9945, 0.9193, 0.019, 0.8764...       NaN       NaN  \n",
       "49  [0.9723, 0.0072, 0.6418, 0.4541, 0.0664, 0.855...       NaN       NaN  \n",
       "50  [0.3623, 0.1507, 0.023, 0.0546, 0.369, 0.9411,...       NaN       NaN  \n",
       "51  [0.0714, 0.0112, 0.0653, 0.0815, 0.0717, 0.613...       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for num_of_run in range(num_repeat_training):            \n",
    "    if train(num_of_run)==0:\n",
    "        break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
