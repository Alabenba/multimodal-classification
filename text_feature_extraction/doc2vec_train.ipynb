{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sushi': 2, 'steak': 1, 'sashimi': 0, 'tiramisu': 3}\n",
      "number of recipes in train dataset:  36\n"
     ]
    }
   ],
   "source": [
    "number_of_iteration = 3\n",
    "models_folder_name = os.path.join(os.getcwd(),'models','train', str(number_of_iteration))\n",
    "summaries_folder_name = os.path.join(os.getcwd(),'summaries','train',str(number_of_iteration))\n",
    "#path_to_preprocessed_texts = os.path.join(os.getcwd(),\n",
    "#                                          'texts','preprocessed_texts_for_doc2vec.pkl')\n",
    "\n",
    "path_to_preprocessed_texts = os.path.join(models_folder_name, 'recipes_train_dataset.pkl')\n",
    "\n",
    "\n",
    "\n",
    "df_preprocessed_texts = pd.read_pickle(path_to_preprocessed_texts)\n",
    "preprocessed_texts = df_preprocessed_texts.preprocessed_texts.values.tolist()\n",
    "labels = df_preprocessed_texts['labels'].values.tolist()\n",
    "\n",
    "unique_labels=sorted(set(labels))\n",
    "number_categories=len(unique_labels)\n",
    "categories_indices=np.linspace(0,number_categories-1,number_categories,dtype=int)\n",
    "labels2integers=dict(zip(unique_labels,categories_indices))\n",
    "\n",
    "print(labels2integers)\n",
    "print(\"number of recipes in train dataset: \", df_preprocessed_texts.labels.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "generations = 150000\n",
    "model_learning_rate = 0.0005\n",
    "\n",
    "embedding_size = 24   #word embedding size\n",
    "doc_embedding_size = 12  #document embedding size\n",
    "concatenated_size = embedding_size + doc_embedding_size\n",
    "\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(preprocessed_texts):\n",
    "    words=[w for words_in_recipe in preprocessed_texts for w in words_in_recipe]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words))\n",
    "    count=sorted(count)\n",
    "    word_dict = {}\n",
    "    for word in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return (word_dict)\n",
    "\n",
    "#replace each word in texts with integer value\n",
    "def text_to_numbers(preprocessed_texts, word_dict):\n",
    "    data = []\n",
    "    for prepr_text in preprocessed_texts:\n",
    "        text_data = []\n",
    "        for word in prepr_text:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            text_data.append(word_ix)\n",
    "        data.append(text_data)\n",
    "    return (data)\n",
    "\n",
    "\n",
    "def create_batch_data(text_with_words_conv_to_numbers, batch_size=batch_size):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    rand_text_ix = int(np.random.choice(len(text_with_words_conv_to_numbers), size=1))\n",
    "    rand_text = text_with_words_conv_to_numbers[rand_text_ix]\n",
    "    word_to_predict_label=np.random.choice(list(set(rand_text)), size=1,replace=False)\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        item_in_batch=[]        \n",
    "        \n",
    "        label_words=np.random.choice(rand_text, size=1,replace=False)\n",
    "\n",
    "        item_in_batch.extend(word_to_predict_label)\n",
    "        item_in_batch.append(rand_text_ix)     \n",
    "        label_data.extend(label_words)\n",
    "        batch_data.append(item_in_batch)\n",
    "\n",
    "        \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array(label_data))\n",
    "\n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oil': 71, 'ginger': 45, 'guacamole': 50, 'torch': 124, 'sheet': 97, 'pinch': 78, 'mat': 60, 'finger': 40, 'space': 103, 'tuna': 127, 'seed': 95, 'carrot': 17, 'cucumber': 29, 'sugar': 114, 'skewer': 100, 'steak': 110, 'cling': 21, 'filling': 39, 'egg': 36, 'pepper': 76, 'ladyfinger': 52, 'beat': 8, 'ball': 5, 'juice': 51, 'wasabi': 132, 'powder': 81, 'tongs': 122, 'noodle': 69, 'vegetable': 129, 'grill': 48, 'mixture': 67, 'white': 136, 'bit': 10, 'cream': 28, 'drain': 35, 'sashimi': 91, 'dipping': 34, 'filet': 38, 'thumb': 119, 'starch': 109, 'brush': 14, 'coffee': 23, 'asparagus': 2, 'yolk': 139, 'whip': 134, 'cake': 16, 'soy': 102, 'strip': 113, 'metal': 65, 'fryer': 43, 'wine': 137, 'leg': 55, 'flesh': 42, 'cut': 31, 'angle': 1, 'garnish': 44, 'whisk': 135, 'matchstick': 61, 'lime': 56, 'cheese': 18, 'batter': 7, 'vinegar': 130, 'bag': 4, 'worcestershire': 138, 'confectioner': 25, 'mascarpone': 59, 'roll': 88, 'nori': 70, 'mixer': 66, 'salt': 90, 'bottom': 13, 'chive': 19, 'wafer': 131, 'layer': 53, 'rice': 86, 'daikon': 33, 'leaf': 54, 'saucepan': 93, 'sushi': 116, 'tomato': 121, 'sieve': 98, 'quantity': 85, 'air': 0, 'cocoa': 22, 'avocado': 3, 'sauce': 92, 'preheat': 82, 'towel': 125, 'spring': 107, 'cone': 24, 'piece': 77, 'spicy': 105, 'grate': 47, 'sesame': 96, 'bamboo': 6, 'marinade': 58, 'medium': 64, 'liqueur': 57, 'sirloin': 99, 'thickness': 118, 'fish': 41, 'tempura': 117, 'cutting': 32, 'curl': 30, 'strawberry': 112, 'press': 83, 'surface': 115, 'speed': 104, 'stick': 111, 'slice': 101, 'part': 74, 'roe': 87, 'crab': 27, 'sea': 94, 'paper': 73, 'vanilla': 128, 'pressure': 84, 'ground': 49, 'salmon': 89, 'pour': 80, 'topping': 123, 'sprinkle': 108, 'peak': 75, 'block': 12, 'coriander': 26, 'tobikko': 120, 'mayonnaise': 62, 'grain': 46, 'zip': 140, 'espresso': 37, 'butter': 15, 'position': 79, 'truffle': 126, 'chocolate': 20, 'mushroom': 68, 'water': 133, 'berry': 9, 'meat': 63, 'blade': 11, 'spread': 106, 'onion': 72}\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "word_dictionary=build_dictionary(preprocessed_texts)\n",
    "vocabulary_size=len(word_dictionary)\n",
    "print(word_dictionary)\n",
    "print(vocabulary_size)\n",
    "\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89, 3, 72, 132, 95, 86, 69, 77, 89, 90, 89, 133, 41, 50, 72, 121, 90, 90, 90, 90, 90, 90, 90, 90, 31, 3, 3, 3, 3, 77, 72, 67, 3, 67, 50, 88, 56, 51, 56, 56, 51, 56, 51, 3, 51, 56, 50, 67, 132, 132, 10, 132, 50, 10, 10, 96, 95, 102, 92, 121, 121, 121, 3, 26, 54, 54, 67, 26, 50, 21, 89, 90, 89, 89, 41, 91, 31, 101, 21, 10, 109, 21, 101, 89, 109, 21, 109, 89, 89, 109, 89, 89, 89, 43, 101, 43, 89, 71, 89, 89, 111, 43, 89, 89, 89, 89, 89, 89, 43, 71, 89, 71, 89, 44, 89, 86, 69, 69, 89, 50, 26, 86, 69, 50], [127, 132, 102, 92, 45, 46, 79, 40, 46, 101], [91, 89, 38, 96, 95, 71, 96, 95, 92, 45, 38, 96, 95, 71, 115, 96, 95, 71, 40, 89, 38, 96, 95, 115, 40, 38, 38, 41, 96, 95, 71, 111, 38, 38, 41, 79, 38, 40, 38, 38, 31, 38, 92, 115, 11, 89, 101, 40, 79, 89, 92, 45, 89, 91, 101, 92], [116, 38, 12, 116, 38, 12, 26, 54, 96, 95, 71, 127, 127, 38, 101, 101, 38, 31, 127, 101, 12, 12, 127, 95, 71, 40, 71, 127, 26, 38, 115, 38, 12, 11, 41, 127, 12, 41, 41, 79, 38, 12, 1, 31, 1, 11, 101, 41, 101, 127, 12, 11, 31, 101, 44, 44, 40, 17, 5, 79, 5, 101, 116, 17, 5, 11, 79, 101, 77, 101, 101, 89, 40, 40, 116, 5, 79, 5, 29, 17, 5, 127, 101, 116, 101, 1, 79, 1, 101, 44, 101, 40, 101, 127, 101, 101, 44, 101, 54, 29, 5, 127, 101, 29, 44, 3, 101, 3, 40, 3, 24, 79, 101, 116, 24, 5, 45, 127, 101, 113, 91, 101, 102, 92, 132], [26, 54, 96, 95, 71, 116, 97, 26, 26, 127, 71, 26, 77, 116, 127, 71, 127, 26, 127, 113, 26, 113, 127, 127, 96, 95, 71, 113, 26, 127, 41, 127, 127, 91, 41, 127, 97, 97, 26, 127, 97, 70, 88, 127, 70, 88, 88, 88, 88, 77, 31, 54, 17, 127, 102, 92, 34], [116, 38, 46, 133, 90, 114, 71, 38, 116, 38, 46, 89, 38, 133, 90, 114, 38, 41, 89, 38, 89, 38, 4, 71, 4, 4, 0, 71, 4, 133, 41, 4, 4, 115, 89, 4, 41, 113, 89, 79, 113, 38, 113, 91, 91, 3, 102, 0, 102, 92, 0, 102, 92, 0, 79, 101, 3, 102, 92, 40, 89, 101, 3, 89], [127, 3, 31, 101, 71, 56, 51, 56, 72, 102, 92, 127, 101, 101, 3, 71, 56, 51, 3, 127, 72, 90, 102, 92, 34, 56], [127, 77, 77, 45, 102, 92, 92, 34, 92, 102, 92, 102, 92, 116, 102, 92, 92, 33, 97, 97, 101, 111, 133, 33, 45, 24, 24, 45, 127, 127, 12, 101, 46, 46, 12, 127, 113, 127, 12, 127, 41, 33, 54, 54, 72, 29, 45, 102, 92, 41, 127, 34, 92, 33, 102, 92, 91], [130, 102, 92, 71, 49, 76, 138, 92, 72, 81, 90, 76, 110, 64, 130, 102, 92, 71, 49, 76, 138, 92, 72, 81, 90, 76, 110, 58, 63, 82, 48, 64, 71, 48, 47, 58], [110, 90, 49, 76, 71, 110, 90, 76, 48, 82, 48, 110, 48, 110, 71, 48, 110, 110], [45, 72, 102, 92, 71, 63, 114, 110, 45, 72, 92, 71, 138, 92, 63, 114, 58, 4, 110, 58, 48, 64, 48, 110, 48, 64], [15, 76, 82, 48, 15, 81, 110, 90, 76, 110, 15], [102, 92, 130, 49, 45, 81, 71, 102, 92, 130, 45, 81, 71, 110, 110, 80, 58, 110, 82, 48, 47, 71, 110, 48, 58, 110], [71, 138, 92, 64, 72, 76, 92, 71, 138, 92, 102, 92, 72, 90, 76, 110, 92, 110, 80, 58, 110, 48, 64, 110, 58, 58, 71, 110, 48, 110], [110, 114, 76, 81, 82, 48, 110, 110, 92, 114, 80, 92, 110, 90, 76, 81, 110, 90, 76, 81, 110, 58, 80, 58, 71, 48, 47, 110, 110, 58], [130, 76, 90, 71, 110, 15, 19, 76, 130, 76, 90, 71, 110, 4, 58, 4, 48, 64, 71, 47, 110, 58, 58, 110, 48, 15, 19, 76, 110, 48, 64, 110, 15], [116, 86, 130, 114, 90, 86, 133, 97, 39, 29, 29, 61, 61, 76, 61, 72, 61, 92, 77, 33, 61, 33, 3, 51, 6, 116, 60, 86, 130, 114, 90, 86, 98, 35, 86, 133, 64, 93, 125, 93, 125, 133, 86, 86, 67, 86, 73, 86, 86, 125, 88, 116, 60, 103, 97, 60, 40, 133, 83, 86, 70, 39, 86, 86, 60, 70, 86, 39, 60, 88, 60, 88, 60, 70, 10, 133, 116, 116, 88, 86, 39, 116, 88, 77, 102, 92], [102, 92, 86, 137, 130, 45, 95, 95, 71, 62, 126, 71, 92, 102, 92, 86, 130, 45, 67, 51, 92, 116, 88, 96, 95, 67, 96, 95, 71, 116, 88, 92, 123, 62, 71, 116, 88, 110, 99, 110, 40, 110, 40, 110, 118, 31, 53, 110, 118, 96, 95, 71, 99, 94, 90, 108, 115, 110, 65, 122, 38, 129, 96, 95, 71, 64, 2, 68, 2, 68, 94, 90, 122, 68, 71, 116, 88, 71, 116, 88, 97, 40, 116, 86, 97, 86, 97, 6, 60, 140, 4, 86, 60, 97, 40, 86, 60, 79, 97, 65, 122, 53, 68, 2, 97, 88, 60, 40, 40, 84, 6, 60, 97, 119, 40, 88, 84, 99, 99, 79, 42, 101, 118, 110, 77, 101, 116, 88, 116, 88, 40, 101, 99, 79, 88, 116, 88, 88, 40, 60, 116, 88, 40, 133, 11, 31, 116, 88, 31, 77, 116, 21, 77, 116, 88, 79, 77, 116, 1, 85, 79, 5, 45, 5, 132, 92, 116, 88, 101, 124, 92, 124, 115, 115, 101, 78, 107, 72, 126, 71, 62, 78, 96, 95, 101], [116, 86, 97, 29, 101, 3, 126, 39, 97, 116, 86, 70, 40, 86, 86, 97, 70, 115, 125, 86, 97, 38, 97, 29, 29, 97, 88, 116, 88, 6, 60, 119, 97, 88, 97, 39, 88, 6, 60, 140, 4, 88, 60, 88, 83, 40, 88, 88, 88, 101, 3, 116, 88, 103, 101, 88, 21, 40, 21, 133, 88, 88, 88, 31, 6, 60, 21, 123, 15, 101, 116, 101, 124, 126, 124, 115, 101, 101, 87, 101, 101, 74, 71, 74, 71, 10, 67, 116, 101, 79, 101, 116, 1, 103, 101, 129, 29, 17, 101, 77, 132], [27, 111, 86, 97, 101, 101, 96, 95, 27, 27, 111, 111, 88, 62, 27, 111, 62, 86, 97, 97, 116, 86, 97, 40, 86, 97, 108, 95, 86, 86, 86, 6, 60, 140, 4, 97, 60, 27, 67, 97, 101, 29, 101, 103, 97, 88, 60, 39, 39, 119, 88, 84, 60, 88, 60, 10, 88, 88, 60, 116, 88, 88, 21, 88, 88, 21, 60, 21, 88, 21, 88, 86, 88, 116, 77, 116, 88, 86, 11, 88, 88, 101, 116, 88, 11, 84, 116, 88, 88, 116, 77, 45, 132, 102, 92], [116, 86, 97, 2, 111, 87, 117, 7, 116, 97, 70, 4, 60, 116, 86, 97, 40, 86, 97, 111, 2, 97, 53, 2, 41, 116, 88, 2, 113, 91, 89, 116, 88, 97, 6, 60, 40, 97, 111, 88, 116, 116, 88, 116, 88, 117, 7, 7, 116, 88, 7, 116, 88, 86, 117, 7, 88, 88, 7, 88, 43, 89, 122, 73, 71, 116, 88, 116, 88, 133, 11, 86, 7, 31, 88, 77, 116, 116, 88, 116, 88, 116, 88, 132, 45, 92], [64, 86, 133, 77, 86, 130, 114, 90, 62, 76, 71, 90, 76, 97, 116, 86, 96, 95, 127, 107, 72, 86, 133, 133, 35, 86, 64, 93, 133, 67, 133, 86, 116, 86, 130, 114, 90, 93, 114, 67, 86, 86, 67, 86, 125, 135, 62, 71, 90, 76, 116, 60, 115, 97, 70, 60, 97, 60, 40, 86, 70, 53, 86, 96, 95, 70, 86, 70, 53, 29, 105, 62, 67, 29, 127, 108, 107, 72, 88, 116, 60, 77, 88], [86, 86, 130, 114, 90, 86, 133, 97, 39, 29, 29, 61, 3, 51, 102, 92, 6, 116, 60, 88, 86, 130, 114, 90, 86, 133, 35, 86, 133, 64, 93, 86, 125, 93, 125, 86, 133, 86, 86, 67, 86, 73, 97, 86, 86, 125, 88, 116, 60, 103, 97, 60, 40, 133, 83, 86, 70, 39, 86, 86, 60, 70, 86, 39, 60, 88, 60, 88, 60, 70, 10, 133, 116, 116, 73, 125, 86, 39, 116, 88, 77, 102, 92], [86, 116, 86, 86, 137, 130, 114, 62, 62, 86, 137, 130, 102, 92, 116, 4, 97, 39, 113, 89, 127, 76, 3, 107, 72, 116, 132, 45, 102, 92, 88, 86, 97, 60, 133, 86, 53, 62, 53, 62, 86, 39, 62, 39, 127, 29, 88, 60, 86, 84, 88, 86, 14, 133, 88, 60, 88, 116, 101, 21, 116, 53, 89, 21, 53, 21, 86, 83, 83, 86, 41, 21, 116, 40, 21, 5, 123, 123, 77, 89, 5, 86, 123, 5, 21, 5], [97, 86, 116, 89, 41, 113, 92, 116, 53, 116, 86, 86, 86, 86, 116, 77, 89, 77, 41, 88, 41, 41, 116, 89, 116, 88, 38, 77, 113, 86, 3, 88, 88, 6, 60, 73, 116, 88, 77, 116, 88, 102, 92, 45], [86, 91, 89, 3, 17, 2, 33, 87, 31, 77, 33, 97, 31, 31, 125, 65, 100, 125, 100, 129, 100, 129, 32, 97, 31, 2, 133, 94, 90, 2, 133, 17, 64, 17, 31, 77, 77, 113, 115, 101, 101, 101, 113, 31, 101, 101, 113, 3, 3, 42, 3, 77, 91, 89, 31, 77, 89, 77, 31, 88, 77, 70, 60, 133, 130, 86, 70, 10, 132, 2, 10, 101, 17, 87, 88, 88, 10, 74, 86, 97, 88, 97, 97, 77, 88, 77, 88, 74, 77], [3, 70, 86, 86, 120, 87, 45, 132, 102, 92, 3, 95, 95, 95, 74, 3, 95, 42, 3, 42, 3, 101, 101, 95, 95, 74, 3, 95, 95, 3, 95, 3, 77, 91, 89, 101, 113, 89, 88, 88, 77, 88, 6, 60, 97, 70, 60, 86, 70, 97, 10, 132, 97, 40, 40, 97, 113, 97, 101, 3, 89, 88, 88, 116, 88, 120, 88, 120, 53, 115, 88, 120, 88, 120, 132, 88, 88, 86, 11, 31, 88, 116, 116, 32, 6, 60, 101, 88, 116, 10, 45, 132, 102, 92], [27, 15, 45, 3, 116, 86, 97, 27, 27, 55, 40, 27, 63, 55, 63, 27, 55, 77, 27, 63, 55, 94, 90, 55, 90, 76, 15, 15, 27, 63, 122, 122, 27, 45, 98, 67, 3, 95, 95, 3, 101, 31, 11, 51, 101, 116, 88, 97, 116, 86, 97, 40, 86, 6, 60, 140, 4, 97, 60, 86, 45, 113, 97, 27, 55, 45, 119, 6, 60, 97, 84, 40, 116, 88, 60, 79, 31, 101, 116, 88, 79, 40, 3, 101, 116, 88, 11, 40, 88, 32, 116, 88, 60, 21, 116, 88, 60, 40, 3, 88, 60, 3, 88, 84, 3, 88, 60, 21, 116, 88, 77, 88, 77, 60, 101, 21, 101, 1, 85, 105, 62, 101, 87], [36, 139, 114, 59, 36, 136, 28, 23, 52, 22, 81, 64, 8, 36, 139, 114, 59, 18, 36, 136, 28, 114, 23, 52, 23, 67, 52, 53, 53, 67, 52, 53, 67, 108, 22], [112, 25, 114, 59, 28, 57, 52, 81, 112, 9, 9, 112, 25, 64, 18, 28, 114, 57, 8, 66, 64, 104, 52, 37, 23, 106, 67, 52, 53, 112, 52, 37, 67, 112, 52, 28, 114, 8, 66, 64, 104, 28, 53, 52, 22, 28, 112, 80, 112], [36, 139, 114, 28, 128, 59, 23, 22, 81, 64, 36, 139, 114, 64, 67, 64, 28, 128, 75, 59, 67, 23, 52, 23, 67, 52, 13, 59, 67, 52, 28, 53, 108, 22], [36, 139, 114, 28, 52, 23, 22, 81, 20, 36, 139, 114, 133, 134, 139, 59, 139, 8, 134, 28, 75, 67, 13, 23, 57, 28, 52, 23, 57, 53, 22, 20, 30, 20, 30, 20], [23, 57, 28, 18, 28, 52, 22, 81, 64, 23, 28, 18, 23, 67, 64, 28, 128, 75, 28, 28, 18, 52, 13, 23, 67, 28, 67, 53, 108, 22], [112, 25, 114, 59, 28, 57, 52, 81, 112, 9, 9, 112, 25, 64, 18, 28, 114, 57, 8, 66, 64, 104, 52, 37, 23, 106, 67, 52, 53, 112, 52, 37, 67, 112, 52, 28, 114, 8, 66, 64, 104, 28, 53, 52, 22, 28, 112, 80, 112], [16, 16, 23, 81, 23, 23, 25, 114, 23, 28, 25, 114, 23, 22, 81, 20, 16, 7, 23, 7, 16, 23, 23, 57, 66, 104, 59, 25, 114, 57, 8, 64, 66, 64, 104, 28, 25, 114, 57, 28, 67, 67, 16, 16, 53, 16, 23, 67, 16, 67, 23, 16, 53, 16, 23, 67, 53, 106, 16, 53, 16, 80, 23, 67, 106, 16, 22, 16, 30, 20, 30, 20], [128, 131, 23, 133, 28, 18, 114, 28, 36, 134, 22, 81, 106, 131, 13, 23, 133, 131, 23, 8, 28, 18, 114, 66, 28, 36, 104, 7, 64, 23, 133, 7, 131, 131, 23, 7, 134, 22, 81]]\n"
     ]
    }
   ],
   "source": [
    "text_data = text_to_numbers(preprocessed_texts, word_dictionary)\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127, 86, 116, 88, 91, 110, 48, 92, 28]\n"
     ]
    }
   ],
   "source": [
    "valid_words = ['tuna', 'rice', 'sushi', 'roll', 'sashimi','steak','grill', 'sauce', 'cream']\n",
    "\n",
    "valid_examples = [word_dictionary[x] for x in valid_words if x in word_dictionary.keys()]\n",
    "print(valid_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['steak' '15']\n",
      " ['steak' '15']\n",
      " ['steak' '15']\n",
      " ['steak' '15']\n",
      " ['steak' '15']\n",
      " ['steak' '15']\n",
      " ['steak' '15']\n",
      " ['steak' '15']]\n",
      "['salt' 'chive' 'steak' 'vinegar' 'pepper' 'marinade' 'chive' 'steak']\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "batch_data, label_data = create_batch_data(preprocessed_texts)\n",
    "print(batch_data)\n",
    "print(label_data)\n",
    "print(np.shape(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "WARNING:tensorflow:From <ipython-input-10-20c022ece0b5>:30: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Starting Training\n",
      "Loss at step 50 : 5.089521408081055\n",
      "Loss at step 100 : 5.371132850646973\n",
      "Loss at step 150 : 4.668432712554932\n",
      "Loss at step 200 : 5.138874053955078\n",
      "Loss at step 250 : 4.8191680908203125\n",
      "Loss at step 300 : 4.847530364990234\n",
      "Loss at step 350 : 4.714554786682129\n",
      "Loss at step 400 : 4.872944355010986\n",
      "Loss at step 450 : 5.225693702697754\n",
      "Loss at step 500 : 5.243710994720459\n",
      "Loss at step 550 : 4.455766677856445\n",
      "Loss at step 600 : 4.982711315155029\n",
      "Loss at step 650 : 4.7843546867370605\n",
      "Loss at step 700 : 5.047980785369873\n",
      "Loss at step 750 : 5.030146598815918\n",
      "Loss at step 800 : 5.238272666931152\n",
      "Loss at step 850 : 4.353737831115723\n",
      "Loss at step 900 : 4.381399154663086\n",
      "Loss at step 950 : 5.138124942779541\n",
      "Loss at step 1000 : 4.170712947845459\n",
      "Loss at step 1050 : 4.915219306945801\n",
      "Loss at step 1100 : 5.098161220550537\n",
      "Loss at step 1150 : 4.608416557312012\n",
      "Loss at step 1200 : 4.455842018127441\n",
      "Loss at step 1250 : 4.334194660186768\n",
      "Loss at step 1300 : 4.282168388366699\n",
      "Loss at step 1350 : 4.389308929443359\n",
      "Loss at step 1400 : 4.9753031730651855\n",
      "Loss at step 1450 : 4.095946788787842\n",
      "Loss at step 1500 : 4.74090576171875\n",
      "Loss at step 1550 : 4.706466197967529\n",
      "Loss at step 1600 : 4.617560863494873\n",
      "Loss at step 1650 : 4.175515174865723\n",
      "Loss at step 1700 : 4.528301239013672\n",
      "Loss at step 1750 : 4.32321834564209\n",
      "Loss at step 1800 : 4.529271125793457\n",
      "Loss at step 1850 : 4.429624557495117\n",
      "Loss at step 1900 : 4.49495792388916\n",
      "Loss at step 1950 : 4.072480201721191\n",
      "Loss at step 2000 : 4.554821014404297\n",
      "Loss at step 2050 : 4.870267391204834\n",
      "Loss at step 2100 : 4.413735866546631\n",
      "Loss at step 2150 : 4.16828727722168\n",
      "Loss at step 2200 : 4.239470481872559\n",
      "Loss at step 2250 : 4.1563639640808105\n",
      "Loss at step 2300 : 4.730624198913574\n",
      "Loss at step 2350 : 4.354424476623535\n",
      "Loss at step 2400 : 4.188846111297607\n",
      "Loss at step 2450 : 4.731749534606934\n",
      "Loss at step 2500 : 4.277928352355957\n",
      "Loss at step 2550 : 4.303132057189941\n",
      "Loss at step 2600 : 4.3076324462890625\n",
      "Loss at step 2650 : 4.332749366760254\n",
      "Loss at step 2700 : 4.0099992752075195\n",
      "Loss at step 2750 : 4.2879791259765625\n",
      "Loss at step 2800 : 3.619506597518921\n",
      "Loss at step 2850 : 4.423990249633789\n",
      "Loss at step 2900 : 4.276556015014648\n",
      "Loss at step 2950 : 4.021883010864258\n",
      "Loss at step 3000 : 4.185629367828369\n",
      "Loss at step 3050 : 4.0472612380981445\n",
      "Loss at step 3100 : 4.84587287902832\n",
      "Loss at step 3150 : 3.4240927696228027\n",
      "Loss at step 3200 : 4.344616889953613\n",
      "Loss at step 3250 : 2.9545764923095703\n",
      "Loss at step 3300 : 4.768415451049805\n",
      "Loss at step 3350 : 3.6620006561279297\n",
      "Loss at step 3400 : 4.937399864196777\n",
      "Loss at step 3450 : 4.071649551391602\n",
      "Loss at step 3500 : 2.9340004920959473\n",
      "Loss at step 3550 : 3.5968666076660156\n",
      "Loss at step 3600 : 3.7609355449676514\n",
      "Loss at step 3650 : 2.5969958305358887\n",
      "Loss at step 3700 : 3.858938455581665\n",
      "Loss at step 3750 : 3.4702823162078857\n",
      "Loss at step 3800 : 3.737107276916504\n",
      "Loss at step 3850 : 4.148127555847168\n",
      "Loss at step 3900 : 3.5406675338745117\n",
      "Loss at step 3950 : 4.564642906188965\n",
      "Loss at step 4000 : 3.6589274406433105\n",
      "Loss at step 4050 : 2.831080436706543\n",
      "Loss at step 4100 : 3.8285038471221924\n",
      "Loss at step 4150 : 3.941816806793213\n",
      "Loss at step 4200 : 2.8513174057006836\n",
      "Loss at step 4250 : 4.109238147735596\n",
      "Loss at step 4300 : 4.016323089599609\n",
      "Loss at step 4350 : 3.144024133682251\n",
      "Loss at step 4400 : 3.7324905395507812\n",
      "Loss at step 4450 : 3.488922357559204\n",
      "Loss at step 4500 : 3.3802337646484375\n",
      "Loss at step 4550 : 3.7166502475738525\n",
      "Loss at step 4600 : 3.957336902618408\n",
      "Loss at step 4650 : 3.5091962814331055\n",
      "Loss at step 4700 : 4.109294891357422\n",
      "Loss at step 4750 : 3.7476911544799805\n",
      "Loss at step 4800 : 4.074654579162598\n",
      "Loss at step 4850 : 1.7364193201065063\n",
      "Loss at step 4900 : 4.595829010009766\n",
      "Loss at step 4950 : 4.5645751953125\n",
      "Loss at step 5000 : 3.1750097274780273\n",
      "Nearest to tuna: position, flesh, daikon, sashimi, ground,\n",
      "Nearest to rice: slice, topping, block, surface, ball,\n",
      "Nearest to sushi: mat, wasabi, finger, daikon, coriander,\n",
      "Nearest to roll: mascarpone, torch, fryer, juice, bag,\n",
      "Nearest to sashimi: ground, roe, sieve, tongs, tuna,\n",
      "Nearest to steak: grate, roe, grill, spring, sashimi,\n",
      "Nearest to grill: ground, steak, vinegar, marinade, meat,\n",
      "Nearest to sauce: cake, vinegar, starch, soy, towel,\n",
      "Nearest to cream: liqueur, sugar, espresso, powder, thumb,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 5050 : 3.670894145965576\n",
      "Loss at step 5100 : 3.218618631362915\n",
      "Loss at step 5150 : 3.580319881439209\n",
      "Loss at step 5200 : 3.789048194885254\n",
      "Loss at step 5250 : 2.688444137573242\n",
      "Loss at step 5300 : 3.5637259483337402\n",
      "Loss at step 5350 : 3.832700729370117\n",
      "Loss at step 5400 : 3.6311163902282715\n",
      "Loss at step 5450 : 2.633533000946045\n",
      "Loss at step 5500 : 3.3532285690307617\n",
      "Loss at step 5550 : 2.8678460121154785\n",
      "Loss at step 5600 : 3.4933698177337646\n",
      "Loss at step 5650 : 4.085676193237305\n",
      "Loss at step 5700 : 3.3267836570739746\n",
      "Loss at step 5750 : 3.5193238258361816\n",
      "Loss at step 5800 : 3.128943920135498\n",
      "Loss at step 5850 : 2.397340774536133\n",
      "Loss at step 5900 : 2.921663522720337\n",
      "Loss at step 5950 : 4.22881555557251\n",
      "Loss at step 6000 : 4.066383361816406\n",
      "Loss at step 6050 : 3.0547924041748047\n",
      "Loss at step 6100 : 3.9717442989349365\n",
      "Loss at step 6150 : 3.2268476486206055\n",
      "Loss at step 6200 : 2.3851354122161865\n",
      "Loss at step 6250 : 3.398791551589966\n",
      "Loss at step 6300 : 4.084776401519775\n",
      "Loss at step 6350 : 2.766592025756836\n",
      "Loss at step 6400 : 2.7767932415008545\n",
      "Loss at step 6450 : 3.7544493675231934\n",
      "Loss at step 6500 : 3.169877767562866\n",
      "Loss at step 6550 : 3.7129368782043457\n",
      "Loss at step 6600 : 3.823115825653076\n",
      "Loss at step 6650 : 3.4992830753326416\n",
      "Loss at step 6700 : 2.2930145263671875\n",
      "Loss at step 6750 : 4.014029026031494\n",
      "Loss at step 6800 : 4.400294303894043\n",
      "Loss at step 6850 : 3.9230334758758545\n",
      "Loss at step 6900 : 3.7140860557556152\n",
      "Loss at step 6950 : 2.9871344566345215\n",
      "Loss at step 7000 : 3.5430703163146973\n",
      "Loss at step 7050 : 3.442788600921631\n",
      "Loss at step 7100 : 4.364470481872559\n",
      "Loss at step 7150 : 2.6081080436706543\n",
      "Loss at step 7200 : 2.782827615737915\n",
      "Loss at step 7250 : 3.4636149406433105\n",
      "Loss at step 7300 : 1.5611939430236816\n",
      "Loss at step 7350 : 4.137262344360352\n",
      "Loss at step 7400 : 2.7889628410339355\n",
      "Loss at step 7450 : 3.5416884422302246\n",
      "Loss at step 7500 : 2.454979658126831\n",
      "Loss at step 7550 : 3.1049022674560547\n",
      "Loss at step 7600 : 3.2802271842956543\n",
      "Loss at step 7650 : 2.6384999752044678\n",
      "Loss at step 7700 : 2.892082452774048\n",
      "Loss at step 7750 : 3.271958589553833\n",
      "Loss at step 7800 : 4.125491142272949\n",
      "Loss at step 7850 : 4.3012285232543945\n",
      "Loss at step 7900 : 3.4372427463531494\n",
      "Loss at step 7950 : 4.050575256347656\n",
      "Loss at step 8000 : 2.6851558685302734\n",
      "Loss at step 8050 : 3.5078978538513184\n",
      "Loss at step 8100 : 2.946605920791626\n",
      "Loss at step 8150 : 3.6931185722351074\n",
      "Loss at step 8200 : 3.2853429317474365\n",
      "Loss at step 8250 : 3.4043679237365723\n",
      "Loss at step 8300 : 3.0439555644989014\n",
      "Loss at step 8350 : 4.656418323516846\n",
      "Loss at step 8400 : 3.134720802307129\n",
      "Loss at step 8450 : 2.448335647583008\n",
      "Loss at step 8500 : 4.496763229370117\n",
      "Loss at step 8550 : 2.838845729827881\n",
      "Loss at step 8600 : 3.5353031158447266\n",
      "Loss at step 8650 : 3.216043472290039\n",
      "Loss at step 8700 : 3.3333373069763184\n",
      "Loss at step 8750 : 2.857388496398926\n",
      "Loss at step 8800 : 3.4776699542999268\n",
      "Loss at step 8850 : 3.231874942779541\n",
      "Loss at step 8900 : 3.0684659481048584\n",
      "Loss at step 8950 : 3.4845900535583496\n",
      "Loss at step 9000 : 2.4559717178344727\n",
      "Loss at step 9050 : 2.5059332847595215\n",
      "Loss at step 9100 : 2.68658447265625\n",
      "Loss at step 9150 : 3.3524153232574463\n",
      "Loss at step 9200 : 3.599893093109131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 9250 : 3.649232864379883\n",
      "Loss at step 9300 : 2.5338401794433594\n",
      "Loss at step 9350 : 3.349475145339966\n",
      "Loss at step 9400 : 1.8582525253295898\n",
      "Loss at step 9450 : 2.799396276473999\n",
      "Loss at step 9500 : 3.9648029804229736\n",
      "Loss at step 9550 : 3.6196727752685547\n",
      "Loss at step 9600 : 2.914489507675171\n",
      "Loss at step 9650 : 4.447295188903809\n",
      "Loss at step 9700 : 3.180666208267212\n",
      "Loss at step 9750 : 3.7228918075561523\n",
      "Loss at step 9800 : 2.494056224822998\n",
      "Loss at step 9850 : 4.779819488525391\n",
      "Loss at step 9900 : 4.450664520263672\n",
      "Loss at step 9950 : 3.0552005767822266\n",
      "Loss at step 10000 : 3.1622233390808105\n",
      "Nearest to tuna: position, flesh, daikon, sashimi, grate,\n",
      "Nearest to rice: topping, block, slice, surface, salmon,\n",
      "Nearest to sushi: mat, wasabi, finger, bamboo, coriander,\n",
      "Nearest to roll: juice, torch, water, bag, fryer,\n",
      "Nearest to sashimi: tuna, position, sieve, ground, roe,\n",
      "Nearest to steak: grate, grill, roe, ground, sashimi,\n",
      "Nearest to grill: steak, ground, marinade, vinegar, grate,\n",
      "Nearest to sauce: soy, cake, vinegar, starch, slice,\n",
      "Nearest to cream: liqueur, powder, espresso, cocoa, sugar,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 10050 : 3.1146507263183594\n",
      "Loss at step 10100 : 3.8849310874938965\n",
      "Loss at step 10150 : 2.9592909812927246\n",
      "Loss at step 10200 : 3.2447586059570312\n",
      "Loss at step 10250 : 2.9522738456726074\n",
      "Loss at step 10300 : 3.3310203552246094\n",
      "Loss at step 10350 : 2.7773101329803467\n",
      "Loss at step 10400 : 3.14005970954895\n",
      "Loss at step 10450 : 3.424011707305908\n",
      "Loss at step 10500 : 3.053037166595459\n",
      "Loss at step 10550 : 2.6252243518829346\n",
      "Loss at step 10600 : 1.9810644388198853\n",
      "Loss at step 10650 : 2.979033946990967\n",
      "Loss at step 10700 : 3.2110671997070312\n",
      "Loss at step 10750 : 3.604473829269409\n",
      "Loss at step 10800 : 3.5006089210510254\n",
      "Loss at step 10850 : 3.816905975341797\n",
      "Loss at step 10900 : 3.761807441711426\n",
      "Loss at step 10950 : 3.115330934524536\n",
      "Loss at step 11000 : 3.9222774505615234\n",
      "Loss at step 11050 : 1.938028335571289\n",
      "Loss at step 11100 : 3.4423274993896484\n",
      "Loss at step 11150 : 2.3058395385742188\n",
      "Loss at step 11200 : 3.459630012512207\n",
      "Loss at step 11250 : 3.1029751300811768\n",
      "Loss at step 11300 : 3.3960955142974854\n",
      "Loss at step 11350 : 2.761229991912842\n",
      "Loss at step 11400 : 2.9504637718200684\n",
      "Loss at step 11450 : 3.165909767150879\n",
      "Loss at step 11500 : 3.394237995147705\n",
      "Loss at step 11550 : 4.12334680557251\n",
      "Loss at step 11600 : 2.3454504013061523\n",
      "Loss at step 11650 : 2.9368388652801514\n",
      "Loss at step 11700 : 3.9727392196655273\n",
      "Loss at step 11750 : 3.380549430847168\n",
      "Loss at step 11800 : 3.300288677215576\n",
      "Loss at step 11850 : 3.145933151245117\n",
      "Loss at step 11900 : 3.0846681594848633\n",
      "Loss at step 11950 : 3.5228824615478516\n",
      "Loss at step 12000 : 2.3917582035064697\n",
      "Loss at step 12050 : 3.834597587585449\n",
      "Loss at step 12100 : 3.3996927738189697\n",
      "Loss at step 12150 : 2.9558229446411133\n",
      "Loss at step 12200 : 2.5262017250061035\n",
      "Loss at step 12250 : 2.7917261123657227\n",
      "Loss at step 12300 : 3.474980592727661\n",
      "Loss at step 12350 : 3.483022689819336\n",
      "Loss at step 12400 : 2.0521399974823\n",
      "Loss at step 12450 : 2.408820629119873\n",
      "Loss at step 12500 : 2.327364683151245\n",
      "Loss at step 12550 : 4.280242443084717\n",
      "Loss at step 12600 : 3.2334165573120117\n",
      "Loss at step 12650 : 2.330296277999878\n",
      "Loss at step 12700 : 3.263322353363037\n",
      "Loss at step 12750 : 3.0317163467407227\n",
      "Loss at step 12800 : 3.4891676902770996\n",
      "Loss at step 12850 : 2.676154613494873\n",
      "Loss at step 12900 : 2.985562562942505\n",
      "Loss at step 12950 : 3.480912685394287\n",
      "Loss at step 13000 : 2.961836338043213\n",
      "Loss at step 13050 : 3.0497384071350098\n",
      "Loss at step 13100 : 2.7901740074157715\n",
      "Loss at step 13150 : 1.9959156513214111\n",
      "Loss at step 13200 : 3.0671627521514893\n",
      "Loss at step 13250 : 3.307860851287842\n",
      "Loss at step 13300 : 2.3941822052001953\n",
      "Loss at step 13350 : 3.5272302627563477\n",
      "Loss at step 13400 : 3.773667573928833\n",
      "Loss at step 13450 : 2.7697882652282715\n",
      "Loss at step 13500 : 2.7131028175354004\n",
      "Loss at step 13550 : 3.7390527725219727\n",
      "Loss at step 13600 : 2.5089731216430664\n",
      "Loss at step 13650 : 4.04367733001709\n",
      "Loss at step 13700 : 2.854606866836548\n",
      "Loss at step 13750 : 2.2818572521209717\n",
      "Loss at step 13800 : 2.8917806148529053\n",
      "Loss at step 13850 : 3.233241558074951\n",
      "Loss at step 13900 : 2.0415875911712646\n",
      "Loss at step 13950 : 3.7256250381469727\n",
      "Loss at step 14000 : 3.915727138519287\n",
      "Loss at step 14050 : 2.9027676582336426\n",
      "Loss at step 14100 : 3.484041929244995\n",
      "Loss at step 14150 : 3.1098599433898926\n",
      "Loss at step 14200 : 3.868422031402588\n",
      "Loss at step 14250 : 2.891510486602783\n",
      "Loss at step 14300 : 3.3704285621643066\n",
      "Loss at step 14350 : 3.623469352722168\n",
      "Loss at step 14400 : 3.3085644245147705\n",
      "Loss at step 14450 : 3.4622550010681152\n",
      "Loss at step 14500 : 3.235159397125244\n",
      "Loss at step 14550 : 3.5047945976257324\n",
      "Loss at step 14600 : 2.9580838680267334\n",
      "Loss at step 14650 : 3.444042921066284\n",
      "Loss at step 14700 : 3.6154918670654297\n",
      "Loss at step 14750 : 2.1438698768615723\n",
      "Loss at step 14800 : 1.6413373947143555\n",
      "Loss at step 14850 : 3.3038275241851807\n",
      "Loss at step 14900 : 4.296435356140137\n",
      "Loss at step 14950 : 2.975334644317627\n",
      "Loss at step 15000 : 4.3855438232421875\n",
      "Nearest to tuna: position, flesh, sashimi, daikon, grate,\n",
      "Nearest to rice: topping, surface, block, slice, wasabi,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: bag, torch, juice, water, fryer,\n",
      "Nearest to sashimi: position, tuna, roe, sieve, soy,\n",
      "Nearest to steak: grate, grill, roe, ground, sashimi,\n",
      "Nearest to grill: steak, ground, marinade, vinegar, grate,\n",
      "Nearest to sauce: soy, vinegar, cake, starch, salt,\n",
      "Nearest to cream: liqueur, cocoa, espresso, powder, cheese,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 15050 : 3.454160213470459\n",
      "Loss at step 15100 : 2.7269718647003174\n",
      "Loss at step 15150 : 2.967538833618164\n",
      "Loss at step 15200 : 4.178229331970215\n",
      "Loss at step 15250 : 3.028907299041748\n",
      "Loss at step 15300 : 2.864255428314209\n",
      "Loss at step 15350 : 2.538611888885498\n",
      "Loss at step 15400 : 2.7225122451782227\n",
      "Loss at step 15450 : 2.599010467529297\n",
      "Loss at step 15500 : 4.51052713394165\n",
      "Loss at step 15550 : 2.455204725265503\n",
      "Loss at step 15600 : 2.8661983013153076\n",
      "Loss at step 15650 : 2.982454776763916\n",
      "Loss at step 15700 : 2.8087267875671387\n",
      "Loss at step 15750 : 3.3030712604522705\n",
      "Loss at step 15800 : 2.439772605895996\n",
      "Loss at step 15850 : 3.146939516067505\n",
      "Loss at step 15900 : 3.5917139053344727\n",
      "Loss at step 15950 : 1.4801056385040283\n",
      "Loss at step 16000 : 2.4647164344787598\n",
      "Loss at step 16050 : 3.7763404846191406\n",
      "Loss at step 16100 : 3.0224947929382324\n",
      "Loss at step 16150 : 3.0604023933410645\n",
      "Loss at step 16200 : 3.5991296768188477\n",
      "Loss at step 16250 : 2.335359573364258\n",
      "Loss at step 16300 : 2.2980358600616455\n",
      "Loss at step 16350 : 3.704855442047119\n",
      "Loss at step 16400 : 2.6542115211486816\n",
      "Loss at step 16450 : 2.763363838195801\n",
      "Loss at step 16500 : 2.544271230697632\n",
      "Loss at step 16550 : 2.7451655864715576\n",
      "Loss at step 16600 : 3.629312038421631\n",
      "Loss at step 16650 : 3.72982120513916\n",
      "Loss at step 16700 : 2.917029857635498\n",
      "Loss at step 16750 : 2.367980480194092\n",
      "Loss at step 16800 : 2.2978720664978027\n",
      "Loss at step 16850 : 2.19273042678833\n",
      "Loss at step 16900 : 3.7093796730041504\n",
      "Loss at step 16950 : 2.827202320098877\n",
      "Loss at step 17000 : 3.7423863410949707\n",
      "Loss at step 17050 : 3.226393938064575\n",
      "Loss at step 17100 : 2.8495471477508545\n",
      "Loss at step 17150 : 2.6020655632019043\n",
      "Loss at step 17200 : 3.753970146179199\n",
      "Loss at step 17250 : 2.702805280685425\n",
      "Loss at step 17300 : 3.5019164085388184\n",
      "Loss at step 17350 : 3.7140932083129883\n",
      "Loss at step 17400 : 3.9803566932678223\n",
      "Loss at step 17450 : 1.6020222902297974\n",
      "Loss at step 17500 : 2.8703620433807373\n",
      "Loss at step 17550 : 2.7542035579681396\n",
      "Loss at step 17600 : 2.7201919555664062\n",
      "Loss at step 17650 : 2.75406551361084\n",
      "Loss at step 17700 : 4.609773635864258\n",
      "Loss at step 17750 : 2.5221457481384277\n",
      "Loss at step 17800 : 3.5334222316741943\n",
      "Loss at step 17850 : 3.953638792037964\n",
      "Loss at step 17900 : 2.984884738922119\n",
      "Loss at step 17950 : 3.5670623779296875\n",
      "Loss at step 18000 : 3.080369234085083\n",
      "Loss at step 18050 : 2.286561965942383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 18100 : 2.009218692779541\n",
      "Loss at step 18150 : 2.8774986267089844\n",
      "Loss at step 18200 : 2.599675416946411\n",
      "Loss at step 18250 : 4.293593406677246\n",
      "Loss at step 18300 : 3.8443222045898438\n",
      "Loss at step 18350 : 3.4211983680725098\n",
      "Loss at step 18400 : 3.4757912158966064\n",
      "Loss at step 18450 : 3.55869722366333\n",
      "Loss at step 18500 : 2.2980666160583496\n",
      "Loss at step 18550 : 3.959000587463379\n",
      "Loss at step 18600 : 2.450881004333496\n",
      "Loss at step 18650 : 3.6532139778137207\n",
      "Loss at step 18700 : 3.323845863342285\n",
      "Loss at step 18750 : 3.231403350830078\n",
      "Loss at step 18800 : 3.0325121879577637\n",
      "Loss at step 18850 : 2.959163188934326\n",
      "Loss at step 18900 : 2.6065261363983154\n",
      "Loss at step 18950 : 3.195510149002075\n",
      "Loss at step 19000 : 2.4377012252807617\n",
      "Loss at step 19050 : 1.9255459308624268\n",
      "Loss at step 19100 : 2.0174758434295654\n",
      "Loss at step 19150 : 4.139527320861816\n",
      "Loss at step 19200 : 4.095195770263672\n",
      "Loss at step 19250 : 4.377018928527832\n",
      "Loss at step 19300 : 2.913100481033325\n",
      "Loss at step 19350 : 3.056595802307129\n",
      "Loss at step 19400 : 4.008177757263184\n",
      "Loss at step 19450 : 3.7917585372924805\n",
      "Loss at step 19500 : 3.325171709060669\n",
      "Loss at step 19550 : 3.1626627445220947\n",
      "Loss at step 19600 : 3.177736759185791\n",
      "Loss at step 19650 : 2.4721627235412598\n",
      "Loss at step 19700 : 3.2550907135009766\n",
      "Loss at step 19750 : 2.6232399940490723\n",
      "Loss at step 19800 : 4.3140740394592285\n",
      "Loss at step 19850 : 2.5983095169067383\n",
      "Loss at step 19900 : 3.824812412261963\n",
      "Loss at step 19950 : 2.429710865020752\n",
      "Loss at step 20000 : 2.736865758895874\n",
      "Nearest to tuna: position, flesh, sashimi, daikon, piece,\n",
      "Nearest to rice: topping, surface, block, slice, wasabi,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, bag, torch, juice, fryer,\n",
      "Nearest to sashimi: position, tuna, roe, sieve, filet,\n",
      "Nearest to steak: grate, grill, roe, ground, butter,\n",
      "Nearest to grill: steak, ground, marinade, vinegar, grate,\n",
      "Nearest to sauce: soy, starch, vinegar, salt, slice,\n",
      "Nearest to cream: cocoa, liqueur, espresso, cheese, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 20050 : 2.563234329223633\n",
      "Loss at step 20100 : 2.5844545364379883\n",
      "Loss at step 20150 : 3.5320887565612793\n",
      "Loss at step 20200 : 4.0377302169799805\n",
      "Loss at step 20250 : 3.430610179901123\n",
      "Loss at step 20300 : 3.426152229309082\n",
      "Loss at step 20350 : 3.350207805633545\n",
      "Loss at step 20400 : 3.829164505004883\n",
      "Loss at step 20450 : 2.9301977157592773\n",
      "Loss at step 20500 : 2.9687066078186035\n",
      "Loss at step 20550 : 2.156977415084839\n",
      "Loss at step 20600 : 2.8754985332489014\n",
      "Loss at step 20650 : 2.4476511478424072\n",
      "Loss at step 20700 : 1.826896071434021\n",
      "Loss at step 20750 : 2.690683364868164\n",
      "Loss at step 20800 : 1.985233187675476\n",
      "Loss at step 20850 : 2.9186761379241943\n",
      "Loss at step 20900 : 3.1692192554473877\n",
      "Loss at step 20950 : 2.689549446105957\n",
      "Loss at step 21000 : 2.482757568359375\n",
      "Loss at step 21050 : 4.18406867980957\n",
      "Loss at step 21100 : 2.9003114700317383\n",
      "Loss at step 21150 : 2.413109540939331\n",
      "Loss at step 21200 : 2.1860013008117676\n",
      "Loss at step 21250 : 2.237330198287964\n",
      "Loss at step 21300 : 2.8610610961914062\n",
      "Loss at step 21350 : 2.8516433238983154\n",
      "Loss at step 21400 : 2.8571834564208984\n",
      "Loss at step 21450 : 3.2554068565368652\n",
      "Loss at step 21500 : 3.5537266731262207\n",
      "Loss at step 21550 : 3.2520358562469482\n",
      "Loss at step 21600 : 3.3524365425109863\n",
      "Loss at step 21650 : 3.578610897064209\n",
      "Loss at step 21700 : 2.5765631198883057\n",
      "Loss at step 21750 : 2.473151683807373\n",
      "Loss at step 21800 : 1.9505109786987305\n",
      "Loss at step 21850 : 2.1595544815063477\n",
      "Loss at step 21900 : 3.5365328788757324\n",
      "Loss at step 21950 : 3.3133745193481445\n",
      "Loss at step 22000 : 1.92388117313385\n",
      "Loss at step 22050 : 3.340059757232666\n",
      "Loss at step 22100 : 3.0605969429016113\n",
      "Loss at step 22150 : 3.267246723175049\n",
      "Loss at step 22200 : 3.424837112426758\n",
      "Loss at step 22250 : 2.8582615852355957\n",
      "Loss at step 22300 : 2.6758151054382324\n",
      "Loss at step 22350 : 3.1374545097351074\n",
      "Loss at step 22400 : 2.962235450744629\n",
      "Loss at step 22450 : 3.1490955352783203\n",
      "Loss at step 22500 : 3.11470890045166\n",
      "Loss at step 22550 : 2.823310136795044\n",
      "Loss at step 22600 : 2.1392807960510254\n",
      "Loss at step 22650 : 2.242281198501587\n",
      "Loss at step 22700 : 3.529506206512451\n",
      "Loss at step 22750 : 4.100226879119873\n",
      "Loss at step 22800 : 2.129481315612793\n",
      "Loss at step 22850 : 3.1306352615356445\n",
      "Loss at step 22900 : 3.7093987464904785\n",
      "Loss at step 22950 : 3.215634822845459\n",
      "Loss at step 23000 : 2.629262924194336\n",
      "Loss at step 23050 : 3.7289044857025146\n",
      "Loss at step 23100 : 2.541370153427124\n",
      "Loss at step 23150 : 2.8030686378479004\n",
      "Loss at step 23200 : 2.4302215576171875\n",
      "Loss at step 23250 : 2.7533392906188965\n",
      "Loss at step 23300 : 3.5783653259277344\n",
      "Loss at step 23350 : 2.1689937114715576\n",
      "Loss at step 23400 : 2.911538600921631\n",
      "Loss at step 23450 : 3.3171911239624023\n",
      "Loss at step 23500 : 2.60915470123291\n",
      "Loss at step 23550 : 2.4579312801361084\n",
      "Loss at step 23600 : 2.133603811264038\n",
      "Loss at step 23650 : 4.283474445343018\n",
      "Loss at step 23700 : 2.7974648475646973\n",
      "Loss at step 23750 : 2.9555373191833496\n",
      "Loss at step 23800 : 2.849775791168213\n",
      "Loss at step 23850 : 2.708415985107422\n",
      "Loss at step 23900 : 3.2328968048095703\n",
      "Loss at step 23950 : 3.1120102405548096\n",
      "Loss at step 24000 : 3.108798027038574\n",
      "Loss at step 24050 : 2.193025588989258\n",
      "Loss at step 24100 : 3.111471176147461\n",
      "Loss at step 24150 : 3.097426414489746\n",
      "Loss at step 24200 : 3.0436863899230957\n",
      "Loss at step 24250 : 3.0972509384155273\n",
      "Loss at step 24300 : 3.1582436561584473\n",
      "Loss at step 24350 : 3.2558131217956543\n",
      "Loss at step 24400 : 2.6152758598327637\n",
      "Loss at step 24450 : 3.8540232181549072\n",
      "Loss at step 24500 : 2.799272060394287\n",
      "Loss at step 24550 : 3.0132243633270264\n",
      "Loss at step 24600 : 2.18526554107666\n",
      "Loss at step 24650 : 2.8634085655212402\n",
      "Loss at step 24700 : 2.2870655059814453\n",
      "Loss at step 24750 : 3.220026969909668\n",
      "Loss at step 24800 : 3.421980857849121\n",
      "Loss at step 24850 : 3.397487163543701\n",
      "Loss at step 24900 : 2.5637166500091553\n",
      "Loss at step 24950 : 3.5313053131103516\n",
      "Loss at step 25000 : 2.5740394592285156\n",
      "Nearest to tuna: position, flesh, sashimi, daikon, piece,\n",
      "Nearest to rice: block, surface, topping, slice, wasabi,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: torch, water, bag, juice, fryer,\n",
      "Nearest to sashimi: position, tuna, roe, sieve, filet,\n",
      "Nearest to steak: grate, grill, roe, ground, juice,\n",
      "Nearest to grill: steak, ground, marinade, grate, vinegar,\n",
      "Nearest to sauce: soy, starch, vinegar, salt, slice,\n",
      "Nearest to cream: cocoa, liqueur, espresso, cheese, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 25050 : 2.6940507888793945\n",
      "Loss at step 25100 : 2.0179386138916016\n",
      "Loss at step 25150 : 3.4263863563537598\n",
      "Loss at step 25200 : 2.988004446029663\n",
      "Loss at step 25250 : 3.295473575592041\n",
      "Loss at step 25300 : 2.2981882095336914\n",
      "Loss at step 25350 : 3.146505832672119\n",
      "Loss at step 25400 : 3.605813503265381\n",
      "Loss at step 25450 : 2.693615436553955\n",
      "Loss at step 25500 : 2.442089796066284\n",
      "Loss at step 25550 : 4.480525016784668\n",
      "Loss at step 25600 : 3.819711208343506\n",
      "Loss at step 25650 : 2.0148189067840576\n",
      "Loss at step 25700 : 3.0804619789123535\n",
      "Loss at step 25750 : 2.5921530723571777\n",
      "Loss at step 25800 : 2.9918928146362305\n",
      "Loss at step 25850 : 2.341420888900757\n",
      "Loss at step 25900 : 1.9974024295806885\n",
      "Loss at step 25950 : 2.344788074493408\n",
      "Loss at step 26000 : 2.7789926528930664\n",
      "Loss at step 26050 : 2.3090295791625977\n",
      "Loss at step 26100 : 2.6644623279571533\n",
      "Loss at step 26150 : 3.4416537284851074\n",
      "Loss at step 26200 : 2.8500938415527344\n",
      "Loss at step 26250 : 2.868319272994995\n",
      "Loss at step 26300 : 3.6842663288116455\n",
      "Loss at step 26350 : 2.241391181945801\n",
      "Loss at step 26400 : 1.9065864086151123\n",
      "Loss at step 26450 : 2.588928699493408\n",
      "Loss at step 26500 : 2.7603042125701904\n",
      "Loss at step 26550 : 2.2833828926086426\n",
      "Loss at step 26600 : 2.523770332336426\n",
      "Loss at step 26650 : 2.946812152862549\n",
      "Loss at step 26700 : 2.9836249351501465\n",
      "Loss at step 26750 : 3.1216979026794434\n",
      "Loss at step 26800 : 3.6712677478790283\n",
      "Loss at step 26850 : 3.0925893783569336\n",
      "Loss at step 26900 : 2.9278078079223633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 26950 : 2.8426249027252197\n",
      "Loss at step 27000 : 2.190997838973999\n",
      "Loss at step 27050 : 2.220132350921631\n",
      "Loss at step 27100 : 2.0090980529785156\n",
      "Loss at step 27150 : 2.3487582206726074\n",
      "Loss at step 27200 : 3.4713969230651855\n",
      "Loss at step 27250 : 3.023409843444824\n",
      "Loss at step 27300 : 3.1392760276794434\n",
      "Loss at step 27350 : 2.8652918338775635\n",
      "Loss at step 27400 : 3.507373571395874\n",
      "Loss at step 27450 : 3.3165183067321777\n",
      "Loss at step 27500 : 4.217070579528809\n",
      "Loss at step 27550 : 4.0329389572143555\n",
      "Loss at step 27600 : 3.180821180343628\n",
      "Loss at step 27650 : 4.037476062774658\n",
      "Loss at step 27700 : 2.1542317867279053\n",
      "Loss at step 27750 : 2.9541101455688477\n",
      "Loss at step 27800 : 2.1044507026672363\n",
      "Loss at step 27850 : 3.653045654296875\n",
      "Loss at step 27900 : 2.6445090770721436\n",
      "Loss at step 27950 : 3.140657424926758\n",
      "Loss at step 28000 : 3.2582435607910156\n",
      "Loss at step 28050 : 2.5198564529418945\n",
      "Loss at step 28100 : 2.9104397296905518\n",
      "Loss at step 28150 : 3.464503288269043\n",
      "Loss at step 28200 : 3.105436325073242\n",
      "Loss at step 28250 : 3.200139284133911\n",
      "Loss at step 28300 : 2.6270840167999268\n",
      "Loss at step 28350 : 2.4459409713745117\n",
      "Loss at step 28400 : 3.626758098602295\n",
      "Loss at step 28450 : 2.7443716526031494\n",
      "Loss at step 28500 : 2.937589645385742\n",
      "Loss at step 28550 : 2.196362257003784\n",
      "Loss at step 28600 : 2.780460834503174\n",
      "Loss at step 28650 : 3.016444444656372\n",
      "Loss at step 28700 : 2.8659169673919678\n",
      "Loss at step 28750 : 3.2431492805480957\n",
      "Loss at step 28800 : 2.31086802482605\n",
      "Loss at step 28850 : 2.90214204788208\n",
      "Loss at step 28900 : 4.25585412979126\n",
      "Loss at step 28950 : 3.134458541870117\n",
      "Loss at step 29000 : 4.573637008666992\n",
      "Loss at step 29050 : 2.6770434379577637\n",
      "Loss at step 29100 : 3.588798999786377\n",
      "Loss at step 29150 : 2.123173713684082\n",
      "Loss at step 29200 : 2.651493549346924\n",
      "Loss at step 29250 : 4.085616588592529\n",
      "Loss at step 29300 : 3.3338587284088135\n",
      "Loss at step 29350 : 3.06087589263916\n",
      "Loss at step 29400 : 1.8031389713287354\n",
      "Loss at step 29450 : 2.424694776535034\n",
      "Loss at step 29500 : 1.7866766452789307\n",
      "Loss at step 29550 : 3.4462475776672363\n",
      "Loss at step 29600 : 3.1964404582977295\n",
      "Loss at step 29650 : 2.9091286659240723\n",
      "Loss at step 29700 : 2.3001363277435303\n",
      "Loss at step 29750 : 3.857592821121216\n",
      "Loss at step 29800 : 3.420895576477051\n",
      "Loss at step 29850 : 2.7740840911865234\n",
      "Loss at step 29900 : 2.416321277618408\n",
      "Loss at step 29950 : 2.5615882873535156\n",
      "Loss at step 30000 : 1.9973275661468506\n",
      "Nearest to tuna: position, flesh, sashimi, daikon, garnish,\n",
      "Nearest to rice: topping, surface, block, slice, zip,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: torch, water, bag, juice, mat,\n",
      "Nearest to sashimi: position, tuna, roe, tongs, filet,\n",
      "Nearest to steak: grate, grill, roe, ground, juice,\n",
      "Nearest to grill: ground, steak, marinade, grate, vinegar,\n",
      "Nearest to sauce: soy, starch, vinegar, salt, tobikko,\n",
      "Nearest to cream: cocoa, liqueur, espresso, cheese, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 30050 : 2.4490246772766113\n",
      "Loss at step 30100 : 3.5041236877441406\n",
      "Loss at step 30150 : 2.940598487854004\n",
      "Loss at step 30200 : 3.6445441246032715\n",
      "Loss at step 30250 : 2.5364038944244385\n",
      "Loss at step 30300 : 2.5862207412719727\n",
      "Loss at step 30350 : 3.1502633094787598\n",
      "Loss at step 30400 : 2.480565071105957\n",
      "Loss at step 30450 : 3.723752975463867\n",
      "Loss at step 30500 : 2.422778606414795\n",
      "Loss at step 30550 : 1.6544041633605957\n",
      "Loss at step 30600 : 3.7799906730651855\n",
      "Loss at step 30650 : 1.7395308017730713\n",
      "Loss at step 30700 : 4.214949607849121\n",
      "Loss at step 30750 : 3.9308524131774902\n",
      "Loss at step 30800 : 2.8516459465026855\n",
      "Loss at step 30850 : 2.7743923664093018\n",
      "Loss at step 30900 : 2.244760513305664\n",
      "Loss at step 30950 : 2.5989110469818115\n",
      "Loss at step 31000 : 3.789696216583252\n",
      "Loss at step 31050 : 2.604841947555542\n",
      "Loss at step 31100 : 1.7663687467575073\n",
      "Loss at step 31150 : 3.013758659362793\n",
      "Loss at step 31200 : 3.919736862182617\n",
      "Loss at step 31250 : 1.9011564254760742\n",
      "Loss at step 31300 : 3.273446559906006\n",
      "Loss at step 31350 : 2.9201135635375977\n",
      "Loss at step 31400 : 3.2561283111572266\n",
      "Loss at step 31450 : 4.093430042266846\n",
      "Loss at step 31500 : 2.1875662803649902\n",
      "Loss at step 31550 : 2.820591688156128\n",
      "Loss at step 31600 : 2.9916739463806152\n",
      "Loss at step 31650 : 2.5004453659057617\n",
      "Loss at step 31700 : 4.74805212020874\n",
      "Loss at step 31750 : 2.7887399196624756\n",
      "Loss at step 31800 : 2.418429374694824\n",
      "Loss at step 31850 : 3.7776875495910645\n",
      "Loss at step 31900 : 3.0001468658447266\n",
      "Loss at step 31950 : 1.8489534854888916\n",
      "Loss at step 32000 : 2.765267848968506\n",
      "Loss at step 32050 : 2.7304887771606445\n",
      "Loss at step 32100 : 3.5560169219970703\n",
      "Loss at step 32150 : 2.2789864540100098\n",
      "Loss at step 32200 : 2.103314161300659\n",
      "Loss at step 32250 : 1.7342453002929688\n",
      "Loss at step 32300 : 3.729367256164551\n",
      "Loss at step 32350 : 2.8841300010681152\n",
      "Loss at step 32400 : 3.1725006103515625\n",
      "Loss at step 32450 : 2.7665982246398926\n",
      "Loss at step 32500 : 2.918893814086914\n",
      "Loss at step 32550 : 2.7799863815307617\n",
      "Loss at step 32600 : 3.6911487579345703\n",
      "Loss at step 32650 : 3.571690559387207\n",
      "Loss at step 32700 : 2.6264290809631348\n",
      "Loss at step 32750 : 1.8728549480438232\n",
      "Loss at step 32800 : 2.3820159435272217\n",
      "Loss at step 32850 : 2.9046168327331543\n",
      "Loss at step 32900 : 2.9297823905944824\n",
      "Loss at step 32950 : 2.7945382595062256\n",
      "Loss at step 33000 : 3.238340139389038\n",
      "Loss at step 33050 : 2.552746534347534\n",
      "Loss at step 33100 : 3.457869052886963\n",
      "Loss at step 33150 : 2.539618968963623\n",
      "Loss at step 33200 : 3.2509102821350098\n",
      "Loss at step 33250 : 1.7518558502197266\n",
      "Loss at step 33300 : 2.6087188720703125\n",
      "Loss at step 33350 : 3.9125325679779053\n",
      "Loss at step 33400 : 4.037641525268555\n",
      "Loss at step 33450 : 3.7753424644470215\n",
      "Loss at step 33500 : 1.5109076499938965\n",
      "Loss at step 33550 : 2.830930709838867\n",
      "Loss at step 33600 : 2.8987112045288086\n",
      "Loss at step 33650 : 1.9416782855987549\n",
      "Loss at step 33700 : 3.632422685623169\n",
      "Loss at step 33750 : 4.080197334289551\n",
      "Loss at step 33800 : 2.701559066772461\n",
      "Loss at step 33850 : 3.5222010612487793\n",
      "Loss at step 33900 : 3.2641491889953613\n",
      "Loss at step 33950 : 3.020247459411621\n",
      "Loss at step 34000 : 2.801847457885742\n",
      "Loss at step 34050 : 2.4873123168945312\n",
      "Loss at step 34100 : 2.5654826164245605\n",
      "Loss at step 34150 : 2.132197380065918\n",
      "Loss at step 34200 : 3.2636232376098633\n",
      "Loss at step 34250 : 3.320694923400879\n",
      "Loss at step 34300 : 3.4905519485473633\n",
      "Loss at step 34350 : 3.069185495376587\n",
      "Loss at step 34400 : 2.0548312664031982\n",
      "Loss at step 34450 : 2.9149723052978516\n",
      "Loss at step 34500 : 2.950084686279297\n",
      "Loss at step 34550 : 2.489673137664795\n",
      "Loss at step 34600 : 2.6039905548095703\n",
      "Loss at step 34650 : 2.9782779216766357\n",
      "Loss at step 34700 : 2.8999013900756836\n",
      "Loss at step 34750 : 2.550706624984741\n",
      "Loss at step 34800 : 3.4749960899353027\n",
      "Loss at step 34850 : 2.781048536300659\n",
      "Loss at step 34900 : 3.0976881980895996\n",
      "Loss at step 34950 : 2.563201665878296\n",
      "Loss at step 35000 : 2.209467649459839\n",
      "Nearest to tuna: position, daikon, flesh, garnish, sashimi,\n",
      "Nearest to rice: topping, surface, block, wasabi, zip,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: torch, water, bag, juice, mat,\n",
      "Nearest to sashimi: position, roe, tongs, tuna, filet,\n",
      "Nearest to steak: grate, grill, roe, ground, juice,\n",
      "Nearest to grill: ground, steak, marinade, grate, vinegar,\n",
      "Nearest to sauce: soy, starch, vinegar, salt, tobikko,\n",
      "Nearest to cream: cocoa, liqueur, espresso, cheese, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 35050 : 2.7506182193756104\n",
      "Loss at step 35100 : 2.9416470527648926\n",
      "Loss at step 35150 : 2.2434611320495605\n",
      "Loss at step 35200 : 3.939060688018799\n",
      "Loss at step 35250 : 3.8430089950561523\n",
      "Loss at step 35300 : 3.032567024230957\n",
      "Loss at step 35350 : 3.043954372406006\n",
      "Loss at step 35400 : 2.895650863647461\n",
      "Loss at step 35450 : 3.755239486694336\n",
      "Loss at step 35500 : 2.7849173545837402\n",
      "Loss at step 35550 : 1.944649338722229\n",
      "Loss at step 35600 : 2.949186325073242\n",
      "Loss at step 35650 : 1.9403653144836426\n",
      "Loss at step 35700 : 2.568556785583496\n",
      "Loss at step 35750 : 1.6658399105072021\n",
      "Loss at step 35800 : 2.4605934619903564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 35850 : 2.510481119155884\n",
      "Loss at step 35900 : 2.197228193283081\n",
      "Loss at step 35950 : 4.019049644470215\n",
      "Loss at step 36000 : 2.589124917984009\n",
      "Loss at step 36050 : 3.1101531982421875\n",
      "Loss at step 36100 : 2.6363205909729004\n",
      "Loss at step 36150 : 2.565025806427002\n",
      "Loss at step 36200 : 3.290708065032959\n",
      "Loss at step 36250 : 2.840714931488037\n",
      "Loss at step 36300 : 2.836008310317993\n",
      "Loss at step 36350 : 2.8455944061279297\n",
      "Loss at step 36400 : 2.485311985015869\n",
      "Loss at step 36450 : 3.121616840362549\n",
      "Loss at step 36500 : 4.505449295043945\n",
      "Loss at step 36550 : 3.4273109436035156\n",
      "Loss at step 36600 : 3.4793219566345215\n",
      "Loss at step 36650 : 3.039824962615967\n",
      "Loss at step 36700 : 3.680051326751709\n",
      "Loss at step 36750 : 2.5760223865509033\n",
      "Loss at step 36800 : 2.7022790908813477\n",
      "Loss at step 36850 : 3.4413013458251953\n",
      "Loss at step 36900 : 2.3608927726745605\n",
      "Loss at step 36950 : 2.462968349456787\n",
      "Loss at step 37000 : 2.9383249282836914\n",
      "Loss at step 37050 : 3.089050769805908\n",
      "Loss at step 37100 : 3.767144203186035\n",
      "Loss at step 37150 : 2.7047319412231445\n",
      "Loss at step 37200 : 3.1936779022216797\n",
      "Loss at step 37250 : 3.4336295127868652\n",
      "Loss at step 37300 : 2.8836464881896973\n",
      "Loss at step 37350 : 1.9443395137786865\n",
      "Loss at step 37400 : 2.558795928955078\n",
      "Loss at step 37450 : 3.512082815170288\n",
      "Loss at step 37500 : 2.3450229167938232\n",
      "Loss at step 37550 : 2.642922878265381\n",
      "Loss at step 37600 : 3.714331865310669\n",
      "Loss at step 37650 : 2.226047992706299\n",
      "Loss at step 37700 : 3.8000452518463135\n",
      "Loss at step 37750 : 2.7197937965393066\n",
      "Loss at step 37800 : 2.704430103302002\n",
      "Loss at step 37850 : 3.1260900497436523\n",
      "Loss at step 37900 : 3.0468814373016357\n",
      "Loss at step 37950 : 3.1355674266815186\n",
      "Loss at step 38000 : 2.8868041038513184\n",
      "Loss at step 38050 : 2.987175703048706\n",
      "Loss at step 38100 : 2.743976354598999\n",
      "Loss at step 38150 : 3.32350492477417\n",
      "Loss at step 38200 : 1.9746042490005493\n",
      "Loss at step 38250 : 2.476680040359497\n",
      "Loss at step 38300 : 3.938992977142334\n",
      "Loss at step 38350 : 2.5383214950561523\n",
      "Loss at step 38400 : 1.7380129098892212\n",
      "Loss at step 38450 : 2.56923770904541\n",
      "Loss at step 38500 : 2.30071759223938\n",
      "Loss at step 38550 : 2.572568416595459\n",
      "Loss at step 38600 : 4.109984397888184\n",
      "Loss at step 38650 : 3.6567420959472656\n",
      "Loss at step 38700 : 2.8780055046081543\n",
      "Loss at step 38750 : 2.1199018955230713\n",
      "Loss at step 38800 : 3.0451080799102783\n",
      "Loss at step 38850 : 3.564356803894043\n",
      "Loss at step 38900 : 2.3894572257995605\n",
      "Loss at step 38950 : 3.0812528133392334\n",
      "Loss at step 39000 : 3.002107620239258\n",
      "Loss at step 39050 : 4.17417573928833\n",
      "Loss at step 39100 : 2.9472124576568604\n",
      "Loss at step 39150 : 2.504711627960205\n",
      "Loss at step 39200 : 3.5271854400634766\n",
      "Loss at step 39250 : 2.4867420196533203\n",
      "Loss at step 39300 : 2.423189640045166\n",
      "Loss at step 39350 : 3.9133074283599854\n",
      "Loss at step 39400 : 2.845827579498291\n",
      "Loss at step 39450 : 2.8854293823242188\n",
      "Loss at step 39500 : 2.514071464538574\n",
      "Loss at step 39550 : 2.5822527408599854\n",
      "Loss at step 39600 : 2.559499502182007\n",
      "Loss at step 39650 : 1.7527530193328857\n",
      "Loss at step 39700 : 3.883216142654419\n",
      "Loss at step 39750 : 3.4176783561706543\n",
      "Loss at step 39800 : 3.1831564903259277\n",
      "Loss at step 39850 : 2.3892526626586914\n",
      "Loss at step 39900 : 2.779726982116699\n",
      "Loss at step 39950 : 2.7718396186828613\n",
      "Loss at step 40000 : 2.4866976737976074\n",
      "Nearest to tuna: position, daikon, garnish, flesh, sashimi,\n",
      "Nearest to rice: topping, surface, block, wasabi, slice,\n",
      "Nearest to sushi: mat, wasabi, finger, bamboo, coriander,\n",
      "Nearest to roll: torch, water, mat, bag, juice,\n",
      "Nearest to sashimi: position, roe, tuna, tongs, filet,\n",
      "Nearest to steak: grate, grill, roe, ground, juice,\n",
      "Nearest to grill: ground, steak, marinade, grate, vinegar,\n",
      "Nearest to sauce: soy, starch, vinegar, salt, tobikko,\n",
      "Nearest to cream: cocoa, liqueur, espresso, cheese, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 40050 : 3.9898219108581543\n",
      "Loss at step 40100 : 1.6075143814086914\n",
      "Loss at step 40150 : 2.821597099304199\n",
      "Loss at step 40200 : 2.8475046157836914\n",
      "Loss at step 40250 : 2.6887078285217285\n",
      "Loss at step 40300 : 3.4317173957824707\n",
      "Loss at step 40350 : 4.619243621826172\n",
      "Loss at step 40400 : 2.6067721843719482\n",
      "Loss at step 40450 : 1.7368754148483276\n",
      "Loss at step 40500 : 2.7515549659729004\n",
      "Loss at step 40550 : 2.5549850463867188\n",
      "Loss at step 40600 : 2.810117244720459\n",
      "Loss at step 40650 : 2.9009242057800293\n",
      "Loss at step 40700 : 2.473615884780884\n",
      "Loss at step 40750 : 2.7192370891571045\n",
      "Loss at step 40800 : 3.414452314376831\n",
      "Loss at step 40850 : 3.2788641452789307\n",
      "Loss at step 40900 : 2.4275646209716797\n",
      "Loss at step 40950 : 2.5704030990600586\n",
      "Loss at step 41000 : 3.266307830810547\n",
      "Loss at step 41050 : 3.077810764312744\n",
      "Loss at step 41100 : 2.320354461669922\n",
      "Loss at step 41150 : 2.073207378387451\n",
      "Loss at step 41200 : 2.9377312660217285\n",
      "Loss at step 41250 : 3.565276622772217\n",
      "Loss at step 41300 : 2.2221193313598633\n",
      "Loss at step 41350 : 2.5251035690307617\n",
      "Loss at step 41400 : 3.596660614013672\n",
      "Loss at step 41450 : 2.9336047172546387\n",
      "Loss at step 41500 : 2.464179039001465\n",
      "Loss at step 41550 : 2.7845258712768555\n",
      "Loss at step 41600 : 3.017519474029541\n",
      "Loss at step 41650 : 3.8694605827331543\n",
      "Loss at step 41700 : 2.190366268157959\n",
      "Loss at step 41750 : 3.114180564880371\n",
      "Loss at step 41800 : 2.981091022491455\n",
      "Loss at step 41850 : 3.523669719696045\n",
      "Loss at step 41900 : 4.344647407531738\n",
      "Loss at step 41950 : 3.182650089263916\n",
      "Loss at step 42000 : 3.618295669555664\n",
      "Loss at step 42050 : 2.715217113494873\n",
      "Loss at step 42100 : 2.935389518737793\n",
      "Loss at step 42150 : 2.544053077697754\n",
      "Loss at step 42200 : 3.1338419914245605\n",
      "Loss at step 42250 : 3.5768916606903076\n",
      "Loss at step 42300 : 2.454887628555298\n",
      "Loss at step 42350 : 2.5080924034118652\n",
      "Loss at step 42400 : 1.6412309408187866\n",
      "Loss at step 42450 : 1.671679973602295\n",
      "Loss at step 42500 : 2.8912997245788574\n",
      "Loss at step 42550 : 2.647777557373047\n",
      "Loss at step 42600 : 2.4335548877716064\n",
      "Loss at step 42650 : 2.581397533416748\n",
      "Loss at step 42700 : 4.269474983215332\n",
      "Loss at step 42750 : 3.4683029651641846\n",
      "Loss at step 42800 : 2.5943593978881836\n",
      "Loss at step 42850 : 3.5830631256103516\n",
      "Loss at step 42900 : 2.4930315017700195\n",
      "Loss at step 42950 : 3.7570924758911133\n",
      "Loss at step 43000 : 4.104454040527344\n",
      "Loss at step 43050 : 2.253990650177002\n",
      "Loss at step 43100 : 2.5809566974639893\n",
      "Loss at step 43150 : 2.334775924682617\n",
      "Loss at step 43200 : 3.6170380115509033\n",
      "Loss at step 43250 : 2.9879822731018066\n",
      "Loss at step 43300 : 2.8214468955993652\n",
      "Loss at step 43350 : 2.5891125202178955\n",
      "Loss at step 43400 : 3.33280086517334\n",
      "Loss at step 43450 : 3.067192554473877\n",
      "Loss at step 43500 : 3.3045811653137207\n",
      "Loss at step 43550 : 2.6623339653015137\n",
      "Loss at step 43600 : 2.4805643558502197\n",
      "Loss at step 43650 : 2.8634870052337646\n",
      "Loss at step 43700 : 4.379940032958984\n",
      "Loss at step 43750 : 2.76724910736084\n",
      "Loss at step 43800 : 2.5726606845855713\n",
      "Loss at step 43850 : 2.7169861793518066\n",
      "Loss at step 43900 : 2.797555446624756\n",
      "Loss at step 43950 : 2.3654286861419678\n",
      "Loss at step 44000 : 2.7764675617218018\n",
      "Loss at step 44050 : 2.2075467109680176\n",
      "Loss at step 44100 : 1.9772911071777344\n",
      "Loss at step 44150 : 2.2470667362213135\n",
      "Loss at step 44200 : 3.429558753967285\n",
      "Loss at step 44250 : 3.127213954925537\n",
      "Loss at step 44300 : 2.7149128913879395\n",
      "Loss at step 44350 : 2.8053464889526367\n",
      "Loss at step 44400 : 1.7989211082458496\n",
      "Loss at step 44450 : 3.0170226097106934\n",
      "Loss at step 44500 : 4.022719383239746\n",
      "Loss at step 44550 : 3.4001522064208984\n",
      "Loss at step 44600 : 2.9418599605560303\n",
      "Loss at step 44650 : 3.0776615142822266\n",
      "Loss at step 44700 : 2.136718511581421\n",
      "Loss at step 44750 : 2.7416319847106934\n",
      "Loss at step 44800 : 3.5267624855041504\n",
      "Loss at step 44850 : 2.914923667907715\n",
      "Loss at step 44900 : 1.957375407218933\n",
      "Loss at step 44950 : 3.169740676879883\n",
      "Loss at step 45000 : 2.697258949279785\n",
      "Nearest to tuna: position, daikon, garnish, flesh, piece,\n",
      "Nearest to rice: topping, surface, wasabi, zip, block,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, bag, mat, torch, juice,\n",
      "Nearest to sashimi: position, roe, tongs, tuna, filet,\n",
      "Nearest to steak: grate, grill, roe, ground, juice,\n",
      "Nearest to grill: ground, steak, marinade, grate, vinegar,\n",
      "Nearest to sauce: soy, starch, vinegar, salt, tobikko,\n",
      "Nearest to cream: cocoa, liqueur, espresso, cheese, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 45050 : 2.2622463703155518\n",
      "Loss at step 45100 : 2.8984599113464355\n",
      "Loss at step 45150 : 3.9720070362091064\n",
      "Loss at step 45200 : 3.019549608230591\n",
      "Loss at step 45250 : 3.0767900943756104\n",
      "Loss at step 45300 : 2.670703411102295\n",
      "Loss at step 45350 : 2.6210598945617676\n",
      "Loss at step 45400 : 3.2682371139526367\n",
      "Loss at step 45450 : 1.6398289203643799\n",
      "Loss at step 45500 : 2.1804065704345703\n",
      "Loss at step 45550 : 2.796889305114746\n",
      "Loss at step 45600 : 2.2347354888916016\n",
      "Loss at step 45650 : 2.1287739276885986\n",
      "Loss at step 45700 : 2.1165733337402344\n",
      "Loss at step 45750 : 2.6296839714050293\n",
      "Loss at step 45800 : 2.650712728500366\n",
      "Loss at step 45850 : 2.694927215576172\n",
      "Loss at step 45900 : 3.7601211071014404\n",
      "Loss at step 45950 : 2.465780258178711\n",
      "Loss at step 46000 : 2.5868310928344727\n",
      "Loss at step 46050 : 2.9670801162719727\n",
      "Loss at step 46100 : 3.0418946743011475\n",
      "Loss at step 46150 : 3.4736995697021484\n",
      "Loss at step 46200 : 2.7048304080963135\n",
      "Loss at step 46250 : 2.7517571449279785\n",
      "Loss at step 46300 : 2.201094388961792\n",
      "Loss at step 46350 : 2.5735857486724854\n",
      "Loss at step 46400 : 2.2046914100646973\n",
      "Loss at step 46450 : 2.872098445892334\n",
      "Loss at step 46500 : 3.986611843109131\n",
      "Loss at step 46550 : 2.2522544860839844\n",
      "Loss at step 46600 : 2.6999106407165527\n",
      "Loss at step 46650 : 2.203343391418457\n",
      "Loss at step 46700 : 1.5353094339370728\n",
      "Loss at step 46750 : 2.097193479537964\n",
      "Loss at step 46800 : 2.686561346054077\n",
      "Loss at step 46850 : 3.5132875442504883\n",
      "Loss at step 46900 : 4.20054817199707\n",
      "Loss at step 46950 : 3.245901584625244\n",
      "Loss at step 47000 : 3.7041406631469727\n",
      "Loss at step 47050 : 3.9009885787963867\n",
      "Loss at step 47100 : 2.982914924621582\n",
      "Loss at step 47150 : 2.63006854057312\n",
      "Loss at step 47200 : 3.611438035964966\n",
      "Loss at step 47250 : 2.7707369327545166\n",
      "Loss at step 47300 : 3.3300936222076416\n",
      "Loss at step 47350 : 4.419855117797852\n",
      "Loss at step 47400 : 2.5560877323150635\n",
      "Loss at step 47450 : 3.754218101501465\n",
      "Loss at step 47500 : 3.082103729248047\n",
      "Loss at step 47550 : 2.6731748580932617\n",
      "Loss at step 47600 : 2.145008087158203\n",
      "Loss at step 47650 : 2.6028506755828857\n",
      "Loss at step 47700 : 3.72200870513916\n",
      "Loss at step 47750 : 4.028322696685791\n",
      "Loss at step 47800 : 2.9653525352478027\n",
      "Loss at step 47850 : 2.8673739433288574\n",
      "Loss at step 47900 : 3.2042243480682373\n",
      "Loss at step 47950 : 3.674346446990967\n",
      "Loss at step 48000 : 2.4339675903320312\n",
      "Loss at step 48050 : 2.887444496154785\n",
      "Loss at step 48100 : 3.2673935890197754\n",
      "Loss at step 48150 : 2.6095821857452393\n",
      "Loss at step 48200 : 2.5149364471435547\n",
      "Loss at step 48250 : 2.9285733699798584\n",
      "Loss at step 48300 : 3.1501882076263428\n",
      "Loss at step 48350 : 1.8104898929595947\n",
      "Loss at step 48400 : 2.9731926918029785\n",
      "Loss at step 48450 : 3.3809707164764404\n",
      "Loss at step 48500 : 2.2826125621795654\n",
      "Loss at step 48550 : 3.4085147380828857\n",
      "Loss at step 48600 : 1.853827714920044\n",
      "Loss at step 48650 : 2.7604193687438965\n",
      "Loss at step 48700 : 2.9642417430877686\n",
      "Loss at step 48750 : 3.008469343185425\n",
      "Loss at step 48800 : 2.3289122581481934\n",
      "Loss at step 48850 : 2.4915459156036377\n",
      "Loss at step 48900 : 3.7603721618652344\n",
      "Loss at step 48950 : 3.137000799179077\n",
      "Loss at step 49000 : 2.879582643508911\n",
      "Loss at step 49050 : 2.736574649810791\n",
      "Loss at step 49100 : 3.390206813812256\n",
      "Loss at step 49150 : 2.962364912033081\n",
      "Loss at step 49200 : 2.396214723587036\n",
      "Loss at step 49250 : 3.0254383087158203\n",
      "Loss at step 49300 : 3.638798713684082\n",
      "Loss at step 49350 : 2.9218802452087402\n",
      "Loss at step 49400 : 2.5784149169921875\n",
      "Loss at step 49450 : 3.256545066833496\n",
      "Loss at step 49500 : 2.270059108734131\n",
      "Loss at step 49550 : 2.7988033294677734\n",
      "Loss at step 49600 : 2.2598042488098145\n",
      "Loss at step 49650 : 2.5788402557373047\n",
      "Loss at step 49700 : 2.6345317363739014\n",
      "Loss at step 49750 : 3.1053500175476074\n",
      "Loss at step 49800 : 3.6003761291503906\n",
      "Loss at step 49850 : 1.566215991973877\n",
      "Loss at step 49900 : 2.004991054534912\n",
      "Loss at step 49950 : 3.4778079986572266\n",
      "Loss at step 50000 : 2.8329010009765625\n",
      "Nearest to tuna: position, garnish, daikon, piece, sashimi,\n",
      "Nearest to rice: topping, wasabi, surface, zip, block,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, mat, bag, torch, juice,\n",
      "Nearest to sashimi: position, roe, tongs, tuna, sieve,\n",
      "Nearest to steak: grate, grill, ground, roe, juice,\n",
      "Nearest to grill: ground, steak, marinade, grate, meat,\n",
      "Nearest to sauce: soy, starch, vinegar, salt, tobikko,\n",
      "Nearest to cream: cocoa, espresso, liqueur, cheese, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 50050 : 3.4439473152160645\n",
      "Loss at step 50100 : 3.3417677879333496\n",
      "Loss at step 50150 : 4.197524070739746\n",
      "Loss at step 50200 : 3.0416035652160645\n",
      "Loss at step 50250 : 3.34602689743042\n",
      "Loss at step 50300 : 2.9294352531433105\n",
      "Loss at step 50350 : 2.2794876098632812\n",
      "Loss at step 50400 : 3.024031162261963\n",
      "Loss at step 50450 : 2.7654647827148438\n",
      "Loss at step 50500 : 3.136972427368164\n",
      "Loss at step 50550 : 2.86084246635437\n",
      "Loss at step 50600 : 3.475294828414917\n",
      "Loss at step 50650 : 2.2992286682128906\n",
      "Loss at step 50700 : 3.2157678604125977\n",
      "Loss at step 50750 : 2.6832664012908936\n",
      "Loss at step 50800 : 3.4569973945617676\n",
      "Loss at step 50850 : 2.4305319786071777\n",
      "Loss at step 50900 : 3.150798797607422\n",
      "Loss at step 50950 : 3.5415773391723633\n",
      "Loss at step 51000 : 2.7296407222747803\n",
      "Loss at step 51050 : 2.846393585205078\n",
      "Loss at step 51100 : 2.6190345287323\n",
      "Loss at step 51150 : 3.8076627254486084\n",
      "Loss at step 51200 : 2.4815244674682617\n",
      "Loss at step 51250 : 2.94807505607605\n",
      "Loss at step 51300 : 2.4334893226623535\n",
      "Loss at step 51350 : 3.1712663173675537\n",
      "Loss at step 51400 : 2.7382187843322754\n",
      "Loss at step 51450 : 2.286755084991455\n",
      "Loss at step 51500 : 2.460206985473633\n",
      "Loss at step 51550 : 2.343353033065796\n",
      "Loss at step 51600 : 2.6839029788970947\n",
      "Loss at step 51650 : 2.515774726867676\n",
      "Loss at step 51700 : 3.3134474754333496\n",
      "Loss at step 51750 : 3.1348557472229004\n",
      "Loss at step 51800 : 2.8648746013641357\n",
      "Loss at step 51850 : 2.4591641426086426\n",
      "Loss at step 51900 : 2.446133613586426\n",
      "Loss at step 51950 : 2.8390791416168213\n",
      "Loss at step 52000 : 3.261542320251465\n",
      "Loss at step 52050 : 2.409282684326172\n",
      "Loss at step 52100 : 2.9141898155212402\n",
      "Loss at step 52150 : 2.3101515769958496\n",
      "Loss at step 52200 : 1.8429856300354004\n",
      "Loss at step 52250 : 2.057269811630249\n",
      "Loss at step 52300 : 3.7253074645996094\n",
      "Loss at step 52350 : 2.230639696121216\n",
      "Loss at step 52400 : 3.3021340370178223\n",
      "Loss at step 52450 : 2.8267979621887207\n",
      "Loss at step 52500 : 2.3956706523895264\n",
      "Loss at step 52550 : 3.4373369216918945\n",
      "Loss at step 52600 : 2.6329545974731445\n",
      "Loss at step 52650 : 3.8877077102661133\n",
      "Loss at step 52700 : 3.5525383949279785\n",
      "Loss at step 52750 : 2.112053394317627\n",
      "Loss at step 52800 : 3.297067403793335\n",
      "Loss at step 52850 : 3.1219441890716553\n",
      "Loss at step 52900 : 2.6757302284240723\n",
      "Loss at step 52950 : 2.4909348487854004\n",
      "Loss at step 53000 : 1.9610928297042847\n",
      "Loss at step 53050 : 2.4447827339172363\n",
      "Loss at step 53100 : 3.119147777557373\n",
      "Loss at step 53150 : 2.6465883255004883\n",
      "Loss at step 53200 : 3.111947536468506\n",
      "Loss at step 53250 : 3.408101797103882\n",
      "Loss at step 53300 : 2.261857748031616\n",
      "Loss at step 53350 : 2.9197890758514404\n",
      "Loss at step 53400 : 2.3255505561828613\n",
      "Loss at step 53450 : 2.3744468688964844\n",
      "Loss at step 53500 : 3.174077272415161\n",
      "Loss at step 53550 : 2.2313132286071777\n",
      "Loss at step 53600 : 4.10621452331543\n",
      "Loss at step 53650 : 2.660159111022949\n",
      "Loss at step 53700 : 3.6664886474609375\n",
      "Loss at step 53750 : 2.6414668560028076\n",
      "Loss at step 53800 : 2.236416816711426\n",
      "Loss at step 53850 : 2.291372299194336\n",
      "Loss at step 53900 : 2.5764050483703613\n",
      "Loss at step 53950 : 2.8565526008605957\n",
      "Loss at step 54000 : 3.208709716796875\n",
      "Loss at step 54050 : 2.674960136413574\n",
      "Loss at step 54100 : 2.461381435394287\n",
      "Loss at step 54150 : 2.3813533782958984\n",
      "Loss at step 54200 : 2.6456403732299805\n",
      "Loss at step 54250 : 1.967496633529663\n",
      "Loss at step 54300 : 3.6201376914978027\n",
      "Loss at step 54350 : 1.8643701076507568\n",
      "Loss at step 54400 : 2.935014009475708\n",
      "Loss at step 54450 : 2.8272271156311035\n",
      "Loss at step 54500 : 2.284909725189209\n",
      "Loss at step 54550 : 3.3536906242370605\n",
      "Loss at step 54600 : 2.6387081146240234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 54650 : 2.5978646278381348\n",
      "Loss at step 54700 : 2.5604376792907715\n",
      "Loss at step 54750 : 2.7710156440734863\n",
      "Loss at step 54800 : 3.5493412017822266\n",
      "Loss at step 54850 : 3.2669787406921387\n",
      "Loss at step 54900 : 2.6971848011016846\n",
      "Loss at step 54950 : 2.895686626434326\n",
      "Loss at step 55000 : 2.313051462173462\n",
      "Nearest to tuna: position, garnish, daikon, piece, flesh,\n",
      "Nearest to rice: topping, wasabi, surface, zip, block,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, mat, bag, torch, juice,\n",
      "Nearest to sashimi: roe, position, tongs, tuna, air,\n",
      "Nearest to steak: grate, grill, ground, roe, butter,\n",
      "Nearest to grill: ground, steak, marinade, grate, meat,\n",
      "Nearest to sauce: soy, starch, vinegar, salt, tobikko,\n",
      "Nearest to cream: cocoa, liqueur, cheese, espresso, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 55050 : 3.000974178314209\n",
      "Loss at step 55100 : 2.367276668548584\n",
      "Loss at step 55150 : 3.7922744750976562\n",
      "Loss at step 55200 : 2.4854559898376465\n",
      "Loss at step 55250 : 2.402188301086426\n",
      "Loss at step 55300 : 2.5723729133605957\n",
      "Loss at step 55350 : 2.1085052490234375\n",
      "Loss at step 55400 : 2.379967212677002\n",
      "Loss at step 55450 : 2.756326198577881\n",
      "Loss at step 55500 : 3.369968891143799\n",
      "Loss at step 55550 : 3.354548215866089\n",
      "Loss at step 55600 : 2.253756046295166\n",
      "Loss at step 55650 : 2.5865559577941895\n",
      "Loss at step 55700 : 3.4367308616638184\n",
      "Loss at step 55750 : 2.273535966873169\n",
      "Loss at step 55800 : 1.7093169689178467\n",
      "Loss at step 55850 : 2.3749990463256836\n",
      "Loss at step 55900 : 3.497453212738037\n",
      "Loss at step 55950 : 4.057746410369873\n",
      "Loss at step 56000 : 2.2395505905151367\n",
      "Loss at step 56050 : 2.7038979530334473\n",
      "Loss at step 56100 : 3.214191436767578\n",
      "Loss at step 56150 : 2.600390911102295\n",
      "Loss at step 56200 : 3.734546661376953\n",
      "Loss at step 56250 : 2.916456460952759\n",
      "Loss at step 56300 : 3.2739758491516113\n",
      "Loss at step 56350 : 2.52817702293396\n",
      "Loss at step 56400 : 2.160438299179077\n",
      "Loss at step 56450 : 3.8594810962677\n",
      "Loss at step 56500 : 2.4367523193359375\n",
      "Loss at step 56550 : 3.189086437225342\n",
      "Loss at step 56600 : 3.057736873626709\n",
      "Loss at step 56650 : 2.3229804039001465\n",
      "Loss at step 56700 : 2.538294792175293\n",
      "Loss at step 56750 : 2.593947410583496\n",
      "Loss at step 56800 : 2.941843032836914\n",
      "Loss at step 56850 : 2.894317150115967\n",
      "Loss at step 56900 : 1.9542045593261719\n",
      "Loss at step 56950 : 2.155160665512085\n",
      "Loss at step 57000 : 2.8420181274414062\n",
      "Loss at step 57050 : 2.630110502243042\n",
      "Loss at step 57100 : 2.788334369659424\n",
      "Loss at step 57150 : 2.758873462677002\n",
      "Loss at step 57200 : 3.3874545097351074\n",
      "Loss at step 57250 : 3.3968000411987305\n",
      "Loss at step 57300 : 3.8121442794799805\n",
      "Loss at step 57350 : 2.140760660171509\n",
      "Loss at step 57400 : 2.779839038848877\n",
      "Loss at step 57450 : 1.5820531845092773\n",
      "Loss at step 57500 : 3.4490840435028076\n",
      "Loss at step 57550 : 3.7230405807495117\n",
      "Loss at step 57600 : 3.7037723064422607\n",
      "Loss at step 57650 : 3.8873751163482666\n",
      "Loss at step 57700 : 3.4187402725219727\n",
      "Loss at step 57750 : 2.4851622581481934\n",
      "Loss at step 57800 : 3.5379552841186523\n",
      "Loss at step 57850 : 3.358811855316162\n",
      "Loss at step 57900 : 3.191457748413086\n",
      "Loss at step 57950 : 3.3351986408233643\n",
      "Loss at step 58000 : 2.406979560852051\n",
      "Loss at step 58050 : 3.2033252716064453\n",
      "Loss at step 58100 : 2.9960622787475586\n",
      "Loss at step 58150 : 2.954611301422119\n",
      "Loss at step 58200 : 2.389206886291504\n",
      "Loss at step 58250 : 3.5591158866882324\n",
      "Loss at step 58300 : 2.5208120346069336\n",
      "Loss at step 58350 : 2.921645164489746\n",
      "Loss at step 58400 : 2.4642910957336426\n",
      "Loss at step 58450 : 2.324948787689209\n",
      "Loss at step 58500 : 2.6002750396728516\n",
      "Loss at step 58550 : 3.559925079345703\n",
      "Loss at step 58600 : 2.1140735149383545\n",
      "Loss at step 58650 : 2.5669302940368652\n",
      "Loss at step 58700 : 2.494189739227295\n",
      "Loss at step 58750 : 2.0685698986053467\n",
      "Loss at step 58800 : 2.7375118732452393\n",
      "Loss at step 58850 : 3.6401748657226562\n",
      "Loss at step 58900 : 2.851496934890747\n",
      "Loss at step 58950 : 2.810199737548828\n",
      "Loss at step 59000 : 2.1673240661621094\n",
      "Loss at step 59050 : 1.9193575382232666\n",
      "Loss at step 59100 : 1.794013261795044\n",
      "Loss at step 59150 : 1.6765658855438232\n",
      "Loss at step 59200 : 2.878847360610962\n",
      "Loss at step 59250 : 2.58609938621521\n",
      "Loss at step 59300 : 2.6006953716278076\n",
      "Loss at step 59350 : 3.403189182281494\n",
      "Loss at step 59400 : 2.235595464706421\n",
      "Loss at step 59450 : 3.1355037689208984\n",
      "Loss at step 59500 : 2.3037829399108887\n",
      "Loss at step 59550 : 2.5567808151245117\n",
      "Loss at step 59600 : 3.5567288398742676\n",
      "Loss at step 59650 : 2.979764461517334\n",
      "Loss at step 59700 : 2.6999337673187256\n",
      "Loss at step 59750 : 4.135124206542969\n",
      "Loss at step 59800 : 4.010747909545898\n",
      "Loss at step 59850 : 2.5997698307037354\n",
      "Loss at step 59900 : 3.2332589626312256\n",
      "Loss at step 59950 : 4.0221171379089355\n",
      "Loss at step 60000 : 2.4131250381469727\n",
      "Nearest to tuna: position, garnish, daikon, piece, sashimi,\n",
      "Nearest to rice: topping, wasabi, zip, surface, nori,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, mat, torch, juice, bag,\n",
      "Nearest to sashimi: position, roe, tongs, tuna, air,\n",
      "Nearest to steak: grill, grate, ground, roe, juice,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, liqueur, cheese, espresso, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 60050 : 2.960751533508301\n",
      "Loss at step 60100 : 2.8565618991851807\n",
      "Loss at step 60150 : 2.8437023162841797\n",
      "Loss at step 60200 : 2.823718309402466\n",
      "Loss at step 60250 : 3.217833995819092\n",
      "Loss at step 60300 : 2.6838252544403076\n",
      "Loss at step 60350 : 2.6618800163269043\n",
      "Loss at step 60400 : 2.9698116779327393\n",
      "Loss at step 60450 : 3.6203794479370117\n",
      "Loss at step 60500 : 2.8415884971618652\n",
      "Loss at step 60550 : 3.1991775035858154\n",
      "Loss at step 60600 : 2.6268744468688965\n",
      "Loss at step 60650 : 2.2381649017333984\n",
      "Loss at step 60700 : 3.772268056869507\n",
      "Loss at step 60750 : 1.6336462497711182\n",
      "Loss at step 60800 : 3.3943772315979004\n",
      "Loss at step 60850 : 3.928356885910034\n",
      "Loss at step 60900 : 2.608098030090332\n",
      "Loss at step 60950 : 2.3475308418273926\n",
      "Loss at step 61000 : 2.5878398418426514\n",
      "Loss at step 61050 : 2.48006272315979\n",
      "Loss at step 61100 : 3.6642746925354004\n",
      "Loss at step 61150 : 2.6342697143554688\n",
      "Loss at step 61200 : 3.808115005493164\n",
      "Loss at step 61250 : 3.049203872680664\n",
      "Loss at step 61300 : 2.903106212615967\n",
      "Loss at step 61350 : 1.8907310962677002\n",
      "Loss at step 61400 : 2.6658573150634766\n",
      "Loss at step 61450 : 2.54990291595459\n",
      "Loss at step 61500 : 2.768347978591919\n",
      "Loss at step 61550 : 3.1074411869049072\n",
      "Loss at step 61600 : 3.5009779930114746\n",
      "Loss at step 61650 : 2.6970221996307373\n",
      "Loss at step 61700 : 3.326598644256592\n",
      "Loss at step 61750 : 3.393538475036621\n",
      "Loss at step 61800 : 3.1944563388824463\n",
      "Loss at step 61850 : 2.601130485534668\n",
      "Loss at step 61900 : 4.026987075805664\n",
      "Loss at step 61950 : 3.2491979598999023\n",
      "Loss at step 62000 : 3.5926051139831543\n",
      "Loss at step 62050 : 3.3164589405059814\n",
      "Loss at step 62100 : 1.5519143342971802\n",
      "Loss at step 62150 : 2.439553737640381\n",
      "Loss at step 62200 : 3.2830111980438232\n",
      "Loss at step 62250 : 2.015986442565918\n",
      "Loss at step 62300 : 2.0794239044189453\n",
      "Loss at step 62350 : 2.075434684753418\n",
      "Loss at step 62400 : 2.2043938636779785\n",
      "Loss at step 62450 : 2.9739270210266113\n",
      "Loss at step 62500 : 3.220405340194702\n",
      "Loss at step 62550 : 2.8466947078704834\n",
      "Loss at step 62600 : 1.652900218963623\n",
      "Loss at step 62650 : 3.6555140018463135\n",
      "Loss at step 62700 : 3.570990562438965\n",
      "Loss at step 62750 : 2.8746466636657715\n",
      "Loss at step 62800 : 2.1106388568878174\n",
      "Loss at step 62850 : 2.8820106983184814\n",
      "Loss at step 62900 : 2.6256141662597656\n",
      "Loss at step 62950 : 2.539344072341919\n",
      "Loss at step 63000 : 3.838442087173462\n",
      "Loss at step 63050 : 2.921248435974121\n",
      "Loss at step 63100 : 2.944044589996338\n",
      "Loss at step 63150 : 2.9861502647399902\n",
      "Loss at step 63200 : 2.3920536041259766\n",
      "Loss at step 63250 : 2.396296262741089\n",
      "Loss at step 63300 : 2.838559627532959\n",
      "Loss at step 63350 : 4.125077247619629\n",
      "Loss at step 63400 : 2.2547788619995117\n",
      "Loss at step 63450 : 2.9986748695373535\n",
      "Loss at step 63500 : 3.2251055240631104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 63550 : 2.5920639038085938\n",
      "Loss at step 63600 : 3.105949878692627\n",
      "Loss at step 63650 : 2.578806161880493\n",
      "Loss at step 63700 : 2.3774783611297607\n",
      "Loss at step 63750 : 2.101844310760498\n",
      "Loss at step 63800 : 2.1079416275024414\n",
      "Loss at step 63850 : 3.2887492179870605\n",
      "Loss at step 63900 : 2.274949312210083\n",
      "Loss at step 63950 : 3.828122615814209\n",
      "Loss at step 64000 : 2.5315439701080322\n",
      "Loss at step 64050 : 2.643406629562378\n",
      "Loss at step 64100 : 2.547614574432373\n",
      "Loss at step 64150 : 3.224214553833008\n",
      "Loss at step 64200 : 3.7410807609558105\n",
      "Loss at step 64250 : 2.8662877082824707\n",
      "Loss at step 64300 : 3.3440961837768555\n",
      "Loss at step 64350 : 2.378122091293335\n",
      "Loss at step 64400 : 2.7946128845214844\n",
      "Loss at step 64450 : 3.801264762878418\n",
      "Loss at step 64500 : 2.3498802185058594\n",
      "Loss at step 64550 : 2.2040812969207764\n",
      "Loss at step 64600 : 2.4151813983917236\n",
      "Loss at step 64650 : 2.567607879638672\n",
      "Loss at step 64700 : 2.7844886779785156\n",
      "Loss at step 64750 : 2.6654105186462402\n",
      "Loss at step 64800 : 2.9452590942382812\n",
      "Loss at step 64850 : 2.8989157676696777\n",
      "Loss at step 64900 : 2.6496782302856445\n",
      "Loss at step 64950 : 2.004204034805298\n",
      "Loss at step 65000 : 1.9295196533203125\n",
      "Nearest to tuna: position, garnish, daikon, piece, sashimi,\n",
      "Nearest to rice: topping, zip, wasabi, surface, nori,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, mat, torch, juice, bag,\n",
      "Nearest to sashimi: position, roe, tongs, tuna, sieve,\n",
      "Nearest to steak: grill, grate, ground, roe, butter,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, liqueur, espresso, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 65050 : 2.4397058486938477\n",
      "Loss at step 65100 : 2.8537893295288086\n",
      "Loss at step 65150 : 3.4320359230041504\n",
      "Loss at step 65200 : 2.169201374053955\n",
      "Loss at step 65250 : 2.7302238941192627\n",
      "Loss at step 65300 : 3.108543634414673\n",
      "Loss at step 65350 : 2.7604568004608154\n",
      "Loss at step 65400 : 2.9722938537597656\n",
      "Loss at step 65450 : 3.025063991546631\n",
      "Loss at step 65500 : 3.295559883117676\n",
      "Loss at step 65550 : 2.2946507930755615\n",
      "Loss at step 65600 : 3.639801263809204\n",
      "Loss at step 65650 : 1.4734342098236084\n",
      "Loss at step 65700 : 3.5003671646118164\n",
      "Loss at step 65750 : 3.7601213455200195\n",
      "Loss at step 65800 : 2.7064425945281982\n",
      "Loss at step 65850 : 2.8414037227630615\n",
      "Loss at step 65900 : 3.3654327392578125\n",
      "Loss at step 65950 : 2.502558708190918\n",
      "Loss at step 66000 : 3.125670909881592\n",
      "Loss at step 66050 : 2.5018043518066406\n",
      "Loss at step 66100 : 2.837272882461548\n",
      "Loss at step 66150 : 1.7700470685958862\n",
      "Loss at step 66200 : 2.7826731204986572\n",
      "Loss at step 66250 : 2.9083147048950195\n",
      "Loss at step 66300 : 3.2801239490509033\n",
      "Loss at step 66350 : 3.0020220279693604\n",
      "Loss at step 66400 : 2.299696922302246\n",
      "Loss at step 66450 : 2.5681018829345703\n",
      "Loss at step 66500 : 1.83418869972229\n",
      "Loss at step 66550 : 3.1434435844421387\n",
      "Loss at step 66600 : 2.6543774604797363\n",
      "Loss at step 66650 : 3.3593931198120117\n",
      "Loss at step 66700 : 2.9079761505126953\n",
      "Loss at step 66750 : 2.7657694816589355\n",
      "Loss at step 66800 : 3.428370475769043\n",
      "Loss at step 66850 : 2.622518301010132\n",
      "Loss at step 66900 : 2.6501026153564453\n",
      "Loss at step 66950 : 2.228482723236084\n",
      "Loss at step 67000 : 2.5620851516723633\n",
      "Loss at step 67050 : 2.7825355529785156\n",
      "Loss at step 67100 : 2.1786489486694336\n",
      "Loss at step 67150 : 2.890653371810913\n",
      "Loss at step 67200 : 2.8404839038848877\n",
      "Loss at step 67250 : 2.535649299621582\n",
      "Loss at step 67300 : 2.696098566055298\n",
      "Loss at step 67350 : 2.297001600265503\n",
      "Loss at step 67400 : 2.9767167568206787\n",
      "Loss at step 67450 : 2.720416784286499\n",
      "Loss at step 67500 : 3.395599365234375\n",
      "Loss at step 67550 : 2.0189268589019775\n",
      "Loss at step 67600 : 2.661785840988159\n",
      "Loss at step 67650 : 2.0083446502685547\n",
      "Loss at step 67700 : 2.143538475036621\n",
      "Loss at step 67750 : 1.4907262325286865\n",
      "Loss at step 67800 : 2.792119026184082\n",
      "Loss at step 67850 : 3.7207956314086914\n",
      "Loss at step 67900 : 3.273700475692749\n",
      "Loss at step 67950 : 2.6845145225524902\n",
      "Loss at step 68000 : 3.3258891105651855\n",
      "Loss at step 68050 : 2.601367235183716\n",
      "Loss at step 68100 : 2.764525890350342\n",
      "Loss at step 68150 : 2.5324056148529053\n",
      "Loss at step 68200 : 1.9185292720794678\n",
      "Loss at step 68250 : 2.883223533630371\n",
      "Loss at step 68300 : 3.1719777584075928\n",
      "Loss at step 68350 : 2.774965286254883\n",
      "Loss at step 68400 : 2.6251046657562256\n",
      "Loss at step 68450 : 2.087876319885254\n",
      "Loss at step 68500 : 3.6108884811401367\n",
      "Loss at step 68550 : 2.1854677200317383\n",
      "Loss at step 68600 : 2.831705093383789\n",
      "Loss at step 68650 : 2.7997188568115234\n",
      "Loss at step 68700 : 3.144432306289673\n",
      "Loss at step 68750 : 3.2354726791381836\n",
      "Loss at step 68800 : 1.8860349655151367\n",
      "Loss at step 68850 : 3.0775957107543945\n",
      "Loss at step 68900 : 2.248054027557373\n",
      "Loss at step 68950 : 2.2152791023254395\n",
      "Loss at step 69000 : 2.3679616451263428\n",
      "Loss at step 69050 : 2.270235538482666\n",
      "Loss at step 69100 : 2.885575771331787\n",
      "Loss at step 69150 : 3.5486273765563965\n",
      "Loss at step 69200 : 2.9538211822509766\n",
      "Loss at step 69250 : 2.866485118865967\n",
      "Loss at step 69300 : 2.91516375541687\n",
      "Loss at step 69350 : 2.8932836055755615\n",
      "Loss at step 69400 : 2.9098334312438965\n",
      "Loss at step 69450 : 3.508357048034668\n",
      "Loss at step 69500 : 2.6375436782836914\n",
      "Loss at step 69550 : 2.625779151916504\n",
      "Loss at step 69600 : 3.0966272354125977\n",
      "Loss at step 69650 : 2.742640495300293\n",
      "Loss at step 69700 : 3.2597193717956543\n",
      "Loss at step 69750 : 2.7520534992218018\n",
      "Loss at step 69800 : 3.6229748725891113\n",
      "Loss at step 69850 : 3.249931573867798\n",
      "Loss at step 69900 : 2.387420177459717\n",
      "Loss at step 69950 : 3.3090381622314453\n",
      "Loss at step 70000 : 2.156670570373535\n",
      "Nearest to tuna: position, garnish, daikon, piece, flesh,\n",
      "Nearest to rice: zip, topping, wasabi, surface, nori,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, mat, bag, juice, torch,\n",
      "Nearest to sashimi: roe, position, tongs, tuna, air,\n",
      "Nearest to steak: grill, grate, ground, roe, butter,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, liqueur, espresso, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 70050 : 3.3525032997131348\n",
      "Loss at step 70100 : 2.923006057739258\n",
      "Loss at step 70150 : 2.784053087234497\n",
      "Loss at step 70200 : 3.0184764862060547\n",
      "Loss at step 70250 : 2.545475482940674\n",
      "Loss at step 70300 : 2.9359869956970215\n",
      "Loss at step 70350 : 3.0057713985443115\n",
      "Loss at step 70400 : 2.866074323654175\n",
      "Loss at step 70450 : 3.4548416137695312\n",
      "Loss at step 70500 : 3.7026968002319336\n",
      "Loss at step 70550 : 2.8888466358184814\n",
      "Loss at step 70600 : 2.7211475372314453\n",
      "Loss at step 70650 : 3.7093069553375244\n",
      "Loss at step 70700 : 3.026179313659668\n",
      "Loss at step 70750 : 3.67022442817688\n",
      "Loss at step 70800 : 2.33784818649292\n",
      "Loss at step 70850 : 2.7220349311828613\n",
      "Loss at step 70900 : 2.539214611053467\n",
      "Loss at step 70950 : 2.828831672668457\n",
      "Loss at step 71000 : 2.6033072471618652\n",
      "Loss at step 71050 : 2.3371129035949707\n",
      "Loss at step 71100 : 3.6101410388946533\n",
      "Loss at step 71150 : 2.4931232929229736\n",
      "Loss at step 71200 : 2.7515151500701904\n",
      "Loss at step 71250 : 2.154675006866455\n",
      "Loss at step 71300 : 3.465609312057495\n",
      "Loss at step 71350 : 2.3706142902374268\n",
      "Loss at step 71400 : 2.9352474212646484\n",
      "Loss at step 71450 : 2.4625964164733887\n",
      "Loss at step 71500 : 2.941223621368408\n",
      "Loss at step 71550 : 3.0722696781158447\n",
      "Loss at step 71600 : 2.799814224243164\n",
      "Loss at step 71650 : 1.6501168012619019\n",
      "Loss at step 71700 : 2.3551888465881348\n",
      "Loss at step 71750 : 2.4088473320007324\n",
      "Loss at step 71800 : 3.1241965293884277\n",
      "Loss at step 71850 : 2.370063543319702\n",
      "Loss at step 71900 : 2.767132043838501\n",
      "Loss at step 71950 : 3.119110107421875\n",
      "Loss at step 72000 : 2.0529427528381348\n",
      "Loss at step 72050 : 2.993778705596924\n",
      "Loss at step 72100 : 2.0128393173217773\n",
      "Loss at step 72150 : 3.0730061531066895\n",
      "Loss at step 72200 : 3.1135993003845215\n",
      "Loss at step 72250 : 3.8405981063842773\n",
      "Loss at step 72300 : 2.320727825164795\n",
      "Loss at step 72350 : 2.214071750640869\n",
      "Loss at step 72400 : 2.940716028213501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 72450 : 2.835239887237549\n",
      "Loss at step 72500 : 2.7771620750427246\n",
      "Loss at step 72550 : 3.227940797805786\n",
      "Loss at step 72600 : 3.8316667079925537\n",
      "Loss at step 72650 : 2.503431558609009\n",
      "Loss at step 72700 : 2.429905891418457\n",
      "Loss at step 72750 : 2.811392307281494\n",
      "Loss at step 72800 : 3.4288277626037598\n",
      "Loss at step 72850 : 2.6396255493164062\n",
      "Loss at step 72900 : 2.793365955352783\n",
      "Loss at step 72950 : 3.232410430908203\n",
      "Loss at step 73000 : 2.7831928730010986\n",
      "Loss at step 73050 : 3.149844169616699\n",
      "Loss at step 73100 : 2.9131360054016113\n",
      "Loss at step 73150 : 2.500272274017334\n",
      "Loss at step 73200 : 3.349250316619873\n",
      "Loss at step 73250 : 2.726983070373535\n",
      "Loss at step 73300 : 3.3615610599517822\n",
      "Loss at step 73350 : 2.90185546875\n",
      "Loss at step 73400 : 3.2320075035095215\n",
      "Loss at step 73450 : 2.6366586685180664\n",
      "Loss at step 73500 : 2.712838649749756\n",
      "Loss at step 73550 : 3.060753345489502\n",
      "Loss at step 73600 : 2.344938278198242\n",
      "Loss at step 73650 : 3.216280937194824\n",
      "Loss at step 73700 : 2.670943260192871\n",
      "Loss at step 73750 : 2.273824691772461\n",
      "Loss at step 73800 : 2.826840400695801\n",
      "Loss at step 73850 : 3.1355502605438232\n",
      "Loss at step 73900 : 2.2983131408691406\n",
      "Loss at step 73950 : 2.8660311698913574\n",
      "Loss at step 74000 : 3.209616184234619\n",
      "Loss at step 74050 : 2.495565891265869\n",
      "Loss at step 74100 : 2.6602067947387695\n",
      "Loss at step 74150 : 2.8487389087677\n",
      "Loss at step 74200 : 2.2033915519714355\n",
      "Loss at step 74250 : 2.4970955848693848\n",
      "Loss at step 74300 : 2.1432013511657715\n",
      "Loss at step 74350 : 1.9811174869537354\n",
      "Loss at step 74400 : 2.66082501411438\n",
      "Loss at step 74450 : 2.771446704864502\n",
      "Loss at step 74500 : 3.921203136444092\n",
      "Loss at step 74550 : 3.3117454051971436\n",
      "Loss at step 74600 : 2.654327392578125\n",
      "Loss at step 74650 : 2.2168126106262207\n",
      "Loss at step 74700 : 2.752852439880371\n",
      "Loss at step 74750 : 2.320089340209961\n",
      "Loss at step 74800 : 3.807832717895508\n",
      "Loss at step 74850 : 2.4456799030303955\n",
      "Loss at step 74900 : 4.53249454498291\n",
      "Loss at step 74950 : 3.386971950531006\n",
      "Loss at step 75000 : 2.614928960800171\n",
      "Nearest to tuna: position, garnish, piece, daikon, sashimi,\n",
      "Nearest to rice: zip, topping, wasabi, nori, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, mat, bag, torch, juice,\n",
      "Nearest to sashimi: position, roe, tongs, sieve, air,\n",
      "Nearest to steak: grill, grate, ground, butter, juice,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, liqueur, espresso, egg,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 75050 : 2.9534711837768555\n",
      "Loss at step 75100 : 3.559579849243164\n",
      "Loss at step 75150 : 3.1250836849212646\n",
      "Loss at step 75200 : 2.5685787200927734\n",
      "Loss at step 75250 : 2.070991277694702\n",
      "Loss at step 75300 : 1.6485925912857056\n",
      "Loss at step 75350 : 2.245429277420044\n",
      "Loss at step 75400 : 2.603874683380127\n",
      "Loss at step 75450 : 3.3508973121643066\n",
      "Loss at step 75500 : 2.983241081237793\n",
      "Loss at step 75550 : 2.83796763420105\n",
      "Loss at step 75600 : 2.977879762649536\n",
      "Loss at step 75650 : 2.616006374359131\n",
      "Loss at step 75700 : 2.842623710632324\n",
      "Loss at step 75750 : 3.2213120460510254\n",
      "Loss at step 75800 : 2.6354329586029053\n",
      "Loss at step 75850 : 2.686680793762207\n",
      "Loss at step 75900 : 3.0095114707946777\n",
      "Loss at step 75950 : 2.4521567821502686\n",
      "Loss at step 76000 : 3.0139594078063965\n",
      "Loss at step 76050 : 2.1385345458984375\n",
      "Loss at step 76100 : 2.4952456951141357\n",
      "Loss at step 76150 : 3.3027539253234863\n",
      "Loss at step 76200 : 2.3765370845794678\n",
      "Loss at step 76250 : 2.695465087890625\n",
      "Loss at step 76300 : 2.815840244293213\n",
      "Loss at step 76350 : 3.277221202850342\n",
      "Loss at step 76400 : 2.9922995567321777\n",
      "Loss at step 76450 : 2.35080623626709\n",
      "Loss at step 76500 : 2.357215166091919\n",
      "Loss at step 76550 : 2.431272029876709\n",
      "Loss at step 76600 : 2.313601493835449\n",
      "Loss at step 76650 : 1.9158531427383423\n",
      "Loss at step 76700 : 3.0431840419769287\n",
      "Loss at step 76750 : 2.315887451171875\n",
      "Loss at step 76800 : 2.791694164276123\n",
      "Loss at step 76850 : 2.6235098838806152\n",
      "Loss at step 76900 : 3.4177675247192383\n",
      "Loss at step 76950 : 3.6121859550476074\n",
      "Loss at step 77000 : 2.616544246673584\n",
      "Loss at step 77050 : 2.6670117378234863\n",
      "Loss at step 77100 : 3.1702451705932617\n",
      "Loss at step 77150 : 2.4143717288970947\n",
      "Loss at step 77200 : 2.27903413772583\n",
      "Loss at step 77250 : 2.693030834197998\n",
      "Loss at step 77300 : 3.1944141387939453\n",
      "Loss at step 77350 : 2.8238024711608887\n",
      "Loss at step 77400 : 3.2677016258239746\n",
      "Loss at step 77450 : 3.105909585952759\n",
      "Loss at step 77500 : 3.624666452407837\n",
      "Loss at step 77550 : 2.7210566997528076\n",
      "Loss at step 77600 : 2.8326940536499023\n",
      "Loss at step 77650 : 2.9679369926452637\n",
      "Loss at step 77700 : 3.464820384979248\n",
      "Loss at step 77750 : 2.6734859943389893\n",
      "Loss at step 77800 : 2.7962310314178467\n",
      "Loss at step 77850 : 4.333502769470215\n",
      "Loss at step 77900 : 2.9681107997894287\n",
      "Loss at step 77950 : 3.2157907485961914\n",
      "Loss at step 78000 : 2.2764785289764404\n",
      "Loss at step 78050 : 3.0052707195281982\n",
      "Loss at step 78100 : 2.4284462928771973\n",
      "Loss at step 78150 : 2.8169922828674316\n",
      "Loss at step 78200 : 2.5137362480163574\n",
      "Loss at step 78250 : 2.395354747772217\n",
      "Loss at step 78300 : 2.6146862506866455\n",
      "Loss at step 78350 : 2.499295949935913\n",
      "Loss at step 78400 : 2.9555296897888184\n",
      "Loss at step 78450 : 2.984802007675171\n",
      "Loss at step 78500 : 1.9569077491760254\n",
      "Loss at step 78550 : 3.0215303897857666\n",
      "Loss at step 78600 : 2.8612775802612305\n",
      "Loss at step 78650 : 2.4358620643615723\n",
      "Loss at step 78700 : 2.834526538848877\n",
      "Loss at step 78750 : 2.7094593048095703\n",
      "Loss at step 78800 : 3.684011697769165\n",
      "Loss at step 78850 : 2.168778896331787\n",
      "Loss at step 78900 : 3.1882882118225098\n",
      "Loss at step 78950 : 2.3524606227874756\n",
      "Loss at step 79000 : 3.652393341064453\n",
      "Loss at step 79050 : 2.9680745601654053\n",
      "Loss at step 79100 : 2.302074670791626\n",
      "Loss at step 79150 : 2.6979856491088867\n",
      "Loss at step 79200 : 2.7492337226867676\n",
      "Loss at step 79250 : 2.6499228477478027\n",
      "Loss at step 79300 : 2.537317991256714\n",
      "Loss at step 79350 : 2.803436279296875\n",
      "Loss at step 79400 : 3.5605547428131104\n",
      "Loss at step 79450 : 3.1692771911621094\n",
      "Loss at step 79500 : 2.7855403423309326\n",
      "Loss at step 79550 : 1.9437456130981445\n",
      "Loss at step 79600 : 2.1924939155578613\n",
      "Loss at step 79650 : 3.311650037765503\n",
      "Loss at step 79700 : 2.5490591526031494\n",
      "Loss at step 79750 : 2.8880748748779297\n",
      "Loss at step 79800 : 2.7264955043792725\n",
      "Loss at step 79850 : 2.418837070465088\n",
      "Loss at step 79900 : 2.4301486015319824\n",
      "Loss at step 79950 : 3.0783889293670654\n",
      "Loss at step 80000 : 2.7944467067718506\n",
      "Nearest to tuna: position, garnish, piece, daikon, sashimi,\n",
      "Nearest to rice: wasabi, zip, topping, nori, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, water, cucumber, bag, juice,\n",
      "Nearest to sashimi: roe, position, tongs, sieve, tuna,\n",
      "Nearest to steak: grill, grate, ground, roe, juice,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, liqueur, espresso, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 80050 : 3.8534345626831055\n",
      "Loss at step 80100 : 2.9652748107910156\n",
      "Loss at step 80150 : 3.326592206954956\n",
      "Loss at step 80200 : 3.1642773151397705\n",
      "Loss at step 80250 : 1.678828477859497\n",
      "Loss at step 80300 : 2.416073799133301\n",
      "Loss at step 80350 : 3.631599187850952\n",
      "Loss at step 80400 : 2.4671530723571777\n",
      "Loss at step 80450 : 2.876664161682129\n",
      "Loss at step 80500 : 3.013561725616455\n",
      "Loss at step 80550 : 2.982807159423828\n",
      "Loss at step 80600 : 2.926480293273926\n",
      "Loss at step 80650 : 2.9363198280334473\n",
      "Loss at step 80700 : 2.6147232055664062\n",
      "Loss at step 80750 : 1.9779090881347656\n",
      "Loss at step 80800 : 3.478975534439087\n",
      "Loss at step 80850 : 2.651841402053833\n",
      "Loss at step 80900 : 2.1753292083740234\n",
      "Loss at step 80950 : 3.176466703414917\n",
      "Loss at step 81000 : 3.508873462677002\n",
      "Loss at step 81050 : 3.72087025642395\n",
      "Loss at step 81100 : 2.4829745292663574\n",
      "Loss at step 81150 : 2.2794816493988037\n",
      "Loss at step 81200 : 2.2260122299194336\n",
      "Loss at step 81250 : 1.7662429809570312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 81300 : 2.011659622192383\n",
      "Loss at step 81350 : 4.499887466430664\n",
      "Loss at step 81400 : 2.5535380840301514\n",
      "Loss at step 81450 : 2.703681468963623\n",
      "Loss at step 81500 : 2.569803237915039\n",
      "Loss at step 81550 : 2.997880458831787\n",
      "Loss at step 81600 : 2.3795268535614014\n",
      "Loss at step 81650 : 2.8243799209594727\n",
      "Loss at step 81700 : 2.890063524246216\n",
      "Loss at step 81750 : 2.6973824501037598\n",
      "Loss at step 81800 : 3.4589743614196777\n",
      "Loss at step 81850 : 2.5159687995910645\n",
      "Loss at step 81900 : 2.248070240020752\n",
      "Loss at step 81950 : 3.391977310180664\n",
      "Loss at step 82000 : 2.7870523929595947\n",
      "Loss at step 82050 : 1.9418023824691772\n",
      "Loss at step 82100 : 2.3644893169403076\n",
      "Loss at step 82150 : 2.2425949573516846\n",
      "Loss at step 82200 : 2.9642066955566406\n",
      "Loss at step 82250 : 2.4838762283325195\n",
      "Loss at step 82300 : 2.933774948120117\n",
      "Loss at step 82350 : 2.3543214797973633\n",
      "Loss at step 82400 : 1.882555365562439\n",
      "Loss at step 82450 : 1.5479092597961426\n",
      "Loss at step 82500 : 2.9891200065612793\n",
      "Loss at step 82550 : 3.6194674968719482\n",
      "Loss at step 82600 : 3.129429340362549\n",
      "Loss at step 82650 : 1.955195665359497\n",
      "Loss at step 82700 : 2.9300360679626465\n",
      "Loss at step 82750 : 2.513209819793701\n",
      "Loss at step 82800 : 3.476327896118164\n",
      "Loss at step 82850 : 2.0830156803131104\n",
      "Loss at step 82900 : 2.3946213722229004\n",
      "Loss at step 82950 : 3.2014427185058594\n",
      "Loss at step 83000 : 3.157012939453125\n",
      "Loss at step 83050 : 2.4808521270751953\n",
      "Loss at step 83100 : 2.107300281524658\n",
      "Loss at step 83150 : 2.8551125526428223\n",
      "Loss at step 83200 : 2.556859016418457\n",
      "Loss at step 83250 : 3.67793607711792\n",
      "Loss at step 83300 : 2.4397401809692383\n",
      "Loss at step 83350 : 2.6881003379821777\n",
      "Loss at step 83400 : 3.1723082065582275\n",
      "Loss at step 83450 : 3.675348997116089\n",
      "Loss at step 83500 : 1.8445378541946411\n",
      "Loss at step 83550 : 1.7605912685394287\n",
      "Loss at step 83600 : 1.8798832893371582\n",
      "Loss at step 83650 : 2.7534399032592773\n",
      "Loss at step 83700 : 2.152848720550537\n",
      "Loss at step 83750 : 2.962845802307129\n",
      "Loss at step 83800 : 2.338771104812622\n",
      "Loss at step 83850 : 2.218287467956543\n",
      "Loss at step 83900 : 3.000901699066162\n",
      "Loss at step 83950 : 3.225214719772339\n",
      "Loss at step 84000 : 2.5558345317840576\n",
      "Loss at step 84050 : 2.709589719772339\n",
      "Loss at step 84100 : 2.5875892639160156\n",
      "Loss at step 84150 : 2.546602725982666\n",
      "Loss at step 84200 : 3.6219089031219482\n",
      "Loss at step 84250 : 2.7707619667053223\n",
      "Loss at step 84300 : 2.3952689170837402\n",
      "Loss at step 84350 : 2.382075786590576\n",
      "Loss at step 84400 : 2.270298719406128\n",
      "Loss at step 84450 : 3.2530930042266846\n",
      "Loss at step 84500 : 2.391627311706543\n",
      "Loss at step 84550 : 2.379906177520752\n",
      "Loss at step 84600 : 2.453223705291748\n",
      "Loss at step 84650 : 2.4546637535095215\n",
      "Loss at step 84700 : 2.6620473861694336\n",
      "Loss at step 84750 : 2.997490167617798\n",
      "Loss at step 84800 : 2.058157444000244\n",
      "Loss at step 84850 : 3.5724704265594482\n",
      "Loss at step 84900 : 2.650282859802246\n",
      "Loss at step 84950 : 2.905350685119629\n",
      "Loss at step 85000 : 2.1109883785247803\n",
      "Nearest to tuna: position, garnish, piece, daikon, sashimi,\n",
      "Nearest to rice: wasabi, topping, nori, zip, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, water, cucumber, juice, bag,\n",
      "Nearest to sashimi: roe, position, tongs, sieve, tuna,\n",
      "Nearest to steak: grill, grate, ground, roe, juice,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, liqueur, espresso, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 85050 : 2.112415313720703\n",
      "Loss at step 85100 : 2.866354465484619\n",
      "Loss at step 85150 : 2.420132637023926\n",
      "Loss at step 85200 : 2.0098373889923096\n",
      "Loss at step 85250 : 1.8511836528778076\n",
      "Loss at step 85300 : 2.7548463344573975\n",
      "Loss at step 85350 : 2.0254018306732178\n",
      "Loss at step 85400 : 2.2960593700408936\n",
      "Loss at step 85450 : 3.3903188705444336\n",
      "Loss at step 85500 : 1.6590745449066162\n",
      "Loss at step 85550 : 2.6521658897399902\n",
      "Loss at step 85600 : 2.8629934787750244\n",
      "Loss at step 85650 : 3.0825753211975098\n",
      "Loss at step 85700 : 3.544240713119507\n",
      "Loss at step 85750 : 3.402583122253418\n",
      "Loss at step 85800 : 2.356487512588501\n",
      "Loss at step 85850 : 3.3760690689086914\n",
      "Loss at step 85900 : 2.6462252140045166\n",
      "Loss at step 85950 : 3.188511848449707\n",
      "Loss at step 86000 : 2.8912525177001953\n",
      "Loss at step 86050 : 4.188030242919922\n",
      "Loss at step 86100 : 1.862922191619873\n",
      "Loss at step 86150 : 2.7588276863098145\n",
      "Loss at step 86200 : 3.153346300125122\n",
      "Loss at step 86250 : 3.0146045684814453\n",
      "Loss at step 86300 : 2.723677158355713\n",
      "Loss at step 86350 : 2.3958611488342285\n",
      "Loss at step 86400 : 2.9472193717956543\n",
      "Loss at step 86450 : 2.566079616546631\n",
      "Loss at step 86500 : 2.3098196983337402\n",
      "Loss at step 86550 : 2.8356399536132812\n",
      "Loss at step 86600 : 2.4351859092712402\n",
      "Loss at step 86650 : 3.7654807567596436\n",
      "Loss at step 86700 : 4.041684150695801\n",
      "Loss at step 86750 : 2.2873072624206543\n",
      "Loss at step 86800 : 2.7854928970336914\n",
      "Loss at step 86850 : 2.831547737121582\n",
      "Loss at step 86900 : 2.2237930297851562\n",
      "Loss at step 86950 : 2.3253650665283203\n",
      "Loss at step 87000 : 3.141970157623291\n",
      "Loss at step 87050 : 3.2245030403137207\n",
      "Loss at step 87100 : 2.7648162841796875\n",
      "Loss at step 87150 : 2.3103647232055664\n",
      "Loss at step 87200 : 2.564701795578003\n",
      "Loss at step 87250 : 3.7459716796875\n",
      "Loss at step 87300 : 4.02269172668457\n",
      "Loss at step 87350 : 2.2040839195251465\n",
      "Loss at step 87400 : 2.7494189739227295\n",
      "Loss at step 87450 : 2.128139019012451\n",
      "Loss at step 87500 : 2.7577900886535645\n",
      "Loss at step 87550 : 2.0643439292907715\n",
      "Loss at step 87600 : 2.516737937927246\n",
      "Loss at step 87650 : 1.9977717399597168\n",
      "Loss at step 87700 : 2.367950201034546\n",
      "Loss at step 87750 : 3.309194326400757\n",
      "Loss at step 87800 : 2.7789807319641113\n",
      "Loss at step 87850 : 2.7476720809936523\n",
      "Loss at step 87900 : 2.998964309692383\n",
      "Loss at step 87950 : 2.2545151710510254\n",
      "Loss at step 88000 : 2.87437105178833\n",
      "Loss at step 88050 : 3.288297414779663\n",
      "Loss at step 88100 : 3.629950523376465\n",
      "Loss at step 88150 : 3.07283878326416\n",
      "Loss at step 88200 : 2.4248547554016113\n",
      "Loss at step 88250 : 3.255608081817627\n",
      "Loss at step 88300 : 3.5071330070495605\n",
      "Loss at step 88350 : 3.1697168350219727\n",
      "Loss at step 88400 : 2.6038031578063965\n",
      "Loss at step 88450 : 2.478228807449341\n",
      "Loss at step 88500 : 2.4561071395874023\n",
      "Loss at step 88550 : 3.161224842071533\n",
      "Loss at step 88600 : 2.715857982635498\n",
      "Loss at step 88650 : 2.2594923973083496\n",
      "Loss at step 88700 : 3.022284507751465\n",
      "Loss at step 88750 : 3.3349215984344482\n",
      "Loss at step 88800 : 3.69175386428833\n",
      "Loss at step 88850 : 1.8407645225524902\n",
      "Loss at step 88900 : 2.68327260017395\n",
      "Loss at step 88950 : 2.9425742626190186\n",
      "Loss at step 89000 : 2.91231107711792\n",
      "Loss at step 89050 : 3.144404888153076\n",
      "Loss at step 89100 : 2.8248515129089355\n",
      "Loss at step 89150 : 1.7877216339111328\n",
      "Loss at step 89200 : 2.9193921089172363\n",
      "Loss at step 89250 : 2.828218698501587\n",
      "Loss at step 89300 : 3.964109182357788\n",
      "Loss at step 89350 : 2.6532206535339355\n",
      "Loss at step 89400 : 2.4876980781555176\n",
      "Loss at step 89450 : 2.592711925506592\n",
      "Loss at step 89500 : 2.4462246894836426\n",
      "Loss at step 89550 : 2.3111777305603027\n",
      "Loss at step 89600 : 2.676663398742676\n",
      "Loss at step 89650 : 3.364394187927246\n",
      "Loss at step 89700 : 2.4938273429870605\n",
      "Loss at step 89750 : 1.891557216644287\n",
      "Loss at step 89800 : 3.09494686126709\n",
      "Loss at step 89850 : 2.3036606311798096\n",
      "Loss at step 89900 : 2.325676679611206\n",
      "Loss at step 89950 : 2.317300319671631\n",
      "Loss at step 90000 : 2.8679423332214355\n",
      "Nearest to tuna: position, garnish, piece, daikon, sashimi,\n",
      "Nearest to rice: wasabi, topping, zip, nori, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: water, mat, juice, cucumber, bag,\n",
      "Nearest to sashimi: position, roe, tongs, sieve, tuna,\n",
      "Nearest to steak: grill, grate, ground, juice, roe,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, liqueur, espresso, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 90050 : 2.8357813358306885\n",
      "Loss at step 90100 : 2.399353265762329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 90150 : 2.754478931427002\n",
      "Loss at step 90200 : 2.5134119987487793\n",
      "Loss at step 90250 : 1.5873562097549438\n",
      "Loss at step 90300 : 3.1257567405700684\n",
      "Loss at step 90350 : 2.881063461303711\n",
      "Loss at step 90400 : 2.8527159690856934\n",
      "Loss at step 90450 : 2.671682119369507\n",
      "Loss at step 90500 : 1.9065659046173096\n",
      "Loss at step 90550 : 2.5710244178771973\n",
      "Loss at step 90600 : 3.6186063289642334\n",
      "Loss at step 90650 : 2.3284924030303955\n",
      "Loss at step 90700 : 2.7137246131896973\n",
      "Loss at step 90750 : 2.5731406211853027\n",
      "Loss at step 90800 : 2.3455657958984375\n",
      "Loss at step 90850 : 3.4828639030456543\n",
      "Loss at step 90900 : 2.655729055404663\n",
      "Loss at step 90950 : 2.6008801460266113\n",
      "Loss at step 91000 : 2.7495505809783936\n",
      "Loss at step 91050 : 2.3223822116851807\n",
      "Loss at step 91100 : 3.132114887237549\n",
      "Loss at step 91150 : 2.538097381591797\n",
      "Loss at step 91200 : 2.3441903591156006\n",
      "Loss at step 91250 : 3.3202552795410156\n",
      "Loss at step 91300 : 3.063765525817871\n",
      "Loss at step 91350 : 3.30767822265625\n",
      "Loss at step 91400 : 2.3684144020080566\n",
      "Loss at step 91450 : 2.7433531284332275\n",
      "Loss at step 91500 : 2.829723834991455\n",
      "Loss at step 91550 : 2.6030800342559814\n",
      "Loss at step 91600 : 2.557241916656494\n",
      "Loss at step 91650 : 2.5784013271331787\n",
      "Loss at step 91700 : 2.65439510345459\n",
      "Loss at step 91750 : 2.3855228424072266\n",
      "Loss at step 91800 : 2.5177948474884033\n",
      "Loss at step 91850 : 2.6235737800598145\n",
      "Loss at step 91900 : 2.5422112941741943\n",
      "Loss at step 91950 : 2.825676441192627\n",
      "Loss at step 92000 : 2.405942440032959\n",
      "Loss at step 92050 : 3.075080394744873\n",
      "Loss at step 92100 : 2.9588117599487305\n",
      "Loss at step 92150 : 2.5294389724731445\n",
      "Loss at step 92200 : 2.717620372772217\n",
      "Loss at step 92250 : 2.196587085723877\n",
      "Loss at step 92300 : 2.7795395851135254\n",
      "Loss at step 92350 : 2.857269763946533\n",
      "Loss at step 92400 : 1.7617825269699097\n",
      "Loss at step 92450 : 3.5313520431518555\n",
      "Loss at step 92500 : 2.5904107093811035\n",
      "Loss at step 92550 : 3.2801284790039062\n",
      "Loss at step 92600 : 1.4274604320526123\n",
      "Loss at step 92650 : 3.4555320739746094\n",
      "Loss at step 92700 : 3.354600191116333\n",
      "Loss at step 92750 : 3.365757465362549\n",
      "Loss at step 92800 : 2.9421277046203613\n",
      "Loss at step 92850 : 3.7710251808166504\n",
      "Loss at step 92900 : 1.7903794050216675\n",
      "Loss at step 92950 : 2.8134307861328125\n",
      "Loss at step 93000 : 2.5424118041992188\n",
      "Loss at step 93050 : 4.275585174560547\n",
      "Loss at step 93100 : 3.4359493255615234\n",
      "Loss at step 93150 : 2.9953432083129883\n",
      "Loss at step 93200 : 2.5759005546569824\n",
      "Loss at step 93250 : 3.676804542541504\n",
      "Loss at step 93300 : 3.0633175373077393\n",
      "Loss at step 93350 : 2.38020658493042\n",
      "Loss at step 93400 : 2.5232419967651367\n",
      "Loss at step 93450 : 3.0283749103546143\n",
      "Loss at step 93500 : 3.135571002960205\n",
      "Loss at step 93550 : 2.023660182952881\n",
      "Loss at step 93600 : 3.504725456237793\n",
      "Loss at step 93650 : 2.942244052886963\n",
      "Loss at step 93700 : 2.3048386573791504\n",
      "Loss at step 93750 : 2.51861834526062\n",
      "Loss at step 93800 : 2.3362600803375244\n",
      "Loss at step 93850 : 3.791560649871826\n",
      "Loss at step 93900 : 2.6453115940093994\n",
      "Loss at step 93950 : 3.455202102661133\n",
      "Loss at step 94000 : 2.685530662536621\n",
      "Loss at step 94050 : 3.146817207336426\n",
      "Loss at step 94100 : 3.0090088844299316\n",
      "Loss at step 94150 : 4.0496978759765625\n",
      "Loss at step 94200 : 2.607341766357422\n",
      "Loss at step 94250 : 3.001132011413574\n",
      "Loss at step 94300 : 1.9266694784164429\n",
      "Loss at step 94350 : 2.9362053871154785\n",
      "Loss at step 94400 : 2.878239631652832\n",
      "Loss at step 94450 : 2.758173942565918\n",
      "Loss at step 94500 : 2.7713308334350586\n",
      "Loss at step 94550 : 2.562501907348633\n",
      "Loss at step 94600 : 2.3412580490112305\n",
      "Loss at step 94650 : 2.2216904163360596\n",
      "Loss at step 94700 : 2.4653544425964355\n",
      "Loss at step 94750 : 3.7389626502990723\n",
      "Loss at step 94800 : 3.041262626647949\n",
      "Loss at step 94850 : 2.8884124755859375\n",
      "Loss at step 94900 : 3.313901424407959\n",
      "Loss at step 94950 : 3.5142159461975098\n",
      "Loss at step 95000 : 2.767326831817627\n",
      "Nearest to tuna: garnish, position, piece, daikon, sashimi,\n",
      "Nearest to rice: wasabi, topping, zip, nori, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, water, juice, cucumber, bag,\n",
      "Nearest to sashimi: roe, position, tongs, tomato, sieve,\n",
      "Nearest to steak: grill, grate, ground, butter, juice,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, espresso, liqueur, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 95050 : 2.5312628746032715\n",
      "Loss at step 95100 : 3.0987160205841064\n",
      "Loss at step 95150 : 2.165802478790283\n",
      "Loss at step 95200 : 4.477654457092285\n",
      "Loss at step 95250 : 2.872218608856201\n",
      "Loss at step 95300 : 3.2677011489868164\n",
      "Loss at step 95350 : 2.294445037841797\n",
      "Loss at step 95400 : 2.1392982006073\n",
      "Loss at step 95450 : 2.858388662338257\n",
      "Loss at step 95500 : 3.1434431076049805\n",
      "Loss at step 95550 : 3.0159189701080322\n",
      "Loss at step 95600 : 2.7971270084381104\n",
      "Loss at step 95650 : 2.757014751434326\n",
      "Loss at step 95700 : 2.275790214538574\n",
      "Loss at step 95750 : 3.1254968643188477\n",
      "Loss at step 95800 : 2.5900943279266357\n",
      "Loss at step 95850 : 2.2275123596191406\n",
      "Loss at step 95900 : 2.3903675079345703\n",
      "Loss at step 95950 : 3.3425731658935547\n",
      "Loss at step 96000 : 2.562045097351074\n",
      "Loss at step 96050 : 2.4732275009155273\n",
      "Loss at step 96100 : 2.3195559978485107\n",
      "Loss at step 96150 : 2.3347246646881104\n",
      "Loss at step 96200 : 2.441758632659912\n",
      "Loss at step 96250 : 2.2280373573303223\n",
      "Loss at step 96300 : 1.846808671951294\n",
      "Loss at step 96350 : 2.9563331604003906\n",
      "Loss at step 96400 : 3.597994565963745\n",
      "Loss at step 96450 : 3.2370872497558594\n",
      "Loss at step 96500 : 2.3339595794677734\n",
      "Loss at step 96550 : 2.668523073196411\n",
      "Loss at step 96600 : 2.8701024055480957\n",
      "Loss at step 96650 : 3.139796733856201\n",
      "Loss at step 96700 : 3.231142044067383\n",
      "Loss at step 96750 : 2.8971667289733887\n",
      "Loss at step 96800 : 2.030860424041748\n",
      "Loss at step 96850 : 3.186211585998535\n",
      "Loss at step 96900 : 3.3168458938598633\n",
      "Loss at step 96950 : 3.2761590480804443\n",
      "Loss at step 97000 : 3.3452088832855225\n",
      "Loss at step 97050 : 2.8841962814331055\n",
      "Loss at step 97100 : 2.931729316711426\n",
      "Loss at step 97150 : 2.741237163543701\n",
      "Loss at step 97200 : 3.188903570175171\n",
      "Loss at step 97250 : 3.222749710083008\n",
      "Loss at step 97300 : 2.5097851753234863\n",
      "Loss at step 97350 : 2.6888980865478516\n",
      "Loss at step 97400 : 2.6646881103515625\n",
      "Loss at step 97450 : 2.6692981719970703\n",
      "Loss at step 97500 : 3.248349666595459\n",
      "Loss at step 97550 : 2.9140686988830566\n",
      "Loss at step 97600 : 3.084923267364502\n",
      "Loss at step 97650 : 2.7792890071868896\n",
      "Loss at step 97700 : 3.3703813552856445\n",
      "Loss at step 97750 : 2.4941799640655518\n",
      "Loss at step 97800 : 2.2398877143859863\n",
      "Loss at step 97850 : 2.854294776916504\n",
      "Loss at step 97900 : 3.7812728881835938\n",
      "Loss at step 97950 : 2.736467123031616\n",
      "Loss at step 98000 : 2.393890857696533\n",
      "Loss at step 98050 : 2.5144152641296387\n",
      "Loss at step 98100 : 2.156627655029297\n",
      "Loss at step 98150 : 2.41042160987854\n",
      "Loss at step 98200 : 2.3002982139587402\n",
      "Loss at step 98250 : 2.533273935317993\n",
      "Loss at step 98300 : 2.838031053543091\n",
      "Loss at step 98350 : 3.766724109649658\n",
      "Loss at step 98400 : 2.298102378845215\n",
      "Loss at step 98450 : 2.427900552749634\n",
      "Loss at step 98500 : 3.2626047134399414\n",
      "Loss at step 98550 : 3.3104288578033447\n",
      "Loss at step 98600 : 3.73740553855896\n",
      "Loss at step 98650 : 1.9130580425262451\n",
      "Loss at step 98700 : 2.33601713180542\n",
      "Loss at step 98750 : 3.418572425842285\n",
      "Loss at step 98800 : 2.727128744125366\n",
      "Loss at step 98850 : 2.894742488861084\n",
      "Loss at step 98900 : 3.6515533924102783\n",
      "Loss at step 98950 : 2.752089500427246\n",
      "Loss at step 99000 : 3.0807995796203613\n",
      "Loss at step 99050 : 3.4110090732574463\n",
      "Loss at step 99100 : 2.447594165802002\n",
      "Loss at step 99150 : 2.9807417392730713\n",
      "Loss at step 99200 : 2.1591506004333496\n",
      "Loss at step 99250 : 2.7580432891845703\n",
      "Loss at step 99300 : 3.3414175510406494\n",
      "Loss at step 99350 : 2.4364919662475586\n",
      "Loss at step 99400 : 3.7018322944641113\n",
      "Loss at step 99450 : 2.435309410095215\n",
      "Loss at step 99500 : 3.1099774837493896\n",
      "Loss at step 99550 : 3.113368511199951\n",
      "Loss at step 99600 : 2.5636508464813232\n",
      "Loss at step 99650 : 2.0050528049468994\n",
      "Loss at step 99700 : 3.3056044578552246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 99750 : 2.69389009475708\n",
      "Loss at step 99800 : 3.037076711654663\n",
      "Loss at step 99850 : 2.225803852081299\n",
      "Loss at step 99900 : 2.5593080520629883\n",
      "Loss at step 99950 : 3.587294578552246\n",
      "Loss at step 100000 : 2.2335100173950195\n",
      "Nearest to tuna: position, garnish, piece, daikon, flesh,\n",
      "Nearest to rice: topping, wasabi, zip, nori, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, water, cucumber, juice, bag,\n",
      "Nearest to sashimi: position, roe, tongs, tomato, sieve,\n",
      "Nearest to steak: grill, grate, ground, butter, juice,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, starch, salt, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, espresso, liqueur, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 100050 : 2.8030056953430176\n",
      "Loss at step 100100 : 3.0960402488708496\n",
      "Loss at step 100150 : 3.174388885498047\n",
      "Loss at step 100200 : 2.4992380142211914\n",
      "Loss at step 100250 : 2.741189956665039\n",
      "Loss at step 100300 : 2.458639621734619\n",
      "Loss at step 100350 : 3.547427177429199\n",
      "Loss at step 100400 : 2.360607147216797\n",
      "Loss at step 100450 : 2.3377180099487305\n",
      "Loss at step 100500 : 2.7411248683929443\n",
      "Loss at step 100550 : 3.20601224899292\n",
      "Loss at step 100600 : 3.3373074531555176\n",
      "Loss at step 100650 : 2.6871085166931152\n",
      "Loss at step 100700 : 2.420048713684082\n",
      "Loss at step 100750 : 2.8168210983276367\n",
      "Loss at step 100800 : 2.867642402648926\n",
      "Loss at step 100850 : 2.852360248565674\n",
      "Loss at step 100900 : 2.7230746746063232\n",
      "Loss at step 100950 : 2.43491792678833\n",
      "Loss at step 101000 : 2.1865837574005127\n",
      "Loss at step 101050 : 2.7137627601623535\n",
      "Loss at step 101100 : 2.2133612632751465\n",
      "Loss at step 101150 : 3.313202381134033\n",
      "Loss at step 101200 : 1.3143644332885742\n",
      "Loss at step 101250 : 1.9467312097549438\n",
      "Loss at step 101300 : 3.0046067237854004\n",
      "Loss at step 101350 : 2.9738998413085938\n",
      "Loss at step 101400 : 3.0213630199432373\n",
      "Loss at step 101450 : 2.326620578765869\n",
      "Loss at step 101500 : 4.7719879150390625\n",
      "Loss at step 101550 : 1.902801513671875\n",
      "Loss at step 101600 : 2.910808563232422\n",
      "Loss at step 101650 : 3.1015212535858154\n",
      "Loss at step 101700 : 3.105745315551758\n",
      "Loss at step 101750 : 2.950427532196045\n",
      "Loss at step 101800 : 2.3821606636047363\n",
      "Loss at step 101850 : 2.642571210861206\n",
      "Loss at step 101900 : 2.6834802627563477\n",
      "Loss at step 101950 : 2.73895263671875\n",
      "Loss at step 102000 : 2.824969530105591\n",
      "Loss at step 102050 : 3.057527542114258\n",
      "Loss at step 102100 : 3.8507561683654785\n",
      "Loss at step 102150 : 3.2865066528320312\n",
      "Loss at step 102200 : 3.305600166320801\n",
      "Loss at step 102250 : 2.5431160926818848\n",
      "Loss at step 102300 : 3.3277087211608887\n",
      "Loss at step 102350 : 2.1643290519714355\n",
      "Loss at step 102400 : 3.4388039112091064\n",
      "Loss at step 102450 : 3.3570594787597656\n",
      "Loss at step 102500 : 2.959721088409424\n",
      "Loss at step 102550 : 2.848497152328491\n",
      "Loss at step 102600 : 2.5918936729431152\n",
      "Loss at step 102650 : 2.99411678314209\n",
      "Loss at step 102700 : 1.687626838684082\n",
      "Loss at step 102750 : 3.199425220489502\n",
      "Loss at step 102800 : 2.2679684162139893\n",
      "Loss at step 102850 : 2.7188775539398193\n",
      "Loss at step 102900 : 3.0028481483459473\n",
      "Loss at step 102950 : 3.2715437412261963\n",
      "Loss at step 103000 : 3.359083414077759\n",
      "Loss at step 103050 : 2.650543451309204\n",
      "Loss at step 103100 : 3.2128958702087402\n",
      "Loss at step 103150 : 2.0752577781677246\n",
      "Loss at step 103200 : 3.2573184967041016\n",
      "Loss at step 103250 : 3.188720703125\n",
      "Loss at step 103300 : 3.09375262260437\n",
      "Loss at step 103350 : 2.3622584342956543\n",
      "Loss at step 103400 : 2.8740100860595703\n",
      "Loss at step 103450 : 3.32827091217041\n",
      "Loss at step 103500 : 2.426835060119629\n",
      "Loss at step 103550 : 4.0797953605651855\n",
      "Loss at step 103600 : 3.38765287399292\n",
      "Loss at step 103650 : 3.0516576766967773\n",
      "Loss at step 103700 : 2.1544833183288574\n",
      "Loss at step 103750 : 3.0724692344665527\n",
      "Loss at step 103800 : 2.3167214393615723\n",
      "Loss at step 103850 : 1.9071303606033325\n",
      "Loss at step 103900 : 2.1549789905548096\n",
      "Loss at step 103950 : 3.1515355110168457\n",
      "Loss at step 104000 : 3.4032516479492188\n",
      "Loss at step 104050 : 2.401203155517578\n",
      "Loss at step 104100 : 2.9919633865356445\n",
      "Loss at step 104150 : 3.3350441455841064\n",
      "Loss at step 104200 : 2.3244597911834717\n",
      "Loss at step 104250 : 3.1613316535949707\n",
      "Loss at step 104300 : 3.576040744781494\n",
      "Loss at step 104350 : 3.037322998046875\n",
      "Loss at step 104400 : 2.259716510772705\n",
      "Loss at step 104450 : 3.0103631019592285\n",
      "Loss at step 104500 : 2.8927547931671143\n",
      "Loss at step 104550 : 3.302227735519409\n",
      "Loss at step 104600 : 3.376833915710449\n",
      "Loss at step 104650 : 2.58212947845459\n",
      "Loss at step 104700 : 2.854367971420288\n",
      "Loss at step 104750 : 3.238551616668701\n",
      "Loss at step 104800 : 2.9203920364379883\n",
      "Loss at step 104850 : 2.789656639099121\n",
      "Loss at step 104900 : 2.8062243461608887\n",
      "Loss at step 104950 : 3.391695022583008\n",
      "Loss at step 105000 : 3.004136085510254\n",
      "Nearest to tuna: garnish, position, piece, daikon, flesh,\n",
      "Nearest to rice: topping, wasabi, zip, nori, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, cucumber, juice, water, fryer,\n",
      "Nearest to sashimi: position, roe, tongs, tomato, sieve,\n",
      "Nearest to steak: grill, grate, ground, roe, butter,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, liqueur, espresso, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 105050 : 3.224562644958496\n",
      "Loss at step 105100 : 2.304142951965332\n",
      "Loss at step 105150 : 2.8775343894958496\n",
      "Loss at step 105200 : 3.6299538612365723\n",
      "Loss at step 105250 : 3.5135691165924072\n",
      "Loss at step 105300 : 2.729635238647461\n",
      "Loss at step 105350 : 3.0027050971984863\n",
      "Loss at step 105400 : 2.400352716445923\n",
      "Loss at step 105450 : 2.87548828125\n",
      "Loss at step 105500 : 2.6217408180236816\n",
      "Loss at step 105550 : 3.0401768684387207\n",
      "Loss at step 105600 : 2.6511712074279785\n",
      "Loss at step 105650 : 2.6681904792785645\n",
      "Loss at step 105700 : 2.909966468811035\n",
      "Loss at step 105750 : 2.751771926879883\n",
      "Loss at step 105800 : 2.8483786582946777\n",
      "Loss at step 105850 : 2.4897241592407227\n",
      "Loss at step 105900 : 3.1816353797912598\n",
      "Loss at step 105950 : 3.306583881378174\n",
      "Loss at step 106000 : 2.490445613861084\n",
      "Loss at step 106050 : 2.9412899017333984\n",
      "Loss at step 106100 : 3.795182466506958\n",
      "Loss at step 106150 : 3.0922751426696777\n",
      "Loss at step 106200 : 3.3764514923095703\n",
      "Loss at step 106250 : 2.426401138305664\n",
      "Loss at step 106300 : 2.6478378772735596\n",
      "Loss at step 106350 : 3.306638479232788\n",
      "Loss at step 106400 : 3.129690408706665\n",
      "Loss at step 106450 : 2.363455057144165\n",
      "Loss at step 106500 : 2.9456372261047363\n",
      "Loss at step 106550 : 2.8874192237854004\n",
      "Loss at step 106600 : 2.998826265335083\n",
      "Loss at step 106650 : 2.918714761734009\n",
      "Loss at step 106700 : 2.1963343620300293\n",
      "Loss at step 106750 : 2.965142250061035\n",
      "Loss at step 106800 : 2.5538039207458496\n",
      "Loss at step 106850 : 2.9601497650146484\n",
      "Loss at step 106900 : 2.131903648376465\n",
      "Loss at step 106950 : 2.7521300315856934\n",
      "Loss at step 107000 : 2.6386637687683105\n",
      "Loss at step 107050 : 2.498351573944092\n",
      "Loss at step 107100 : 2.49611496925354\n",
      "Loss at step 107150 : 3.4973068237304688\n",
      "Loss at step 107200 : 3.0364062786102295\n",
      "Loss at step 107250 : 2.7696118354797363\n",
      "Loss at step 107300 : 2.8692493438720703\n",
      "Loss at step 107350 : 2.4986393451690674\n",
      "Loss at step 107400 : 3.1903984546661377\n",
      "Loss at step 107450 : 3.6811485290527344\n",
      "Loss at step 107500 : 2.775991439819336\n",
      "Loss at step 107550 : 1.9717693328857422\n",
      "Loss at step 107600 : 3.0979790687561035\n",
      "Loss at step 107650 : 2.6685824394226074\n",
      "Loss at step 107700 : 2.489280939102173\n",
      "Loss at step 107750 : 3.162900447845459\n",
      "Loss at step 107800 : 3.137539863586426\n",
      "Loss at step 107850 : 2.4077014923095703\n",
      "Loss at step 107900 : 2.5369808673858643\n",
      "Loss at step 107950 : 3.1598434448242188\n",
      "Loss at step 108000 : 2.8251285552978516\n",
      "Loss at step 108050 : 3.1578211784362793\n",
      "Loss at step 108100 : 2.3558239936828613\n",
      "Loss at step 108150 : 1.740813970565796\n",
      "Loss at step 108200 : 2.964291572570801\n",
      "Loss at step 108250 : 2.698617935180664\n",
      "Loss at step 108300 : 2.511587142944336\n",
      "Loss at step 108350 : 2.571078062057495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 108400 : 2.0848398208618164\n",
      "Loss at step 108450 : 2.3084802627563477\n",
      "Loss at step 108500 : 2.975525140762329\n",
      "Loss at step 108550 : 3.1846909523010254\n",
      "Loss at step 108600 : 3.064213752746582\n",
      "Loss at step 108650 : 2.6032116413116455\n",
      "Loss at step 108700 : 2.1921956539154053\n",
      "Loss at step 108750 : 2.848402500152588\n",
      "Loss at step 108800 : 3.223109722137451\n",
      "Loss at step 108850 : 3.4826738834381104\n",
      "Loss at step 108900 : 2.57625675201416\n",
      "Loss at step 108950 : 1.8792083263397217\n",
      "Loss at step 109000 : 1.9469335079193115\n",
      "Loss at step 109050 : 3.0029666423797607\n",
      "Loss at step 109100 : 3.2528493404388428\n",
      "Loss at step 109150 : 2.3698477745056152\n",
      "Loss at step 109200 : 2.7410011291503906\n",
      "Loss at step 109250 : 2.619145393371582\n",
      "Loss at step 109300 : 3.3910136222839355\n",
      "Loss at step 109350 : 2.399054527282715\n",
      "Loss at step 109400 : 2.921407461166382\n",
      "Loss at step 109450 : 2.554615020751953\n",
      "Loss at step 109500 : 3.694849729537964\n",
      "Loss at step 109550 : 3.2086081504821777\n",
      "Loss at step 109600 : 2.756709575653076\n",
      "Loss at step 109650 : 3.135673761367798\n",
      "Loss at step 109700 : 2.8541219234466553\n",
      "Loss at step 109750 : 2.554192066192627\n",
      "Loss at step 109800 : 2.5351343154907227\n",
      "Loss at step 109850 : 2.7433114051818848\n",
      "Loss at step 109900 : 3.049220085144043\n",
      "Loss at step 109950 : 2.438779830932617\n",
      "Loss at step 110000 : 2.7264206409454346\n",
      "Nearest to tuna: garnish, position, piece, daikon, sashimi,\n",
      "Nearest to rice: topping, wasabi, zip, nori, slice,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, cucumber, juice, fryer, water,\n",
      "Nearest to sashimi: roe, position, tongs, tomato, tuna,\n",
      "Nearest to steak: grill, grate, ground, roe, butter,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, liqueur, espresso, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 110050 : 2.940485954284668\n",
      "Loss at step 110100 : 3.477652072906494\n",
      "Loss at step 110150 : 3.964694023132324\n",
      "Loss at step 110200 : 2.798475980758667\n",
      "Loss at step 110250 : 2.7568435668945312\n",
      "Loss at step 110300 : 3.0467045307159424\n",
      "Loss at step 110350 : 3.2668778896331787\n",
      "Loss at step 110400 : 2.597797393798828\n",
      "Loss at step 110450 : 1.8341400623321533\n",
      "Loss at step 110500 : 3.4663283824920654\n",
      "Loss at step 110550 : 3.369361400604248\n",
      "Loss at step 110600 : 2.412038803100586\n",
      "Loss at step 110650 : 3.012486696243286\n",
      "Loss at step 110700 : 3.3879785537719727\n",
      "Loss at step 110750 : 2.568293333053589\n",
      "Loss at step 110800 : 2.768712043762207\n",
      "Loss at step 110850 : 3.4368884563446045\n",
      "Loss at step 110900 : 3.2994651794433594\n",
      "Loss at step 110950 : 2.4915060997009277\n",
      "Loss at step 111000 : 3.5291547775268555\n",
      "Loss at step 111050 : 2.555455207824707\n",
      "Loss at step 111100 : 2.742739200592041\n",
      "Loss at step 111150 : 2.838925361633301\n",
      "Loss at step 111200 : 2.7835633754730225\n",
      "Loss at step 111250 : 3.2110910415649414\n",
      "Loss at step 111300 : 2.450503349304199\n",
      "Loss at step 111350 : 3.1515023708343506\n",
      "Loss at step 111400 : 3.402592658996582\n",
      "Loss at step 111450 : 2.257871150970459\n",
      "Loss at step 111500 : 2.9076309204101562\n",
      "Loss at step 111550 : 2.27266263961792\n",
      "Loss at step 111600 : 2.3253915309906006\n",
      "Loss at step 111650 : 3.254824161529541\n",
      "Loss at step 111700 : 2.881833553314209\n",
      "Loss at step 111750 : 3.5736782550811768\n",
      "Loss at step 111800 : 2.8469645977020264\n",
      "Loss at step 111850 : 2.472062587738037\n",
      "Loss at step 111900 : 3.005495548248291\n",
      "Loss at step 111950 : 1.7886101007461548\n",
      "Loss at step 112000 : 3.678800106048584\n",
      "Loss at step 112050 : 3.4179844856262207\n",
      "Loss at step 112100 : 2.9482338428497314\n",
      "Loss at step 112150 : 2.616623640060425\n",
      "Loss at step 112200 : 2.3615882396698\n",
      "Loss at step 112250 : 1.847091794013977\n",
      "Loss at step 112300 : 2.837986946105957\n",
      "Loss at step 112350 : 3.4729957580566406\n",
      "Loss at step 112400 : 1.4069894552230835\n",
      "Loss at step 112450 : 2.9791512489318848\n",
      "Loss at step 112500 : 3.6925389766693115\n",
      "Loss at step 112550 : 2.6861424446105957\n",
      "Loss at step 112600 : 2.376020908355713\n",
      "Loss at step 112650 : 1.7907087802886963\n",
      "Loss at step 112700 : 2.6997761726379395\n",
      "Loss at step 112750 : 2.3149986267089844\n",
      "Loss at step 112800 : 3.1587576866149902\n",
      "Loss at step 112850 : 2.870687484741211\n",
      "Loss at step 112900 : 2.6940999031066895\n",
      "Loss at step 112950 : 3.6434431076049805\n",
      "Loss at step 113000 : 2.484127998352051\n",
      "Loss at step 113050 : 3.449075698852539\n",
      "Loss at step 113100 : 3.5347747802734375\n",
      "Loss at step 113150 : 2.729487895965576\n",
      "Loss at step 113200 : 3.050902843475342\n",
      "Loss at step 113250 : 2.2125744819641113\n",
      "Loss at step 113300 : 2.272382974624634\n",
      "Loss at step 113350 : 3.020665168762207\n",
      "Loss at step 113400 : 3.31648588180542\n",
      "Loss at step 113450 : 2.3110108375549316\n",
      "Loss at step 113500 : 2.95127010345459\n",
      "Loss at step 113550 : 2.395033359527588\n",
      "Loss at step 113600 : 2.575469493865967\n",
      "Loss at step 113650 : 2.0965304374694824\n",
      "Loss at step 113700 : 2.7924108505249023\n",
      "Loss at step 113750 : 2.645167112350464\n",
      "Loss at step 113800 : 2.4179482460021973\n",
      "Loss at step 113850 : 3.1641201972961426\n",
      "Loss at step 113900 : 2.284909725189209\n",
      "Loss at step 113950 : 3.7048792839050293\n",
      "Loss at step 114000 : 2.8114006519317627\n",
      "Loss at step 114050 : 2.5093612670898438\n",
      "Loss at step 114100 : 3.3311426639556885\n",
      "Loss at step 114150 : 2.5044922828674316\n",
      "Loss at step 114200 : 2.1890344619750977\n",
      "Loss at step 114250 : 2.1489715576171875\n",
      "Loss at step 114300 : 2.33449649810791\n",
      "Loss at step 114350 : 2.7560486793518066\n",
      "Loss at step 114400 : 3.132777690887451\n",
      "Loss at step 114450 : 2.28639817237854\n",
      "Loss at step 114500 : 3.371316432952881\n",
      "Loss at step 114550 : 2.088015079498291\n",
      "Loss at step 114600 : 2.5838732719421387\n",
      "Loss at step 114650 : 3.0259757041931152\n",
      "Loss at step 114700 : 2.3670780658721924\n",
      "Loss at step 114750 : 2.252563953399658\n",
      "Loss at step 114800 : 2.385451078414917\n",
      "Loss at step 114850 : 2.8009629249572754\n",
      "Loss at step 114900 : 2.443373680114746\n",
      "Loss at step 114950 : 3.35386323928833\n",
      "Loss at step 115000 : 3.2437734603881836\n",
      "Nearest to tuna: garnish, position, piece, daikon, flesh,\n",
      "Nearest to rice: wasabi, topping, zip, slice, nori,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, cucumber, juice, fryer, bag,\n",
      "Nearest to sashimi: roe, position, tongs, tomato, salt,\n",
      "Nearest to steak: grill, grate, ground, roe, butter,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, tobikko,\n",
      "Nearest to cream: cocoa, cheese, espresso, liqueur, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 115050 : 2.230536460876465\n",
      "Loss at step 115100 : 2.6484456062316895\n",
      "Loss at step 115150 : 2.4042954444885254\n",
      "Loss at step 115200 : 3.408047676086426\n",
      "Loss at step 115250 : 1.9082890748977661\n",
      "Loss at step 115300 : 1.7709109783172607\n",
      "Loss at step 115350 : 2.6126832962036133\n",
      "Loss at step 115400 : 3.035402297973633\n",
      "Loss at step 115450 : 3.006328582763672\n",
      "Loss at step 115500 : 1.9518983364105225\n",
      "Loss at step 115550 : 2.058896780014038\n",
      "Loss at step 115600 : 2.0323452949523926\n",
      "Loss at step 115650 : 2.9724550247192383\n",
      "Loss at step 115700 : 3.0638620853424072\n",
      "Loss at step 115750 : 3.346756935119629\n",
      "Loss at step 115800 : 2.898510694503784\n",
      "Loss at step 115850 : 2.60945200920105\n",
      "Loss at step 115900 : 2.192471504211426\n",
      "Loss at step 115950 : 3.9504663944244385\n",
      "Loss at step 116000 : 2.965592384338379\n",
      "Loss at step 116050 : 2.959803581237793\n",
      "Loss at step 116100 : 3.3655266761779785\n",
      "Loss at step 116150 : 3.157442331314087\n",
      "Loss at step 116200 : 2.772754430770874\n",
      "Loss at step 116250 : 2.4079785346984863\n",
      "Loss at step 116300 : 3.696359872817993\n",
      "Loss at step 116350 : 2.2706780433654785\n",
      "Loss at step 116400 : 2.2053627967834473\n",
      "Loss at step 116450 : 2.3026657104492188\n",
      "Loss at step 116500 : 1.8161444664001465\n",
      "Loss at step 116550 : 2.6462602615356445\n",
      "Loss at step 116600 : 3.0347342491149902\n",
      "Loss at step 116650 : 2.9262425899505615\n",
      "Loss at step 116700 : 2.646749496459961\n",
      "Loss at step 116750 : 3.1199209690093994\n",
      "Loss at step 116800 : 2.6638755798339844\n",
      "Loss at step 116850 : 2.649423599243164\n",
      "Loss at step 116900 : 1.391700029373169\n",
      "Loss at step 116950 : 2.184887409210205\n",
      "Loss at step 117000 : 2.0068583488464355\n",
      "Loss at step 117050 : 2.5686287879943848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 117100 : 2.4730947017669678\n",
      "Loss at step 117150 : 2.4122138023376465\n",
      "Loss at step 117200 : 2.6966304779052734\n",
      "Loss at step 117250 : 3.1598126888275146\n",
      "Loss at step 117300 : 2.2808499336242676\n",
      "Loss at step 117350 : 3.5021047592163086\n",
      "Loss at step 117400 : 3.343390941619873\n",
      "Loss at step 117450 : 2.1424031257629395\n",
      "Loss at step 117500 : 2.3209517002105713\n",
      "Loss at step 117550 : 3.5543582439422607\n",
      "Loss at step 117600 : 3.3199617862701416\n",
      "Loss at step 117650 : 2.3815622329711914\n",
      "Loss at step 117700 : 2.666400909423828\n",
      "Loss at step 117750 : 2.602346897125244\n",
      "Loss at step 117800 : 2.860574722290039\n",
      "Loss at step 117850 : 2.934966564178467\n",
      "Loss at step 117900 : 2.343893527984619\n",
      "Loss at step 117950 : 3.2052860260009766\n",
      "Loss at step 118000 : 2.5860743522644043\n",
      "Loss at step 118050 : 1.9199939966201782\n",
      "Loss at step 118100 : 2.1926040649414062\n",
      "Loss at step 118150 : 2.9700584411621094\n",
      "Loss at step 118200 : 2.4439964294433594\n",
      "Loss at step 118250 : 2.647764205932617\n",
      "Loss at step 118300 : 3.2780723571777344\n",
      "Loss at step 118350 : 3.683649778366089\n",
      "Loss at step 118400 : 2.7772817611694336\n",
      "Loss at step 118450 : 2.1009247303009033\n",
      "Loss at step 118500 : 3.4941649436950684\n",
      "Loss at step 118550 : 2.0361342430114746\n",
      "Loss at step 118600 : 2.676732063293457\n",
      "Loss at step 118650 : 2.5723910331726074\n",
      "Loss at step 118700 : 3.7121920585632324\n",
      "Loss at step 118750 : 2.7613866329193115\n",
      "Loss at step 118800 : 2.637977123260498\n",
      "Loss at step 118850 : 2.8491549491882324\n",
      "Loss at step 118900 : 3.3373427391052246\n",
      "Loss at step 118950 : 3.1060197353363037\n",
      "Loss at step 119000 : 3.301093578338623\n",
      "Loss at step 119050 : 2.228517532348633\n",
      "Loss at step 119100 : 2.35850191116333\n",
      "Loss at step 119150 : 3.0059099197387695\n",
      "Loss at step 119200 : 3.097280502319336\n",
      "Loss at step 119250 : 3.081578254699707\n",
      "Loss at step 119300 : 3.20263934135437\n",
      "Loss at step 119350 : 2.531599283218384\n",
      "Loss at step 119400 : 3.0798163414001465\n",
      "Loss at step 119450 : 2.727667808532715\n",
      "Loss at step 119500 : 3.0746378898620605\n",
      "Loss at step 119550 : 2.717522144317627\n",
      "Loss at step 119600 : 2.410569667816162\n",
      "Loss at step 119650 : 1.6845630407333374\n",
      "Loss at step 119700 : 2.4461326599121094\n",
      "Loss at step 119750 : 3.4863295555114746\n",
      "Loss at step 119800 : 3.136157989501953\n",
      "Loss at step 119850 : 2.7857797145843506\n",
      "Loss at step 119900 : 3.220669746398926\n",
      "Loss at step 119950 : 3.056812286376953\n",
      "Loss at step 120000 : 2.3069710731506348\n",
      "Nearest to tuna: garnish, position, piece, daikon, sashimi,\n",
      "Nearest to rice: zip, wasabi, topping, slice, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, juice, cucumber, fryer, torch,\n",
      "Nearest to sashimi: roe, position, tongs, sieve, tomato,\n",
      "Nearest to steak: grill, grate, ground, roe, butter,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, slice,\n",
      "Nearest to cream: cocoa, cheese, espresso, liqueur, mixer,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 120050 : 3.6790590286254883\n",
      "Loss at step 120100 : 2.4913296699523926\n",
      "Loss at step 120150 : 2.3810131549835205\n",
      "Loss at step 120200 : 2.9009220600128174\n",
      "Loss at step 120250 : 3.4344747066497803\n",
      "Loss at step 120300 : 2.363363027572632\n",
      "Loss at step 120350 : 2.9854061603546143\n",
      "Loss at step 120400 : 3.2721948623657227\n",
      "Loss at step 120450 : 2.579545497894287\n",
      "Loss at step 120500 : 3.1960651874542236\n",
      "Loss at step 120550 : 3.1979637145996094\n",
      "Loss at step 120600 : 3.7942557334899902\n",
      "Loss at step 120650 : 2.353787422180176\n",
      "Loss at step 120700 : 3.091390609741211\n",
      "Loss at step 120750 : 3.237118721008301\n",
      "Loss at step 120800 : 1.8484978675842285\n",
      "Loss at step 120850 : 3.439060926437378\n",
      "Loss at step 120900 : 2.9747376441955566\n",
      "Loss at step 120950 : 2.1413979530334473\n",
      "Loss at step 121000 : 3.259206771850586\n",
      "Loss at step 121050 : 2.60252046585083\n",
      "Loss at step 121100 : 2.451555013656616\n",
      "Loss at step 121150 : 2.3188488483428955\n",
      "Loss at step 121200 : 2.672691583633423\n",
      "Loss at step 121250 : 3.0366413593292236\n",
      "Loss at step 121300 : 3.066352128982544\n",
      "Loss at step 121350 : 2.9740676879882812\n",
      "Loss at step 121400 : 2.7842230796813965\n",
      "Loss at step 121450 : 2.775792121887207\n",
      "Loss at step 121500 : 3.522399663925171\n",
      "Loss at step 121550 : 2.5544047355651855\n",
      "Loss at step 121600 : 2.903651237487793\n",
      "Loss at step 121650 : 2.614074945449829\n",
      "Loss at step 121700 : 2.815384864807129\n",
      "Loss at step 121750 : 2.801985740661621\n",
      "Loss at step 121800 : 1.8462169170379639\n",
      "Loss at step 121850 : 2.65673828125\n",
      "Loss at step 121900 : 2.9522805213928223\n",
      "Loss at step 121950 : 2.9669899940490723\n",
      "Loss at step 122000 : 2.653809070587158\n",
      "Loss at step 122050 : 3.619790554046631\n",
      "Loss at step 122100 : 2.431807041168213\n",
      "Loss at step 122150 : 2.523756742477417\n",
      "Loss at step 122200 : 3.659318447113037\n",
      "Loss at step 122250 : 3.106912612915039\n",
      "Loss at step 122300 : 3.218496084213257\n",
      "Loss at step 122350 : 2.8319146633148193\n",
      "Loss at step 122400 : 3.316267490386963\n",
      "Loss at step 122450 : 2.180565118789673\n",
      "Loss at step 122500 : 3.3885140419006348\n",
      "Loss at step 122550 : 2.753143787384033\n",
      "Loss at step 122600 : 2.51282000541687\n",
      "Loss at step 122650 : 2.526247262954712\n",
      "Loss at step 122700 : 2.53688383102417\n",
      "Loss at step 122750 : 2.703413963317871\n",
      "Loss at step 122800 : 2.6767256259918213\n",
      "Loss at step 122850 : 3.4716544151306152\n",
      "Loss at step 122900 : 3.5084972381591797\n",
      "Loss at step 122950 : 2.5278892517089844\n",
      "Loss at step 123000 : 2.727410316467285\n",
      "Loss at step 123050 : 2.84810733795166\n",
      "Loss at step 123100 : 2.5294265747070312\n",
      "Loss at step 123150 : 2.814659595489502\n",
      "Loss at step 123200 : 2.201066732406616\n",
      "Loss at step 123250 : 3.2266695499420166\n",
      "Loss at step 123300 : 2.512463092803955\n",
      "Loss at step 123350 : 2.5133750438690186\n",
      "Loss at step 123400 : 3.0856199264526367\n",
      "Loss at step 123450 : 3.1147170066833496\n",
      "Loss at step 123500 : 2.191793918609619\n",
      "Loss at step 123550 : 3.1930453777313232\n",
      "Loss at step 123600 : 2.6312735080718994\n",
      "Loss at step 123650 : 4.083565711975098\n",
      "Loss at step 123700 : 3.295158863067627\n",
      "Loss at step 123750 : 2.9783713817596436\n",
      "Loss at step 123800 : 2.430792808532715\n",
      "Loss at step 123850 : 3.0809526443481445\n",
      "Loss at step 123900 : 2.3111908435821533\n",
      "Loss at step 123950 : 2.677152156829834\n",
      "Loss at step 124000 : 3.0716397762298584\n",
      "Loss at step 124050 : 3.01347017288208\n",
      "Loss at step 124100 : 2.670520305633545\n",
      "Loss at step 124150 : 2.7969634532928467\n",
      "Loss at step 124200 : 1.8975186347961426\n",
      "Loss at step 124250 : 2.715752124786377\n",
      "Loss at step 124300 : 2.549201488494873\n",
      "Loss at step 124350 : 2.7631406784057617\n",
      "Loss at step 124400 : 2.7331528663635254\n",
      "Loss at step 124450 : 3.5412802696228027\n",
      "Loss at step 124500 : 2.8820390701293945\n",
      "Loss at step 124550 : 3.0687966346740723\n",
      "Loss at step 124600 : 3.01243257522583\n",
      "Loss at step 124650 : 2.7592501640319824\n",
      "Loss at step 124700 : 2.8601021766662598\n",
      "Loss at step 124750 : 2.3560433387756348\n",
      "Loss at step 124800 : 2.765103816986084\n",
      "Loss at step 124850 : 2.5685741901397705\n",
      "Loss at step 124900 : 2.96531081199646\n",
      "Loss at step 124950 : 3.640057325363159\n",
      "Loss at step 125000 : 2.951385974884033\n",
      "Nearest to tuna: garnish, position, piece, daikon, flesh,\n",
      "Nearest to rice: zip, topping, wasabi, roe, surface,\n",
      "Nearest to sushi: mat, finger, wasabi, coriander, bamboo,\n",
      "Nearest to roll: mat, juice, bag, cucumber, fryer,\n",
      "Nearest to sashimi: roe, position, tongs, sieve, tuna,\n",
      "Nearest to steak: grill, grate, ground, butter, roe,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, slice,\n",
      "Nearest to cream: cocoa, cheese, espresso, liqueur, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 125050 : 3.042353868484497\n",
      "Loss at step 125100 : 2.5313892364501953\n",
      "Loss at step 125150 : 3.2934842109680176\n",
      "Loss at step 125200 : 2.9859814643859863\n",
      "Loss at step 125250 : 3.007009744644165\n",
      "Loss at step 125300 : 2.0628886222839355\n",
      "Loss at step 125350 : 2.121255397796631\n",
      "Loss at step 125400 : 2.9455413818359375\n",
      "Loss at step 125450 : 2.0568950176239014\n",
      "Loss at step 125500 : 2.855926513671875\n",
      "Loss at step 125550 : 3.687070369720459\n",
      "Loss at step 125600 : 3.1170239448547363\n",
      "Loss at step 125650 : 2.3532555103302\n",
      "Loss at step 125700 : 3.2089414596557617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 125750 : 2.6168832778930664\n",
      "Loss at step 125800 : 2.561819076538086\n",
      "Loss at step 125850 : 3.469022750854492\n",
      "Loss at step 125900 : 2.579840660095215\n",
      "Loss at step 125950 : 2.0793581008911133\n",
      "Loss at step 126000 : 2.8418796062469482\n",
      "Loss at step 126050 : 3.437980890274048\n",
      "Loss at step 126100 : 2.5949440002441406\n",
      "Loss at step 126150 : 2.982875347137451\n",
      "Loss at step 126200 : 3.9629790782928467\n",
      "Loss at step 126250 : 2.2363693714141846\n",
      "Loss at step 126300 : 3.058103322982788\n",
      "Loss at step 126350 : 2.9096641540527344\n",
      "Loss at step 126400 : 3.2689194679260254\n",
      "Loss at step 126450 : 2.999751091003418\n",
      "Loss at step 126500 : 2.4088375568389893\n",
      "Loss at step 126550 : 3.511256694793701\n",
      "Loss at step 126600 : 2.6139168739318848\n",
      "Loss at step 126650 : 2.649796724319458\n",
      "Loss at step 126700 : 2.4519379138946533\n",
      "Loss at step 126750 : 4.096801280975342\n",
      "Loss at step 126800 : 2.4029364585876465\n",
      "Loss at step 126850 : 2.9331753253936768\n",
      "Loss at step 126900 : 2.985381603240967\n",
      "Loss at step 126950 : 3.2213594913482666\n",
      "Loss at step 127000 : 2.8597638607025146\n",
      "Loss at step 127050 : 2.225036859512329\n",
      "Loss at step 127100 : 3.3135857582092285\n",
      "Loss at step 127150 : 3.157578945159912\n",
      "Loss at step 127200 : 2.258789300918579\n",
      "Loss at step 127250 : 2.425093650817871\n",
      "Loss at step 127300 : 3.366687774658203\n",
      "Loss at step 127350 : 2.653059482574463\n",
      "Loss at step 127400 : 2.6308069229125977\n",
      "Loss at step 127450 : 2.974435567855835\n",
      "Loss at step 127500 : 2.248852252960205\n",
      "Loss at step 127550 : 2.800098180770874\n",
      "Loss at step 127600 : 2.3126726150512695\n",
      "Loss at step 127650 : 2.90789794921875\n",
      "Loss at step 127700 : 2.401768684387207\n",
      "Loss at step 127750 : 3.5323617458343506\n",
      "Loss at step 127800 : 2.841662883758545\n",
      "Loss at step 127850 : 2.7085325717926025\n",
      "Loss at step 127900 : 2.525444984436035\n",
      "Loss at step 127950 : 2.358391046524048\n",
      "Loss at step 128000 : 2.464890718460083\n",
      "Loss at step 128050 : 1.9520657062530518\n",
      "Loss at step 128100 : 2.8982222080230713\n",
      "Loss at step 128150 : 2.831254482269287\n",
      "Loss at step 128200 : 2.991053342819214\n",
      "Loss at step 128250 : 1.7015286684036255\n",
      "Loss at step 128300 : 3.0823447704315186\n",
      "Loss at step 128350 : 3.0427403450012207\n",
      "Loss at step 128400 : 2.8954758644104004\n",
      "Loss at step 128450 : 3.4787259101867676\n",
      "Loss at step 128500 : 2.5344767570495605\n",
      "Loss at step 128550 : 2.6672396659851074\n",
      "Loss at step 128600 : 3.3469183444976807\n",
      "Loss at step 128650 : 2.8705618381500244\n",
      "Loss at step 128700 : 3.0374159812927246\n",
      "Loss at step 128750 : 2.5502219200134277\n",
      "Loss at step 128800 : 3.116694927215576\n",
      "Loss at step 128850 : 3.782899856567383\n",
      "Loss at step 128900 : 2.5389866828918457\n",
      "Loss at step 128950 : 3.3367815017700195\n",
      "Loss at step 129000 : 3.2453129291534424\n",
      "Loss at step 129050 : 2.619961977005005\n",
      "Loss at step 129100 : 2.9455349445343018\n",
      "Loss at step 129150 : 3.568519353866577\n",
      "Loss at step 129200 : 3.0159876346588135\n",
      "Loss at step 129250 : 2.7772116661071777\n",
      "Loss at step 129300 : 2.843245267868042\n",
      "Loss at step 129350 : 2.867783308029175\n",
      "Loss at step 129400 : 2.471142292022705\n",
      "Loss at step 129450 : 2.230191230773926\n",
      "Loss at step 129500 : 2.6729586124420166\n",
      "Loss at step 129550 : 1.7841262817382812\n",
      "Loss at step 129600 : 2.714779853820801\n",
      "Loss at step 129650 : 2.4244868755340576\n",
      "Loss at step 129700 : 3.240386486053467\n",
      "Loss at step 129750 : 2.7238192558288574\n",
      "Loss at step 129800 : 2.5185701847076416\n",
      "Loss at step 129850 : 3.2351036071777344\n",
      "Loss at step 129900 : 2.616894245147705\n",
      "Loss at step 129950 : 2.851285219192505\n",
      "Loss at step 130000 : 2.849674701690674\n",
      "Nearest to tuna: garnish, position, piece, daikon, cucumber,\n",
      "Nearest to rice: wasabi, topping, zip, slice, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, cucumber, juice, bag, torch,\n",
      "Nearest to sashimi: roe, position, tongs, tomato, sieve,\n",
      "Nearest to steak: grill, grate, ground, butter, marinade,\n",
      "Nearest to grill: ground, steak, grate, marinade, tobikko,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, sieve,\n",
      "Nearest to cream: cocoa, cheese, espresso, spread, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 130050 : 2.775019645690918\n",
      "Loss at step 130100 : 2.901277780532837\n",
      "Loss at step 130150 : 2.9239797592163086\n",
      "Loss at step 130200 : 2.2285256385803223\n",
      "Loss at step 130250 : 2.5174388885498047\n",
      "Loss at step 130300 : 2.8179993629455566\n",
      "Loss at step 130350 : 2.4281764030456543\n",
      "Loss at step 130400 : 2.012796401977539\n",
      "Loss at step 130450 : 2.4411895275115967\n",
      "Loss at step 130500 : 3.4356465339660645\n",
      "Loss at step 130550 : 3.169985294342041\n",
      "Loss at step 130600 : 2.0150489807128906\n",
      "Loss at step 130650 : 1.993399739265442\n",
      "Loss at step 130700 : 2.4837398529052734\n",
      "Loss at step 130750 : 2.3563363552093506\n",
      "Loss at step 130800 : 3.0403101444244385\n",
      "Loss at step 130850 : 3.57779860496521\n",
      "Loss at step 130900 : 2.898761034011841\n",
      "Loss at step 130950 : 3.249640464782715\n",
      "Loss at step 131000 : 1.5808727741241455\n",
      "Loss at step 131050 : 2.4031734466552734\n",
      "Loss at step 131100 : 2.1837239265441895\n",
      "Loss at step 131150 : 2.7570855617523193\n",
      "Loss at step 131200 : 2.48174786567688\n",
      "Loss at step 131250 : 2.7447609901428223\n",
      "Loss at step 131300 : 2.754831075668335\n",
      "Loss at step 131350 : 2.4629132747650146\n",
      "Loss at step 131400 : 1.8816372156143188\n",
      "Loss at step 131450 : 2.937990665435791\n",
      "Loss at step 131500 : 2.151790142059326\n",
      "Loss at step 131550 : 2.2527825832366943\n",
      "Loss at step 131600 : 2.6219615936279297\n",
      "Loss at step 131650 : 2.9486770629882812\n",
      "Loss at step 131700 : 2.372720956802368\n",
      "Loss at step 131750 : 2.8054537773132324\n",
      "Loss at step 131800 : 3.0665953159332275\n",
      "Loss at step 131850 : 2.544724941253662\n",
      "Loss at step 131900 : 3.0466108322143555\n",
      "Loss at step 131950 : 3.4888272285461426\n",
      "Loss at step 132000 : 3.854175567626953\n",
      "Loss at step 132050 : 2.768345832824707\n",
      "Loss at step 132100 : 2.735224723815918\n",
      "Loss at step 132150 : 3.5747971534729004\n",
      "Loss at step 132200 : 2.3923451900482178\n",
      "Loss at step 132250 : 2.3308212757110596\n",
      "Loss at step 132300 : 2.914318323135376\n",
      "Loss at step 132350 : 1.5384035110473633\n",
      "Loss at step 132400 : 3.5858852863311768\n",
      "Loss at step 132450 : 3.769749164581299\n",
      "Loss at step 132500 : 2.572916030883789\n",
      "Loss at step 132550 : 1.969651460647583\n",
      "Loss at step 132600 : 2.7956414222717285\n",
      "Loss at step 132650 : 2.775087833404541\n",
      "Loss at step 132700 : 2.9960789680480957\n",
      "Loss at step 132750 : 2.317187786102295\n",
      "Loss at step 132800 : 2.8406624794006348\n",
      "Loss at step 132850 : 2.759692668914795\n",
      "Loss at step 132900 : 2.6203601360321045\n",
      "Loss at step 132950 : 3.5813231468200684\n",
      "Loss at step 133000 : 2.7090249061584473\n",
      "Loss at step 133050 : 2.275174140930176\n",
      "Loss at step 133100 : 2.7870326042175293\n",
      "Loss at step 133150 : 3.0860931873321533\n",
      "Loss at step 133200 : 2.420933485031128\n",
      "Loss at step 133250 : 2.6980104446411133\n",
      "Loss at step 133300 : 3.5068166255950928\n",
      "Loss at step 133350 : 2.5468623638153076\n",
      "Loss at step 133400 : 2.0099639892578125\n",
      "Loss at step 133450 : 3.073669910430908\n",
      "Loss at step 133500 : 3.1367006301879883\n",
      "Loss at step 133550 : 2.6988883018493652\n",
      "Loss at step 133600 : 3.3566174507141113\n",
      "Loss at step 133650 : 2.8001084327697754\n",
      "Loss at step 133700 : 2.810934543609619\n",
      "Loss at step 133750 : 3.31893253326416\n",
      "Loss at step 133800 : 2.766683340072632\n",
      "Loss at step 133850 : 3.0453810691833496\n",
      "Loss at step 133900 : 1.788301944732666\n",
      "Loss at step 133950 : 3.0185587406158447\n",
      "Loss at step 134000 : 3.497133731842041\n",
      "Loss at step 134050 : 3.9392998218536377\n",
      "Loss at step 134100 : 2.5040838718414307\n",
      "Loss at step 134150 : 2.7154526710510254\n",
      "Loss at step 134200 : 3.115910053253174\n",
      "Loss at step 134250 : 3.6727776527404785\n",
      "Loss at step 134300 : 2.206331729888916\n",
      "Loss at step 134350 : 2.7183709144592285\n",
      "Loss at step 134400 : 2.2819137573242188\n",
      "Loss at step 134450 : 2.8978304862976074\n",
      "Loss at step 134500 : 3.7951138019561768\n",
      "Loss at step 134550 : 3.801525115966797\n",
      "Loss at step 134600 : 2.1087541580200195\n",
      "Loss at step 134650 : 2.6923348903656006\n",
      "Loss at step 134700 : 2.6722164154052734\n",
      "Loss at step 134750 : 3.824242115020752\n",
      "Loss at step 134800 : 3.2375662326812744\n",
      "Loss at step 134850 : 2.979550838470459\n",
      "Loss at step 134900 : 3.434027910232544\n",
      "Loss at step 134950 : 2.4259450435638428\n",
      "Loss at step 135000 : 2.740227699279785\n",
      "Nearest to tuna: garnish, position, piece, daikon, flesh,\n",
      "Nearest to rice: topping, wasabi, zip, surface, slice,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, juice, cucumber, bag, coriander,\n",
      "Nearest to sashimi: roe, position, tongs, sieve, tomato,\n",
      "Nearest to steak: grill, grate, ground, butter, marinade,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, sieve,\n",
      "Nearest to cream: cocoa, cheese, espresso, liqueur, spread,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 135050 : 2.7496399879455566\n",
      "Loss at step 135100 : 2.8176350593566895\n",
      "Loss at step 135150 : 2.0006027221679688\n",
      "Loss at step 135200 : 2.703397750854492\n",
      "Loss at step 135250 : 2.9042506217956543\n",
      "Loss at step 135300 : 3.518138885498047\n",
      "Loss at step 135350 : 2.605198621749878\n",
      "Loss at step 135400 : 3.1075048446655273\n",
      "Loss at step 135450 : 1.793884038925171\n",
      "Loss at step 135500 : 2.2500569820404053\n",
      "Loss at step 135550 : 2.134159564971924\n",
      "Loss at step 135600 : 2.702195644378662\n",
      "Loss at step 135650 : 2.6887593269348145\n",
      "Loss at step 135700 : 2.769425868988037\n",
      "Loss at step 135750 : 3.108044147491455\n",
      "Loss at step 135800 : 3.1715450286865234\n",
      "Loss at step 135850 : 2.3753268718719482\n",
      "Loss at step 135900 : 3.898165225982666\n",
      "Loss at step 135950 : 2.5351128578186035\n",
      "Loss at step 136000 : 2.698298692703247\n",
      "Loss at step 136050 : 3.061746120452881\n",
      "Loss at step 136100 : 2.741283893585205\n",
      "Loss at step 136150 : 2.927743434906006\n",
      "Loss at step 136200 : 2.786742687225342\n",
      "Loss at step 136250 : 2.958181619644165\n",
      "Loss at step 136300 : 2.5046372413635254\n",
      "Loss at step 136350 : 2.830873727798462\n",
      "Loss at step 136400 : 2.617717742919922\n",
      "Loss at step 136450 : 2.6561570167541504\n",
      "Loss at step 136500 : 2.293477773666382\n",
      "Loss at step 136550 : 2.5092780590057373\n",
      "Loss at step 136600 : 3.2272043228149414\n",
      "Loss at step 136650 : 2.721587896347046\n",
      "Loss at step 136700 : 2.8040547370910645\n",
      "Loss at step 136750 : 2.6777637004852295\n",
      "Loss at step 136800 : 2.7758936882019043\n",
      "Loss at step 136850 : 1.402864694595337\n",
      "Loss at step 136900 : 2.637979030609131\n",
      "Loss at step 136950 : 2.511943817138672\n",
      "Loss at step 137000 : 2.542712926864624\n",
      "Loss at step 137050 : 1.99555504322052\n",
      "Loss at step 137100 : 2.903987169265747\n",
      "Loss at step 137150 : 2.3007888793945312\n",
      "Loss at step 137200 : 3.1553711891174316\n",
      "Loss at step 137250 : 2.2414495944976807\n",
      "Loss at step 137300 : 2.6778652667999268\n",
      "Loss at step 137350 : 2.460805892944336\n",
      "Loss at step 137400 : 2.0496487617492676\n",
      "Loss at step 137450 : 2.861389636993408\n",
      "Loss at step 137500 : 3.686232566833496\n",
      "Loss at step 137550 : 2.462515115737915\n",
      "Loss at step 137600 : 2.750870704650879\n",
      "Loss at step 137650 : 2.7102134227752686\n",
      "Loss at step 137700 : 2.85073184967041\n",
      "Loss at step 137750 : 2.7231383323669434\n",
      "Loss at step 137800 : 2.1424663066864014\n",
      "Loss at step 137850 : 2.446824312210083\n",
      "Loss at step 137900 : 2.375880718231201\n",
      "Loss at step 137950 : 3.8195996284484863\n",
      "Loss at step 138000 : 2.8485817909240723\n",
      "Loss at step 138050 : 3.1378254890441895\n",
      "Loss at step 138100 : 3.295898914337158\n",
      "Loss at step 138150 : 2.365537643432617\n",
      "Loss at step 138200 : 3.00734806060791\n",
      "Loss at step 138250 : 3.0116937160491943\n",
      "Loss at step 138300 : 2.7769289016723633\n",
      "Loss at step 138350 : 2.3073017597198486\n",
      "Loss at step 138400 : 3.1186492443084717\n",
      "Loss at step 138450 : 2.426847219467163\n",
      "Loss at step 138500 : 2.9284582138061523\n",
      "Loss at step 138550 : 3.223001480102539\n",
      "Loss at step 138600 : 3.824664354324341\n",
      "Loss at step 138650 : 3.598735809326172\n",
      "Loss at step 138700 : 2.6836097240448\n",
      "Loss at step 138750 : 3.614353895187378\n",
      "Loss at step 138800 : 2.3582539558410645\n",
      "Loss at step 138850 : 2.8858447074890137\n",
      "Loss at step 138900 : 2.928231716156006\n",
      "Loss at step 138950 : 2.9754257202148438\n",
      "Loss at step 139000 : 1.833411693572998\n",
      "Loss at step 139050 : 2.3508496284484863\n",
      "Loss at step 139100 : 2.1642706394195557\n",
      "Loss at step 139150 : 2.5699338912963867\n",
      "Loss at step 139200 : 2.843122959136963\n",
      "Loss at step 139250 : 3.263361930847168\n",
      "Loss at step 139300 : 2.87739634513855\n",
      "Loss at step 139350 : 2.302356719970703\n",
      "Loss at step 139400 : 2.355165481567383\n",
      "Loss at step 139450 : 2.984119176864624\n",
      "Loss at step 139500 : 2.3562843799591064\n",
      "Loss at step 139550 : 2.9278008937835693\n",
      "Loss at step 139600 : 2.6172471046447754\n",
      "Loss at step 139650 : 3.574932336807251\n",
      "Loss at step 139700 : 3.319361686706543\n",
      "Loss at step 139750 : 2.70963454246521\n",
      "Loss at step 139800 : 2.1983466148376465\n",
      "Loss at step 139850 : 3.4428787231445312\n",
      "Loss at step 139900 : 2.793958902359009\n",
      "Loss at step 139950 : 3.3515098094940186\n",
      "Loss at step 140000 : 3.000202178955078\n",
      "Nearest to tuna: garnish, position, piece, daikon, flesh,\n",
      "Nearest to rice: zip, topping, slice, wasabi, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, cucumber, juice, bag, sushi,\n",
      "Nearest to sashimi: roe, position, tongs, sieve, tomato,\n",
      "Nearest to steak: grill, grate, ground, butter, juice,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, sieve,\n",
      "Nearest to cream: cocoa, cheese, espresso, spread, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 140050 : 2.4789631366729736\n",
      "Loss at step 140100 : 1.9471838474273682\n",
      "Loss at step 140150 : 3.9059665203094482\n",
      "Loss at step 140200 : 2.839630603790283\n",
      "Loss at step 140250 : 3.004361629486084\n",
      "Loss at step 140300 : 2.1396820545196533\n",
      "Loss at step 140350 : 2.5657596588134766\n",
      "Loss at step 140400 : 2.01725435256958\n",
      "Loss at step 140450 : 2.602344512939453\n",
      "Loss at step 140500 : 2.1983590126037598\n",
      "Loss at step 140550 : 2.764887809753418\n",
      "Loss at step 140600 : 2.3073666095733643\n",
      "Loss at step 140650 : 3.131227731704712\n",
      "Loss at step 140700 : 3.5089149475097656\n",
      "Loss at step 140750 : 2.7764129638671875\n",
      "Loss at step 140800 : 2.667153835296631\n",
      "Loss at step 140850 : 2.9261555671691895\n",
      "Loss at step 140900 : 1.7300642728805542\n",
      "Loss at step 140950 : 2.5682597160339355\n",
      "Loss at step 141000 : 2.6201772689819336\n",
      "Loss at step 141050 : 2.741245746612549\n",
      "Loss at step 141100 : 2.7100257873535156\n",
      "Loss at step 141150 : 2.4232711791992188\n",
      "Loss at step 141200 : 2.810044527053833\n",
      "Loss at step 141250 : 2.572762966156006\n",
      "Loss at step 141300 : 3.605943202972412\n",
      "Loss at step 141350 : 3.6386218070983887\n",
      "Loss at step 141400 : 2.818861484527588\n",
      "Loss at step 141450 : 2.786808490753174\n",
      "Loss at step 141500 : 2.25393009185791\n",
      "Loss at step 141550 : 2.263380289077759\n",
      "Loss at step 141600 : 2.5611047744750977\n",
      "Loss at step 141650 : 2.0279340744018555\n",
      "Loss at step 141700 : 2.403156280517578\n",
      "Loss at step 141750 : 2.347904920578003\n",
      "Loss at step 141800 : 2.6510376930236816\n",
      "Loss at step 141850 : 3.377023220062256\n",
      "Loss at step 141900 : 2.6310973167419434\n",
      "Loss at step 141950 : 3.5670528411865234\n",
      "Loss at step 142000 : 2.5077643394470215\n",
      "Loss at step 142050 : 1.6628391742706299\n",
      "Loss at step 142100 : 2.4686903953552246\n",
      "Loss at step 142150 : 2.7154016494750977\n",
      "Loss at step 142200 : 2.559469223022461\n",
      "Loss at step 142250 : 2.932544708251953\n",
      "Loss at step 142300 : 2.6042914390563965\n",
      "Loss at step 142350 : 2.574456214904785\n",
      "Loss at step 142400 : 2.9233779907226562\n",
      "Loss at step 142450 : 3.193105697631836\n",
      "Loss at step 142500 : 2.9765639305114746\n",
      "Loss at step 142550 : 2.940354824066162\n",
      "Loss at step 142600 : 3.0156712532043457\n",
      "Loss at step 142650 : 2.950277805328369\n",
      "Loss at step 142700 : 2.9598095417022705\n",
      "Loss at step 142750 : 2.4196159839630127\n",
      "Loss at step 142800 : 3.675527572631836\n",
      "Loss at step 142850 : 2.8304495811462402\n",
      "Loss at step 142900 : 2.8099489212036133\n",
      "Loss at step 142950 : 3.1865041255950928\n",
      "Loss at step 143000 : 2.2186427116394043\n",
      "Loss at step 143050 : 3.0988101959228516\n",
      "Loss at step 143100 : 1.6347664594650269\n",
      "Loss at step 143150 : 2.1362411975860596\n",
      "Loss at step 143200 : 2.4049360752105713\n",
      "Loss at step 143250 : 1.7993090152740479\n",
      "Loss at step 143300 : 3.262953996658325\n",
      "Loss at step 143350 : 3.452507495880127\n",
      "Loss at step 143400 : 2.9938488006591797\n",
      "Loss at step 143450 : 3.092043876647949\n",
      "Loss at step 143500 : 2.03208589553833\n",
      "Loss at step 143550 : 2.7080137729644775\n",
      "Loss at step 143600 : 2.8712892532348633\n",
      "Loss at step 143650 : 2.9172234535217285\n",
      "Loss at step 143700 : 3.2451395988464355\n",
      "Loss at step 143750 : 2.441075325012207\n",
      "Loss at step 143800 : 1.9736665487289429\n",
      "Loss at step 143850 : 3.7001185417175293\n",
      "Loss at step 143900 : 2.5369627475738525\n",
      "Loss at step 143950 : 2.987701416015625\n",
      "Loss at step 144000 : 2.996885299682617\n",
      "Loss at step 144050 : 2.970369815826416\n",
      "Loss at step 144100 : 2.2271060943603516\n",
      "Loss at step 144150 : 2.311488151550293\n",
      "Loss at step 144200 : 2.8716135025024414\n",
      "Loss at step 144250 : 2.4589216709136963\n",
      "Loss at step 144300 : 2.8384604454040527\n",
      "Loss at step 144350 : 2.776541233062744\n",
      "Loss at step 144400 : 2.7600297927856445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 144450 : 2.5048327445983887\n",
      "Loss at step 144500 : 1.6058177947998047\n",
      "Loss at step 144550 : 2.5688512325286865\n",
      "Loss at step 144600 : 2.8496179580688477\n",
      "Loss at step 144650 : 2.653223991394043\n",
      "Loss at step 144700 : 3.5360212326049805\n",
      "Loss at step 144750 : 2.8213844299316406\n",
      "Loss at step 144800 : 2.6500370502471924\n",
      "Loss at step 144850 : 3.151747226715088\n",
      "Loss at step 144900 : 2.591503620147705\n",
      "Loss at step 144950 : 2.436814308166504\n",
      "Loss at step 145000 : 2.690741539001465\n",
      "Nearest to tuna: garnish, position, piece, daikon, flesh,\n",
      "Nearest to rice: zip, topping, slice, wasabi, surface,\n",
      "Nearest to sushi: mat, wasabi, finger, coriander, bamboo,\n",
      "Nearest to roll: mat, cucumber, juice, bag, fryer,\n",
      "Nearest to sashimi: roe, position, tongs, tomato, sieve,\n",
      "Nearest to steak: grill, grate, ground, butter, roe,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, sieve,\n",
      "Nearest to cream: cocoa, cheese, espresso, spread, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 145050 : 2.518711566925049\n",
      "Loss at step 145100 : 2.501990556716919\n",
      "Loss at step 145150 : 3.046255111694336\n",
      "Loss at step 145200 : 2.2180533409118652\n",
      "Loss at step 145250 : 2.020331382751465\n",
      "Loss at step 145300 : 2.627223491668701\n",
      "Loss at step 145350 : 2.4726641178131104\n",
      "Loss at step 145400 : 2.395921230316162\n",
      "Loss at step 145450 : 2.1855125427246094\n",
      "Loss at step 145500 : 2.603306770324707\n",
      "Loss at step 145550 : 2.550605297088623\n",
      "Loss at step 145600 : 2.7092792987823486\n",
      "Loss at step 145650 : 2.2937235832214355\n",
      "Loss at step 145700 : 2.422530174255371\n",
      "Loss at step 145750 : 2.5989575386047363\n",
      "Loss at step 145800 : 2.5828211307525635\n",
      "Loss at step 145850 : 2.444993495941162\n",
      "Loss at step 145900 : 1.8808863162994385\n",
      "Loss at step 145950 : 2.3754591941833496\n",
      "Loss at step 146000 : 2.945805311203003\n",
      "Loss at step 146050 : 2.6945807933807373\n",
      "Loss at step 146100 : 3.1697139739990234\n",
      "Loss at step 146150 : 3.197634696960449\n",
      "Loss at step 146200 : 2.736022472381592\n",
      "Loss at step 146250 : 3.119192600250244\n",
      "Loss at step 146300 : 2.661198139190674\n",
      "Loss at step 146350 : 2.961587429046631\n",
      "Loss at step 146400 : 3.8899028301239014\n",
      "Loss at step 146450 : 2.9232230186462402\n",
      "Loss at step 146500 : 3.943049192428589\n",
      "Loss at step 146550 : 2.575004816055298\n",
      "Loss at step 146600 : 2.5965700149536133\n",
      "Loss at step 146650 : 1.919865608215332\n",
      "Loss at step 146700 : 2.9411187171936035\n",
      "Loss at step 146750 : 1.7968709468841553\n",
      "Loss at step 146800 : 3.4318840503692627\n",
      "Loss at step 146850 : 2.409830093383789\n",
      "Loss at step 146900 : 2.18170166015625\n",
      "Loss at step 146950 : 2.208002805709839\n",
      "Loss at step 147000 : 3.334885835647583\n",
      "Loss at step 147050 : 3.3883590698242188\n",
      "Loss at step 147100 : 2.8795104026794434\n",
      "Loss at step 147150 : 2.0241899490356445\n",
      "Loss at step 147200 : 2.9884190559387207\n",
      "Loss at step 147250 : 3.1614904403686523\n",
      "Loss at step 147300 : 2.9552149772644043\n",
      "Loss at step 147350 : 3.0671701431274414\n",
      "Loss at step 147400 : 2.351421356201172\n",
      "Loss at step 147450 : 2.8229265213012695\n",
      "Loss at step 147500 : 2.7187905311584473\n",
      "Loss at step 147550 : 3.3066818714141846\n",
      "Loss at step 147600 : 2.200747013092041\n",
      "Loss at step 147650 : 2.5387046337127686\n",
      "Loss at step 147700 : 2.01999568939209\n",
      "Loss at step 147750 : 2.7293193340301514\n",
      "Loss at step 147800 : 2.2717628479003906\n",
      "Loss at step 147850 : 2.751067638397217\n",
      "Loss at step 147900 : 1.7089122533798218\n",
      "Loss at step 147950 : 2.6819231510162354\n",
      "Loss at step 148000 : 2.7702269554138184\n",
      "Loss at step 148050 : 2.423830509185791\n",
      "Loss at step 148100 : 3.3807003498077393\n",
      "Loss at step 148150 : 3.587317943572998\n",
      "Loss at step 148200 : 3.74489164352417\n",
      "Loss at step 148250 : 2.589977741241455\n",
      "Loss at step 148300 : 2.1736083030700684\n",
      "Loss at step 148350 : 2.9053030014038086\n",
      "Loss at step 148400 : 2.200329303741455\n",
      "Loss at step 148450 : 2.227217435836792\n",
      "Loss at step 148500 : 2.7428627014160156\n",
      "Loss at step 148550 : 2.763096809387207\n",
      "Loss at step 148600 : 3.361693859100342\n",
      "Loss at step 148650 : 3.1671218872070312\n",
      "Loss at step 148700 : 3.266746997833252\n",
      "Loss at step 148750 : 2.286201000213623\n",
      "Loss at step 148800 : 2.9371914863586426\n",
      "Loss at step 148850 : 3.418570041656494\n",
      "Loss at step 148900 : 3.1492161750793457\n",
      "Loss at step 148950 : 2.1638376712799072\n",
      "Loss at step 149000 : 1.6867029666900635\n",
      "Loss at step 149050 : 3.151702404022217\n",
      "Loss at step 149100 : 2.7652835845947266\n",
      "Loss at step 149150 : 1.7885494232177734\n",
      "Loss at step 149200 : 3.357377052307129\n",
      "Loss at step 149250 : 2.3244221210479736\n",
      "Loss at step 149300 : 2.194895029067993\n",
      "Loss at step 149350 : 2.6041877269744873\n",
      "Loss at step 149400 : 3.206979513168335\n",
      "Loss at step 149450 : 2.559284210205078\n",
      "Loss at step 149500 : 2.6968588829040527\n",
      "Loss at step 149550 : 2.968266487121582\n",
      "Loss at step 149600 : 3.377166748046875\n",
      "Loss at step 149650 : 3.124727725982666\n",
      "Loss at step 149700 : 3.7701895236968994\n",
      "Loss at step 149750 : 3.476222038269043\n",
      "Loss at step 149800 : 2.860718011856079\n",
      "Loss at step 149850 : 2.5304460525512695\n",
      "Loss at step 149900 : 3.079625129699707\n",
      "Loss at step 149950 : 2.660231590270996\n",
      "Loss at step 150000 : 2.633059024810791\n",
      "Nearest to tuna: garnish, position, piece, daikon, flesh,\n",
      "Nearest to rice: zip, slice, topping, wasabi, surface,\n",
      "Nearest to sushi: mat, finger, wasabi, coriander, bamboo,\n",
      "Nearest to roll: mat, juice, cucumber, bag, sushi,\n",
      "Nearest to sashimi: roe, position, tongs, tomato, sieve,\n",
      "Nearest to steak: grill, grate, ground, butter, juice,\n",
      "Nearest to grill: ground, steak, grate, marinade, meat,\n",
      "Nearest to sauce: soy, salt, starch, vinegar, sieve,\n",
      "Nearest to cream: cocoa, cheese, espresso, spread, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/train/3/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name=\"word_embeddings\")\n",
    "doc_embeddings = tf.Variable(tf.random_uniform([len(preprocessed_texts), doc_embedding_size], -1.0, 1.0), name=\"doc_embeddings\")\n",
    "\n",
    "decoder_weights = tf.Variable(tf.truncated_normal([vocabulary_size, concatenated_size],\n",
    "                                               stddev=1.0 / np.sqrt(concatenated_size)),\n",
    "                                               name=\"decoder_weights\")\n",
    "decoder_biases = tf.Variable(tf.zeros([vocabulary_size]), name=\"decoder_biases\")\n",
    "\n",
    "\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[None, 2]) #1 for word index and 1 for doc index\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embed= tf.nn.embedding_lookup(embeddings, x_inputs[:, 0])\n",
    "    \n",
    "doc_indices = tf.slice(x_inputs, [0,1],[batch_size,1])\n",
    "doc_embed = tf.nn.embedding_lookup(doc_embeddings,doc_indices)\n",
    "final_embed = tf.concat([embed, tf.squeeze(doc_embed)],1)\n",
    "\n",
    "logits = tf.matmul(final_embed, tf.transpose(decoder_weights)) + decoder_biases\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_target))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=model_learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "#cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True, name=\"cosine_similarity\")\n",
    "\n",
    "with tf.name_scope(\"performance\"):\n",
    "    loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "    loss_summary = tf.summary.scalar('loss', loss_ph)\n",
    "performance_summaries = tf.summary.merge([loss_summary])\n",
    "\n",
    "#saver = tf.train.Saver({\"embeddings\": embeddings, \"doc_embeddings\": doc_embeddings})\n",
    "saver = tf.train.Saver({\"embeddings\": embeddings, \"doc_embeddings\": doc_embeddings, \"decoder_weights\":decoder_weights, \"decoder_biases\":decoder_biases})\n",
    "summ_writer = tf.summary.FileWriter(summaries_folder_name, sess.graph)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "print('Starting Training')\n",
    "\n",
    "#loss_vec = []\n",
    "#loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = create_batch_data(text_data)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    #run the train step\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    #return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        summ = sess.run(performance_summaries, feed_dict={loss_ph:loss_val})\n",
    "        summ_writer.add_summary(summ, i+1)\n",
    "        #loss_vec.append(loss_val)\n",
    "        #loss_x_vec.append(i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "        \n",
    "    #validation\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    #save dictionary + embeddings\n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        #save vocabulary dictionary\n",
    "        with open(os.path.join(models_folder_name,'doc2vec_recipes_dict_words_integers.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        #save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),models_folder_name,'doc2vec_recipes_checkpoint.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))\n",
    "        \n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
