{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tiramisu': 3, 'sashimi': 0, 'sushi': 2, 'steak': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>number_of_important_words</th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>text_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>124</td>\n",
       "      <td>[salmon, avocado, onion, wasabi, seed, rice, n...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>10</td>\n",
       "      <td>[tuna, wasabi, soy, sauce, ginger, grain, posi...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>56</td>\n",
       "      <td>[sashimi, salmon, filet, sesame, seed, oil, se...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>125</td>\n",
       "      <td>[sushi, filet, block, sushi, filet, block, cor...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>57</td>\n",
       "      <td>[coriander, leaf, sesame, seed, oil, sushi, sh...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>64</td>\n",
       "      <td>[sushi, filet, grain, water, salt, sugar, oil,...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>19</td>\n",
       "      <td>[sushi, coriander, tuna, sashimi, piece, tuna,...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>26</td>\n",
       "      <td>[tuna, avocado, cut, slice, oil, lime, juice, ...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>57</td>\n",
       "      <td>[tuna, piece, piece, ginger, soy, sauce, sauce...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sashimi</td>\n",
       "      <td>42</td>\n",
       "      <td>[sushi, rice, rice, vinegar, vinegar, sugar, s...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>steak</td>\n",
       "      <td>36</td>\n",
       "      <td>[vinegar, soy, sauce, oil, ground, pepper, wor...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>steak</td>\n",
       "      <td>26</td>\n",
       "      <td>[onion, oil, vinegar, soy, sauce, pepper, stea...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>steak</td>\n",
       "      <td>18</td>\n",
       "      <td>[steak, salt, ground, pepper, oil, steak, salt...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>steak</td>\n",
       "      <td>26</td>\n",
       "      <td>[ginger, onion, soy, sauce, oil, meat, sugar, ...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>steak</td>\n",
       "      <td>11</td>\n",
       "      <td>[butter, pepper, preheat, grill, butter, powde...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>steak</td>\n",
       "      <td>26</td>\n",
       "      <td>[soy, sauce, vinegar, ground, ginger, powder, ...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>steak</td>\n",
       "      <td>30</td>\n",
       "      <td>[oil, worcestershire, sauce, medium, onion, pe...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>steak</td>\n",
       "      <td>30</td>\n",
       "      <td>[steak, sugar, pepper, powder, preheat, grill,...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>steak</td>\n",
       "      <td>33</td>\n",
       "      <td>[vinegar, pepper, salt, oil, steak, butter, ch...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>steak</td>\n",
       "      <td>30</td>\n",
       "      <td>[butter, chive, pepper, salt, onion, powder, g...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sushi</td>\n",
       "      <td>86</td>\n",
       "      <td>[sushi, rice, vinegar, sugar, salt, rice, wate...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sushi</td>\n",
       "      <td>185</td>\n",
       "      <td>[soy, sauce, rice, wine, vinegar, ginger, seed...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sushi</td>\n",
       "      <td>96</td>\n",
       "      <td>[sushi, salmon, raspberry, daikon, salmon, roe...</td>\n",
       "      <td>11.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sushi</td>\n",
       "      <td>101</td>\n",
       "      <td>[sushi, rice, sheet, cucumber, slice, avocado,...</td>\n",
       "      <td>12.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sushi</td>\n",
       "      <td>156</td>\n",
       "      <td>[breast, sesame, oil, oil, sea, salt, teriyaki...</td>\n",
       "      <td>13.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sushi</td>\n",
       "      <td>95</td>\n",
       "      <td>[crab, stick, rice, sheet, slice, slice, sesam...</td>\n",
       "      <td>14.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sushi</td>\n",
       "      <td>86</td>\n",
       "      <td>[sushi, rice, sheet, asparagus, stick, roe, te...</td>\n",
       "      <td>15.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sushi</td>\n",
       "      <td>83</td>\n",
       "      <td>[medium, rice, water, piece, rice, vinegar, su...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sushi</td>\n",
       "      <td>83</td>\n",
       "      <td>[rice, rice, vinegar, sugar, salt, rice, water...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sushi</td>\n",
       "      <td>53</td>\n",
       "      <td>[shrimp, tempura, sesame, seed, salt, oil, shr...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sushi</td>\n",
       "      <td>86</td>\n",
       "      <td>[rice, sushi, rice, rice, wine, vinegar, sugar...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sushi</td>\n",
       "      <td>44</td>\n",
       "      <td>[sheet, rice, sushi, salmon, fish, strip, sauc...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sushi</td>\n",
       "      <td>89</td>\n",
       "      <td>[rice, sashimi, salmon, avocado, carrot, aspar...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sushi</td>\n",
       "      <td>94</td>\n",
       "      <td>[avocado, nori, rice, rice, tobikko, roe, ging...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sushi</td>\n",
       "      <td>123</td>\n",
       "      <td>[crab, butter, ginger, avocado, sushi, rice, s...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>35</td>\n",
       "      <td>[egg, yolk, sugar, mascarpone, egg, white, cre...</td>\n",
       "      <td>1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>49</td>\n",
       "      <td>[strawberry, confectioner, sugar, mascarpone, ...</td>\n",
       "      <td>10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>34</td>\n",
       "      <td>[egg, yolk, sugar, cream, vanilla, mascarpone,...</td>\n",
       "      <td>2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>36</td>\n",
       "      <td>[egg, yolk, sugar, cream, ladyfinger, coffee, ...</td>\n",
       "      <td>3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>46</td>\n",
       "      <td>[sugar, egg, espresso, sugar, ladyfinger, powd...</td>\n",
       "      <td>4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>30</td>\n",
       "      <td>[coffee, liqueur, cream, cheese, cream, ladyfi...</td>\n",
       "      <td>5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>23</td>\n",
       "      <td>[egg, yolk, sugar, mascarpone, ladyfinger, cof...</td>\n",
       "      <td>6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>49</td>\n",
       "      <td>[strawberry, confectioner, sugar, mascarpone, ...</td>\n",
       "      <td>7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>72</td>\n",
       "      <td>[cake, cake, coffee, powder, coffee, coffee, c...</td>\n",
       "      <td>8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tiramisu</td>\n",
       "      <td>39</td>\n",
       "      <td>[vanilla, wafer, coffee, water, cream, cheese,...</td>\n",
       "      <td>9.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels  number_of_important_words  \\\n",
       "0    sashimi                        124   \n",
       "1    sashimi                         10   \n",
       "2    sashimi                         56   \n",
       "3    sashimi                        125   \n",
       "4    sashimi                         57   \n",
       "5    sashimi                         64   \n",
       "6    sashimi                         19   \n",
       "7    sashimi                         26   \n",
       "8    sashimi                         57   \n",
       "9    sashimi                         42   \n",
       "10     steak                         36   \n",
       "11     steak                         26   \n",
       "12     steak                         18   \n",
       "13     steak                         26   \n",
       "14     steak                         11   \n",
       "15     steak                         26   \n",
       "16     steak                         30   \n",
       "17     steak                         30   \n",
       "18     steak                         33   \n",
       "19     steak                         30   \n",
       "20     sushi                         86   \n",
       "21     sushi                        185   \n",
       "22     sushi                         96   \n",
       "23     sushi                        101   \n",
       "24     sushi                        156   \n",
       "25     sushi                         95   \n",
       "26     sushi                         86   \n",
       "27     sushi                         83   \n",
       "28     sushi                         83   \n",
       "29     sushi                         53   \n",
       "30     sushi                         86   \n",
       "31     sushi                         44   \n",
       "32     sushi                         89   \n",
       "33     sushi                         94   \n",
       "34     sushi                        123   \n",
       "35  tiramisu                         35   \n",
       "36  tiramisu                         49   \n",
       "37  tiramisu                         34   \n",
       "38  tiramisu                         36   \n",
       "39  tiramisu                         46   \n",
       "40  tiramisu                         30   \n",
       "41  tiramisu                         23   \n",
       "42  tiramisu                         49   \n",
       "43  tiramisu                         72   \n",
       "44  tiramisu                         39   \n",
       "\n",
       "                                   preprocessed_texts text_names  \n",
       "0   [salmon, avocado, onion, wasabi, seed, rice, n...      1.txt  \n",
       "1   [tuna, wasabi, soy, sauce, ginger, grain, posi...     10.txt  \n",
       "2   [sashimi, salmon, filet, sesame, seed, oil, se...      2.txt  \n",
       "3   [sushi, filet, block, sushi, filet, block, cor...      3.txt  \n",
       "4   [coriander, leaf, sesame, seed, oil, sushi, sh...      4.txt  \n",
       "5   [sushi, filet, grain, water, salt, sugar, oil,...      5.txt  \n",
       "6   [sushi, coriander, tuna, sashimi, piece, tuna,...      6.txt  \n",
       "7   [tuna, avocado, cut, slice, oil, lime, juice, ...      7.txt  \n",
       "8   [tuna, piece, piece, ginger, soy, sauce, sauce...      8.txt  \n",
       "9   [sushi, rice, rice, vinegar, vinegar, sugar, s...      9.txt  \n",
       "10  [vinegar, soy, sauce, oil, ground, pepper, wor...      1.txt  \n",
       "11  [onion, oil, vinegar, soy, sauce, pepper, stea...     10.txt  \n",
       "12  [steak, salt, ground, pepper, oil, steak, salt...      2.txt  \n",
       "13  [ginger, onion, soy, sauce, oil, meat, sugar, ...      3.txt  \n",
       "14  [butter, pepper, preheat, grill, butter, powde...      4.txt  \n",
       "15  [soy, sauce, vinegar, ground, ginger, powder, ...      5.txt  \n",
       "16  [oil, worcestershire, sauce, medium, onion, pe...      6.txt  \n",
       "17  [steak, sugar, pepper, powder, preheat, grill,...      7.txt  \n",
       "18  [vinegar, pepper, salt, oil, steak, butter, ch...      8.txt  \n",
       "19  [butter, chive, pepper, salt, onion, powder, g...      9.txt  \n",
       "20  [sushi, rice, vinegar, sugar, salt, rice, wate...      1.txt  \n",
       "21  [soy, sauce, rice, wine, vinegar, ginger, seed...     10.txt  \n",
       "22  [sushi, salmon, raspberry, daikon, salmon, roe...     11.txt  \n",
       "23  [sushi, rice, sheet, cucumber, slice, avocado,...     12.txt  \n",
       "24  [breast, sesame, oil, oil, sea, salt, teriyaki...     13.txt  \n",
       "25  [crab, stick, rice, sheet, slice, slice, sesam...     14.txt  \n",
       "26  [sushi, rice, sheet, asparagus, stick, roe, te...     15.txt  \n",
       "27  [medium, rice, water, piece, rice, vinegar, su...      2.txt  \n",
       "28  [rice, rice, vinegar, sugar, salt, rice, water...      3.txt  \n",
       "29  [shrimp, tempura, sesame, seed, salt, oil, shr...      4.txt  \n",
       "30  [rice, sushi, rice, rice, wine, vinegar, sugar...      5.txt  \n",
       "31  [sheet, rice, sushi, salmon, fish, strip, sauc...      6.txt  \n",
       "32  [rice, sashimi, salmon, avocado, carrot, aspar...      7.txt  \n",
       "33  [avocado, nori, rice, rice, tobikko, roe, ging...      8.txt  \n",
       "34  [crab, butter, ginger, avocado, sushi, rice, s...      9.txt  \n",
       "35  [egg, yolk, sugar, mascarpone, egg, white, cre...      1.txt  \n",
       "36  [strawberry, confectioner, sugar, mascarpone, ...     10.txt  \n",
       "37  [egg, yolk, sugar, cream, vanilla, mascarpone,...      2.txt  \n",
       "38  [egg, yolk, sugar, cream, ladyfinger, coffee, ...      3.txt  \n",
       "39  [sugar, egg, espresso, sugar, ladyfinger, powd...      4.txt  \n",
       "40  [coffee, liqueur, cream, cheese, cream, ladyfi...      5.txt  \n",
       "41  [egg, yolk, sugar, mascarpone, ladyfinger, cof...      6.txt  \n",
       "42  [strawberry, confectioner, sugar, mascarpone, ...      7.txt  \n",
       "43  [cake, cake, coffee, powder, coffee, coffee, c...      8.txt  \n",
       "44  [vanilla, wafer, coffee, water, cream, cheese,...      9.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_iteration=3\n",
    "models_folder_name_test = os.path.join(os.getcwd(),'models','test',str(number_of_iteration))\n",
    "models_folder_name_train = os.path.join(os.getcwd(),'models','train',str(number_of_iteration))\n",
    "summaries_folder_name = os.path.join(os.getcwd(),'summaries','test', str(number_of_iteration))\n",
    "\n",
    "path_to_preprocessed_texts_test = os.path.join(models_folder_name_test,'recipes_test_dataset.pkl')\n",
    "path_to_preprocessed_texts_train = os.path.join(models_folder_name_train,'recipes_train_dataset.pkl')\n",
    "\n",
    "df_preprocessed_texts_test = pd.read_pickle(path_to_preprocessed_texts_test)\n",
    "df_preprocessed_texts_train = pd.read_pickle(path_to_preprocessed_texts_train)\n",
    "df_preprocessed_texts_all=pd.concat([df_preprocessed_texts_test,df_preprocessed_texts_train]).sort_index()\n",
    "words_to_ints = pd.read_pickle(os.path.join(models_folder_name_train, 'doc2vec_recipes_dict_words_integers.pkl'))\n",
    "\n",
    "preprocessed_texts_test = df_preprocessed_texts_test.preprocessed_texts.values.tolist()\n",
    "preprocessed_texts_train = df_preprocessed_texts_train.preprocessed_texts.values.tolist()\n",
    "preprocessed_texts_all = df_preprocessed_texts_all.preprocessed_texts.values.tolist()\n",
    "labels = df_preprocessed_texts_train['labels'].values.tolist()\n",
    "\n",
    "unique_labels=sorted(set(labels))\n",
    "number_categories=len(unique_labels)\n",
    "categories_indices=np.linspace(0,number_categories-1,number_categories,dtype=int)\n",
    "labels2integers=dict(zip(unique_labels,categories_indices))\n",
    "\n",
    "print(labels2integers)\n",
    "df_preprocessed_texts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "generations = 75000\n",
    "model_learning_rate = 0.0005\n",
    "\n",
    "embedding_size = 24   #word embedding size\n",
    "doc_embedding_size = 12  #document embedding size\n",
    "concatenated_size = embedding_size + doc_embedding_size\n",
    "\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace each word in texts with integer value\n",
    "def text_to_numbers(preprocessed_texts, word_dict):\n",
    "    data = []\n",
    "    for prepr_text in preprocessed_texts:\n",
    "        text_data = []\n",
    "        for word in prepr_text:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "                text_data.append(word_ix)\n",
    "            #else:\n",
    "            #    word_ix = 0\n",
    "            #text_data.append(word_ix)\n",
    "        data.append(text_data)\n",
    "    return (data)\n",
    "\n",
    "\n",
    "def create_batch_data(text_with_words_conv_to_numbers, batch_size=batch_size):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    rand_text_ix = int(np.random.choice(len(text_with_words_conv_to_numbers), size=1))\n",
    "    rand_text = text_with_words_conv_to_numbers[rand_text_ix]\n",
    "    word_to_predict_label=np.random.choice(list(set(rand_text)), size=1,replace=False)\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        item_in_batch=[]        \n",
    "        \n",
    "        label_words=np.random.choice(rand_text, size=1,replace=False)\n",
    "\n",
    "        item_in_batch.extend(word_to_predict_label)\n",
    "        item_in_batch.append(rand_text_ix)     \n",
    "        label_data.extend(label_words)\n",
    "        batch_data.append(item_in_batch)\n",
    "\n",
    "        \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array(label_data))\n",
    "\n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'air': 0, 'crab': 30, 'roll': 92, 'preheat': 85, 'cake': 17, 'tuna': 133, 'yolk': 144, 'part': 77, 'soy': 107, 'finger': 43, 'guacamole': 53, 'filet': 41, 'egg': 39, 'cream': 31, 'mayonnaise': 65, 'berry': 9, 'truffle': 132, 'bamboo': 6, 'roe': 91, 'pinch': 81, 'liqueur': 60, 'chopstick': 23, 'drain': 38, 'ball': 5, 'vanilla': 134, 'seed': 100, 'mushroom': 71, 'chive': 21, 'salmon': 94, 'blade': 11, 'sauce': 97, 'brush': 15, 'speed': 109, 'breast': 14, 'marinade': 61, 'grate': 50, 'whip': 140, 'strip': 119, 'tongs': 128, 'coffee': 26, 'slice': 106, 'leg': 58, 'sushi': 122, 'onion': 75, 'strawberry': 118, 'sugar': 120, 'worcestershire': 143, 'piece': 80, 'steak': 115, 'saucepan': 98, 'surface': 121, 'cling': 24, 'cut': 34, 'espresso': 40, 'wafer': 137, 'salt': 95, 'asparagus': 2, 'topping': 129, 'grain': 49, 'carrot': 18, 'lime': 59, 'water': 139, 'raspberry': 89, 'bottom': 13, 'wasabi': 138, 'row': 93, 'medium': 67, 'cutting': 35, 'garnish': 47, 'pour': 83, 'press': 86, 'sieve': 103, 'spring': 112, 'dipping': 37, 'block': 12, 'towel': 131, 'sprinkle': 113, 'metal': 68, 'grill': 51, 'mixer': 69, 'paper': 76, 'ginger': 48, 'layer': 56, 'sirloin': 104, 'avocado': 3, 'thickness': 124, 'rice': 90, 'tomato': 127, 'juice': 54, 'ground': 52, 'spicy': 110, 'pepper': 79, 'matchstick': 64, 'fryer': 46, 'cucumber': 32, 'zip': 145, 'thumb': 125, 'vegetable': 135, 'oil': 74, 'quantity': 88, 'mixture': 70, 'pressure': 87, 'mascarpone': 62, 'torch': 130, 'sheet': 102, 'cheese': 19, 'space': 108, 'stick': 116, 'noodle': 72, 'powder': 84, 'whisk': 141, 'meat': 66, 'skewer': 105, 'sashimi': 96, 'starch': 114, 'wine': 142, 'sesame': 101, 'position': 82, 'nori': 73, 'curl': 33, 'ladyfinger': 55, 'stone': 117, 'filling': 42, 'batter': 7, 'chicken': 20, 'beat': 8, 'bit': 10, 'spread': 111, 'vinegar': 136, 'bag': 4, 'daikon': 36, 'coriander': 29, 'peak': 78, 'tobikko': 126, 'chocolate': 22, 'cone': 27, 'flesh': 45, 'leaf': 57, 'angle': 1, 'confectioner': 28, 'butter': 16, 'sea': 99, 'teriyaki': 123, 'mat': 63, 'fish': 44, 'cocoa': 25}\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "word_dictionary=words_to_ints\n",
    "vocabulary_size=len(word_dictionary)\n",
    "print(word_dictionary)\n",
    "print(vocabulary_size)\n",
    "\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "(36,)\n",
      "(45,)\n",
      "[[94, 3, 75, 138, 100, 90, 72, 80, 94, 95, 94, 139, 44, 53, 75, 127, 95, 95, 95, 95, 95, 95, 95, 95, 34, 3, 3, 3, 3, 80, 75, 70, 3, 70, 53, 92, 59, 54, 59, 59, 54, 59, 54, 3, 54, 59, 53, 70, 138, 138, 10, 138, 53, 10, 10, 101, 100, 107, 97, 127, 127, 127, 3, 29, 57, 57, 70, 29, 53, 24, 94, 95, 94, 94, 44, 96, 34, 106, 24, 10, 114, 24, 106, 94, 114, 24, 114, 94, 94, 114, 94, 94, 94, 46, 106, 46, 94, 74, 94, 94, 116, 46, 94, 94, 94, 94, 94, 94, 46, 74, 94, 74, 94, 47, 94, 90, 72, 72, 94, 53, 29, 90, 72, 53], [133, 138, 107, 97, 48, 49, 82, 43, 49, 106], [122, 41, 12, 122, 41, 12, 29, 57, 101, 100, 74, 133, 133, 41, 106, 106, 41, 34, 133, 106, 12, 12, 133, 100, 74, 43, 74, 133, 29, 41, 121, 41, 12, 11, 44, 133, 12, 44, 44, 82, 41, 12, 1, 34, 1, 11, 106, 44, 106, 133, 12, 11, 34, 106, 47, 47, 43, 18, 5, 82, 5, 106, 122, 18, 5, 11, 82, 106, 80, 106, 106, 94, 43, 43, 122, 5, 82, 5, 32, 18, 5, 133, 106, 122, 106, 1, 82, 1, 106, 47, 106, 43, 106, 133, 106, 106, 47, 106, 57, 32, 5, 133, 106, 32, 47, 3, 106, 3, 43, 3, 27, 82, 106, 122, 27, 5, 48, 133, 106, 119, 96, 106, 107, 97, 138], [29, 57, 101, 100, 74, 122, 102, 29, 29, 133, 74, 29, 80, 122, 133, 74, 133, 29, 133, 119, 29, 119, 133, 133, 101, 100, 74, 119, 29, 133, 44, 133, 133, 96, 44, 133, 102, 102, 29, 133, 102, 73, 92, 133, 73, 92, 92, 92, 92, 80, 34, 57, 18, 133, 107, 97, 37], [122, 41, 49, 139, 95, 120, 74, 41, 122, 41, 49, 94, 41, 139, 95, 120, 41, 44, 94, 41, 94, 41, 4, 74, 4, 4, 0, 74, 4, 139, 44, 4, 4, 121, 94, 4, 44, 119, 94, 82, 119, 41, 119, 96, 96, 3, 107, 0, 107, 97, 0, 107, 97, 0, 82, 106, 3, 107, 97, 43, 94, 106, 3, 94], [133, 3, 34, 106, 74, 59, 54, 59, 75, 107, 97, 133, 106, 106, 3, 74, 59, 54, 3, 133, 75, 95, 107, 97, 37, 59], [133, 80, 80, 48, 107, 97, 97, 37, 97, 107, 97, 107, 97, 122, 107, 97, 97, 36, 102, 102, 106, 116, 139, 36, 48, 27, 27, 48, 133, 133, 12, 106, 49, 49, 12, 133, 119, 133, 12, 133, 44, 36, 57, 57, 75, 32, 48, 107, 97, 44, 133, 37, 97, 36, 107, 97, 96], [122, 90, 90, 136, 136, 120, 95, 94, 3, 80, 73, 119, 97, 107, 138, 101, 90, 90, 139, 90, 90, 136, 120, 95, 120, 95, 90, 136, 70, 90, 94, 3, 73, 107, 97, 138, 97, 138, 138, 107, 97, 97], [136, 107, 97, 74, 52, 79, 143, 97, 75, 84, 95, 79, 115, 67, 136, 107, 97, 74, 52, 79, 143, 97, 75, 84, 95, 79, 115, 61, 66, 85, 51, 67, 74, 51, 50, 61], [75, 74, 136, 107, 97, 79, 115, 75, 74, 136, 107, 97, 95, 79, 115, 4, 83, 61, 115, 51, 51, 50, 74, 61, 115, 51], [115, 95, 52, 79, 74, 115, 95, 79, 51, 85, 51, 115, 51, 115, 74, 51, 115, 115], [107, 97, 136, 52, 48, 84, 74, 107, 97, 136, 48, 84, 74, 115, 115, 83, 61, 115, 85, 51, 50, 74, 115, 51, 61, 115], [74, 143, 97, 67, 75, 79, 97, 74, 143, 97, 107, 97, 75, 95, 79, 115, 97, 115, 83, 61, 115, 51, 67, 115, 61, 61, 74, 115, 51, 115], [115, 120, 79, 84, 85, 51, 115, 115, 97, 120, 83, 97, 115, 95, 79, 84, 115, 95, 79, 84, 115, 61, 83, 61, 74, 51, 50, 115, 115, 61], [136, 79, 95, 74, 115, 16, 21, 79, 136, 79, 95, 74, 115, 4, 61, 4, 51, 67, 74, 50, 115, 61, 61, 115, 51, 16, 21, 79, 115, 51, 67, 115, 16], [16, 21, 79, 95, 75, 84, 52, 79, 115, 74, 85, 51, 16, 21, 79, 95, 75, 84, 79, 115, 74, 66, 74, 51, 50, 66, 51, 51, 51, 16], [107, 97, 90, 142, 136, 48, 100, 100, 74, 65, 132, 74, 97, 107, 97, 90, 136, 48, 70, 54, 97, 122, 92, 101, 100, 70, 101, 100, 74, 122, 92, 97, 129, 65, 74, 122, 92, 115, 104, 115, 43, 115, 43, 115, 124, 34, 56, 115, 124, 101, 100, 74, 104, 99, 95, 113, 121, 115, 68, 128, 41, 135, 101, 100, 74, 67, 2, 71, 2, 71, 99, 95, 128, 71, 74, 122, 92, 74, 122, 92, 102, 43, 122, 90, 102, 90, 102, 6, 63, 145, 4, 90, 63, 102, 43, 90, 63, 82, 102, 68, 128, 56, 71, 2, 102, 92, 63, 43, 43, 87, 6, 63, 102, 125, 43, 92, 87, 104, 104, 82, 45, 106, 124, 115, 80, 106, 122, 92, 122, 92, 43, 106, 104, 82, 92, 122, 92, 92, 43, 63, 122, 92, 43, 139, 11, 34, 122, 92, 34, 80, 122, 24, 80, 122, 92, 82, 80, 122, 1, 88, 82, 5, 48, 5, 138, 97, 122, 92, 106, 130, 97, 130, 121, 121, 106, 81, 112, 75, 132, 74, 65, 81, 101, 100, 106], [122, 94, 89, 36, 94, 91, 3, 3, 117, 3, 117, 117, 117, 3, 125, 3, 80, 3, 82, 3, 1, 3, 43, 106, 124, 11, 43, 3, 3, 106, 93, 24, 93, 106, 93, 106, 93, 122, 92, 92, 82, 80, 122, 94, 41, 93, 93, 24, 3, 41, 24, 87, 43, 122, 92, 122, 92, 80, 89, 82, 89, 103, 68, 89, 103, 54, 103, 54, 15, 15, 89, 121, 23, 3, 92, 80, 89, 15, 82, 106, 23, 94, 34, 80, 36, 88, 11, 88, 3, 92, 23, 94, 91, 106, 23, 87], [122, 90, 102, 32, 106, 3, 132, 42, 102, 122, 90, 73, 43, 90, 90, 102, 73, 121, 131, 90, 102, 41, 102, 32, 32, 102, 92, 122, 92, 6, 63, 125, 102, 92, 102, 42, 92, 6, 63, 145, 4, 92, 63, 92, 86, 43, 92, 92, 92, 106, 3, 122, 92, 108, 106, 92, 24, 43, 24, 139, 92, 92, 92, 34, 6, 63, 24, 129, 16, 106, 122, 106, 130, 132, 130, 121, 106, 106, 91, 106, 106, 77, 74, 77, 74, 10, 70, 122, 106, 82, 106, 122, 1, 108, 106, 135, 32, 18, 106, 80, 138], [14, 101, 74, 74, 99, 95, 123, 97, 102, 90, 20, 97, 122, 90, 74, 74, 80, 20, 14, 106, 80, 92, 80, 81, 99, 95, 81, 95, 20, 14, 20, 66, 20, 14, 105, 77, 54, 77, 20, 66, 20, 123, 97, 66, 20, 14, 123, 123, 20, 92, 92, 122, 92, 6, 63, 145, 4, 90, 63, 102, 73, 63, 139, 90, 136, 90, 122, 90, 121, 102, 90, 102, 90, 63, 80, 123, 20, 14, 102, 32, 20, 92, 10, 123, 97, 32, 20, 92, 122, 92, 6, 63, 42, 92, 87, 6, 63, 87, 63, 92, 92, 10, 92, 90, 3, 3, 106, 106, 3, 34, 3, 3, 10, 106, 3, 92, 122, 92, 3, 106, 106, 122, 92, 92, 102, 92, 6, 63, 4, 87, 92, 63, 24, 90, 11, 92, 80, 80, 92, 34, 35, 24, 122, 122, 48, 123, 97, 122, 80, 101, 100, 92, 123, 20, 122, 92], [30, 116, 90, 102, 106, 106, 101, 100, 30, 30, 116, 116, 92, 65, 30, 116, 65, 90, 102, 102, 122, 90, 102, 43, 90, 102, 113, 100, 90, 90, 90, 6, 63, 145, 4, 102, 63, 30, 70, 102, 106, 32, 106, 108, 102, 92, 63, 42, 42, 125, 92, 87, 63, 92, 63, 10, 92, 92, 63, 122, 92, 92, 24, 92, 92, 24, 63, 24, 92, 24, 92, 90, 92, 122, 80, 122, 92, 90, 11, 92, 92, 106, 122, 92, 11, 87, 122, 92, 92, 122, 80, 48, 138, 107, 97], [67, 90, 139, 80, 90, 136, 120, 95, 65, 79, 74, 95, 79, 102, 122, 90, 101, 100, 133, 112, 75, 90, 139, 139, 38, 90, 67, 98, 139, 70, 139, 90, 122, 90, 136, 120, 95, 98, 120, 70, 90, 90, 70, 90, 131, 141, 65, 74, 95, 79, 122, 63, 121, 102, 73, 63, 102, 63, 43, 90, 73, 56, 90, 101, 100, 73, 90, 73, 56, 32, 110, 65, 70, 32, 133, 113, 112, 75, 92, 122, 63, 80, 92], [90, 90, 136, 120, 95, 90, 139, 102, 42, 32, 32, 64, 3, 54, 107, 97, 6, 122, 63, 92, 90, 136, 120, 95, 90, 139, 38, 90, 139, 67, 98, 90, 131, 98, 131, 90, 139, 90, 90, 70, 90, 76, 102, 90, 90, 131, 92, 122, 63, 108, 102, 63, 43, 139, 86, 90, 73, 42, 90, 90, 63, 73, 90, 42, 63, 92, 63, 92, 63, 73, 10, 139, 122, 122, 76, 131, 90, 42, 122, 92, 80, 107, 97], [90, 122, 90, 90, 142, 136, 120, 65, 65, 90, 142, 136, 107, 97, 122, 4, 102, 42, 119, 94, 133, 79, 3, 112, 75, 122, 138, 48, 107, 97, 92, 90, 102, 63, 139, 90, 56, 65, 56, 65, 90, 42, 65, 42, 133, 32, 92, 63, 90, 87, 92, 90, 15, 139, 92, 63, 92, 122, 106, 24, 122, 56, 94, 24, 56, 24, 90, 86, 86, 90, 44, 24, 122, 43, 24, 5, 129, 129, 80, 94, 5, 90, 129, 5, 24, 5], [102, 90, 122, 94, 44, 119, 97, 122, 56, 122, 90, 90, 90, 90, 122, 80, 94, 80, 44, 92, 44, 44, 122, 94, 122, 92, 41, 80, 119, 90, 3, 92, 92, 6, 63, 76, 122, 92, 80, 122, 92, 107, 97, 48], [90, 96, 94, 3, 18, 2, 36, 91, 34, 80, 36, 102, 34, 34, 131, 68, 105, 131, 105, 135, 105, 135, 35, 102, 34, 2, 139, 99, 95, 2, 139, 18, 67, 18, 34, 80, 80, 119, 121, 106, 106, 106, 119, 34, 106, 106, 119, 3, 3, 45, 3, 80, 96, 94, 34, 80, 94, 80, 34, 92, 80, 73, 63, 139, 136, 90, 73, 10, 138, 2, 10, 106, 18, 91, 92, 92, 10, 77, 90, 102, 92, 102, 102, 80, 92, 80, 92, 77, 80], [3, 73, 90, 90, 126, 91, 48, 138, 107, 97, 3, 100, 100, 100, 77, 3, 100, 45, 3, 45, 3, 106, 106, 100, 100, 77, 3, 100, 100, 3, 100, 3, 80, 96, 94, 106, 119, 94, 92, 92, 80, 92, 6, 63, 102, 73, 63, 90, 73, 102, 10, 138, 102, 43, 43, 102, 119, 102, 106, 3, 94, 92, 92, 122, 92, 126, 92, 126, 56, 121, 92, 126, 92, 126, 138, 92, 92, 90, 11, 34, 92, 122, 122, 35, 6, 63, 106, 92, 122, 10, 48, 138, 107, 97], [30, 16, 48, 3, 122, 90, 102, 30, 30, 58, 43, 30, 66, 58, 66, 30, 58, 80, 30, 66, 58, 99, 95, 58, 95, 79, 16, 16, 30, 66, 128, 128, 30, 48, 103, 70, 3, 100, 100, 3, 106, 34, 11, 54, 106, 122, 92, 102, 122, 90, 102, 43, 90, 6, 63, 145, 4, 102, 63, 90, 48, 119, 102, 30, 58, 48, 125, 6, 63, 102, 87, 43, 122, 92, 63, 82, 34, 106, 122, 92, 82, 43, 3, 106, 122, 92, 11, 43, 92, 35, 122, 92, 63, 24, 122, 92, 63, 43, 3, 92, 63, 3, 92, 87, 3, 92, 63, 24, 122, 92, 80, 92, 80, 63, 106, 24, 106, 1, 88, 110, 65, 106, 91], [118, 28, 120, 62, 31, 60, 55, 84, 118, 9, 9, 118, 28, 67, 19, 31, 120, 60, 8, 69, 67, 109, 55, 40, 26, 111, 70, 55, 56, 118, 55, 40, 70, 118, 55, 31, 120, 8, 69, 67, 109, 31, 56, 55, 25, 31, 118, 83, 118], [39, 144, 120, 31, 134, 62, 26, 25, 84, 67, 39, 144, 120, 67, 70, 67, 31, 134, 78, 62, 70, 26, 55, 26, 70, 55, 13, 62, 70, 55, 31, 56, 113, 25], [39, 144, 120, 31, 55, 26, 25, 84, 22, 39, 144, 120, 139, 140, 144, 62, 144, 8, 140, 31, 78, 70, 13, 26, 60, 31, 55, 26, 60, 56, 25, 22, 33, 22, 33, 22], [26, 60, 31, 19, 31, 55, 25, 84, 67, 26, 31, 19, 26, 70, 67, 31, 134, 78, 31, 31, 19, 55, 13, 26, 70, 31, 70, 56, 113, 25], [39, 144, 120, 62, 55, 26, 84, 67, 144, 120, 134, 62, 70, 55, 26, 13, 111, 62, 70, 55, 62, 113, 25], [118, 28, 120, 62, 31, 60, 55, 84, 118, 9, 9, 118, 28, 67, 19, 31, 120, 60, 8, 69, 67, 109, 55, 40, 26, 111, 70, 55, 56, 118, 55, 40, 70, 118, 55, 31, 120, 8, 69, 67, 109, 31, 56, 55, 25, 31, 118, 83, 118], [17, 17, 26, 84, 26, 26, 28, 120, 26, 31, 28, 120, 26, 25, 84, 22, 17, 7, 26, 7, 17, 26, 26, 60, 69, 109, 62, 28, 120, 60, 8, 67, 69, 67, 109, 31, 28, 120, 60, 31, 70, 70, 17, 17, 56, 17, 26, 70, 17, 70, 26, 17, 56, 17, 26, 70, 56, 111, 17, 56, 17, 83, 26, 70, 111, 17, 25, 17, 33, 22, 33, 22], [134, 137, 26, 139, 31, 19, 120, 31, 39, 140, 25, 84, 111, 137, 13, 26, 139, 137, 26, 8, 31, 19, 120, 69, 31, 39, 109, 7, 67, 26, 139, 7, 137, 137, 26, 7, 140, 25, 84], [96, 94, 41, 101, 100, 74, 101, 100, 97, 48, 41, 101, 100, 74, 121, 101, 100, 74, 43, 94, 41, 101, 100, 121, 43, 41, 41, 44, 101, 100, 74, 116, 41, 41, 44, 82, 41, 43, 41, 41, 34, 41, 97, 121, 11, 94, 106, 43, 82, 94, 97, 48, 94, 96, 106, 97], [122, 29, 133, 96, 80, 133, 44, 133, 44, 133, 133, 12, 29, 133, 29, 133, 116, 74, 106], [48, 75, 107, 97, 74, 66, 120, 115, 48, 75, 97, 74, 143, 97, 66, 120, 61, 4, 115, 61, 51, 67, 51, 115, 51, 67], [16, 79, 85, 51, 16, 84, 115, 95, 79, 115, 16], [122, 90, 136, 120, 95, 90, 139, 102, 42, 32, 32, 64, 64, 79, 64, 75, 64, 97, 80, 36, 64, 36, 3, 54, 6, 122, 63, 90, 136, 120, 95, 90, 103, 38, 90, 139, 67, 98, 131, 98, 131, 139, 90, 90, 70, 90, 76, 90, 90, 131, 92, 122, 63, 108, 102, 63, 43, 139, 86, 90, 73, 42, 90, 90, 63, 73, 90, 42, 63, 92, 63, 92, 63, 73, 10, 139, 122, 122, 92, 90, 42, 122, 92, 80, 107, 97], [122, 90, 102, 2, 116, 91, 7, 122, 102, 73, 4, 63, 122, 90, 102, 43, 90, 102, 116, 2, 102, 56, 2, 44, 122, 92, 2, 119, 96, 94, 122, 92, 102, 6, 63, 43, 102, 116, 92, 122, 122, 92, 122, 92, 7, 7, 122, 92, 7, 122, 92, 90, 7, 92, 92, 7, 92, 46, 94, 128, 76, 74, 122, 92, 122, 92, 139, 11, 90, 7, 34, 92, 80, 122, 122, 92, 122, 92, 122, 92, 138, 48, 97], [101, 100, 95, 74, 110, 97, 65, 97, 90, 142, 136, 54, 141, 7, 141, 101, 100, 95, 54, 7, 43, 46, 74, 74, 98, 74, 7, 46, 38, 76, 131, 102, 113, 95, 46, 110, 97, 141, 65, 90, 142, 136, 54, 110, 97], [39, 144, 120, 62, 39, 31, 26, 55, 25, 84, 67, 8, 39, 144, 120, 62, 19, 39, 31, 120, 26, 55, 26, 70, 55, 56, 56, 70, 55, 56, 70, 113, 25], [120, 39, 40, 120, 55, 84, 8, 120, 39, 144, 69, 39, 70, 8, 8, 39, 120, 69, 78, 39, 62, 70, 83, 40, 55, 40, 55, 111, 62, 70, 55, 56, 25, 84, 55, 40, 62, 70, 25, 84, 55, 40, 62, 70]]\n"
     ]
    }
   ],
   "source": [
    "text_data_test = text_to_numbers(preprocessed_texts_test, word_dictionary)\n",
    "text_data_train = text_to_numbers(preprocessed_texts_train, word_dictionary)\n",
    "print(np.shape(text_data_test))\n",
    "print(np.shape(text_data_train))\n",
    "text_data = []\n",
    "text_data.extend(text_data_train)\n",
    "text_data.extend(text_data_test)\n",
    "print(np.shape(text_data))\n",
    "print(text_data)\n",
    "\n",
    "#print(text_data_test)\n",
    "#print(text_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[133, 90, 122, 92, 96, 115, 51, 97, 31]\n"
     ]
    }
   ],
   "source": [
    "valid_words = ['tuna', 'rice', 'sushi', 'roll', 'sashimi','steak','grill', 'sauce', 'cream']\n",
    "\n",
    "valid_examples = [word_dictionary[x] for x in valid_words if x in word_dictionary.keys()]\n",
    "print(valid_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mascarpone' '35']\n",
      " ['mascarpone' '35']\n",
      " ['mascarpone' '35']\n",
      " ['mascarpone' '35']\n",
      " ['mascarpone' '35']\n",
      " ['mascarpone' '35']\n",
      " ['mascarpone' '35']\n",
      " ['mascarpone' '35']]\n",
      "['medium' 'powder' 'yolk' 'ladyfinger' 'sugar' 'yolk' 'ladyfinger'\n",
      " 'mixture']\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "batch_data, label_data = create_batch_data(preprocessed_texts_all)\n",
    "print(batch_data)\n",
    "print(label_data)\n",
    "print(np.shape(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "embeddings = tf.get_variable(\"embeddings\", shape=[vocabulary_size, embedding_size], trainable=False)\n",
    "doc_embeddings = tf.get_variable(\"doc_embeddings\", shape=[len(preprocessed_texts_train), doc_embedding_size])\n",
    "decoder_weights = tf.get_variable(\"decoder_weights\", shape=[vocabulary_size, concatenated_size], trainable=False)\n",
    "decoder_biases = tf.get_variable(\"decoder_biases\", shape=[vocabulary_size], trainable=False)\n",
    "print(embeddings.trainable)\n",
    "restorer = tf.train.Saver(name=\"restoring\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /notebooks/school/text_feature_extraction/models/train/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Model restored.\n",
      "embeddings : [[ 0.16318983  0.0157128   1.1473163  ... -0.36404973 -0.47158304\n",
      "  -0.9127127 ]\n",
      " [-0.3714574   0.68322307 -0.08932335 ...  0.77678955 -0.42184588\n",
      "  -0.627995  ]\n",
      " [ 0.36375847  0.5762724   0.46977198 ... -0.35758272  0.12533186\n",
      "  -0.36692593]\n",
      " ...\n",
      " [ 0.9877567  -0.24879673  0.5887008  ...  0.09593772 -0.69610685\n",
      "   0.7749497 ]\n",
      " [-0.01066543 -0.54074144  0.9609981  ... -0.19811237 -0.07805609\n",
      "   0.06442046]\n",
      " [-0.16597985 -0.22206676  0.48183683 ... -0.00731345  0.35793105\n",
      "  -0.35809818]]\n",
      "doc_embeddings : [[-0.0643751   0.3981141  -1.2441322   1.4530942  -3.1478906   0.5494276\n",
      "  -1.0748886   0.18218173 -0.04573985 -0.7446831  -1.5485166   2.43101   ]\n",
      " [-2.0126398   1.6969712  -1.9203137  -3.0515747   1.0123584  -1.4938012\n",
      "   0.40120342  4.2204633  -0.4455142  -1.768436   -2.3949094  -0.20840812]\n",
      " [-2.4571276   0.10651758 -0.8569757  -0.70818424 -0.8523971   1.4471573\n",
      "   0.41887343  1.4806932  -2.0237527  -2.152493   -1.8041952   0.65761167]\n",
      " [-2.7140858   1.9150152  -0.18812945  2.0674143  -0.8175861  -0.43002146\n",
      "   3.6118755   1.360848   -0.8361883  -0.30008394 -1.4006175  -1.1313317 ]\n",
      " [-1.3221245   1.9489923  -1.14558     0.7445086   2.276329    0.20025684\n",
      "  -1.4505372   0.18279143  0.16065466 -2.7811837  -0.26919192  3.466219  ]\n",
      " [ 1.6929657  -0.5079416  -0.2881327   0.5261325  -0.55803657 -2.536103\n",
      "   2.2173674   2.5348225  -1.9723467  -2.6004677  -1.1887987   2.053267  ]\n",
      " [ 0.5856818   1.0910133  -1.1198268  -1.0732218   1.2671013   1.7229106\n",
      "   1.9070039   0.4707286   0.37850976 -2.323913   -3.1360683  -0.2517939 ]\n",
      " [-0.5227082   1.851856   -1.161949    1.5236841  -1.0418109  -1.4631646\n",
      "  -0.17479305  1.305369    2.7562714   1.3099134  -1.0717701   2.9562943 ]\n",
      " [ 2.7388766   0.48254135 -1.0506053  -0.3036467   0.30947807 -2.89423\n",
      "   1.137015    0.13182773 -0.12243772  0.10547423  1.0797005  -1.0163652 ]\n",
      " [ 1.2684834   3.979794   -2.0277226  -1.6056451   0.7387421  -0.59155107\n",
      "   0.04453915 -0.37393987 -0.20206563  0.15309699  1.2614521  -0.20180847]\n",
      " [-0.43901724 -0.05518144 -1.8777165  -2.5750575  -1.2931272  -3.4765332\n",
      "  -0.6532629  -1.9541463  -0.1634202  -2.007302    1.551814    0.65901077]\n",
      " [ 1.2574296   2.5703814   0.07589774 -0.930648    0.93612504 -2.340191\n",
      "  -1.6330067   1.156523   -0.20032732 -0.30640164  1.3058968  -1.7860905 ]\n",
      " [ 1.578024    1.5333128  -1.6748668  -1.2018675   1.2968057  -2.2844434\n",
      "   2.6401536  -1.3008665  -0.5097022  -0.80389106 -0.3005728  -0.92113686]\n",
      " [ 1.2469703   2.0756214   0.9962977  -1.6463838   0.55792767 -3.423298\n",
      "   0.1138091  -1.8393698   1.9792219  -1.8626982   0.21035562  0.3936175 ]\n",
      " [-0.27429304  1.578533   -1.3444104   0.35046288 -0.8187835  -0.19523846\n",
      "   1.292715   -1.6763828   0.86081904 -0.84716123  2.8701506  -1.6292652 ]\n",
      " [ 1.4701601   0.01249771 -0.8378371  -1.0665344  -2.5223584  -1.0418153\n",
      "  -0.20405968  0.03851751 -1.0354968  -1.226444    2.765795   -0.8120659 ]\n",
      " [-1.4663334   0.7653927   1.3910428  -1.8724865  -0.33750874 -0.39811185\n",
      "  -0.2684823   1.2077951   0.18942517  1.044608    0.1991731  -0.08580736]\n",
      " [-0.73516935  0.28893465  3.0367742  -1.7625699  -2.4116523   2.272625\n",
      "   0.25396648  1.6752713   0.6107726  -3.4727669   1.1471409   1.7544751 ]\n",
      " [-2.704527   -2.0165944   1.393029   -1.2243738   1.7597073   0.20120524\n",
      "   0.486818    0.54402727 -0.1299847   0.20203084  0.8609586   1.2543064 ]\n",
      " [-2.2022805  -1.3971536  -0.6290449  -0.09020542 -0.2194055  -0.3407634\n",
      "   0.83856845  0.85568655  1.4372287   0.4094849   1.2582586   0.3900898 ]\n",
      " [-1.4766146  -0.8036592   0.10503244 -2.5002189  -0.06941269  2.102342\n",
      "   0.2597947   1.0300077   2.6542006   0.5374419  -0.50687027 -0.3799461 ]\n",
      " [-0.04928866  1.5241182   1.1188555  -1.1906655  -1.1248807   0.62989444\n",
      "   2.265073   -0.68192804 -0.22659013  2.6390414  -1.4650697   1.8231909 ]\n",
      " [ 0.72809166  0.04908406  1.398152    0.39057252  1.8865441   0.9320936\n",
      "   2.7507584   0.53905797  0.30200434  1.2208865   0.97403204  2.0735896 ]\n",
      " [ 0.37764296 -0.26813242 -2.260267   -0.39153248 -0.3327652   2.7300947\n",
      "   0.9167978   1.6173666   1.5413138   1.3158971  -0.78675026  0.73543537]\n",
      " [-0.787634    1.7577305   0.7292998   2.1110272   2.02828     2.0973215\n",
      "   1.2103733   3.096388    2.07532    -0.37595728  0.18138003  0.26846007]\n",
      " [-1.981813   -1.516931    1.8418013   0.29970828 -0.23384291 -1.4028689\n",
      "   1.7048979  -0.58714813  2.0674226  -1.4621428  -1.1444551   1.4325485 ]\n",
      " [-2.3183966  -0.96586996  0.5991149   0.88095194 -0.15755911 -0.6092531\n",
      "   0.38425246  3.5919473   1.3190715   0.4306768  -1.4203988  -0.28631094]\n",
      " [-0.6227491  -1.1027685   1.6748178  -1.3907139  -0.57026076  0.69660723\n",
      "   1.1570427   2.2473605  -0.03064554 -0.09427179  1.0093523  -0.2809704 ]\n",
      " [-0.572914   -0.28601995  2.0955288   2.2006118   0.96245205 -0.41173902\n",
      "  -2.0241406  -2.2693076  -0.71577525 -0.18745366 -1.4380969  -0.98834944]\n",
      " [ 2.7692513  -0.6774223   1.2827331   1.8983535   0.02465452 -0.19852386\n",
      "   0.03222304 -0.1713174   1.7339352   0.90917325 -2.218946   -1.2811772 ]\n",
      " [ 2.2521477  -2.0291352   1.0158166   1.424927    2.4102893  -1.2230997\n",
      "  -2.4504979   1.1799259   1.1384122   1.8216891  -0.7769204   0.48783243]\n",
      " [ 2.5763102  -0.780736    0.65058446  2.0306206  -0.3994261   2.2523196\n",
      "  -1.4217678  -0.7426417   0.79345876 -1.0621865  -2.3383725  -1.9730629 ]\n",
      " [ 1.9935315  -0.87242615  1.95618     0.40727594 -0.61538124 -1.729941\n",
      "  -0.5695448  -1.6057673   2.9675038   0.67233276 -2.7921104  -1.3122412 ]\n",
      " [-0.5106725  -0.10131303  1.8077866   2.4171984   0.7258924  -0.5987683\n",
      "  -2.2015576  -2.3206763  -0.6743119  -0.12232488 -1.521172   -1.0560654 ]\n",
      " [ 1.1371473   0.13244207  1.4276187   1.3242452   1.1393371   1.1512908\n",
      "  -2.5761673  -2.8900957   0.7546444   1.5910254   0.84437287 -0.27404875]\n",
      " [ 2.6083982  -2.4706585  -1.037573    0.84515     2.2787864  -0.62051636\n",
      "  -0.83252364 -2.2659338   1.6041884  -1.4638019  -0.3074463   1.0165093 ]]\n",
      "decoder_weights : [[-0.35245752 -0.30302903 -0.44567543 ... -0.258707    0.48023435\n",
      "   0.37490928]\n",
      " [-0.17965324 -0.08616088 -0.2643445  ... -0.38566095  0.09285305\n",
      "  -0.08991795]\n",
      " [-0.20626613 -0.10138493 -0.57350576 ...  0.07415877 -0.2410398\n",
      "  -0.12384877]\n",
      " ...\n",
      " [-0.19564344 -0.2332499  -0.28647518 ...  0.76233184 -0.14838801\n",
      "  -0.83642715]\n",
      " [-0.28510603 -0.39306197 -0.3272772  ...  0.9545751  -0.3606761\n",
      "  -0.5517504 ]\n",
      " [-0.10780848 -0.13948168 -0.2930405  ...  0.34023967  0.72842425\n",
      "  -0.9280385 ]]\n",
      "decoder_biases : [-1.226448   -0.8216166  -1.0292898   0.5418953   0.30393425 -0.62069637\n",
      " -0.4135722  -0.8035477  -0.36589107 -0.85347533 -0.15588276 -0.6845776\n",
      " -0.6807857  -0.5851849  -1.1008667  -1.3451252  -0.19632086 -0.71783745\n",
      " -0.99392074 -0.5062424  -0.8452127  -0.7598388  -0.81069726 -1.4619381\n",
      "  0.16070038 -0.02367592  0.05630049 -0.88376796 -0.53905106 -0.7233258\n",
      " -0.980677    0.25021696  0.0350536  -0.98219967  0.02568795 -0.9504644\n",
      " -0.8098323  -0.8324431  -1.0741534  -0.6793329  -0.74966544 -0.7160941\n",
      " -0.541498   -0.14458491 -0.29177243 -1.2442365  -1.2906766  -1.1250414\n",
      "  0.25073627 -0.968384   -0.40164348  0.04973027 -0.47156236 -1.1857859\n",
      " -0.02281175 -0.0553884   0.60433245 -0.5468886  -1.1209664  -0.7980294\n",
      " -0.31775153 -0.22453694 -0.37800944  0.3513974  -1.5582783  -0.4884757\n",
      " -0.16386     1.5844696  -1.127703   -0.43877226  1.2048093  -1.2679858\n",
      " -1.2581737  -0.3429171   1.4942617   0.4615588  -1.1917734  -1.0215226\n",
      " -0.8276895   0.3729033   0.6813259  -0.9585293  -0.5782941  -0.30867335\n",
      "  0.75490224 -0.53753364 -0.8125607  -0.5299608  -1.2558048  -1.4070553\n",
      "  0.6882219  -0.94538045  0.7800171  -1.4297359  -0.06475233  1.0242178\n",
      " -0.46327513  1.1628563  -0.91928977 -0.73161477 -0.11182375 -0.27651688\n",
      "  0.68177474 -1.2423183  -1.3121705  -1.2792943   0.7591648   0.9946454\n",
      " -1.0893164  -0.41901857 -1.1735009  -0.5284575  -0.80951744 -0.20547004\n",
      " -1.325053    0.535882   -0.5968579  -1.5579587  -0.5658618  -0.21613392\n",
      "  0.9072645  -0.35132423  0.8170926  -1.0033824  -1.1680207  -1.0074909\n",
      " -1.5121049  -1.1940135  -0.8540748  -0.7116408  -1.1859037  -0.7392487\n",
      " -1.1517155  -0.26164898 -0.5906346  -1.2360983   0.7891644  -0.87921053\n",
      " -0.28508538  0.74442756 -0.97623605 -1.6021172  -0.9799155  -0.86295\n",
      " -0.76349896 -0.7304428 ]\n"
     ]
    }
   ],
   "source": [
    "restorer.restore(sess, os.path.join(models_folder_name_train,\"doc2vec_recipes_checkpoint.ckpt\"))\n",
    "print(\"Model restored.\")\n",
    "# Check the values of the variables\n",
    "print(\"embeddings : %s\" % embeddings.eval())\n",
    "print(\"doc_embeddings : %s\" % doc_embeddings.eval())\n",
    "print(\"decoder_weights : %s\" % decoder_weights.eval())\n",
    "print(\"decoder_biases : %s\" % decoder_biases.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables_names = [v.name for v in tf.trainable_variables()]\n",
    "#values = sess.run(variables_names)\n",
    "#for k, v in zip(variables_names, values):\n",
    "#    print(\"Variable: \", k)\n",
    "#    print(\"Shape: \", v.shape)\n",
    "#    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "(36, 12)\n",
      "(45, 12)\n",
      "WARNING:tensorflow:From <ipython-input-13-a5e7fa9e2995>:35: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Starting Training\n",
      "Loss at step 50 : 3.058952569961548\n",
      "Loss at step 100 : 2.8579609394073486\n",
      "Loss at step 150 : 2.6407299041748047\n",
      "Loss at step 200 : 2.4341206550598145\n",
      "Loss at step 250 : 2.806662082672119\n",
      "Loss at step 300 : 3.3923468589782715\n",
      "Loss at step 350 : 2.295071840286255\n",
      "Loss at step 400 : 3.9047164916992188\n",
      "Loss at step 450 : 2.7947142124176025\n",
      "Loss at step 500 : 3.8241539001464844\n",
      "Loss at step 550 : 2.741347551345825\n",
      "Loss at step 600 : 2.588061571121216\n",
      "Loss at step 650 : 2.8121068477630615\n",
      "Loss at step 700 : 3.637629508972168\n",
      "Loss at step 750 : 2.487825632095337\n",
      "Loss at step 800 : 2.6784987449645996\n",
      "Loss at step 850 : 2.650233745574951\n",
      "Loss at step 900 : 2.7082042694091797\n",
      "Loss at step 950 : 2.3866844177246094\n",
      "Loss at step 1000 : 2.2769904136657715\n",
      "Loss at step 1050 : 4.945453643798828\n",
      "Loss at step 1100 : 2.3853907585144043\n",
      "Loss at step 1150 : 3.171874761581421\n",
      "Loss at step 1200 : 6.062747955322266\n",
      "Loss at step 1250 : 2.726408004760742\n",
      "Loss at step 1300 : 3.0661115646362305\n",
      "Loss at step 1350 : 3.0937275886535645\n",
      "Loss at step 1400 : 4.422055721282959\n",
      "Loss at step 1450 : 3.1486334800720215\n",
      "Loss at step 1500 : 3.021092653274536\n",
      "Loss at step 1550 : 2.981983184814453\n",
      "Loss at step 1600 : 2.7862608432769775\n",
      "Loss at step 1650 : 3.4200992584228516\n",
      "Loss at step 1700 : 5.149219036102295\n",
      "Loss at step 1750 : 3.2774710655212402\n",
      "Loss at step 1800 : 3.061886787414551\n",
      "Loss at step 1850 : 2.5938024520874023\n",
      "Loss at step 1900 : 2.9600729942321777\n",
      "Loss at step 1950 : 2.5639731884002686\n",
      "Loss at step 2000 : 4.776242256164551\n",
      "Loss at step 2050 : 2.3969597816467285\n",
      "Loss at step 2100 : 3.438762664794922\n",
      "Loss at step 2150 : 2.4997944831848145\n",
      "Loss at step 2200 : 2.866732120513916\n",
      "Loss at step 2250 : 3.3743820190429688\n",
      "Loss at step 2300 : 2.9053280353546143\n",
      "Loss at step 2350 : 4.048815727233887\n",
      "Loss at step 2400 : 4.082437992095947\n",
      "Loss at step 2450 : 3.4303083419799805\n",
      "Loss at step 2500 : 2.5050063133239746\n",
      "Loss at step 2550 : 2.1757330894470215\n",
      "Loss at step 2600 : 3.931259870529175\n",
      "Loss at step 2650 : 1.7888778448104858\n",
      "Loss at step 2700 : 3.5520567893981934\n",
      "Loss at step 2750 : 2.7421693801879883\n",
      "Loss at step 2800 : 2.5108590126037598\n",
      "Loss at step 2850 : 2.658803939819336\n",
      "Loss at step 2900 : 3.0373318195343018\n",
      "Loss at step 2950 : 4.26611328125\n",
      "Loss at step 3000 : 2.3185698986053467\n",
      "Loss at step 3050 : 2.999274253845215\n",
      "Loss at step 3100 : 3.9415693283081055\n",
      "Loss at step 3150 : 1.7097203731536865\n",
      "Loss at step 3200 : 3.214538812637329\n",
      "Loss at step 3250 : 2.5689585208892822\n",
      "Loss at step 3300 : 4.010798454284668\n",
      "Loss at step 3350 : 2.0656657218933105\n",
      "Loss at step 3400 : 2.602733612060547\n",
      "Loss at step 3450 : 1.6286399364471436\n",
      "Loss at step 3500 : 6.2772369384765625\n",
      "Loss at step 3550 : 2.5601015090942383\n",
      "Loss at step 3600 : 4.251814842224121\n",
      "Loss at step 3650 : 2.1125707626342773\n",
      "Loss at step 3700 : 3.4144375324249268\n",
      "Loss at step 3750 : 2.3797495365142822\n",
      "Loss at step 3800 : 2.661848545074463\n",
      "Loss at step 3850 : 5.848384380340576\n",
      "Loss at step 3900 : 2.6350622177124023\n",
      "Loss at step 3950 : 2.8269736766815186\n",
      "Loss at step 4000 : 2.2825095653533936\n",
      "Loss at step 4050 : 4.622097969055176\n",
      "Loss at step 4100 : 2.399346351623535\n",
      "Loss at step 4150 : 4.994675636291504\n",
      "Loss at step 4200 : 3.6599490642547607\n",
      "Loss at step 4250 : 2.1153459548950195\n",
      "Loss at step 4300 : 3.5509510040283203\n",
      "Loss at step 4350 : 2.645289897918701\n",
      "Loss at step 4400 : 3.476914882659912\n",
      "Loss at step 4450 : 3.2151119709014893\n",
      "Loss at step 4500 : 5.116594314575195\n",
      "Loss at step 4550 : 2.65632700920105\n",
      "Loss at step 4600 : 5.120004653930664\n",
      "Loss at step 4650 : 2.944415807723999\n",
      "Loss at step 4700 : 2.609323740005493\n",
      "Loss at step 4750 : 2.7710185050964355\n",
      "Loss at step 4800 : 3.847811460494995\n",
      "Loss at step 4850 : 2.839594841003418\n",
      "Loss at step 4900 : 5.482071876525879\n",
      "Loss at step 4950 : 2.5891799926757812\n",
      "Loss at step 5000 : 4.118448257446289\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 5050 : 3.703831672668457\n",
      "Loss at step 5100 : 2.1259546279907227\n",
      "Loss at step 5150 : 2.2226457595825195\n",
      "Loss at step 5200 : 3.80275297164917\n",
      "Loss at step 5250 : 3.516399383544922\n",
      "Loss at step 5300 : 2.663151264190674\n",
      "Loss at step 5350 : 5.465465068817139\n",
      "Loss at step 5400 : 1.9457361698150635\n",
      "Loss at step 5450 : 3.2571816444396973\n",
      "Loss at step 5500 : 2.8318898677825928\n",
      "Loss at step 5550 : 2.3460376262664795\n",
      "Loss at step 5600 : 2.6365692615509033\n",
      "Loss at step 5650 : 4.7807464599609375\n",
      "Loss at step 5700 : 2.507983922958374\n",
      "Loss at step 5750 : 2.588921546936035\n",
      "Loss at step 5800 : 3.143864870071411\n",
      "Loss at step 5850 : 3.536078929901123\n",
      "Loss at step 5900 : 3.1006088256835938\n",
      "Loss at step 5950 : 2.4061713218688965\n",
      "Loss at step 6000 : 3.1277642250061035\n",
      "Loss at step 6050 : 5.530869483947754\n",
      "Loss at step 6100 : 5.8311357498168945\n",
      "Loss at step 6150 : 3.1596879959106445\n",
      "Loss at step 6200 : 2.780609607696533\n",
      "Loss at step 6250 : 3.531003475189209\n",
      "Loss at step 6300 : 3.241684675216675\n",
      "Loss at step 6350 : 2.2187588214874268\n",
      "Loss at step 6400 : 2.8475513458251953\n",
      "Loss at step 6450 : 4.712311744689941\n",
      "Loss at step 6500 : 3.5526208877563477\n",
      "Loss at step 6550 : 2.520659923553467\n",
      "Loss at step 6600 : 2.5368235111236572\n",
      "Loss at step 6650 : 2.334271192550659\n",
      "Loss at step 6700 : 3.924461841583252\n",
      "Loss at step 6750 : 3.2738943099975586\n",
      "Loss at step 6800 : 4.260011672973633\n",
      "Loss at step 6850 : 3.0775585174560547\n",
      "Loss at step 6900 : 2.979785203933716\n",
      "Loss at step 6950 : 2.371737003326416\n",
      "Loss at step 7000 : 3.1288928985595703\n",
      "Loss at step 7050 : 3.30441951751709\n",
      "Loss at step 7100 : 3.734027862548828\n",
      "Loss at step 7150 : 2.1207518577575684\n",
      "Loss at step 7200 : 1.653397798538208\n",
      "Loss at step 7250 : 2.7658088207244873\n",
      "Loss at step 7300 : 6.068428993225098\n",
      "Loss at step 7350 : 4.114793300628662\n",
      "Loss at step 7400 : 3.212833881378174\n",
      "Loss at step 7450 : 2.3070945739746094\n",
      "Loss at step 7500 : 3.6858928203582764\n",
      "Loss at step 7550 : 4.083569049835205\n",
      "Loss at step 7600 : 2.2595419883728027\n",
      "Loss at step 7650 : 1.9489041566848755\n",
      "Loss at step 7700 : 2.5746164321899414\n",
      "Loss at step 7750 : 2.515476703643799\n",
      "Loss at step 7800 : 2.6633846759796143\n",
      "Loss at step 7850 : 5.475516319274902\n",
      "Loss at step 7900 : 1.6114672422409058\n",
      "Loss at step 7950 : 3.9320144653320312\n",
      "Loss at step 8000 : 3.0673141479492188\n",
      "Loss at step 8050 : 2.83121919631958\n",
      "Loss at step 8100 : 3.219717502593994\n",
      "Loss at step 8150 : 2.8264732360839844\n",
      "Loss at step 8200 : 3.0928878784179688\n",
      "Loss at step 8250 : 3.3356213569641113\n",
      "Loss at step 8300 : 3.747796058654785\n",
      "Loss at step 8350 : 3.3216981887817383\n",
      "Loss at step 8400 : 3.459660291671753\n",
      "Loss at step 8450 : 3.4942378997802734\n",
      "Loss at step 8500 : 2.2131600379943848\n",
      "Loss at step 8550 : 2.9647326469421387\n",
      "Loss at step 8600 : 2.766028881072998\n",
      "Loss at step 8650 : 3.293308734893799\n",
      "Loss at step 8700 : 2.1466712951660156\n",
      "Loss at step 8750 : 3.030776023864746\n",
      "Loss at step 8800 : 3.020383596420288\n",
      "Loss at step 8850 : 3.0149905681610107\n",
      "Loss at step 8900 : 2.5159788131713867\n",
      "Loss at step 8950 : 1.6153228282928467\n",
      "Loss at step 9000 : 3.5599722862243652\n",
      "Loss at step 9050 : 2.4010725021362305\n",
      "Loss at step 9100 : 2.6433205604553223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 9150 : 2.930691719055176\n",
      "Loss at step 9200 : 4.0857110023498535\n",
      "Loss at step 9250 : 2.8266191482543945\n",
      "Loss at step 9300 : 3.566781520843506\n",
      "Loss at step 9350 : 2.4716339111328125\n",
      "Loss at step 9400 : 5.131914138793945\n",
      "Loss at step 9450 : 2.446767807006836\n",
      "Loss at step 9500 : 2.481318950653076\n",
      "Loss at step 9550 : 2.385599136352539\n",
      "Loss at step 9600 : 2.7752575874328613\n",
      "Loss at step 9650 : 2.328319549560547\n",
      "Loss at step 9700 : 3.378056526184082\n",
      "Loss at step 9750 : 2.5716638565063477\n",
      "Loss at step 9800 : 2.9947280883789062\n",
      "Loss at step 9850 : 3.794292449951172\n",
      "Loss at step 9900 : 2.9429702758789062\n",
      "Loss at step 9950 : 3.3360559940338135\n",
      "Loss at step 10000 : 2.7206146717071533\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 10050 : 2.527853488922119\n",
      "Loss at step 10100 : 3.3898813724517822\n",
      "Loss at step 10150 : 3.0927748680114746\n",
      "Loss at step 10200 : 3.716696262359619\n",
      "Loss at step 10250 : 2.3537843227386475\n",
      "Loss at step 10300 : 3.9681901931762695\n",
      "Loss at step 10350 : 3.4852147102355957\n",
      "Loss at step 10400 : 3.1106138229370117\n",
      "Loss at step 10450 : 2.879638433456421\n",
      "Loss at step 10500 : 2.5016164779663086\n",
      "Loss at step 10550 : 3.660970687866211\n",
      "Loss at step 10600 : 2.528318405151367\n",
      "Loss at step 10650 : 3.1740713119506836\n",
      "Loss at step 10700 : 3.6238210201263428\n",
      "Loss at step 10750 : 1.4613006114959717\n",
      "Loss at step 10800 : 1.9498164653778076\n",
      "Loss at step 10850 : 2.7294023036956787\n",
      "Loss at step 10900 : 3.6416678428649902\n",
      "Loss at step 10950 : 3.303030014038086\n",
      "Loss at step 11000 : 4.068336486816406\n",
      "Loss at step 11050 : 3.1582517623901367\n",
      "Loss at step 11100 : 3.5831832885742188\n",
      "Loss at step 11150 : 2.8681018352508545\n",
      "Loss at step 11200 : 2.525017261505127\n",
      "Loss at step 11250 : 2.5092577934265137\n",
      "Loss at step 11300 : 3.3290555477142334\n",
      "Loss at step 11350 : 2.4277219772338867\n",
      "Loss at step 11400 : 3.040025472640991\n",
      "Loss at step 11450 : 2.9332456588745117\n",
      "Loss at step 11500 : 2.6996402740478516\n",
      "Loss at step 11550 : 3.6067874431610107\n",
      "Loss at step 11600 : 3.7605092525482178\n",
      "Loss at step 11650 : 2.6774566173553467\n",
      "Loss at step 11700 : 2.3725082874298096\n",
      "Loss at step 11750 : 1.8789851665496826\n",
      "Loss at step 11800 : 3.2622172832489014\n",
      "Loss at step 11850 : 6.036306381225586\n",
      "Loss at step 11900 : 2.1975879669189453\n",
      "Loss at step 11950 : 3.4427666664123535\n",
      "Loss at step 12000 : 3.019390821456909\n",
      "Loss at step 12050 : 2.9151463508605957\n",
      "Loss at step 12100 : 2.6949026584625244\n",
      "Loss at step 12150 : 3.044567108154297\n",
      "Loss at step 12200 : 4.025540351867676\n",
      "Loss at step 12250 : 2.9900054931640625\n",
      "Loss at step 12300 : 2.476637601852417\n",
      "Loss at step 12350 : 2.558004379272461\n",
      "Loss at step 12400 : 3.9391860961914062\n",
      "Loss at step 12450 : 2.394299030303955\n",
      "Loss at step 12500 : 4.20713996887207\n",
      "Loss at step 12550 : 2.5595297813415527\n",
      "Loss at step 12600 : 3.534329652786255\n",
      "Loss at step 12650 : 3.3424975872039795\n",
      "Loss at step 12700 : 3.1752583980560303\n",
      "Loss at step 12750 : 2.309475898742676\n",
      "Loss at step 12800 : 2.837125778198242\n",
      "Loss at step 12850 : 4.636882305145264\n",
      "Loss at step 12900 : 2.949208974838257\n",
      "Loss at step 12950 : 2.498507022857666\n",
      "Loss at step 13000 : 1.9804245233535767\n",
      "Loss at step 13050 : 2.4308834075927734\n",
      "Loss at step 13100 : 2.634239912033081\n",
      "Loss at step 13150 : 3.1917147636413574\n",
      "Loss at step 13200 : 4.365019798278809\n",
      "Loss at step 13250 : 3.6540658473968506\n",
      "Loss at step 13300 : 4.518714904785156\n",
      "Loss at step 13350 : 3.2233076095581055\n",
      "Loss at step 13400 : 3.1023342609405518\n",
      "Loss at step 13450 : 2.718362331390381\n",
      "Loss at step 13500 : 2.385684013366699\n",
      "Loss at step 13550 : 3.5503463745117188\n",
      "Loss at step 13600 : 3.5491981506347656\n",
      "Loss at step 13650 : 3.7696235179901123\n",
      "Loss at step 13700 : 3.551440715789795\n",
      "Loss at step 13750 : 3.3969950675964355\n",
      "Loss at step 13800 : 2.7076056003570557\n",
      "Loss at step 13850 : 2.494569778442383\n",
      "Loss at step 13900 : 2.543200969696045\n",
      "Loss at step 13950 : 2.716701030731201\n",
      "Loss at step 14000 : 3.292215347290039\n",
      "Loss at step 14050 : 1.7624250650405884\n",
      "Loss at step 14100 : 2.4195334911346436\n",
      "Loss at step 14150 : 2.6116490364074707\n",
      "Loss at step 14200 : 3.769704818725586\n",
      "Loss at step 14250 : 3.3306939601898193\n",
      "Loss at step 14300 : 2.8100621700286865\n",
      "Loss at step 14350 : 1.970154047012329\n",
      "Loss at step 14400 : 3.3661131858825684\n",
      "Loss at step 14450 : 2.448920249938965\n",
      "Loss at step 14500 : 3.1150646209716797\n",
      "Loss at step 14550 : 3.5238466262817383\n",
      "Loss at step 14600 : 2.504025936126709\n",
      "Loss at step 14650 : 2.611823558807373\n",
      "Loss at step 14700 : 2.3676414489746094\n",
      "Loss at step 14750 : 2.3364124298095703\n",
      "Loss at step 14800 : 2.662627696990967\n",
      "Loss at step 14850 : 2.8744471073150635\n",
      "Loss at step 14900 : 2.846461296081543\n",
      "Loss at step 14950 : 4.096115589141846\n",
      "Loss at step 15000 : 3.20237135887146\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 15050 : 3.1095380783081055\n",
      "Loss at step 15100 : 4.575878143310547\n",
      "Loss at step 15150 : 2.4903972148895264\n",
      "Loss at step 15200 : 3.7609992027282715\n",
      "Loss at step 15250 : 2.791790723800659\n",
      "Loss at step 15300 : 2.1881794929504395\n",
      "Loss at step 15350 : 2.3735532760620117\n",
      "Loss at step 15400 : 2.8576674461364746\n",
      "Loss at step 15450 : 3.4719078540802\n",
      "Loss at step 15500 : 3.0413057804107666\n",
      "Loss at step 15550 : 4.504538536071777\n",
      "Loss at step 15600 : 2.274914503097534\n",
      "Loss at step 15650 : 2.977816104888916\n",
      "Loss at step 15700 : 2.5809662342071533\n",
      "Loss at step 15750 : 2.776944637298584\n",
      "Loss at step 15800 : 2.867612838745117\n",
      "Loss at step 15850 : 3.436495304107666\n",
      "Loss at step 15900 : 4.1249542236328125\n",
      "Loss at step 15950 : 2.522437572479248\n",
      "Loss at step 16000 : 2.9333226680755615\n",
      "Loss at step 16050 : 2.0832741260528564\n",
      "Loss at step 16100 : 2.7372794151306152\n",
      "Loss at step 16150 : 2.627812385559082\n",
      "Loss at step 16200 : 3.923647403717041\n",
      "Loss at step 16250 : 3.224139451980591\n",
      "Loss at step 16300 : 3.000105857849121\n",
      "Loss at step 16350 : 2.408703088760376\n",
      "Loss at step 16400 : 2.59360933303833\n",
      "Loss at step 16450 : 2.6658949851989746\n",
      "Loss at step 16500 : 2.4111862182617188\n",
      "Loss at step 16550 : 2.1432065963745117\n",
      "Loss at step 16600 : 2.7291600704193115\n",
      "Loss at step 16650 : 2.656771659851074\n",
      "Loss at step 16700 : 3.43709135055542\n",
      "Loss at step 16750 : 2.365267276763916\n",
      "Loss at step 16800 : 2.8930859565734863\n",
      "Loss at step 16850 : 2.659625768661499\n",
      "Loss at step 16900 : 3.6269431114196777\n",
      "Loss at step 16950 : 2.319261074066162\n",
      "Loss at step 17000 : 2.8101749420166016\n",
      "Loss at step 17050 : 3.9806177616119385\n",
      "Loss at step 17100 : 2.889352560043335\n",
      "Loss at step 17150 : 2.6478629112243652\n",
      "Loss at step 17200 : 3.3947787284851074\n",
      "Loss at step 17250 : 2.468170642852783\n",
      "Loss at step 17300 : 3.3445563316345215\n",
      "Loss at step 17350 : 2.591775894165039\n",
      "Loss at step 17400 : 2.8401403427124023\n",
      "Loss at step 17450 : 3.9258623123168945\n",
      "Loss at step 17500 : 2.109955310821533\n",
      "Loss at step 17550 : 4.123416423797607\n",
      "Loss at step 17600 : 2.4594573974609375\n",
      "Loss at step 17650 : 2.957439422607422\n",
      "Loss at step 17700 : 1.7956674098968506\n",
      "Loss at step 17750 : 3.508262872695923\n",
      "Loss at step 17800 : 2.3300271034240723\n",
      "Loss at step 17850 : 3.2199811935424805\n",
      "Loss at step 17900 : 2.5972843170166016\n",
      "Loss at step 17950 : 3.251509189605713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 18000 : 2.313159465789795\n",
      "Loss at step 18050 : 2.3962340354919434\n",
      "Loss at step 18100 : 2.726264238357544\n",
      "Loss at step 18150 : 2.339568853378296\n",
      "Loss at step 18200 : 2.4435911178588867\n",
      "Loss at step 18250 : 2.3175196647644043\n",
      "Loss at step 18300 : 4.184787273406982\n",
      "Loss at step 18350 : 3.184863567352295\n",
      "Loss at step 18400 : 3.1035966873168945\n",
      "Loss at step 18450 : 2.820713520050049\n",
      "Loss at step 18500 : 5.439957141876221\n",
      "Loss at step 18550 : 2.454987049102783\n",
      "Loss at step 18600 : 1.398200511932373\n",
      "Loss at step 18650 : 2.779201030731201\n",
      "Loss at step 18700 : 2.3195924758911133\n",
      "Loss at step 18750 : 2.950563430786133\n",
      "Loss at step 18800 : 2.170604944229126\n",
      "Loss at step 18850 : 3.3816018104553223\n",
      "Loss at step 18900 : 3.273651361465454\n",
      "Loss at step 18950 : 2.157461643218994\n",
      "Loss at step 19000 : 3.2127790451049805\n",
      "Loss at step 19050 : 2.544257879257202\n",
      "Loss at step 19100 : 1.8404470682144165\n",
      "Loss at step 19150 : 3.6875367164611816\n",
      "Loss at step 19200 : 3.84039568901062\n",
      "Loss at step 19250 : 2.9375696182250977\n",
      "Loss at step 19300 : 3.4673542976379395\n",
      "Loss at step 19350 : 2.7678399085998535\n",
      "Loss at step 19400 : 3.9199604988098145\n",
      "Loss at step 19450 : 2.5112366676330566\n",
      "Loss at step 19500 : 2.7326436042785645\n",
      "Loss at step 19550 : 3.0935449600219727\n",
      "Loss at step 19600 : 2.065359354019165\n",
      "Loss at step 19650 : 3.051530361175537\n",
      "Loss at step 19700 : 2.3023128509521484\n",
      "Loss at step 19750 : 3.483985185623169\n",
      "Loss at step 19800 : 2.2826995849609375\n",
      "Loss at step 19850 : 2.613154172897339\n",
      "Loss at step 19900 : 2.7680978775024414\n",
      "Loss at step 19950 : 2.8558647632598877\n",
      "Loss at step 20000 : 3.481395721435547\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 20050 : 3.3577845096588135\n",
      "Loss at step 20100 : 3.8645777702331543\n",
      "Loss at step 20150 : 2.41471529006958\n",
      "Loss at step 20200 : 2.9366559982299805\n",
      "Loss at step 20250 : 4.608712196350098\n",
      "Loss at step 20300 : 2.735318660736084\n",
      "Loss at step 20350 : 2.473627805709839\n",
      "Loss at step 20400 : 2.4235892295837402\n",
      "Loss at step 20450 : 3.243896484375\n",
      "Loss at step 20500 : 2.3062334060668945\n",
      "Loss at step 20550 : 2.4301867485046387\n",
      "Loss at step 20600 : 2.9734106063842773\n",
      "Loss at step 20650 : 1.635717511177063\n",
      "Loss at step 20700 : 2.5706706047058105\n",
      "Loss at step 20750 : 2.7445788383483887\n",
      "Loss at step 20800 : 3.222440719604492\n",
      "Loss at step 20850 : 2.12308931350708\n",
      "Loss at step 20900 : 3.0782856941223145\n",
      "Loss at step 20950 : 2.4327149391174316\n",
      "Loss at step 21000 : 4.242786407470703\n",
      "Loss at step 21050 : 3.0050714015960693\n",
      "Loss at step 21100 : 3.531546115875244\n",
      "Loss at step 21150 : 4.139329433441162\n",
      "Loss at step 21200 : 2.2799062728881836\n",
      "Loss at step 21250 : 2.5172948837280273\n",
      "Loss at step 21300 : 1.6929104328155518\n",
      "Loss at step 21350 : 2.56640625\n",
      "Loss at step 21400 : 2.9442105293273926\n",
      "Loss at step 21450 : 2.620844602584839\n",
      "Loss at step 21500 : 2.608325481414795\n",
      "Loss at step 21550 : 3.5173189640045166\n",
      "Loss at step 21600 : 2.2817330360412598\n",
      "Loss at step 21650 : 2.0549750328063965\n",
      "Loss at step 21700 : 3.4794044494628906\n",
      "Loss at step 21750 : 3.807020664215088\n",
      "Loss at step 21800 : 3.0819945335388184\n",
      "Loss at step 21850 : 2.8223555088043213\n",
      "Loss at step 21900 : 3.1745119094848633\n",
      "Loss at step 21950 : 3.250046730041504\n",
      "Loss at step 22000 : 2.663787841796875\n",
      "Loss at step 22050 : 2.9766335487365723\n",
      "Loss at step 22100 : 2.509983777999878\n",
      "Loss at step 22150 : 3.195962905883789\n",
      "Loss at step 22200 : 2.418623685836792\n",
      "Loss at step 22250 : 2.2062363624572754\n",
      "Loss at step 22300 : 4.742336750030518\n",
      "Loss at step 22350 : 2.1638498306274414\n",
      "Loss at step 22400 : 5.461644172668457\n",
      "Loss at step 22450 : 2.3524422645568848\n",
      "Loss at step 22500 : 2.292734384536743\n",
      "Loss at step 22550 : 2.7929205894470215\n",
      "Loss at step 22600 : 2.895557403564453\n",
      "Loss at step 22650 : 3.6819076538085938\n",
      "Loss at step 22700 : 3.357466697692871\n",
      "Loss at step 22750 : 2.534186363220215\n",
      "Loss at step 22800 : 3.278394937515259\n",
      "Loss at step 22850 : 3.5461435317993164\n",
      "Loss at step 22900 : 2.652751922607422\n",
      "Loss at step 22950 : 3.0261754989624023\n",
      "Loss at step 23000 : 2.4489119052886963\n",
      "Loss at step 23050 : 2.8055591583251953\n",
      "Loss at step 23100 : 2.8227415084838867\n",
      "Loss at step 23150 : 2.320317268371582\n",
      "Loss at step 23200 : 4.7530198097229\n",
      "Loss at step 23250 : 2.6101603507995605\n",
      "Loss at step 23300 : 2.6647543907165527\n",
      "Loss at step 23350 : 2.3311173915863037\n",
      "Loss at step 23400 : 3.528337001800537\n",
      "Loss at step 23450 : 2.5317227840423584\n",
      "Loss at step 23500 : 4.180967330932617\n",
      "Loss at step 23550 : 2.601562261581421\n",
      "Loss at step 23600 : 3.507648229598999\n",
      "Loss at step 23650 : 3.450488805770874\n",
      "Loss at step 23700 : 2.6562910079956055\n",
      "Loss at step 23750 : 2.2588603496551514\n",
      "Loss at step 23800 : 2.5525646209716797\n",
      "Loss at step 23850 : 2.344360589981079\n",
      "Loss at step 23900 : 1.6702404022216797\n",
      "Loss at step 23950 : 3.2385215759277344\n",
      "Loss at step 24000 : 3.1831297874450684\n",
      "Loss at step 24050 : 2.8743631839752197\n",
      "Loss at step 24100 : 3.0161070823669434\n",
      "Loss at step 24150 : 2.2796730995178223\n",
      "Loss at step 24200 : 3.0071635246276855\n",
      "Loss at step 24250 : 2.4139604568481445\n",
      "Loss at step 24300 : 2.817749261856079\n",
      "Loss at step 24350 : 1.8402411937713623\n",
      "Loss at step 24400 : 2.544752836227417\n",
      "Loss at step 24450 : 3.120375633239746\n",
      "Loss at step 24500 : 2.080821990966797\n",
      "Loss at step 24550 : 2.465101957321167\n",
      "Loss at step 24600 : 2.4112038612365723\n",
      "Loss at step 24650 : 2.5141429901123047\n",
      "Loss at step 24700 : 3.5786995887756348\n",
      "Loss at step 24750 : 2.1360790729522705\n",
      "Loss at step 24800 : 2.8042049407958984\n",
      "Loss at step 24850 : 3.041001319885254\n",
      "Loss at step 24900 : 3.759023427963257\n",
      "Loss at step 24950 : 4.885318756103516\n",
      "Loss at step 25000 : 3.0645909309387207\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 25050 : 3.6989266872406006\n",
      "Loss at step 25100 : 3.511566638946533\n",
      "Loss at step 25150 : 2.391021251678467\n",
      "Loss at step 25200 : 3.648874044418335\n",
      "Loss at step 25250 : 2.83817982673645\n",
      "Loss at step 25300 : 3.286627769470215\n",
      "Loss at step 25350 : 2.5492868423461914\n",
      "Loss at step 25400 : 2.5466084480285645\n",
      "Loss at step 25450 : 2.2303876876831055\n",
      "Loss at step 25500 : 2.4722132682800293\n",
      "Loss at step 25550 : 2.509458303451538\n",
      "Loss at step 25600 : 2.7430615425109863\n",
      "Loss at step 25650 : 2.7030296325683594\n",
      "Loss at step 25700 : 3.4904940128326416\n",
      "Loss at step 25750 : 2.2187657356262207\n",
      "Loss at step 25800 : 2.802553653717041\n",
      "Loss at step 25850 : 2.964890956878662\n",
      "Loss at step 25900 : 3.002979278564453\n",
      "Loss at step 25950 : 3.015650987625122\n",
      "Loss at step 26000 : 3.2964181900024414\n",
      "Loss at step 26050 : 3.006788730621338\n",
      "Loss at step 26100 : 2.4712917804718018\n",
      "Loss at step 26150 : 3.5680692195892334\n",
      "Loss at step 26200 : 3.025261402130127\n",
      "Loss at step 26250 : 2.6214218139648438\n",
      "Loss at step 26300 : 2.6010777950286865\n",
      "Loss at step 26350 : 3.1763458251953125\n",
      "Loss at step 26400 : 3.0565133094787598\n",
      "Loss at step 26450 : 2.4748806953430176\n",
      "Loss at step 26500 : 3.215769052505493\n",
      "Loss at step 26550 : 2.333615303039551\n",
      "Loss at step 26600 : 5.123775482177734\n",
      "Loss at step 26650 : 2.6579790115356445\n",
      "Loss at step 26700 : 2.4755496978759766\n",
      "Loss at step 26750 : 2.3768420219421387\n",
      "Loss at step 26800 : 3.5665125846862793\n",
      "Loss at step 26850 : 2.559920072555542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 26900 : 3.5088276863098145\n",
      "Loss at step 26950 : 2.992300510406494\n",
      "Loss at step 27000 : 3.714038610458374\n",
      "Loss at step 27050 : 2.6908795833587646\n",
      "Loss at step 27100 : 2.6797518730163574\n",
      "Loss at step 27150 : 1.2789390087127686\n",
      "Loss at step 27200 : 3.0313377380371094\n",
      "Loss at step 27250 : 2.2482502460479736\n",
      "Loss at step 27300 : 3.199490547180176\n",
      "Loss at step 27350 : 2.2635364532470703\n",
      "Loss at step 27400 : 2.2613110542297363\n",
      "Loss at step 27450 : 2.3949344158172607\n",
      "Loss at step 27500 : 3.022629499435425\n",
      "Loss at step 27550 : 2.201231002807617\n",
      "Loss at step 27600 : 2.6531901359558105\n",
      "Loss at step 27650 : 3.2377724647521973\n",
      "Loss at step 27700 : 3.2486677169799805\n",
      "Loss at step 27750 : 4.949773788452148\n",
      "Loss at step 27800 : 3.434231758117676\n",
      "Loss at step 27850 : 3.0469167232513428\n",
      "Loss at step 27900 : 3.6374711990356445\n",
      "Loss at step 27950 : 2.801950454711914\n",
      "Loss at step 28000 : 2.633666515350342\n",
      "Loss at step 28050 : 2.4437761306762695\n",
      "Loss at step 28100 : 2.562628746032715\n",
      "Loss at step 28150 : 2.4119527339935303\n",
      "Loss at step 28200 : 3.2768354415893555\n",
      "Loss at step 28250 : 3.156799793243408\n",
      "Loss at step 28300 : 2.1175522804260254\n",
      "Loss at step 28350 : 2.9084725379943848\n",
      "Loss at step 28400 : 3.7738945484161377\n",
      "Loss at step 28450 : 1.903648853302002\n",
      "Loss at step 28500 : 2.961475133895874\n",
      "Loss at step 28550 : 2.6665358543395996\n",
      "Loss at step 28600 : 2.4283721446990967\n",
      "Loss at step 28650 : 2.3371636867523193\n",
      "Loss at step 28700 : 3.776743173599243\n",
      "Loss at step 28750 : 3.110365152359009\n",
      "Loss at step 28800 : 2.98734450340271\n",
      "Loss at step 28850 : 2.6198179721832275\n",
      "Loss at step 28900 : 3.210783004760742\n",
      "Loss at step 28950 : 3.0165395736694336\n",
      "Loss at step 29000 : 2.7056846618652344\n",
      "Loss at step 29050 : 2.8584842681884766\n",
      "Loss at step 29100 : 2.5122694969177246\n",
      "Loss at step 29150 : 2.9367029666900635\n",
      "Loss at step 29200 : 2.988825559616089\n",
      "Loss at step 29250 : 3.8650050163269043\n",
      "Loss at step 29300 : 2.484363555908203\n",
      "Loss at step 29350 : 2.671147584915161\n",
      "Loss at step 29400 : 3.380918025970459\n",
      "Loss at step 29450 : 3.5739502906799316\n",
      "Loss at step 29500 : 2.490832805633545\n",
      "Loss at step 29550 : 2.6644670963287354\n",
      "Loss at step 29600 : 2.7349705696105957\n",
      "Loss at step 29650 : 2.8100762367248535\n",
      "Loss at step 29700 : 2.7393274307250977\n",
      "Loss at step 29750 : 2.7824606895446777\n",
      "Loss at step 29800 : 3.4988131523132324\n",
      "Loss at step 29850 : 2.9899704456329346\n",
      "Loss at step 29900 : 3.222903251647949\n",
      "Loss at step 29950 : 2.4097976684570312\n",
      "Loss at step 30000 : 2.875474452972412\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 30050 : 2.642007350921631\n",
      "Loss at step 30100 : 3.9838244915008545\n",
      "Loss at step 30150 : 2.876817464828491\n",
      "Loss at step 30200 : 2.9340085983276367\n",
      "Loss at step 30250 : 3.7689177989959717\n",
      "Loss at step 30300 : 2.38015079498291\n",
      "Loss at step 30350 : 3.452239751815796\n",
      "Loss at step 30400 : 2.807699680328369\n",
      "Loss at step 30450 : 2.745441198348999\n",
      "Loss at step 30500 : 2.7804198265075684\n",
      "Loss at step 30550 : 2.86301851272583\n",
      "Loss at step 30600 : 2.534139633178711\n",
      "Loss at step 30650 : 4.204200744628906\n",
      "Loss at step 30700 : 3.124140977859497\n",
      "Loss at step 30750 : 2.7364182472229004\n",
      "Loss at step 30800 : 2.248274326324463\n",
      "Loss at step 30850 : 2.7908854484558105\n",
      "Loss at step 30900 : 3.0114073753356934\n",
      "Loss at step 30950 : 2.609508991241455\n",
      "Loss at step 31000 : 3.58005428314209\n",
      "Loss at step 31050 : 2.5159029960632324\n",
      "Loss at step 31100 : 3.0602200031280518\n",
      "Loss at step 31150 : 3.422215700149536\n",
      "Loss at step 31200 : 2.8794214725494385\n",
      "Loss at step 31250 : 2.428607940673828\n",
      "Loss at step 31300 : 3.3142809867858887\n",
      "Loss at step 31350 : 2.326932191848755\n",
      "Loss at step 31400 : 2.4262588024139404\n",
      "Loss at step 31450 : 3.0191049575805664\n",
      "Loss at step 31500 : 5.391489505767822\n",
      "Loss at step 31550 : 2.869459867477417\n",
      "Loss at step 31600 : 4.495797634124756\n",
      "Loss at step 31650 : 3.4268150329589844\n",
      "Loss at step 31700 : 2.5625619888305664\n",
      "Loss at step 31750 : 3.432880163192749\n",
      "Loss at step 31800 : 3.103497266769409\n",
      "Loss at step 31850 : 2.5641698837280273\n",
      "Loss at step 31900 : 1.5714020729064941\n",
      "Loss at step 31950 : 2.249640703201294\n",
      "Loss at step 32000 : 3.431942939758301\n",
      "Loss at step 32050 : 2.697436809539795\n",
      "Loss at step 32100 : 2.8477883338928223\n",
      "Loss at step 32150 : 2.60687255859375\n",
      "Loss at step 32200 : 3.5493905544281006\n",
      "Loss at step 32250 : 2.3859195709228516\n",
      "Loss at step 32300 : 2.415445327758789\n",
      "Loss at step 32350 : 3.0024116039276123\n",
      "Loss at step 32400 : 3.313213586807251\n",
      "Loss at step 32450 : 3.087890148162842\n",
      "Loss at step 32500 : 2.6235551834106445\n",
      "Loss at step 32550 : 5.08992862701416\n",
      "Loss at step 32600 : 2.583415985107422\n",
      "Loss at step 32650 : 3.3337252140045166\n",
      "Loss at step 32700 : 2.574294090270996\n",
      "Loss at step 32750 : 3.8149046897888184\n",
      "Loss at step 32800 : 2.7058403491973877\n",
      "Loss at step 32850 : 2.3072447776794434\n",
      "Loss at step 32900 : 2.6350784301757812\n",
      "Loss at step 32950 : 3.232923984527588\n",
      "Loss at step 33000 : 2.530925750732422\n",
      "Loss at step 33050 : 2.5722403526306152\n",
      "Loss at step 33100 : 2.612077236175537\n",
      "Loss at step 33150 : 2.4416327476501465\n",
      "Loss at step 33200 : 3.3033370971679688\n",
      "Loss at step 33250 : 2.23138689994812\n",
      "Loss at step 33300 : 2.612977981567383\n",
      "Loss at step 33350 : 2.559168815612793\n",
      "Loss at step 33400 : 2.9528300762176514\n",
      "Loss at step 33450 : 2.485459089279175\n",
      "Loss at step 33500 : 2.214506149291992\n",
      "Loss at step 33550 : 2.9870667457580566\n",
      "Loss at step 33600 : 3.083761692047119\n",
      "Loss at step 33650 : 3.3265018463134766\n",
      "Loss at step 33700 : 2.8085086345672607\n",
      "Loss at step 33750 : 3.9492299556732178\n",
      "Loss at step 33800 : 3.1100361347198486\n",
      "Loss at step 33850 : 2.4721639156341553\n",
      "Loss at step 33900 : 4.267100811004639\n",
      "Loss at step 33950 : 2.6344685554504395\n",
      "Loss at step 34000 : 2.647792339324951\n",
      "Loss at step 34050 : 2.656278133392334\n",
      "Loss at step 34100 : 4.215519905090332\n",
      "Loss at step 34150 : 2.580636978149414\n",
      "Loss at step 34200 : 2.3738114833831787\n",
      "Loss at step 34250 : 2.7915024757385254\n",
      "Loss at step 34300 : 2.446647882461548\n",
      "Loss at step 34350 : 2.637000560760498\n",
      "Loss at step 34400 : 1.9196175336837769\n",
      "Loss at step 34450 : 2.6906564235687256\n",
      "Loss at step 34500 : 2.8740663528442383\n",
      "Loss at step 34550 : 2.782362461090088\n",
      "Loss at step 34600 : 3.153348445892334\n",
      "Loss at step 34650 : 2.175294876098633\n",
      "Loss at step 34700 : 2.015061140060425\n",
      "Loss at step 34750 : 3.288525104522705\n",
      "Loss at step 34800 : 2.528778076171875\n",
      "Loss at step 34850 : 2.3197503089904785\n",
      "Loss at step 34900 : 1.77150297164917\n",
      "Loss at step 34950 : 3.4317879676818848\n",
      "Loss at step 35000 : 2.8455569744110107\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 35050 : 2.552614212036133\n",
      "Loss at step 35100 : 3.450627088546753\n",
      "Loss at step 35150 : 2.849764108657837\n",
      "Loss at step 35200 : 2.9588305950164795\n",
      "Loss at step 35250 : 2.417468547821045\n",
      "Loss at step 35300 : 3.2368531227111816\n",
      "Loss at step 35350 : 3.1501736640930176\n",
      "Loss at step 35400 : 2.716381549835205\n",
      "Loss at step 35450 : 3.6318697929382324\n",
      "Loss at step 35500 : 2.985598087310791\n",
      "Loss at step 35550 : 3.3849079608917236\n",
      "Loss at step 35600 : 2.190423011779785\n",
      "Loss at step 35650 : 2.878880739212036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 35700 : 2.9197592735290527\n",
      "Loss at step 35750 : 2.7628705501556396\n",
      "Loss at step 35800 : 2.196964740753174\n",
      "Loss at step 35850 : 3.074491262435913\n",
      "Loss at step 35900 : 2.915529489517212\n",
      "Loss at step 35950 : 3.5876684188842773\n",
      "Loss at step 36000 : 3.3381896018981934\n",
      "Loss at step 36050 : 2.9049363136291504\n",
      "Loss at step 36100 : 4.298178672790527\n",
      "Loss at step 36150 : 4.756310939788818\n",
      "Loss at step 36200 : 4.365467071533203\n",
      "Loss at step 36250 : 2.73508620262146\n",
      "Loss at step 36300 : 3.120785713195801\n",
      "Loss at step 36350 : 3.5026278495788574\n",
      "Loss at step 36400 : 3.356668472290039\n",
      "Loss at step 36450 : 5.014613628387451\n",
      "Loss at step 36500 : 3.869354248046875\n",
      "Loss at step 36550 : 3.190053939819336\n",
      "Loss at step 36600 : 3.0029754638671875\n",
      "Loss at step 36650 : 2.8165903091430664\n",
      "Loss at step 36700 : 2.2005350589752197\n",
      "Loss at step 36750 : 2.4575252532958984\n",
      "Loss at step 36800 : 3.2301025390625\n",
      "Loss at step 36850 : 2.4443411827087402\n",
      "Loss at step 36900 : 2.6079750061035156\n",
      "Loss at step 36950 : 2.5292601585388184\n",
      "Loss at step 37000 : 2.0602612495422363\n",
      "Loss at step 37050 : 2.4493823051452637\n",
      "Loss at step 37100 : 3.0632779598236084\n",
      "Loss at step 37150 : 3.312957763671875\n",
      "Loss at step 37200 : 2.9518661499023438\n",
      "Loss at step 37250 : 3.630673885345459\n",
      "Loss at step 37300 : 2.736584186553955\n",
      "Loss at step 37350 : 2.620016098022461\n",
      "Loss at step 37400 : 2.730085849761963\n",
      "Loss at step 37450 : 3.1885344982147217\n",
      "Loss at step 37500 : 2.3641514778137207\n",
      "Loss at step 37550 : 2.231840133666992\n",
      "Loss at step 37600 : 1.690733551979065\n",
      "Loss at step 37650 : 2.789381265640259\n",
      "Loss at step 37700 : 2.412055253982544\n",
      "Loss at step 37750 : 2.0254855155944824\n",
      "Loss at step 37800 : 2.487821578979492\n",
      "Loss at step 37850 : 3.557208776473999\n",
      "Loss at step 37900 : 2.409759521484375\n",
      "Loss at step 37950 : 2.686117649078369\n",
      "Loss at step 38000 : 2.89538836479187\n",
      "Loss at step 38050 : 2.76920223236084\n",
      "Loss at step 38100 : 3.1314499378204346\n",
      "Loss at step 38150 : 3.081192970275879\n",
      "Loss at step 38200 : 2.9313905239105225\n",
      "Loss at step 38250 : 2.680255174636841\n",
      "Loss at step 38300 : 2.518751621246338\n",
      "Loss at step 38350 : 3.3632686138153076\n",
      "Loss at step 38400 : 3.351076126098633\n",
      "Loss at step 38450 : 2.912433385848999\n",
      "Loss at step 38500 : 3.5145325660705566\n",
      "Loss at step 38550 : 3.368767738342285\n",
      "Loss at step 38600 : 2.6586344242095947\n",
      "Loss at step 38650 : 2.879395008087158\n",
      "Loss at step 38700 : 2.785762071609497\n",
      "Loss at step 38750 : 2.929265022277832\n",
      "Loss at step 38800 : 2.7757983207702637\n",
      "Loss at step 38850 : 2.857769012451172\n",
      "Loss at step 38900 : 2.936793327331543\n",
      "Loss at step 38950 : 2.759056329727173\n",
      "Loss at step 39000 : 3.317544937133789\n",
      "Loss at step 39050 : 3.7772326469421387\n",
      "Loss at step 39100 : 2.75970196723938\n",
      "Loss at step 39150 : 3.1058714389801025\n",
      "Loss at step 39200 : 2.334726095199585\n",
      "Loss at step 39250 : 3.030407428741455\n",
      "Loss at step 39300 : 2.532787322998047\n",
      "Loss at step 39350 : 3.577106475830078\n",
      "Loss at step 39400 : 3.2052559852600098\n",
      "Loss at step 39450 : 2.5744516849517822\n",
      "Loss at step 39500 : 4.687472343444824\n",
      "Loss at step 39550 : 2.6783814430236816\n",
      "Loss at step 39600 : 2.3052010536193848\n",
      "Loss at step 39650 : 1.953997254371643\n",
      "Loss at step 39700 : 2.1444220542907715\n",
      "Loss at step 39750 : 3.3219122886657715\n",
      "Loss at step 39800 : 3.472156524658203\n",
      "Loss at step 39850 : 5.300858020782471\n",
      "Loss at step 39900 : 2.8536014556884766\n",
      "Loss at step 39950 : 3.291302442550659\n",
      "Loss at step 40000 : 2.6940009593963623\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 40050 : 2.8089611530303955\n",
      "Loss at step 40100 : 3.3689815998077393\n",
      "Loss at step 40150 : 2.8852427005767822\n",
      "Loss at step 40200 : 2.8066272735595703\n",
      "Loss at step 40250 : 2.6891136169433594\n",
      "Loss at step 40300 : 4.90640926361084\n",
      "Loss at step 40350 : 2.7236335277557373\n",
      "Loss at step 40400 : 2.4938759803771973\n",
      "Loss at step 40450 : 2.867793560028076\n",
      "Loss at step 40500 : 2.4720070362091064\n",
      "Loss at step 40550 : 3.327561855316162\n",
      "Loss at step 40600 : 2.675424098968506\n",
      "Loss at step 40650 : 2.454483985900879\n",
      "Loss at step 40700 : 3.680391788482666\n",
      "Loss at step 40750 : 2.2639822959899902\n",
      "Loss at step 40800 : 2.179682731628418\n",
      "Loss at step 40850 : 3.1271371841430664\n",
      "Loss at step 40900 : 3.1443634033203125\n",
      "Loss at step 40950 : 2.415982246398926\n",
      "Loss at step 41000 : 3.6011884212493896\n",
      "Loss at step 41050 : 3.2769274711608887\n",
      "Loss at step 41100 : 3.3925182819366455\n",
      "Loss at step 41150 : 5.012004852294922\n",
      "Loss at step 41200 : 2.7463321685791016\n",
      "Loss at step 41250 : 2.4716086387634277\n",
      "Loss at step 41300 : 3.4375128746032715\n",
      "Loss at step 41350 : 2.7661123275756836\n",
      "Loss at step 41400 : 2.49491548538208\n",
      "Loss at step 41450 : 2.7411112785339355\n",
      "Loss at step 41500 : 2.7892329692840576\n",
      "Loss at step 41550 : 2.433641195297241\n",
      "Loss at step 41600 : 2.653919219970703\n",
      "Loss at step 41650 : 2.5177745819091797\n",
      "Loss at step 41700 : 3.2554659843444824\n",
      "Loss at step 41750 : 2.7649645805358887\n",
      "Loss at step 41800 : 2.320345640182495\n",
      "Loss at step 41850 : 3.090839385986328\n",
      "Loss at step 41900 : 2.347461700439453\n",
      "Loss at step 41950 : 3.0665881633758545\n",
      "Loss at step 42000 : 2.9818265438079834\n",
      "Loss at step 42050 : 2.7098448276519775\n",
      "Loss at step 42100 : 3.6333305835723877\n",
      "Loss at step 42150 : 3.1212730407714844\n",
      "Loss at step 42200 : 2.091881275177002\n",
      "Loss at step 42250 : 2.4440784454345703\n",
      "Loss at step 42300 : 3.177992820739746\n",
      "Loss at step 42350 : 2.4719436168670654\n",
      "Loss at step 42400 : 4.262807369232178\n",
      "Loss at step 42450 : 2.0411112308502197\n",
      "Loss at step 42500 : 2.5585670471191406\n",
      "Loss at step 42550 : 3.098276138305664\n",
      "Loss at step 42600 : 2.4659481048583984\n",
      "Loss at step 42650 : 3.399022102355957\n",
      "Loss at step 42700 : 2.663933038711548\n",
      "Loss at step 42750 : 2.9512581825256348\n",
      "Loss at step 42800 : 2.8556742668151855\n",
      "Loss at step 42850 : 2.7266368865966797\n",
      "Loss at step 42900 : 4.355532646179199\n",
      "Loss at step 42950 : 4.117979526519775\n",
      "Loss at step 43000 : 2.5648350715637207\n",
      "Loss at step 43050 : 2.712444305419922\n",
      "Loss at step 43100 : 2.6144979000091553\n",
      "Loss at step 43150 : 2.6369009017944336\n",
      "Loss at step 43200 : 3.009915828704834\n",
      "Loss at step 43250 : 2.852700710296631\n",
      "Loss at step 43300 : 2.65555739402771\n",
      "Loss at step 43350 : 2.784545421600342\n",
      "Loss at step 43400 : 2.8750991821289062\n",
      "Loss at step 43450 : 2.7450361251831055\n",
      "Loss at step 43500 : 3.961358070373535\n",
      "Loss at step 43550 : 2.1953659057617188\n",
      "Loss at step 43600 : 3.450125217437744\n",
      "Loss at step 43650 : 3.0231618881225586\n",
      "Loss at step 43700 : 3.2847869396209717\n",
      "Loss at step 43750 : 2.4763386249542236\n",
      "Loss at step 43800 : 3.4772379398345947\n",
      "Loss at step 43850 : 3.212332248687744\n",
      "Loss at step 43900 : 3.1504499912261963\n",
      "Loss at step 43950 : 3.781129837036133\n",
      "Loss at step 44000 : 3.369875907897949\n",
      "Loss at step 44050 : 2.4204211235046387\n",
      "Loss at step 44100 : 2.5498239994049072\n",
      "Loss at step 44150 : 2.764697790145874\n",
      "Loss at step 44200 : 2.5280261039733887\n",
      "Loss at step 44250 : 2.1931803226470947\n",
      "Loss at step 44300 : 1.49598228931427\n",
      "Loss at step 44350 : 3.0924785137176514\n",
      "Loss at step 44400 : 4.06002950668335\n",
      "Loss at step 44450 : 3.4263739585876465\n",
      "Loss at step 44500 : 2.623229742050171\n",
      "Loss at step 44550 : 3.425013542175293\n",
      "Loss at step 44600 : 2.6419544219970703\n",
      "Loss at step 44650 : 2.246190309524536\n",
      "Loss at step 44700 : 2.8714284896850586\n",
      "Loss at step 44750 : 2.599768877029419\n",
      "Loss at step 44800 : 2.982428550720215\n",
      "Loss at step 44850 : 3.227792739868164\n",
      "Loss at step 44900 : 2.9468207359313965\n",
      "Loss at step 44950 : 3.3458704948425293\n",
      "Loss at step 45000 : 2.745012044906616\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 45050 : 2.7766833305358887\n",
      "Loss at step 45100 : 2.51812744140625\n",
      "Loss at step 45150 : 2.907332181930542\n",
      "Loss at step 45200 : 2.5258026123046875\n",
      "Loss at step 45250 : 3.744718313217163\n",
      "Loss at step 45300 : 2.8791375160217285\n",
      "Loss at step 45350 : 2.885841131210327\n",
      "Loss at step 45400 : 3.114434242248535\n",
      "Loss at step 45450 : 3.214751958847046\n",
      "Loss at step 45500 : 3.423844575881958\n",
      "Loss at step 45550 : 3.0075254440307617\n",
      "Loss at step 45600 : 3.203125\n",
      "Loss at step 45650 : 2.7756049633026123\n",
      "Loss at step 45700 : 3.190863847732544\n",
      "Loss at step 45750 : 2.39113187789917\n",
      "Loss at step 45800 : 2.731549024581909\n",
      "Loss at step 45850 : 2.5115129947662354\n",
      "Loss at step 45900 : 3.3870108127593994\n",
      "Loss at step 45950 : 2.389962673187256\n",
      "Loss at step 46000 : 2.1191203594207764\n",
      "Loss at step 46050 : 2.771040678024292\n",
      "Loss at step 46100 : 2.5731544494628906\n",
      "Loss at step 46150 : 2.567490577697754\n",
      "Loss at step 46200 : 3.028702974319458\n",
      "Loss at step 46250 : 2.7533888816833496\n",
      "Loss at step 46300 : 2.336042881011963\n",
      "Loss at step 46350 : 2.8565497398376465\n",
      "Loss at step 46400 : 2.5941083431243896\n",
      "Loss at step 46450 : 3.2509005069732666\n",
      "Loss at step 46500 : 2.9598240852355957\n",
      "Loss at step 46550 : 2.5818428993225098\n",
      "Loss at step 46600 : 2.1584856510162354\n",
      "Loss at step 46650 : 3.2825253009796143\n",
      "Loss at step 46700 : 1.8836466073989868\n",
      "Loss at step 46750 : 3.2605268955230713\n",
      "Loss at step 46800 : 3.004351854324341\n",
      "Loss at step 46850 : 2.7883365154266357\n",
      "Loss at step 46900 : 2.811980724334717\n",
      "Loss at step 46950 : 2.2563419342041016\n",
      "Loss at step 47000 : 2.5349106788635254\n",
      "Loss at step 47050 : 2.3456759452819824\n",
      "Loss at step 47100 : 2.0972931385040283\n",
      "Loss at step 47150 : 3.877358913421631\n",
      "Loss at step 47200 : 2.8658995628356934\n",
      "Loss at step 47250 : 2.2970733642578125\n",
      "Loss at step 47300 : 2.975912094116211\n",
      "Loss at step 47350 : 3.0727603435516357\n",
      "Loss at step 47400 : 2.6333627700805664\n",
      "Loss at step 47450 : 3.1044507026672363\n",
      "Loss at step 47500 : 2.8721179962158203\n",
      "Loss at step 47550 : 2.6896257400512695\n",
      "Loss at step 47600 : 2.530928611755371\n",
      "Loss at step 47650 : 4.462889671325684\n",
      "Loss at step 47700 : 5.267998695373535\n",
      "Loss at step 47750 : 2.4322509765625\n",
      "Loss at step 47800 : 3.4483156204223633\n",
      "Loss at step 47850 : 2.525754690170288\n",
      "Loss at step 47900 : 2.699892520904541\n",
      "Loss at step 47950 : 4.676884651184082\n",
      "Loss at step 48000 : 2.6215288639068604\n",
      "Loss at step 48050 : 3.3782997131347656\n",
      "Loss at step 48100 : 2.3069772720336914\n",
      "Loss at step 48150 : 2.592240571975708\n",
      "Loss at step 48200 : 2.8141937255859375\n",
      "Loss at step 48250 : 2.6642775535583496\n",
      "Loss at step 48300 : 2.811509132385254\n",
      "Loss at step 48350 : 2.7182633876800537\n",
      "Loss at step 48400 : 3.6036901473999023\n",
      "Loss at step 48450 : 3.643848419189453\n",
      "Loss at step 48500 : 3.556311845779419\n",
      "Loss at step 48550 : 3.694258689880371\n",
      "Loss at step 48600 : 2.8283820152282715\n",
      "Loss at step 48650 : 2.511263370513916\n",
      "Loss at step 48700 : 2.5158798694610596\n",
      "Loss at step 48750 : 3.617384195327759\n",
      "Loss at step 48800 : 2.8536148071289062\n",
      "Loss at step 48850 : 3.492098808288574\n",
      "Loss at step 48900 : 2.0432119369506836\n",
      "Loss at step 48950 : 3.068086624145508\n",
      "Loss at step 49000 : 2.8378806114196777\n",
      "Loss at step 49050 : 2.3795197010040283\n",
      "Loss at step 49100 : 2.602468490600586\n",
      "Loss at step 49150 : 3.0132250785827637\n",
      "Loss at step 49200 : 2.678443670272827\n",
      "Loss at step 49250 : 2.671171188354492\n",
      "Loss at step 49300 : 3.1004793643951416\n",
      "Loss at step 49350 : 2.826504707336426\n",
      "Loss at step 49400 : 1.9776835441589355\n",
      "Loss at step 49450 : 2.4811344146728516\n",
      "Loss at step 49500 : 2.2822022438049316\n",
      "Loss at step 49550 : 3.1331722736358643\n",
      "Loss at step 49600 : 2.9168307781219482\n",
      "Loss at step 49650 : 3.231968641281128\n",
      "Loss at step 49700 : 3.0332446098327637\n",
      "Loss at step 49750 : 4.376010894775391\n",
      "Loss at step 49800 : 2.661508798599243\n",
      "Loss at step 49850 : 2.1316115856170654\n",
      "Loss at step 49900 : 2.105724811553955\n",
      "Loss at step 49950 : 3.003309488296509\n",
      "Loss at step 50000 : 2.885498523712158\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 50050 : 2.6962549686431885\n",
      "Loss at step 50100 : 3.199388265609741\n",
      "Loss at step 50150 : 2.790200710296631\n",
      "Loss at step 50200 : 3.562542676925659\n",
      "Loss at step 50250 : 3.1221656799316406\n",
      "Loss at step 50300 : 3.014881134033203\n",
      "Loss at step 50350 : 2.773216485977173\n",
      "Loss at step 50400 : 2.3726916313171387\n",
      "Loss at step 50450 : 3.1356699466705322\n",
      "Loss at step 50500 : 2.977527618408203\n",
      "Loss at step 50550 : 3.4227969646453857\n",
      "Loss at step 50600 : 3.4532811641693115\n",
      "Loss at step 50650 : 3.46128511428833\n",
      "Loss at step 50700 : 2.004626512527466\n",
      "Loss at step 50750 : 2.6736197471618652\n",
      "Loss at step 50800 : 4.468973159790039\n",
      "Loss at step 50850 : 2.5295557975769043\n",
      "Loss at step 50900 : 3.83286714553833\n",
      "Loss at step 50950 : 2.8085265159606934\n",
      "Loss at step 51000 : 1.95453941822052\n",
      "Loss at step 51050 : 2.801570415496826\n",
      "Loss at step 51100 : 2.635089635848999\n",
      "Loss at step 51150 : 4.2847466468811035\n",
      "Loss at step 51200 : 3.4584708213806152\n",
      "Loss at step 51250 : 3.0304365158081055\n",
      "Loss at step 51300 : 3.0954267978668213\n",
      "Loss at step 51350 : 3.192066192626953\n",
      "Loss at step 51400 : 2.9255943298339844\n",
      "Loss at step 51450 : 2.895646572113037\n",
      "Loss at step 51500 : 3.067108154296875\n",
      "Loss at step 51550 : 3.0101022720336914\n",
      "Loss at step 51600 : 2.7124016284942627\n",
      "Loss at step 51650 : 2.5293068885803223\n",
      "Loss at step 51700 : 2.595005512237549\n",
      "Loss at step 51750 : 2.27486515045166\n",
      "Loss at step 51800 : 2.814021110534668\n",
      "Loss at step 51850 : 2.0578629970550537\n",
      "Loss at step 51900 : 3.392282485961914\n",
      "Loss at step 51950 : 4.115750789642334\n",
      "Loss at step 52000 : 2.8416409492492676\n",
      "Loss at step 52050 : 3.0939855575561523\n",
      "Loss at step 52100 : 3.401780605316162\n",
      "Loss at step 52150 : 2.8762741088867188\n",
      "Loss at step 52200 : 3.7370758056640625\n",
      "Loss at step 52250 : 2.163038730621338\n",
      "Loss at step 52300 : 2.2225232124328613\n",
      "Loss at step 52350 : 2.363607406616211\n",
      "Loss at step 52400 : 2.51072359085083\n",
      "Loss at step 52450 : 3.1889002323150635\n",
      "Loss at step 52500 : 2.3104588985443115\n",
      "Loss at step 52550 : 1.4820899963378906\n",
      "Loss at step 52600 : 3.1354007720947266\n",
      "Loss at step 52650 : 2.468503475189209\n",
      "Loss at step 52700 : 2.673882246017456\n",
      "Loss at step 52750 : 3.029021739959717\n",
      "Loss at step 52800 : 2.3737940788269043\n",
      "Loss at step 52850 : 2.9085936546325684\n",
      "Loss at step 52900 : 2.458116054534912\n",
      "Loss at step 52950 : 3.5168659687042236\n",
      "Loss at step 53000 : 2.0310184955596924\n",
      "Loss at step 53050 : 3.257380485534668\n",
      "Loss at step 53100 : 3.6610045433044434\n",
      "Loss at step 53150 : 5.041568756103516\n",
      "Loss at step 53200 : 4.144880294799805\n",
      "Loss at step 53250 : 2.996217727661133\n",
      "Loss at step 53300 : 2.515209197998047\n",
      "Loss at step 53350 : 2.578582286834717\n",
      "Loss at step 53400 : 2.5069384574890137\n",
      "Loss at step 53450 : 4.192586421966553\n",
      "Loss at step 53500 : 2.447510242462158\n",
      "Loss at step 53550 : 1.8989477157592773\n",
      "Loss at step 53600 : 3.2597897052764893\n",
      "Loss at step 53650 : 2.194605588912964\n",
      "Loss at step 53700 : 2.408851385116577\n",
      "Loss at step 53750 : 2.8528432846069336\n",
      "Loss at step 53800 : 2.3777074813842773\n",
      "Loss at step 53850 : 2.1631903648376465\n",
      "Loss at step 53900 : 1.7044183015823364\n",
      "Loss at step 53950 : 1.5114412307739258\n",
      "Loss at step 54000 : 3.453886032104492\n",
      "Loss at step 54050 : 2.0782594680786133\n",
      "Loss at step 54100 : 2.904426097869873\n",
      "Loss at step 54150 : 3.8254806995391846\n",
      "Loss at step 54200 : 2.8403830528259277\n",
      "Loss at step 54250 : 3.6097817420959473\n",
      "Loss at step 54300 : 2.9091439247131348\n",
      "Loss at step 54350 : 2.349294900894165\n",
      "Loss at step 54400 : 3.7424659729003906\n",
      "Loss at step 54450 : 2.5222344398498535\n",
      "Loss at step 54500 : 3.275721311569214\n",
      "Loss at step 54550 : 2.7130813598632812\n",
      "Loss at step 54600 : 3.4045469760894775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 54650 : 2.454263687133789\n",
      "Loss at step 54700 : 3.0572915077209473\n",
      "Loss at step 54750 : 4.671723365783691\n",
      "Loss at step 54800 : 2.222893714904785\n",
      "Loss at step 54850 : 4.06284761428833\n",
      "Loss at step 54900 : 3.5888454914093018\n",
      "Loss at step 54950 : 2.460597515106201\n",
      "Loss at step 55000 : 2.5278046131134033\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 55050 : 2.5386617183685303\n",
      "Loss at step 55100 : 2.96366286277771\n",
      "Loss at step 55150 : 2.922990322113037\n",
      "Loss at step 55200 : 2.2873666286468506\n",
      "Loss at step 55250 : 2.787668228149414\n",
      "Loss at step 55300 : 2.792006015777588\n",
      "Loss at step 55350 : 2.545546770095825\n",
      "Loss at step 55400 : 3.3955039978027344\n",
      "Loss at step 55450 : 3.0499844551086426\n",
      "Loss at step 55500 : 2.8432493209838867\n",
      "Loss at step 55550 : 2.4948697090148926\n",
      "Loss at step 55600 : 3.529279947280884\n",
      "Loss at step 55650 : 2.617863416671753\n",
      "Loss at step 55700 : 2.727586269378662\n",
      "Loss at step 55750 : 3.120431900024414\n",
      "Loss at step 55800 : 3.1241257190704346\n",
      "Loss at step 55850 : 1.96291184425354\n",
      "Loss at step 55900 : 2.5978522300720215\n",
      "Loss at step 55950 : 2.449389696121216\n",
      "Loss at step 56000 : 3.2180819511413574\n",
      "Loss at step 56050 : 2.56252384185791\n",
      "Loss at step 56100 : 2.363689422607422\n",
      "Loss at step 56150 : 4.103414535522461\n",
      "Loss at step 56200 : 3.0071606636047363\n",
      "Loss at step 56250 : 2.59584903717041\n",
      "Loss at step 56300 : 3.5803585052490234\n",
      "Loss at step 56350 : 2.856172800064087\n",
      "Loss at step 56400 : 4.669356346130371\n",
      "Loss at step 56450 : 4.8121514320373535\n",
      "Loss at step 56500 : 2.778503894805908\n",
      "Loss at step 56550 : 2.9643044471740723\n",
      "Loss at step 56600 : 2.7785892486572266\n",
      "Loss at step 56650 : 2.7112536430358887\n",
      "Loss at step 56700 : 3.3066153526306152\n",
      "Loss at step 56750 : 2.654468297958374\n",
      "Loss at step 56800 : 2.8848319053649902\n",
      "Loss at step 56850 : 3.189483165740967\n",
      "Loss at step 56900 : 2.9493672847747803\n",
      "Loss at step 56950 : 3.038276195526123\n",
      "Loss at step 57000 : 1.7616592645645142\n",
      "Loss at step 57050 : 2.9407994747161865\n",
      "Loss at step 57100 : 3.934507131576538\n",
      "Loss at step 57150 : 1.8222911357879639\n",
      "Loss at step 57200 : 4.550562858581543\n",
      "Loss at step 57250 : 3.0465781688690186\n",
      "Loss at step 57300 : 2.632659435272217\n",
      "Loss at step 57350 : 2.614871025085449\n",
      "Loss at step 57400 : 2.5777251720428467\n",
      "Loss at step 57450 : 2.338095188140869\n",
      "Loss at step 57500 : 3.233043670654297\n",
      "Loss at step 57550 : 2.600623369216919\n",
      "Loss at step 57600 : 2.4449610710144043\n",
      "Loss at step 57650 : 2.2591097354888916\n",
      "Loss at step 57700 : 2.6450095176696777\n",
      "Loss at step 57750 : 2.181206703186035\n",
      "Loss at step 57800 : 2.7577648162841797\n",
      "Loss at step 57850 : 2.5551247596740723\n",
      "Loss at step 57900 : 2.306328535079956\n",
      "Loss at step 57950 : 2.6225745677948\n",
      "Loss at step 58000 : 3.1223132610321045\n",
      "Loss at step 58050 : 3.408266544342041\n",
      "Loss at step 58100 : 3.8220856189727783\n",
      "Loss at step 58150 : 2.856436014175415\n",
      "Loss at step 58200 : 3.7567038536071777\n",
      "Loss at step 58250 : 2.1414546966552734\n",
      "Loss at step 58300 : 1.9901708364486694\n",
      "Loss at step 58350 : 3.1690778732299805\n",
      "Loss at step 58400 : 2.7240419387817383\n",
      "Loss at step 58450 : 2.8468515872955322\n",
      "Loss at step 58500 : 3.8152825832366943\n",
      "Loss at step 58550 : 2.2698283195495605\n",
      "Loss at step 58600 : 2.856076240539551\n",
      "Loss at step 58650 : 2.2022197246551514\n",
      "Loss at step 58700 : 2.612112522125244\n",
      "Loss at step 58750 : 2.8938398361206055\n",
      "Loss at step 58800 : 2.8230018615722656\n",
      "Loss at step 58850 : 2.1503214836120605\n",
      "Loss at step 58900 : 2.866398572921753\n",
      "Loss at step 58950 : 2.656877040863037\n",
      "Loss at step 59000 : 3.0399527549743652\n",
      "Loss at step 59050 : 2.388793468475342\n",
      "Loss at step 59100 : 2.9015564918518066\n",
      "Loss at step 59150 : 3.264932632446289\n",
      "Loss at step 59200 : 2.646793842315674\n",
      "Loss at step 59250 : 2.8280203342437744\n",
      "Loss at step 59300 : 2.39911150932312\n",
      "Loss at step 59350 : 2.5334882736206055\n",
      "Loss at step 59400 : 2.1499838829040527\n",
      "Loss at step 59450 : 2.868734359741211\n",
      "Loss at step 59500 : 3.0579915046691895\n",
      "Loss at step 59550 : 2.6330838203430176\n",
      "Loss at step 59600 : 2.710134506225586\n",
      "Loss at step 59650 : 1.9160795211791992\n",
      "Loss at step 59700 : 2.5990777015686035\n",
      "Loss at step 59750 : 3.032358169555664\n",
      "Loss at step 59800 : 2.4109573364257812\n",
      "Loss at step 59850 : 2.4154117107391357\n",
      "Loss at step 59900 : 2.7799437046051025\n",
      "Loss at step 59950 : 2.4697823524475098\n",
      "Loss at step 60000 : 3.5133297443389893\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 60050 : 2.7070465087890625\n",
      "Loss at step 60100 : 2.565195083618164\n",
      "Loss at step 60150 : 1.8037034273147583\n",
      "Loss at step 60200 : 2.6345252990722656\n",
      "Loss at step 60250 : 1.990821123123169\n",
      "Loss at step 60300 : 2.472426414489746\n",
      "Loss at step 60350 : 2.610276699066162\n",
      "Loss at step 60400 : 3.307253360748291\n",
      "Loss at step 60450 : 4.9933624267578125\n",
      "Loss at step 60500 : 2.8755125999450684\n",
      "Loss at step 60550 : 3.007524251937866\n",
      "Loss at step 60600 : 2.610191583633423\n",
      "Loss at step 60650 : 2.557502508163452\n",
      "Loss at step 60700 : 2.084533929824829\n",
      "Loss at step 60750 : 3.523405075073242\n",
      "Loss at step 60800 : 3.169764995574951\n",
      "Loss at step 60850 : 2.701444387435913\n",
      "Loss at step 60900 : 2.4384045600891113\n",
      "Loss at step 60950 : 2.4699056148529053\n",
      "Loss at step 61000 : 3.523536205291748\n",
      "Loss at step 61050 : 3.116974353790283\n",
      "Loss at step 61100 : 4.012790679931641\n",
      "Loss at step 61150 : 2.3210291862487793\n",
      "Loss at step 61200 : 3.6593122482299805\n",
      "Loss at step 61250 : 2.6056089401245117\n",
      "Loss at step 61300 : 3.4964611530303955\n",
      "Loss at step 61350 : 2.4874110221862793\n",
      "Loss at step 61400 : 2.7505581378936768\n",
      "Loss at step 61450 : 4.514421463012695\n",
      "Loss at step 61500 : 4.047941207885742\n",
      "Loss at step 61550 : 2.239457607269287\n",
      "Loss at step 61600 : 2.77526593208313\n",
      "Loss at step 61650 : 2.521751880645752\n",
      "Loss at step 61700 : 3.5126333236694336\n",
      "Loss at step 61750 : 2.474717378616333\n",
      "Loss at step 61800 : 2.747561454772949\n",
      "Loss at step 61850 : 3.6482796669006348\n",
      "Loss at step 61900 : 2.986873149871826\n",
      "Loss at step 61950 : 3.6492204666137695\n",
      "Loss at step 62000 : 3.0700912475585938\n",
      "Loss at step 62050 : 1.5914541482925415\n",
      "Loss at step 62100 : 3.1892166137695312\n",
      "Loss at step 62150 : 3.7813405990600586\n",
      "Loss at step 62200 : 2.2153093814849854\n",
      "Loss at step 62250 : 2.5122599601745605\n",
      "Loss at step 62300 : 2.33750581741333\n",
      "Loss at step 62350 : 2.704418659210205\n",
      "Loss at step 62400 : 3.4707369804382324\n",
      "Loss at step 62450 : 3.0095221996307373\n",
      "Loss at step 62500 : 2.2355916500091553\n",
      "Loss at step 62550 : 2.094653606414795\n",
      "Loss at step 62600 : 3.662198305130005\n",
      "Loss at step 62650 : 3.615814208984375\n",
      "Loss at step 62700 : 2.35710072517395\n",
      "Loss at step 62750 : 2.589081048965454\n",
      "Loss at step 62800 : 2.6163511276245117\n",
      "Loss at step 62850 : 3.01727294921875\n",
      "Loss at step 62900 : 2.7640533447265625\n",
      "Loss at step 62950 : 3.411832332611084\n",
      "Loss at step 63000 : 3.538142681121826\n",
      "Loss at step 63050 : 2.713521957397461\n",
      "Loss at step 63100 : 2.844503164291382\n",
      "Loss at step 63150 : 2.780655860900879\n",
      "Loss at step 63200 : 3.019439220428467\n",
      "Loss at step 63250 : 2.4906554222106934\n",
      "Loss at step 63300 : 3.631828546524048\n",
      "Loss at step 63350 : 2.8244833946228027\n",
      "Loss at step 63400 : 2.328433036804199\n",
      "Loss at step 63450 : 3.2662179470062256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 63500 : 2.4254953861236572\n",
      "Loss at step 63550 : 3.180607557296753\n",
      "Loss at step 63600 : 2.5792343616485596\n",
      "Loss at step 63650 : 3.5115184783935547\n",
      "Loss at step 63700 : 2.5311546325683594\n",
      "Loss at step 63750 : 3.9828438758850098\n",
      "Loss at step 63800 : 1.9277875423431396\n",
      "Loss at step 63850 : 3.7658839225769043\n",
      "Loss at step 63900 : 3.1013307571411133\n",
      "Loss at step 63950 : 2.1662216186523438\n",
      "Loss at step 64000 : 2.330043077468872\n",
      "Loss at step 64050 : 3.168379783630371\n",
      "Loss at step 64100 : 4.915131092071533\n",
      "Loss at step 64150 : 3.3536505699157715\n",
      "Loss at step 64200 : 3.025909423828125\n",
      "Loss at step 64250 : 2.8458433151245117\n",
      "Loss at step 64300 : 3.515390157699585\n",
      "Loss at step 64350 : 2.8466413021087646\n",
      "Loss at step 64400 : 2.193826675415039\n",
      "Loss at step 64450 : 2.6212551593780518\n",
      "Loss at step 64500 : 1.9885929822921753\n",
      "Loss at step 64550 : 3.225348949432373\n",
      "Loss at step 64600 : 3.1525657176971436\n",
      "Loss at step 64650 : 3.386655569076538\n",
      "Loss at step 64700 : 2.2179551124572754\n",
      "Loss at step 64750 : 3.51900053024292\n",
      "Loss at step 64800 : 2.6056435108184814\n",
      "Loss at step 64850 : 3.0707926750183105\n",
      "Loss at step 64900 : 2.406586170196533\n",
      "Loss at step 64950 : 3.3756885528564453\n",
      "Loss at step 65000 : 3.0088119506835938\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 65050 : 3.1238503456115723\n",
      "Loss at step 65100 : 2.8476178646087646\n",
      "Loss at step 65150 : 2.7638940811157227\n",
      "Loss at step 65200 : 3.252302646636963\n",
      "Loss at step 65250 : 1.9444952011108398\n",
      "Loss at step 65300 : 4.276328086853027\n",
      "Loss at step 65350 : 2.5192675590515137\n",
      "Loss at step 65400 : 2.0429134368896484\n",
      "Loss at step 65450 : 2.6855649948120117\n",
      "Loss at step 65500 : 3.1581006050109863\n",
      "Loss at step 65550 : 2.0801901817321777\n",
      "Loss at step 65600 : 2.466452121734619\n",
      "Loss at step 65650 : 4.018206596374512\n",
      "Loss at step 65700 : 3.3188278675079346\n",
      "Loss at step 65750 : 1.8967543840408325\n",
      "Loss at step 65800 : 2.841588020324707\n",
      "Loss at step 65850 : 2.9585418701171875\n",
      "Loss at step 65900 : 4.4638214111328125\n",
      "Loss at step 65950 : 2.624469757080078\n",
      "Loss at step 66000 : 3.1536927223205566\n",
      "Loss at step 66050 : 2.112396478652954\n",
      "Loss at step 66100 : 4.0722479820251465\n",
      "Loss at step 66150 : 2.477820634841919\n",
      "Loss at step 66200 : 3.7537431716918945\n",
      "Loss at step 66250 : 2.626558780670166\n",
      "Loss at step 66300 : 2.730247974395752\n",
      "Loss at step 66350 : 2.3697736263275146\n",
      "Loss at step 66400 : 2.37298583984375\n",
      "Loss at step 66450 : 3.0806961059570312\n",
      "Loss at step 66500 : 2.326847553253174\n",
      "Loss at step 66550 : 3.0386056900024414\n",
      "Loss at step 66600 : 1.9753748178482056\n",
      "Loss at step 66650 : 2.846958637237549\n",
      "Loss at step 66700 : 2.662226676940918\n",
      "Loss at step 66750 : 2.917084217071533\n",
      "Loss at step 66800 : 1.7353230714797974\n",
      "Loss at step 66850 : 3.03330659866333\n",
      "Loss at step 66900 : 2.6679515838623047\n",
      "Loss at step 66950 : 3.379392385482788\n",
      "Loss at step 67000 : 3.3746142387390137\n",
      "Loss at step 67050 : 3.1882529258728027\n",
      "Loss at step 67100 : 1.803836464881897\n",
      "Loss at step 67150 : 3.003213405609131\n",
      "Loss at step 67200 : 2.9580752849578857\n",
      "Loss at step 67250 : 2.333066463470459\n",
      "Loss at step 67300 : 2.296444892883301\n",
      "Loss at step 67350 : 3.3363189697265625\n",
      "Loss at step 67400 : 2.566185474395752\n",
      "Loss at step 67450 : 2.927840232849121\n",
      "Loss at step 67500 : 3.3743808269500732\n",
      "Loss at step 67550 : 2.8546130657196045\n",
      "Loss at step 67600 : 3.2803101539611816\n",
      "Loss at step 67650 : 2.5778353214263916\n",
      "Loss at step 67700 : 2.188913345336914\n",
      "Loss at step 67750 : 5.057950496673584\n",
      "Loss at step 67800 : 2.9948153495788574\n",
      "Loss at step 67850 : 2.5750198364257812\n",
      "Loss at step 67900 : 2.606553077697754\n",
      "Loss at step 67950 : 2.5310511589050293\n",
      "Loss at step 68000 : 3.180243492126465\n",
      "Loss at step 68050 : 3.2379848957061768\n",
      "Loss at step 68100 : 2.773118495941162\n",
      "Loss at step 68150 : 3.0246238708496094\n",
      "Loss at step 68200 : 2.3740525245666504\n",
      "Loss at step 68250 : 2.6634323596954346\n",
      "Loss at step 68300 : 2.2298712730407715\n",
      "Loss at step 68350 : 2.1810359954833984\n",
      "Loss at step 68400 : 2.8450846672058105\n",
      "Loss at step 68450 : 2.8710131645202637\n",
      "Loss at step 68500 : 2.669966220855713\n",
      "Loss at step 68550 : 2.1003241539001465\n",
      "Loss at step 68600 : 2.990515947341919\n",
      "Loss at step 68650 : 2.647439956665039\n",
      "Loss at step 68700 : 2.6405365467071533\n",
      "Loss at step 68750 : 2.3890061378479004\n",
      "Loss at step 68800 : 2.423168420791626\n",
      "Loss at step 68850 : 3.2331676483154297\n",
      "Loss at step 68900 : 2.994138479232788\n",
      "Loss at step 68950 : 3.7066688537597656\n",
      "Loss at step 69000 : 3.108593225479126\n",
      "Loss at step 69050 : 2.3582262992858887\n",
      "Loss at step 69100 : 2.7499890327453613\n",
      "Loss at step 69150 : 2.7303578853607178\n",
      "Loss at step 69200 : 2.2960193157196045\n",
      "Loss at step 69250 : 2.049201011657715\n",
      "Loss at step 69300 : 2.6059367656707764\n",
      "Loss at step 69350 : 3.5931906700134277\n",
      "Loss at step 69400 : 3.367880344390869\n",
      "Loss at step 69450 : 3.4612746238708496\n",
      "Loss at step 69500 : 2.3970208168029785\n",
      "Loss at step 69550 : 3.193308115005493\n",
      "Loss at step 69600 : 3.3743650913238525\n",
      "Loss at step 69650 : 3.204864501953125\n",
      "Loss at step 69700 : 3.98569655418396\n",
      "Loss at step 69750 : 2.19698429107666\n",
      "Loss at step 69800 : 3.1683719158172607\n",
      "Loss at step 69850 : 3.794689178466797\n",
      "Loss at step 69900 : 2.3166792392730713\n",
      "Loss at step 69950 : 1.7825064659118652\n",
      "Loss at step 70000 : 2.5917015075683594\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 70050 : 3.153475761413574\n",
      "Loss at step 70100 : 2.8839380741119385\n",
      "Loss at step 70150 : 2.9295716285705566\n",
      "Loss at step 70200 : 2.8428874015808105\n",
      "Loss at step 70250 : 3.9459993839263916\n",
      "Loss at step 70300 : 2.864562749862671\n",
      "Loss at step 70350 : 2.369748592376709\n",
      "Loss at step 70400 : 3.4463138580322266\n",
      "Loss at step 70450 : 2.3958053588867188\n",
      "Loss at step 70500 : 2.5088698863983154\n",
      "Loss at step 70550 : 2.9694252014160156\n",
      "Loss at step 70600 : 3.419259548187256\n",
      "Loss at step 70650 : 3.1703381538391113\n",
      "Loss at step 70700 : 2.999232053756714\n",
      "Loss at step 70750 : 3.0305607318878174\n",
      "Loss at step 70800 : 2.4989817142486572\n",
      "Loss at step 70850 : 3.8282151222229004\n",
      "Loss at step 70900 : 2.612125873565674\n",
      "Loss at step 70950 : 3.112309455871582\n",
      "Loss at step 71000 : 3.1749155521392822\n",
      "Loss at step 71050 : 1.985732078552246\n",
      "Loss at step 71100 : 3.0030274391174316\n",
      "Loss at step 71150 : 3.076277732849121\n",
      "Loss at step 71200 : 2.6221871376037598\n",
      "Loss at step 71250 : 2.7299442291259766\n",
      "Loss at step 71300 : 2.8193771839141846\n",
      "Loss at step 71350 : 2.4140195846557617\n",
      "Loss at step 71400 : 2.7451062202453613\n",
      "Loss at step 71450 : 2.943931818008423\n",
      "Loss at step 71500 : 3.0723323822021484\n",
      "Loss at step 71550 : 3.0111536979675293\n",
      "Loss at step 71600 : 3.5085463523864746\n",
      "Loss at step 71650 : 2.796051502227783\n",
      "Loss at step 71700 : 2.7687807083129883\n",
      "Loss at step 71750 : 2.0526185035705566\n",
      "Loss at step 71800 : 2.5300545692443848\n",
      "Loss at step 71850 : 2.6985220909118652\n",
      "Loss at step 71900 : 3.7622110843658447\n",
      "Loss at step 71950 : 3.936537504196167\n",
      "Loss at step 72000 : 3.6479694843292236\n",
      "Loss at step 72050 : 2.4714651107788086\n",
      "Loss at step 72100 : 3.927219867706299\n",
      "Loss at step 72150 : 2.3972995281219482\n",
      "Loss at step 72200 : 3.1568603515625\n",
      "Loss at step 72250 : 1.589341163635254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 72300 : 3.949219226837158\n",
      "Loss at step 72350 : 2.194791316986084\n",
      "Loss at step 72400 : 2.4383316040039062\n",
      "Loss at step 72450 : 3.191035747528076\n",
      "Loss at step 72500 : 3.2878313064575195\n",
      "Loss at step 72550 : 2.8235697746276855\n",
      "Loss at step 72600 : 2.4027280807495117\n",
      "Loss at step 72650 : 2.1969780921936035\n",
      "Loss at step 72700 : 2.84018611907959\n",
      "Loss at step 72750 : 2.2164368629455566\n",
      "Loss at step 72800 : 2.584534168243408\n",
      "Loss at step 72850 : 3.7449822425842285\n",
      "Loss at step 72900 : 2.8225159645080566\n",
      "Loss at step 72950 : 2.7600834369659424\n",
      "Loss at step 73000 : 2.699350595474243\n",
      "Loss at step 73050 : 3.231889009475708\n",
      "Loss at step 73100 : 2.7057671546936035\n",
      "Loss at step 73150 : 2.0836024284362793\n",
      "Loss at step 73200 : 2.9320526123046875\n",
      "Loss at step 73250 : 2.158360004425049\n",
      "Loss at step 73300 : 2.52298641204834\n",
      "Loss at step 73350 : 2.671820640563965\n",
      "Loss at step 73400 : 3.109055519104004\n",
      "Loss at step 73450 : 4.115716934204102\n",
      "Loss at step 73500 : 3.259422540664673\n",
      "Loss at step 73550 : 2.8980064392089844\n",
      "Loss at step 73600 : 2.439791679382324\n",
      "Loss at step 73650 : 4.050602912902832\n",
      "Loss at step 73700 : 2.493636131286621\n",
      "Loss at step 73750 : 3.164475917816162\n",
      "Loss at step 73800 : 2.8427841663360596\n",
      "Loss at step 73850 : 2.4368038177490234\n",
      "Loss at step 73900 : 3.08125901222229\n",
      "Loss at step 73950 : 2.6681010723114014\n",
      "Loss at step 74000 : 2.705636501312256\n",
      "Loss at step 74050 : 2.3576271533966064\n",
      "Loss at step 74100 : 3.1192688941955566\n",
      "Loss at step 74150 : 2.6597542762756348\n",
      "Loss at step 74200 : 2.7418606281280518\n",
      "Loss at step 74250 : 2.5100371837615967\n",
      "Loss at step 74300 : 3.089355945587158\n",
      "Loss at step 74350 : 3.0433473587036133\n",
      "Loss at step 74400 : 3.0572757720947266\n",
      "Loss at step 74450 : 2.5670838356018066\n",
      "Loss at step 74500 : 2.0688347816467285\n",
      "Loss at step 74550 : 3.0441222190856934\n",
      "Loss at step 74600 : 2.918123245239258\n",
      "Loss at step 74650 : 2.4809765815734863\n",
      "Loss at step 74700 : 2.1559481620788574\n",
      "Loss at step 74750 : 3.3511173725128174\n",
      "Loss at step 74800 : 2.2925777435302734\n",
      "Loss at step 74850 : 2.546610116958618\n",
      "Loss at step 74900 : 2.665985584259033\n",
      "Loss at step 74950 : 3.985167980194092\n",
      "Loss at step 75000 : 4.116802215576172\n",
      "Nearest to tuna: grain, leaf, brush, juice, seed,\n",
      "Nearest to rice: roe, surface, filling, blade, brush,\n",
      "Nearest to sushi: piece, space, carrot, sashimi, zip,\n",
      "Nearest to roll: carrot, crab, coriander, paper, teriyaki,\n",
      "Nearest to sashimi: sesame, starch, fish, leaf, nori,\n",
      "Nearest to steak: preheat, ground, grill, salt, vinegar,\n",
      "Nearest to grill: preheat, grate, marinade, ground, worcestershire,\n",
      "Nearest to sauce: onion, sesame, truffle, salt, cone,\n",
      "Nearest to cream: coffee, cheese, confectioner, bottom, liqueur,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/test/2/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "\n",
    "#doc_embeddings_test = tf.Variable(tf.random_uniform([len(preprocessed_texts_test), doc_embedding_size], -1.0, 1.0), name=\"doc_embeddings_test\")\n",
    "#sess.run(tf.variables_initializer([doc_embeddings_test]))\n",
    "doc_embeddings_test_initial_tensor = tf.random_uniform([len(preprocessed_texts_test), doc_embedding_size], -1.0, 1.0)\n",
    "#doc_embeddings_data = tf.get_default_graph().get_tensor_by_name(\"doc_embeddings:0\")\n",
    "doc_embeddings_all_initial_tensor=tf.concat([doc_embeddings, doc_embeddings_test_initial_tensor],0,\"concat\")\n",
    "#resize_var = tf.assign(doc_embeddings, doc_embeddings_all_data, validate_shape=False)\n",
    "doc_embeddings_all=tf.Variable(doc_embeddings_all_initial_tensor,name = \"doc_embeddings_all\")\n",
    "#sess.run(resize_var)\n",
    "sess.run(tf.initialize_variables([doc_embeddings_all]))\n",
    "\n",
    "print(doc_embeddings.shape)\n",
    "print(doc_embeddings_all.shape)\n",
    "\n",
    "\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[None, 2]) #1 for word index and 1 for doc index\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embed= tf.nn.embedding_lookup(embeddings, x_inputs[:, 0])\n",
    "    \n",
    "doc_indices = tf.slice(x_inputs, [0,1],[batch_size,1])\n",
    "doc_embed = tf.nn.embedding_lookup(doc_embeddings_all,doc_indices)\n",
    "final_embed = tf.concat([embed, tf.squeeze(doc_embed)],1)\n",
    "\n",
    "logits = tf.matmul(final_embed, tf.transpose(decoder_weights)) + decoder_biases\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_target))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=model_learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "#cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True, name=\"cosine_similarity\")\n",
    "\n",
    "with tf.name_scope(\"performance\"):\n",
    "    loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "    loss_summary = tf.summary.scalar('loss', loss_ph)\n",
    "performance_summaries = tf.summary.merge([loss_summary])\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summ_writer = tf.summary.FileWriter(summaries_folder_name, sess.graph)\n",
    "\n",
    "#sess.run(tf.initialize_variables([doc_embeddings]))\n",
    "#sess.run(tf.variables_initializer([doc_embeddings_test]))\n",
    "sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Starting Training')\n",
    "\n",
    "\n",
    "for i in range(generations):\n",
    "    #batch_inputs, batch_labels = create_batch_data(text_data)\n",
    "    batch_inputs, batch_labels = create_batch_data(text_data)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    #run the train step\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    #return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        summ = sess.run(performance_summaries, feed_dict={loss_ph:loss_val})\n",
    "        summ_writer.add_summary(summ, i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "        \n",
    "    #validation\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    #save dictionary + embeddings\n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        #save vocabulary dictionary\n",
    "        with open(os.path.join(models_folder_name_test,'doc2vec_recipes_dict_words_integers.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        #save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),models_folder_name_test,'doc2vec_recipes_checkpoint.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))\n",
    "        \n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
