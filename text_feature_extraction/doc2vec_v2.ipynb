{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sushi': 2, 'steak': 1, 'tiramisu': 3, 'sashimi': 0}\n"
     ]
    }
   ],
   "source": [
    "models_folder_name = os.path.join(os.getcwd(),'models')\n",
    "path_to_preprocessed_texts = os.path.join(os.getcwd(),\n",
    "                                          'texts','preprocessed_texts_for_doc2vec.pkl') \n",
    "\n",
    "df_preprocessed_texts = pd.read_pickle(path_to_preprocessed_texts)\n",
    "\n",
    "preprocessed_texts = df_preprocessed_texts.preprocessed_texts.values.tolist()\n",
    "labels = df_preprocessed_texts['labels'].values.tolist()\n",
    "\n",
    "unique_labels=sorted(set(labels))\n",
    "number_categories=len(unique_labels)\n",
    "categories_indices=np.linspace(0,number_categories-1,number_categories,dtype=int)\n",
    "labels2integers=dict(zip(unique_labels,categories_indices))\n",
    "\n",
    "print(labels2integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "generations = 100000\n",
    "model_learning_rate = 0.001\n",
    "\n",
    "embedding_size = 24   #word embedding size\n",
    "doc_embedding_size = 12  #document embedding size\n",
    "concatenated_size = embedding_size + doc_embedding_size\n",
    "\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(preprocessed_texts):\n",
    "    words=[w for words_in_recipe in preprocessed_texts for w in words_in_recipe]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words))\n",
    "    count=sorted(count)\n",
    "    word_dict = {}\n",
    "    for word in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return (word_dict)\n",
    "\n",
    "#replace each word in texts with integer value\n",
    "def text_to_numbers(preprocessed_texts, word_dict):\n",
    "    data = []\n",
    "    for prepr_text in preprocessed_texts:\n",
    "        text_data = []\n",
    "        for word in prepr_text:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            text_data.append(word_ix)\n",
    "        data.append(text_data)\n",
    "    return (data)\n",
    "\n",
    "\n",
    "def create_batch_data(text_with_words_conv_to_numbers, batch_size=batch_size):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    rand_text_ix = int(np.random.choice(len(text_with_words_conv_to_numbers), size=1))\n",
    "    rand_text = text_with_words_conv_to_numbers[rand_text_ix]\n",
    "    word_to_predict_label=np.random.choice(list(set(rand_text)), size=1,replace=False)\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        item_in_batch=[]        \n",
    "        \n",
    "        label_words=np.random.choice(rand_text, size=1,replace=False)\n",
    "\n",
    "        item_in_batch.extend(word_to_predict_label)\n",
    "        item_in_batch.append(rand_text_ix)     \n",
    "        label_data.extend(label_words)\n",
    "        batch_data.append(item_in_batch)\n",
    "\n",
    "        \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array(label_data))\n",
    "\n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 100, 'surface': 122, 'sashimi': 96, 'starch': 115, 'press': 86, 'garnish': 47, 'stick': 117, 'soy': 108, 'onion': 75, 'tempura': 124, 'sauce': 97, 'breast': 14, 'speed': 110, 'carrot': 18, 'curl': 33, 'paper': 76, 'pinch': 81, 'noodle': 72, 'skewer': 106, 'rice': 90, 'vinegar': 138, 'cream': 31, 'quantity': 88, 'bag': 4, 'spicy': 111, 'pressure': 87, 'roe': 91, 'guacamole': 53, 'butter': 16, 'shrimp': 103, 'sesame': 101, 'thumb': 127, 'salt': 95, 'water': 141, 'confectioner': 28, 'pepper': 79, 'sirloin': 105, 'spread': 112, 'bottom': 13, 'liqueur': 60, 'leg': 58, 'preheat': 85, 'batter': 7, 'lime': 59, 'tomato': 129, 'chicken': 20, 'tuna': 135, 'yolk': 147, 'juice': 54, 'daikon': 36, 'salmon': 94, 'wafer': 139, 'chocolate': 22, 'grill': 51, 'sushi': 123, 'leaf': 57, 'strawberry': 119, 'mixture': 70, 'sugar': 121, 'teriyaki': 125, 'coffee': 26, 'mushroom': 71, 'pour': 83, 'sprinkle': 114, 'grate': 50, 'wasabi': 140, 'grain': 49, 'finger': 43, 'space': 109, 'tobikko': 128, 'powder': 84, 'position': 82, 'brush': 15, 'cut': 34, 'stone': 118, 'steak': 116, 'vanilla': 136, 'raspberry': 89, 'bamboo': 6, 'cling': 24, 'cake': 17, 'slice': 107, 'vegetable': 137, 'topping': 131, 'roll': 92, 'cocoa': 25, 'cheese': 19, 'chive': 21, 'cucumber': 32, 'egg': 39, 'torch': 132, 'marinade': 61, 'ladyfinger': 55, 'matchstick': 64, 'strip': 120, 'ball': 5, 'mascarpone': 62, 'block': 12, 'white': 144, 'truffle': 134, 'cutting': 35, 'piece': 80, 'metal': 68, 'ginger': 48, 'towel': 133, 'whisk': 143, 'bit': 10, 'peak': 78, 'avocado': 3, 'mat': 63, 'fish': 44, 'chopstick': 23, 'whip': 142, 'flesh': 45, 'row': 93, 'berry': 9, 'coriander': 29, 'cone': 27, 'nori': 73, 'layer': 56, 'wine': 145, 'air': 0, 'asparagus': 2, 'mixer': 69, 'beat': 8, 'meat': 66, 'dipping': 37, 'sieve': 104, 'zip': 148, 'saucepan': 98, 'oil': 74, 'angle': 1, 'sheet': 102, 'mayonnaise': 65, 'thickness': 126, 'crab': 30, 'fryer': 46, 'drain': 38, 'filet': 41, 'blade': 11, 'espresso': 40, 'medium': 67, 'part': 77, 'sea': 99, 'spring': 113, 'tongs': 130, 'ground': 52, 'filling': 42, 'worcestershire': 146}\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "word_dictionary=build_dictionary(preprocessed_texts)\n",
    "vocabulary_size=len(word_dictionary)\n",
    "print(word_dictionary)\n",
    "print(vocabulary_size)\n",
    "\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94, 3, 75, 140, 100, 90, 72, 80, 94, 95, 94, 141, 44, 53, 75, 129, 95, 95, 95, 95, 95, 95, 95, 95, 34, 3, 3, 3, 3, 80, 75, 70, 3, 70, 53, 92, 59, 54, 59, 59, 54, 59, 54, 3, 54, 59, 53, 70, 140, 140, 10, 140, 53, 10, 10, 101, 100, 108, 97, 129, 129, 129, 3, 29, 57, 57, 70, 29, 53, 24, 94, 95, 94, 94, 44, 96, 34, 107, 24, 10, 115, 24, 107, 94, 115, 24, 115, 94, 94, 115, 94, 94, 94, 46, 107, 46, 94, 74, 94, 94, 117, 46, 94, 94, 94, 94, 94, 94, 46, 74, 94, 74, 94, 47, 94, 90, 72, 72, 94, 53, 29, 90, 72, 53], [135, 140, 108, 97, 48, 49, 82, 43, 49, 107], [96, 94, 41, 101, 100, 74, 101, 100, 97, 48, 41, 101, 100, 74, 122, 101, 100, 74, 43, 94, 41, 101, 100, 122, 43, 41, 41, 44, 101, 100, 74, 117, 41, 41, 44, 82, 41, 43, 41, 41, 34, 41, 97, 122, 11, 94, 107, 43, 82, 94, 97, 48, 94, 96, 107, 97], [123, 41, 12, 123, 41, 12, 29, 57, 101, 100, 74, 135, 135, 41, 107, 107, 41, 34, 135, 107, 12, 12, 135, 100, 74, 43, 74, 135, 29, 41, 122, 41, 12, 11, 44, 135, 12, 44, 44, 82, 41, 12, 1, 34, 1, 11, 107, 44, 107, 135, 12, 11, 34, 107, 47, 47, 43, 18, 5, 82, 5, 107, 123, 18, 5, 11, 82, 107, 80, 107, 107, 94, 43, 43, 123, 5, 82, 5, 32, 18, 5, 135, 107, 123, 107, 1, 82, 1, 107, 47, 107, 43, 107, 135, 107, 107, 47, 107, 57, 32, 5, 135, 107, 32, 47, 3, 107, 3, 43, 3, 27, 82, 107, 123, 27, 5, 48, 135, 107, 120, 96, 107, 108, 97, 140], [29, 57, 101, 100, 74, 123, 102, 29, 29, 135, 74, 29, 80, 123, 135, 74, 135, 29, 135, 120, 29, 120, 135, 135, 101, 100, 74, 120, 29, 135, 44, 135, 135, 96, 44, 135, 102, 102, 29, 135, 102, 73, 92, 135, 73, 92, 92, 92, 92, 80, 34, 57, 18, 135, 108, 97, 37], [123, 41, 49, 141, 95, 121, 74, 41, 123, 41, 49, 94, 41, 141, 95, 121, 41, 44, 94, 41, 94, 41, 4, 74, 4, 4, 0, 74, 4, 141, 44, 4, 4, 122, 94, 4, 44, 120, 94, 82, 120, 41, 120, 96, 96, 3, 108, 0, 108, 97, 0, 108, 97, 0, 82, 107, 3, 108, 97, 43, 94, 107, 3, 94], [123, 29, 135, 96, 80, 135, 44, 135, 44, 135, 135, 12, 29, 135, 29, 135, 117, 74, 107], [135, 3, 34, 107, 74, 59, 54, 59, 75, 108, 97, 135, 107, 107, 3, 74, 59, 54, 3, 135, 75, 95, 108, 97, 37, 59], [135, 80, 80, 48, 108, 97, 97, 37, 97, 108, 97, 108, 97, 123, 108, 97, 97, 36, 102, 102, 107, 117, 141, 36, 48, 27, 27, 48, 135, 135, 12, 107, 49, 49, 12, 135, 120, 135, 12, 135, 44, 36, 57, 57, 75, 32, 48, 108, 97, 44, 135, 37, 97, 36, 108, 97, 96], [123, 90, 90, 138, 138, 121, 95, 94, 3, 80, 73, 120, 97, 108, 140, 101, 90, 90, 141, 90, 90, 138, 121, 95, 121, 95, 90, 138, 70, 90, 94, 3, 73, 108, 97, 140, 97, 140, 140, 108, 97, 97], [138, 108, 97, 74, 52, 79, 146, 97, 75, 84, 95, 79, 116, 67, 138, 108, 97, 74, 52, 79, 146, 97, 75, 84, 95, 79, 116, 61, 66, 85, 51, 67, 74, 51, 50, 61], [75, 74, 138, 108, 97, 79, 116, 75, 74, 138, 108, 97, 95, 79, 116, 4, 83, 61, 116, 51, 51, 50, 74, 61, 116, 51], [116, 95, 52, 79, 74, 116, 95, 79, 51, 85, 51, 116, 51, 116, 74, 51, 116, 116], [48, 75, 108, 97, 74, 66, 121, 116, 48, 75, 97, 74, 146, 97, 66, 121, 61, 4, 116, 61, 51, 67, 51, 116, 51, 67], [16, 79, 85, 51, 16, 84, 116, 95, 79, 116, 16], [108, 97, 138, 52, 48, 84, 74, 108, 97, 138, 48, 84, 74, 116, 116, 83, 61, 116, 85, 51, 50, 74, 116, 51, 61, 116], [74, 146, 97, 67, 75, 79, 97, 74, 146, 97, 108, 97, 75, 95, 79, 116, 97, 116, 83, 61, 116, 51, 67, 116, 61, 61, 74, 116, 51, 116], [116, 121, 79, 84, 85, 51, 116, 116, 97, 121, 83, 97, 116, 95, 79, 84, 116, 95, 79, 84, 116, 61, 83, 61, 74, 51, 50, 116, 116, 61], [138, 79, 95, 74, 116, 16, 21, 79, 138, 79, 95, 74, 116, 4, 61, 4, 51, 67, 74, 50, 116, 61, 61, 116, 51, 16, 21, 79, 116, 51, 67, 116, 16], [16, 21, 79, 95, 75, 84, 52, 79, 116, 74, 85, 51, 16, 21, 79, 95, 75, 84, 79, 116, 74, 66, 74, 51, 50, 66, 51, 51, 51, 16], [123, 90, 138, 121, 95, 90, 141, 102, 42, 32, 32, 64, 64, 79, 64, 75, 64, 97, 80, 36, 64, 36, 3, 54, 6, 123, 63, 90, 138, 121, 95, 90, 104, 38, 90, 141, 67, 98, 133, 98, 133, 141, 90, 90, 70, 90, 76, 90, 90, 133, 92, 123, 63, 109, 102, 63, 43, 141, 86, 90, 73, 42, 90, 90, 63, 73, 90, 42, 63, 92, 63, 92, 63, 73, 10, 141, 123, 123, 92, 90, 42, 123, 92, 80, 108, 97], [108, 97, 90, 145, 138, 48, 100, 100, 74, 65, 134, 74, 97, 108, 97, 90, 138, 48, 70, 54, 97, 123, 92, 101, 100, 70, 101, 100, 74, 123, 92, 97, 131, 65, 74, 123, 92, 116, 105, 116, 43, 116, 43, 116, 126, 34, 56, 116, 126, 101, 100, 74, 105, 99, 95, 114, 122, 116, 68, 130, 41, 137, 101, 100, 74, 67, 2, 71, 2, 71, 99, 95, 130, 71, 74, 123, 92, 74, 123, 92, 102, 43, 123, 90, 102, 90, 102, 6, 63, 148, 4, 90, 63, 102, 43, 90, 63, 82, 102, 68, 130, 56, 71, 2, 102, 92, 63, 43, 43, 87, 6, 63, 102, 127, 43, 92, 87, 105, 105, 82, 45, 107, 126, 116, 80, 107, 123, 92, 123, 92, 43, 107, 105, 82, 92, 123, 92, 92, 43, 63, 123, 92, 43, 141, 11, 34, 123, 92, 34, 80, 123, 24, 80, 123, 92, 82, 80, 123, 1, 88, 82, 5, 48, 5, 140, 97, 123, 92, 107, 132, 97, 132, 122, 122, 107, 81, 113, 75, 134, 74, 65, 81, 101, 100, 107], [123, 94, 89, 36, 94, 91, 3, 3, 118, 3, 118, 118, 118, 3, 127, 3, 80, 3, 82, 3, 1, 3, 43, 107, 126, 11, 43, 3, 3, 107, 93, 24, 93, 107, 93, 107, 93, 123, 92, 92, 82, 80, 123, 94, 41, 93, 93, 24, 3, 41, 24, 87, 43, 123, 92, 123, 92, 80, 89, 82, 89, 104, 68, 89, 104, 54, 104, 54, 15, 15, 89, 122, 23, 3, 92, 80, 89, 15, 82, 107, 23, 94, 34, 80, 36, 88, 11, 88, 3, 92, 23, 94, 91, 107, 23, 87], [123, 90, 102, 32, 107, 3, 134, 42, 102, 123, 90, 73, 43, 90, 90, 102, 73, 122, 133, 90, 102, 41, 102, 32, 32, 102, 92, 123, 92, 6, 63, 127, 102, 92, 102, 42, 92, 6, 63, 148, 4, 92, 63, 92, 86, 43, 92, 92, 92, 107, 3, 123, 92, 109, 107, 92, 24, 43, 24, 141, 92, 92, 92, 34, 6, 63, 24, 131, 16, 107, 123, 107, 132, 134, 132, 122, 107, 107, 91, 107, 107, 77, 74, 77, 74, 10, 70, 123, 107, 82, 107, 123, 1, 109, 107, 137, 32, 18, 107, 80, 140], [14, 101, 74, 74, 99, 95, 125, 97, 102, 90, 20, 97, 123, 90, 74, 74, 80, 20, 14, 107, 80, 92, 80, 81, 99, 95, 81, 95, 20, 14, 20, 66, 20, 14, 106, 77, 54, 77, 20, 66, 20, 125, 97, 66, 20, 14, 125, 125, 20, 92, 92, 123, 92, 6, 63, 148, 4, 90, 63, 102, 73, 63, 141, 90, 138, 90, 123, 90, 122, 102, 90, 102, 90, 63, 80, 125, 20, 14, 102, 32, 20, 92, 10, 125, 97, 32, 20, 92, 123, 92, 6, 63, 42, 92, 87, 6, 63, 87, 63, 92, 92, 10, 92, 90, 3, 3, 107, 107, 3, 34, 3, 3, 10, 107, 3, 92, 123, 92, 3, 107, 107, 123, 92, 92, 102, 92, 6, 63, 4, 87, 92, 63, 24, 90, 11, 92, 80, 80, 92, 34, 35, 24, 123, 123, 48, 125, 97, 123, 80, 101, 100, 92, 125, 20, 123, 92], [30, 117, 90, 102, 107, 107, 101, 100, 30, 30, 117, 117, 92, 65, 30, 117, 65, 90, 102, 102, 123, 90, 102, 43, 90, 102, 114, 100, 90, 90, 90, 6, 63, 148, 4, 102, 63, 30, 70, 102, 107, 32, 107, 109, 102, 92, 63, 42, 42, 127, 92, 87, 63, 92, 63, 10, 92, 92, 63, 123, 92, 92, 24, 92, 92, 24, 63, 24, 92, 24, 92, 90, 92, 123, 80, 123, 92, 90, 11, 92, 92, 107, 123, 92, 11, 87, 123, 92, 92, 123, 80, 48, 140, 108, 97], [123, 90, 102, 2, 117, 91, 124, 7, 123, 102, 73, 4, 63, 123, 90, 102, 43, 90, 102, 117, 2, 102, 56, 2, 44, 123, 92, 2, 120, 96, 94, 123, 92, 102, 6, 63, 43, 102, 117, 92, 123, 123, 92, 123, 92, 124, 7, 7, 123, 92, 7, 123, 92, 90, 124, 7, 92, 92, 7, 92, 46, 94, 130, 76, 74, 123, 92, 123, 92, 141, 11, 90, 7, 34, 92, 80, 123, 123, 92, 123, 92, 123, 92, 140, 48, 97], [67, 90, 141, 80, 90, 138, 121, 95, 65, 79, 74, 95, 79, 102, 123, 90, 101, 100, 135, 113, 75, 90, 141, 141, 38, 90, 67, 98, 141, 70, 141, 90, 123, 90, 138, 121, 95, 98, 121, 70, 90, 90, 70, 90, 133, 143, 65, 74, 95, 79, 123, 63, 122, 102, 73, 63, 102, 63, 43, 90, 73, 56, 90, 101, 100, 73, 90, 73, 56, 32, 111, 65, 70, 32, 135, 114, 113, 75, 92, 123, 63, 80, 92], [90, 90, 138, 121, 95, 90, 141, 102, 42, 32, 32, 64, 3, 54, 108, 97, 6, 123, 63, 92, 90, 138, 121, 95, 90, 141, 38, 90, 141, 67, 98, 90, 133, 98, 133, 90, 141, 90, 90, 70, 90, 76, 102, 90, 90, 133, 92, 123, 63, 109, 102, 63, 43, 141, 86, 90, 73, 42, 90, 90, 63, 73, 90, 42, 63, 92, 63, 92, 63, 73, 10, 141, 123, 123, 76, 133, 90, 42, 123, 92, 80, 108, 97], [103, 124, 101, 100, 95, 74, 103, 111, 97, 65, 97, 90, 145, 138, 54, 103, 124, 143, 7, 143, 101, 100, 95, 54, 7, 43, 46, 74, 74, 98, 74, 103, 7, 46, 38, 76, 133, 102, 114, 95, 103, 46, 111, 97, 143, 65, 90, 145, 138, 54, 103, 111, 97], [90, 123, 90, 90, 145, 138, 121, 65, 65, 90, 145, 138, 108, 97, 123, 4, 102, 42, 120, 94, 135, 79, 3, 113, 75, 123, 140, 48, 108, 97, 92, 90, 102, 63, 141, 90, 56, 65, 56, 65, 90, 42, 65, 42, 135, 32, 92, 63, 90, 87, 92, 90, 15, 141, 92, 63, 92, 123, 107, 24, 123, 56, 94, 24, 56, 24, 90, 86, 86, 90, 44, 24, 123, 43, 24, 5, 131, 131, 80, 94, 5, 90, 131, 5, 24, 5], [102, 90, 123, 94, 44, 120, 97, 123, 56, 123, 90, 90, 90, 90, 123, 80, 94, 80, 44, 92, 44, 44, 123, 94, 123, 92, 41, 80, 120, 90, 3, 92, 92, 6, 63, 76, 123, 92, 80, 123, 92, 108, 97, 48], [90, 96, 94, 3, 18, 2, 36, 91, 34, 80, 36, 102, 34, 34, 133, 68, 106, 133, 106, 137, 106, 137, 35, 102, 34, 2, 141, 99, 95, 2, 141, 18, 67, 18, 34, 80, 80, 120, 122, 107, 107, 107, 120, 34, 107, 107, 120, 3, 3, 45, 3, 80, 96, 94, 34, 80, 94, 80, 34, 92, 80, 73, 63, 141, 138, 90, 73, 10, 140, 2, 10, 107, 18, 91, 92, 92, 10, 77, 90, 102, 92, 102, 102, 80, 92, 80, 92, 77, 80], [3, 73, 90, 90, 128, 91, 48, 140, 108, 97, 3, 100, 100, 100, 77, 3, 100, 45, 3, 45, 3, 107, 107, 100, 100, 77, 3, 100, 100, 3, 100, 3, 80, 96, 94, 107, 120, 94, 92, 92, 80, 92, 6, 63, 102, 73, 63, 90, 73, 102, 10, 140, 102, 43, 43, 102, 120, 102, 107, 3, 94, 92, 92, 123, 92, 128, 92, 128, 56, 122, 92, 128, 92, 128, 140, 92, 92, 90, 11, 34, 92, 123, 123, 35, 6, 63, 107, 92, 123, 10, 48, 140, 108, 97], [30, 16, 48, 3, 123, 90, 102, 30, 30, 58, 43, 30, 66, 58, 66, 30, 58, 80, 30, 66, 58, 99, 95, 58, 95, 79, 16, 16, 30, 66, 130, 130, 30, 48, 104, 70, 3, 100, 100, 3, 107, 34, 11, 54, 107, 123, 92, 102, 123, 90, 102, 43, 90, 6, 63, 148, 4, 102, 63, 90, 48, 120, 102, 30, 58, 48, 127, 6, 63, 102, 87, 43, 123, 92, 63, 82, 34, 107, 123, 92, 82, 43, 3, 107, 123, 92, 11, 43, 92, 35, 123, 92, 63, 24, 123, 92, 63, 43, 3, 92, 63, 3, 92, 87, 3, 92, 63, 24, 123, 92, 80, 92, 80, 63, 107, 24, 107, 1, 88, 111, 65, 107, 91], [39, 147, 121, 62, 39, 144, 31, 26, 55, 25, 84, 67, 8, 39, 147, 121, 62, 19, 39, 144, 31, 121, 26, 55, 26, 70, 55, 56, 56, 70, 55, 56, 70, 114, 25], [119, 28, 121, 62, 31, 60, 55, 84, 119, 9, 9, 119, 28, 67, 19, 31, 121, 60, 8, 69, 67, 110, 55, 40, 26, 112, 70, 55, 56, 119, 55, 40, 70, 119, 55, 31, 121, 8, 69, 67, 110, 31, 56, 55, 25, 31, 119, 83, 119], [39, 147, 121, 31, 136, 62, 26, 25, 84, 67, 39, 147, 121, 67, 70, 67, 31, 136, 78, 62, 70, 26, 55, 26, 70, 55, 13, 62, 70, 55, 31, 56, 114, 25], [39, 147, 121, 31, 55, 26, 25, 84, 22, 39, 147, 121, 141, 142, 147, 62, 147, 8, 142, 31, 78, 70, 13, 26, 60, 31, 55, 26, 60, 56, 25, 22, 33, 22, 33, 22], [121, 39, 40, 121, 55, 84, 8, 121, 39, 147, 69, 39, 70, 8, 8, 39, 144, 121, 69, 78, 39, 144, 62, 70, 83, 40, 55, 40, 55, 112, 62, 70, 55, 56, 25, 84, 55, 40, 62, 70, 25, 84, 55, 40, 62, 70], [26, 60, 31, 19, 31, 55, 25, 84, 67, 26, 31, 19, 26, 70, 67, 31, 136, 78, 31, 31, 19, 55, 13, 26, 70, 31, 70, 56, 114, 25], [39, 147, 121, 62, 55, 26, 84, 67, 147, 121, 136, 62, 70, 55, 26, 13, 112, 62, 70, 55, 62, 114, 25], [119, 28, 121, 62, 31, 60, 55, 84, 119, 9, 9, 119, 28, 67, 19, 31, 121, 60, 8, 69, 67, 110, 55, 40, 26, 112, 70, 55, 56, 119, 55, 40, 70, 119, 55, 31, 121, 8, 69, 67, 110, 31, 56, 55, 25, 31, 119, 83, 119], [17, 17, 26, 84, 26, 26, 28, 121, 26, 31, 28, 121, 26, 25, 84, 22, 17, 7, 26, 7, 17, 26, 26, 60, 69, 110, 62, 28, 121, 60, 8, 67, 69, 67, 110, 31, 28, 121, 60, 31, 70, 70, 17, 17, 56, 17, 26, 70, 17, 70, 26, 17, 56, 17, 26, 70, 56, 112, 17, 56, 17, 83, 26, 70, 112, 17, 25, 17, 33, 22, 33, 22], [136, 139, 26, 141, 31, 19, 121, 31, 39, 142, 25, 84, 112, 139, 13, 26, 141, 139, 26, 8, 31, 19, 121, 69, 31, 39, 110, 7, 67, 26, 141, 7, 139, 139, 26, 7, 142, 25, 84]]\n"
     ]
    }
   ],
   "source": [
    "text_data = text_to_numbers(preprocessed_texts, word_dictionary)\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135, 90, 123, 92, 96, 116, 51, 97, 31]\n"
     ]
    }
   ],
   "source": [
    "valid_words = ['tuna', 'rice', 'sushi', 'roll', 'sashimi','steak','grill', 'sauce', 'cream']\n",
    "\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]\n",
    "print(valid_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['soy' '28']\n",
      " ['soy' '28']]\n",
      "['rice' 'water']\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "batch_data, label_data = create_batch_data(preprocessed_texts)\n",
    "print(batch_data)\n",
    "print(label_data)\n",
    "print(np.shape(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "WARNING:tensorflow:From <ipython-input-10-d34d1eb53ae6>:33: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Starting Training\n",
      "Loss at step 50 : 5.709970951080322\n",
      "Loss at step 100 : 4.424684047698975\n",
      "Loss at step 150 : 5.260358810424805\n",
      "Loss at step 200 : 5.357036590576172\n",
      "Loss at step 250 : 5.188162803649902\n",
      "Loss at step 300 : 5.177769660949707\n",
      "Loss at step 350 : 5.223682403564453\n",
      "Loss at step 400 : 5.191035270690918\n",
      "Loss at step 450 : 4.744154930114746\n",
      "Loss at step 500 : 5.264348030090332\n",
      "Loss at step 550 : 4.773748397827148\n",
      "Loss at step 600 : 4.752451419830322\n",
      "Loss at step 650 : 5.3109259605407715\n",
      "Loss at step 700 : 4.93099308013916\n",
      "Loss at step 750 : 4.642880916595459\n",
      "Loss at step 800 : 4.118608474731445\n",
      "Loss at step 850 : 4.740657806396484\n",
      "Loss at step 900 : 5.834427833557129\n",
      "Loss at step 950 : 4.929481029510498\n",
      "Loss at step 1000 : 4.926198482513428\n",
      "Loss at step 1050 : 4.7685370445251465\n",
      "Loss at step 1100 : 4.662642002105713\n",
      "Loss at step 1150 : 4.029271125793457\n",
      "Loss at step 1200 : 4.76768159866333\n",
      "Loss at step 1250 : 4.747049808502197\n",
      "Loss at step 1300 : 5.639206886291504\n",
      "Loss at step 1350 : 4.1062726974487305\n",
      "Loss at step 1400 : 4.354877471923828\n",
      "Loss at step 1450 : 5.125063419342041\n",
      "Loss at step 1500 : 4.930086135864258\n",
      "Loss at step 1550 : 4.74617862701416\n",
      "Loss at step 1600 : 3.892651081085205\n",
      "Loss at step 1650 : 3.8846118450164795\n",
      "Loss at step 1700 : 4.335028648376465\n",
      "Loss at step 1750 : 4.307797431945801\n",
      "Loss at step 1800 : 4.6086626052856445\n",
      "Loss at step 1850 : 4.656818866729736\n",
      "Loss at step 1900 : 4.658201217651367\n",
      "Loss at step 1950 : 4.616362571716309\n",
      "Loss at step 2000 : 3.3595285415649414\n",
      "Loss at step 2050 : 5.28785514831543\n",
      "Loss at step 2100 : 5.04079532623291\n",
      "Loss at step 2150 : 4.909867286682129\n",
      "Loss at step 2200 : 3.1044042110443115\n",
      "Loss at step 2250 : 2.984340190887451\n",
      "Loss at step 2300 : 4.925090789794922\n",
      "Loss at step 2350 : 5.115265846252441\n",
      "Loss at step 2400 : 4.179989337921143\n",
      "Loss at step 2450 : 3.5278749465942383\n",
      "Loss at step 2500 : 4.11164665222168\n",
      "Loss at step 2550 : 4.690629005432129\n",
      "Loss at step 2600 : 4.402135848999023\n",
      "Loss at step 2650 : 4.896193504333496\n",
      "Loss at step 2700 : 4.694545269012451\n",
      "Loss at step 2750 : 3.61352801322937\n",
      "Loss at step 2800 : 4.82143497467041\n",
      "Loss at step 2850 : 3.2131080627441406\n",
      "Loss at step 2900 : 4.1532063484191895\n",
      "Loss at step 2950 : 4.105815887451172\n",
      "Loss at step 3000 : 3.794454336166382\n",
      "Loss at step 3050 : 4.140377044677734\n",
      "Loss at step 3100 : 4.3518266677856445\n",
      "Loss at step 3150 : 2.5911717414855957\n",
      "Loss at step 3200 : 2.9592418670654297\n",
      "Loss at step 3250 : 4.36929988861084\n",
      "Loss at step 3300 : 4.151027679443359\n",
      "Loss at step 3350 : 4.432029724121094\n",
      "Loss at step 3400 : 4.335176944732666\n",
      "Loss at step 3450 : 3.476229190826416\n",
      "Loss at step 3500 : 4.349259376525879\n",
      "Loss at step 3550 : 3.528438091278076\n",
      "Loss at step 3600 : 3.761629104614258\n",
      "Loss at step 3650 : 4.573548793792725\n",
      "Loss at step 3700 : 4.6517229080200195\n",
      "Loss at step 3750 : 2.7889771461486816\n",
      "Loss at step 3800 : 3.2267189025878906\n",
      "Loss at step 3850 : 5.104094505310059\n",
      "Loss at step 3900 : 4.73465633392334\n",
      "Loss at step 3950 : 4.265707969665527\n",
      "Loss at step 4000 : 4.469116687774658\n",
      "Loss at step 4050 : 3.7910714149475098\n",
      "Loss at step 4100 : 2.7845230102539062\n",
      "Loss at step 4150 : 2.475088596343994\n",
      "Loss at step 4200 : 3.4153709411621094\n",
      "Loss at step 4250 : 3.257810592651367\n",
      "Loss at step 4300 : 4.61967658996582\n",
      "Loss at step 4350 : 3.6624300479888916\n",
      "Loss at step 4400 : 2.5807881355285645\n",
      "Loss at step 4450 : 4.686394691467285\n",
      "Loss at step 4500 : 3.720484733581543\n",
      "Loss at step 4550 : 4.163752555847168\n",
      "Loss at step 4600 : 4.539088249206543\n",
      "Loss at step 4650 : 3.511580467224121\n",
      "Loss at step 4700 : 3.542776346206665\n",
      "Loss at step 4750 : 3.4899790287017822\n",
      "Loss at step 4800 : 4.85869026184082\n",
      "Loss at step 4850 : 4.277341365814209\n",
      "Loss at step 4900 : 3.680701732635498\n",
      "Loss at step 4950 : 2.3376965522766113\n",
      "Loss at step 5000 : 2.42608380317688\n",
      "Nearest to tuna: espresso, salmon, mayonnaise, pinch, spring,\n",
      "Nearest to rice: cling, position, cucumber, layer, mixer,\n",
      "Nearest to sushi: zip, towel, flesh, vinegar, fish,\n",
      "Nearest to roll: flesh, sesame, sauce, cone, dipping,\n",
      "Nearest to sashimi: piece, torch, topping, daikon, filling,\n",
      "Nearest to steak: salt, leaf, grain, onion, ground,\n",
      "Nearest to grill: grain, worcestershire, grate, sieve, shrimp,\n",
      "Nearest to sauce: cone, sesame, daikon, roll, piece,\n",
      "Nearest to cream: liqueur, mayonnaise, pour, stone, chicken,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 5050 : 2.9195384979248047\n",
      "Loss at step 5100 : 3.009934425354004\n",
      "Loss at step 5150 : 4.007842063903809\n",
      "Loss at step 5200 : 3.864168167114258\n",
      "Loss at step 5250 : 1.9538719654083252\n",
      "Loss at step 5300 : 2.7499523162841797\n",
      "Loss at step 5350 : 2.4590721130371094\n",
      "Loss at step 5400 : 2.741420269012451\n",
      "Loss at step 5450 : 3.6407859325408936\n",
      "Loss at step 5500 : 2.8925564289093018\n",
      "Loss at step 5550 : 3.4601528644561768\n",
      "Loss at step 5600 : 5.042517185211182\n",
      "Loss at step 5650 : 3.645186424255371\n",
      "Loss at step 5700 : 4.176197528839111\n",
      "Loss at step 5750 : 4.719552516937256\n",
      "Loss at step 5800 : 1.4322203397750854\n",
      "Loss at step 5850 : 2.443066358566284\n",
      "Loss at step 5900 : 3.1800742149353027\n",
      "Loss at step 5950 : 3.264057159423828\n",
      "Loss at step 6000 : 2.849308967590332\n",
      "Loss at step 6050 : 3.370556592941284\n",
      "Loss at step 6100 : 3.9507546424865723\n",
      "Loss at step 6150 : 4.387792587280273\n",
      "Loss at step 6200 : 2.096111536026001\n",
      "Loss at step 6250 : 3.2390244007110596\n",
      "Loss at step 6300 : 3.951329231262207\n",
      "Loss at step 6350 : 1.18220853805542\n",
      "Loss at step 6400 : 3.7351622581481934\n",
      "Loss at step 6450 : 3.377516746520996\n",
      "Loss at step 6500 : 3.3135733604431152\n",
      "Loss at step 6550 : 3.3990871906280518\n",
      "Loss at step 6600 : 3.0261547565460205\n",
      "Loss at step 6650 : 3.8359858989715576\n",
      "Loss at step 6700 : 3.614967107772827\n",
      "Loss at step 6750 : 3.252741813659668\n",
      "Loss at step 6800 : 4.338567733764648\n",
      "Loss at step 6850 : 4.235344886779785\n",
      "Loss at step 6900 : 2.57443904876709\n",
      "Loss at step 6950 : 4.4652814865112305\n",
      "Loss at step 7000 : 3.7754335403442383\n",
      "Loss at step 7050 : 3.601942300796509\n",
      "Loss at step 7100 : 3.919980764389038\n",
      "Loss at step 7150 : 4.199004650115967\n",
      "Loss at step 7200 : 2.8532681465148926\n",
      "Loss at step 7250 : 3.624868869781494\n",
      "Loss at step 7300 : 5.714044094085693\n",
      "Loss at step 7350 : 3.7153167724609375\n",
      "Loss at step 7400 : 3.642533779144287\n",
      "Loss at step 7450 : 4.424792289733887\n",
      "Loss at step 7500 : 2.6088171005249023\n",
      "Loss at step 7550 : 4.80654764175415\n",
      "Loss at step 7600 : 3.5230798721313477\n",
      "Loss at step 7650 : 5.481602191925049\n",
      "Loss at step 7700 : 3.8960494995117188\n",
      "Loss at step 7750 : 2.971583843231201\n",
      "Loss at step 7800 : 2.9160897731781006\n",
      "Loss at step 7850 : 4.153205871582031\n",
      "Loss at step 7900 : 2.2043023109436035\n",
      "Loss at step 7950 : 3.316795825958252\n",
      "Loss at step 8000 : 4.138968467712402\n",
      "Loss at step 8050 : 3.1642518043518066\n",
      "Loss at step 8100 : 3.566636562347412\n",
      "Loss at step 8150 : 2.934662342071533\n",
      "Loss at step 8200 : 1.541574239730835\n",
      "Loss at step 8250 : 4.469685077667236\n",
      "Loss at step 8300 : 4.6737775802612305\n",
      "Loss at step 8350 : 4.163784503936768\n",
      "Loss at step 8400 : 4.639472961425781\n",
      "Loss at step 8450 : 3.912614345550537\n",
      "Loss at step 8500 : 5.133506774902344\n",
      "Loss at step 8550 : 3.8208796977996826\n",
      "Loss at step 8600 : 2.935116767883301\n",
      "Loss at step 8650 : 4.550319671630859\n",
      "Loss at step 8700 : 4.155533313751221\n",
      "Loss at step 8750 : 3.332498788833618\n",
      "Loss at step 8800 : 4.839861869812012\n",
      "Loss at step 8850 : 2.832555055618286\n",
      "Loss at step 8900 : 6.012880325317383\n",
      "Loss at step 8950 : 4.669734477996826\n",
      "Loss at step 9000 : 3.819943904876709\n",
      "Loss at step 9050 : 2.9250094890594482\n",
      "Loss at step 9100 : 6.578767776489258\n",
      "Loss at step 9150 : 2.704436779022217\n",
      "Loss at step 9200 : 3.8246240615844727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 9250 : 2.464491128921509\n",
      "Loss at step 9300 : 3.1034622192382812\n",
      "Loss at step 9350 : 3.695023536682129\n",
      "Loss at step 9400 : 4.652491569519043\n",
      "Loss at step 9450 : 3.9373040199279785\n",
      "Loss at step 9500 : 5.968338966369629\n",
      "Loss at step 9550 : 2.275705337524414\n",
      "Loss at step 9600 : 3.187108278274536\n",
      "Loss at step 9650 : 3.0416078567504883\n",
      "Loss at step 9700 : 4.031565189361572\n",
      "Loss at step 9750 : 4.243646144866943\n",
      "Loss at step 9800 : 2.9661710262298584\n",
      "Loss at step 9850 : 4.129406929016113\n",
      "Loss at step 9900 : 3.2145445346832275\n",
      "Loss at step 9950 : 3.6891138553619385\n",
      "Loss at step 10000 : 3.98911714553833\n",
      "Nearest to tuna: salmon, espresso, wafer, batter, spring,\n",
      "Nearest to rice: cling, position, cucumber, space, sauce,\n",
      "Nearest to sushi: towel, vinegar, flesh, zip, strip,\n",
      "Nearest to roll: flesh, sesame, sauce, cone, dipping,\n",
      "Nearest to sashimi: piece, leaf, torch, topping, daikon,\n",
      "Nearest to steak: salt, leaf, onion, pepper, ground,\n",
      "Nearest to grill: grate, worcestershire, chive, salt, marinade,\n",
      "Nearest to sauce: cone, sesame, meat, grain, ground,\n",
      "Nearest to cream: liqueur, mascarpone, mayonnaise, layer, wafer,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 10050 : 2.402188301086426\n",
      "Loss at step 10100 : 3.167630434036255\n",
      "Loss at step 10150 : 2.9585318565368652\n",
      "Loss at step 10200 : 4.728275775909424\n",
      "Loss at step 10250 : 4.60430908203125\n",
      "Loss at step 10300 : 5.186712265014648\n",
      "Loss at step 10350 : 3.382894992828369\n",
      "Loss at step 10400 : 4.323918342590332\n",
      "Loss at step 10450 : 2.3712875843048096\n",
      "Loss at step 10500 : 5.624856948852539\n",
      "Loss at step 10550 : 4.557100772857666\n",
      "Loss at step 10600 : 3.72385573387146\n",
      "Loss at step 10650 : 2.779249429702759\n",
      "Loss at step 10700 : 2.516167640686035\n",
      "Loss at step 10750 : 2.3297486305236816\n",
      "Loss at step 10800 : 2.8403100967407227\n",
      "Loss at step 10850 : 1.8720390796661377\n",
      "Loss at step 10900 : 4.013002395629883\n",
      "Loss at step 10950 : 3.802171230316162\n",
      "Loss at step 11000 : 4.329580307006836\n",
      "Loss at step 11050 : 5.082890033721924\n",
      "Loss at step 11100 : 5.045971870422363\n",
      "Loss at step 11150 : 3.244969129562378\n",
      "Loss at step 11200 : 3.9536995887756348\n",
      "Loss at step 11250 : 2.5040624141693115\n",
      "Loss at step 11300 : 1.4796643257141113\n",
      "Loss at step 11350 : 4.324403762817383\n",
      "Loss at step 11400 : 2.87359619140625\n",
      "Loss at step 11450 : 3.039363384246826\n",
      "Loss at step 11500 : 2.8238699436187744\n",
      "Loss at step 11550 : 1.9794094562530518\n",
      "Loss at step 11600 : 1.3843783140182495\n",
      "Loss at step 11650 : 3.5492935180664062\n",
      "Loss at step 11700 : 2.018472671508789\n",
      "Loss at step 11750 : 4.032220363616943\n",
      "Loss at step 11800 : 2.37241268157959\n",
      "Loss at step 11850 : 0.9905704855918884\n",
      "Loss at step 11900 : 4.373298168182373\n",
      "Loss at step 11950 : 4.2148756980896\n",
      "Loss at step 12000 : 3.249056100845337\n",
      "Loss at step 12050 : 4.219826698303223\n",
      "Loss at step 12100 : 4.9533610343933105\n",
      "Loss at step 12150 : 1.6840592622756958\n",
      "Loss at step 12200 : 4.694383144378662\n",
      "Loss at step 12250 : 3.996056318283081\n",
      "Loss at step 12300 : 4.445467948913574\n",
      "Loss at step 12350 : 2.4593024253845215\n",
      "Loss at step 12400 : 2.49003267288208\n",
      "Loss at step 12450 : 5.673327445983887\n",
      "Loss at step 12500 : 3.948305130004883\n",
      "Loss at step 12550 : 4.287242889404297\n",
      "Loss at step 12600 : 3.0526649951934814\n",
      "Loss at step 12650 : 2.4749248027801514\n",
      "Loss at step 12700 : 2.658414125442505\n",
      "Loss at step 12750 : 3.0421810150146484\n",
      "Loss at step 12800 : 2.4099485874176025\n",
      "Loss at step 12850 : 2.683257818222046\n",
      "Loss at step 12900 : 4.425597190856934\n",
      "Loss at step 12950 : 2.78176212310791\n",
      "Loss at step 13000 : 2.931408166885376\n",
      "Loss at step 13050 : 3.3133671283721924\n",
      "Loss at step 13100 : 4.326922416687012\n",
      "Loss at step 13150 : 3.460904836654663\n",
      "Loss at step 13200 : 5.072243690490723\n",
      "Loss at step 13250 : 3.6898603439331055\n",
      "Loss at step 13300 : 3.4319257736206055\n",
      "Loss at step 13350 : 1.8116168975830078\n",
      "Loss at step 13400 : 3.0151548385620117\n",
      "Loss at step 13450 : 3.284576654434204\n",
      "Loss at step 13500 : 2.115326404571533\n",
      "Loss at step 13550 : 3.4926722049713135\n",
      "Loss at step 13600 : 2.291591167449951\n",
      "Loss at step 13650 : 4.219140529632568\n",
      "Loss at step 13700 : 2.5301427841186523\n",
      "Loss at step 13750 : 6.137513637542725\n",
      "Loss at step 13800 : 3.0222387313842773\n",
      "Loss at step 13850 : 2.3191988468170166\n",
      "Loss at step 13900 : 3.1721572875976562\n",
      "Loss at step 13950 : 2.3654708862304688\n",
      "Loss at step 14000 : 2.208810806274414\n",
      "Loss at step 14050 : 4.372950077056885\n",
      "Loss at step 14100 : 2.825562000274658\n",
      "Loss at step 14150 : 4.890841484069824\n",
      "Loss at step 14200 : 4.513710021972656\n",
      "Loss at step 14250 : 4.344812393188477\n",
      "Loss at step 14300 : 2.6282923221588135\n",
      "Loss at step 14350 : 3.1180591583251953\n",
      "Loss at step 14400 : 2.8423447608947754\n",
      "Loss at step 14450 : 3.440037488937378\n",
      "Loss at step 14500 : 3.5746240615844727\n",
      "Loss at step 14550 : 2.3713626861572266\n",
      "Loss at step 14600 : 3.2300493717193604\n",
      "Loss at step 14650 : 4.555627822875977\n",
      "Loss at step 14700 : 3.909867763519287\n",
      "Loss at step 14750 : 3.561245918273926\n",
      "Loss at step 14800 : 3.5971755981445312\n",
      "Loss at step 14850 : 1.5789759159088135\n",
      "Loss at step 14900 : 3.2205615043640137\n",
      "Loss at step 14950 : 3.053950309753418\n",
      "Loss at step 15000 : 2.158472776412964\n",
      "Nearest to tuna: salmon, wafer, mushroom, espresso, worcestershire,\n",
      "Nearest to rice: cling, position, cucumber, space, nori,\n",
      "Nearest to sushi: vinegar, strip, fish, towel, matchstick,\n",
      "Nearest to roll: sesame, flesh, dipping, sauce, piece,\n",
      "Nearest to sashimi: piece, leaf, torch, topping, daikon,\n",
      "Nearest to steak: salt, onion, leaf, pepper, ground,\n",
      "Nearest to grill: worcestershire, grate, chive, marinade, salt,\n",
      "Nearest to sauce: grain, cone, meat, roll, sesame,\n",
      "Nearest to cream: liqueur, mascarpone, layer, coffee, mayonnaise,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 15050 : 3.1700406074523926\n",
      "Loss at step 15100 : 3.2020208835601807\n",
      "Loss at step 15150 : 2.092806100845337\n",
      "Loss at step 15200 : 3.708686351776123\n",
      "Loss at step 15250 : 3.4687044620513916\n",
      "Loss at step 15300 : 4.1325554847717285\n",
      "Loss at step 15350 : 2.011586904525757\n",
      "Loss at step 15400 : 3.1929731369018555\n",
      "Loss at step 15450 : 2.9442100524902344\n",
      "Loss at step 15500 : 5.110943794250488\n",
      "Loss at step 15550 : 3.829432249069214\n",
      "Loss at step 15600 : 6.137366771697998\n",
      "Loss at step 15650 : 2.832493305206299\n",
      "Loss at step 15700 : 5.018604755401611\n",
      "Loss at step 15750 : 2.6781628131866455\n",
      "Loss at step 15800 : 3.7171175479888916\n",
      "Loss at step 15850 : 3.4425971508026123\n",
      "Loss at step 15900 : 3.158921241760254\n",
      "Loss at step 15950 : 2.2698628902435303\n",
      "Loss at step 16000 : 5.016814708709717\n",
      "Loss at step 16050 : 3.3796308040618896\n",
      "Loss at step 16100 : 2.7846264839172363\n",
      "Loss at step 16150 : 2.365525245666504\n",
      "Loss at step 16200 : 3.4784560203552246\n",
      "Loss at step 16250 : 2.030322551727295\n",
      "Loss at step 16300 : 3.398355007171631\n",
      "Loss at step 16350 : 2.28005313873291\n",
      "Loss at step 16400 : 2.256612539291382\n",
      "Loss at step 16450 : 3.168488025665283\n",
      "Loss at step 16500 : 2.760247230529785\n",
      "Loss at step 16550 : 2.256136178970337\n",
      "Loss at step 16600 : 2.8316218852996826\n",
      "Loss at step 16650 : 5.27906608581543\n",
      "Loss at step 16700 : 2.444551467895508\n",
      "Loss at step 16750 : 3.266777515411377\n",
      "Loss at step 16800 : 2.468505382537842\n",
      "Loss at step 16850 : 2.7469632625579834\n",
      "Loss at step 16900 : 2.082378387451172\n",
      "Loss at step 16950 : 3.5157785415649414\n",
      "Loss at step 17000 : 2.583704948425293\n",
      "Loss at step 17050 : 2.8495545387268066\n",
      "Loss at step 17100 : 2.9800288677215576\n",
      "Loss at step 17150 : 3.202971935272217\n",
      "Loss at step 17200 : 2.509934902191162\n",
      "Loss at step 17250 : 2.803347587585449\n",
      "Loss at step 17300 : 3.527627944946289\n",
      "Loss at step 17350 : 3.5789170265197754\n",
      "Loss at step 17400 : 2.1469528675079346\n",
      "Loss at step 17450 : 3.394472599029541\n",
      "Loss at step 17500 : 2.906193494796753\n",
      "Loss at step 17550 : 5.121100902557373\n",
      "Loss at step 17600 : 4.54193639755249\n",
      "Loss at step 17650 : 1.013400673866272\n",
      "Loss at step 17700 : 3.6359810829162598\n",
      "Loss at step 17750 : 3.156371831893921\n",
      "Loss at step 17800 : 2.6180672645568848\n",
      "Loss at step 17850 : 4.252203941345215\n",
      "Loss at step 17900 : 2.063408136367798\n",
      "Loss at step 17950 : 3.992121934890747\n",
      "Loss at step 18000 : 2.5364842414855957\n",
      "Loss at step 18050 : 3.2843034267425537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 18100 : 2.9585819244384766\n",
      "Loss at step 18150 : 3.6475372314453125\n",
      "Loss at step 18200 : 5.69390344619751\n",
      "Loss at step 18250 : 4.089812755584717\n",
      "Loss at step 18300 : 3.378868818283081\n",
      "Loss at step 18350 : 3.0530407428741455\n",
      "Loss at step 18400 : 0.8624075055122375\n",
      "Loss at step 18450 : 4.986883640289307\n",
      "Loss at step 18500 : 4.409065246582031\n",
      "Loss at step 18550 : 2.4723587036132812\n",
      "Loss at step 18600 : 4.020962238311768\n",
      "Loss at step 18650 : 3.7619361877441406\n",
      "Loss at step 18700 : 3.6376419067382812\n",
      "Loss at step 18750 : 2.9598886966705322\n",
      "Loss at step 18800 : 3.5508806705474854\n",
      "Loss at step 18850 : 1.8267490863800049\n",
      "Loss at step 18900 : 5.214532852172852\n",
      "Loss at step 18950 : 3.141719341278076\n",
      "Loss at step 19000 : 4.310990810394287\n",
      "Loss at step 19050 : 2.5568935871124268\n",
      "Loss at step 19100 : 3.5694637298583984\n",
      "Loss at step 19150 : 2.9866292476654053\n",
      "Loss at step 19200 : 3.4422473907470703\n",
      "Loss at step 19250 : 0.9102305769920349\n",
      "Loss at step 19300 : 2.7261962890625\n",
      "Loss at step 19350 : 2.298706293106079\n",
      "Loss at step 19400 : 2.695056200027466\n",
      "Loss at step 19450 : 4.58106803894043\n",
      "Loss at step 19500 : 3.1832761764526367\n",
      "Loss at step 19550 : 2.365541458129883\n",
      "Loss at step 19600 : 5.155925750732422\n",
      "Loss at step 19650 : 3.84879207611084\n",
      "Loss at step 19700 : 2.3102269172668457\n",
      "Loss at step 19750 : 2.899656295776367\n",
      "Loss at step 19800 : 3.4152941703796387\n",
      "Loss at step 19850 : 4.032673358917236\n",
      "Loss at step 19900 : 2.446126937866211\n",
      "Loss at step 19950 : 3.058600902557373\n",
      "Loss at step 20000 : 2.8427560329437256\n",
      "Nearest to tuna: salmon, wafer, mushroom, worcestershire, piece,\n",
      "Nearest to rice: cling, cucumber, position, vinegar, strip,\n",
      "Nearest to sushi: strip, fish, vinegar, towel, matchstick,\n",
      "Nearest to roll: sesame, flesh, dipping, piece, sauce,\n",
      "Nearest to sashimi: piece, leaf, torch, coriander, daikon,\n",
      "Nearest to steak: onion, salt, leaf, pepper, ground,\n",
      "Nearest to grill: worcestershire, grate, preheat, marinade, chive,\n",
      "Nearest to sauce: grain, cone, meat, ginger, worcestershire,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, layer, yolk,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 20050 : 3.294928550720215\n",
      "Loss at step 20100 : 3.7479214668273926\n",
      "Loss at step 20150 : 3.875277519226074\n",
      "Loss at step 20200 : 1.9593037366867065\n",
      "Loss at step 20250 : 2.878218650817871\n",
      "Loss at step 20300 : 3.5412254333496094\n",
      "Loss at step 20350 : 4.114035129547119\n",
      "Loss at step 20400 : 3.8695287704467773\n",
      "Loss at step 20450 : 2.9712626934051514\n",
      "Loss at step 20500 : 3.5706262588500977\n",
      "Loss at step 20550 : 4.201944828033447\n",
      "Loss at step 20600 : 3.7341601848602295\n",
      "Loss at step 20650 : 3.4766182899475098\n",
      "Loss at step 20700 : 3.314701795578003\n",
      "Loss at step 20750 : 3.2752480506896973\n",
      "Loss at step 20800 : 5.85233211517334\n",
      "Loss at step 20850 : 2.906162738800049\n",
      "Loss at step 20900 : 2.954594850540161\n",
      "Loss at step 20950 : 3.6315813064575195\n",
      "Loss at step 21000 : 3.3913064002990723\n",
      "Loss at step 21050 : 2.5566468238830566\n",
      "Loss at step 21100 : 2.1235451698303223\n",
      "Loss at step 21150 : 5.011183738708496\n",
      "Loss at step 21200 : 2.941462993621826\n",
      "Loss at step 21250 : 2.267448902130127\n",
      "Loss at step 21300 : 3.534587860107422\n",
      "Loss at step 21350 : 2.095890522003174\n",
      "Loss at step 21400 : 4.328068733215332\n",
      "Loss at step 21450 : 3.5935487747192383\n",
      "Loss at step 21500 : 2.8039979934692383\n",
      "Loss at step 21550 : 2.826847553253174\n",
      "Loss at step 21600 : 4.092866897583008\n",
      "Loss at step 21650 : 3.123727321624756\n",
      "Loss at step 21700 : 4.686894416809082\n",
      "Loss at step 21750 : 2.8714468479156494\n",
      "Loss at step 21800 : 3.290106773376465\n",
      "Loss at step 21850 : 1.8531451225280762\n",
      "Loss at step 21900 : 3.376786231994629\n",
      "Loss at step 21950 : 3.7041802406311035\n",
      "Loss at step 22000 : 3.117307662963867\n",
      "Loss at step 22050 : 2.995903491973877\n",
      "Loss at step 22100 : 2.2570695877075195\n",
      "Loss at step 22150 : 2.3031179904937744\n",
      "Loss at step 22200 : 2.3632397651672363\n",
      "Loss at step 22250 : 3.1723215579986572\n",
      "Loss at step 22300 : 3.1075146198272705\n",
      "Loss at step 22350 : 2.2113280296325684\n",
      "Loss at step 22400 : 2.4557247161865234\n",
      "Loss at step 22450 : 4.014808654785156\n",
      "Loss at step 22500 : 4.718478679656982\n",
      "Loss at step 22550 : 2.9635157585144043\n",
      "Loss at step 22600 : 2.7494492530822754\n",
      "Loss at step 22650 : 3.1135382652282715\n",
      "Loss at step 22700 : 5.1055402755737305\n",
      "Loss at step 22750 : 2.506782293319702\n",
      "Loss at step 22800 : 3.4808616638183594\n",
      "Loss at step 22850 : 2.9280385971069336\n",
      "Loss at step 22900 : 3.330711841583252\n",
      "Loss at step 22950 : 2.3233892917633057\n",
      "Loss at step 23000 : 4.130814552307129\n",
      "Loss at step 23050 : 6.030695915222168\n",
      "Loss at step 23100 : 3.5084123611450195\n",
      "Loss at step 23150 : 4.1545233726501465\n",
      "Loss at step 23200 : 4.483194351196289\n",
      "Loss at step 23250 : 2.5753631591796875\n",
      "Loss at step 23300 : 3.6604557037353516\n",
      "Loss at step 23350 : 2.6646628379821777\n",
      "Loss at step 23400 : 2.3014659881591797\n",
      "Loss at step 23450 : 5.6334757804870605\n",
      "Loss at step 23500 : 3.820068836212158\n",
      "Loss at step 23550 : 3.5537405014038086\n",
      "Loss at step 23600 : 2.459102153778076\n",
      "Loss at step 23650 : 2.378715753555298\n",
      "Loss at step 23700 : 3.3392274379730225\n",
      "Loss at step 23750 : 2.9485881328582764\n",
      "Loss at step 23800 : 3.8685760498046875\n",
      "Loss at step 23850 : 4.13834285736084\n",
      "Loss at step 23900 : 3.8591065406799316\n",
      "Loss at step 23950 : 2.522714376449585\n",
      "Loss at step 24000 : 3.5336039066314697\n",
      "Loss at step 24050 : 3.974250316619873\n",
      "Loss at step 24100 : 3.7878358364105225\n",
      "Loss at step 24150 : 3.534092903137207\n",
      "Loss at step 24200 : 2.1143059730529785\n",
      "Loss at step 24250 : 2.336850166320801\n",
      "Loss at step 24300 : 2.2584638595581055\n",
      "Loss at step 24350 : 2.0865108966827393\n",
      "Loss at step 24400 : 1.738163948059082\n",
      "Loss at step 24450 : 3.114320755004883\n",
      "Loss at step 24500 : 3.4785985946655273\n",
      "Loss at step 24550 : 2.7215471267700195\n",
      "Loss at step 24600 : 3.146367073059082\n",
      "Loss at step 24650 : 3.469207286834717\n",
      "Loss at step 24700 : 1.6438300609588623\n",
      "Loss at step 24750 : 4.468245506286621\n",
      "Loss at step 24800 : 2.7997946739196777\n",
      "Loss at step 24850 : 4.097053527832031\n",
      "Loss at step 24900 : 2.954122543334961\n",
      "Loss at step 24950 : 3.137694835662842\n",
      "Loss at step 25000 : 2.5803143978118896\n",
      "Nearest to tuna: mushroom, salmon, wafer, piece, worcestershire,\n",
      "Nearest to rice: cling, position, cucumber, strip, salmon,\n",
      "Nearest to sushi: fish, strip, towel, vinegar, matchstick,\n",
      "Nearest to roll: sesame, flesh, dipping, soy, sushi,\n",
      "Nearest to sashimi: piece, leaf, coriander, torch, tempura,\n",
      "Nearest to steak: salt, onion, leaf, pepper, ground,\n",
      "Nearest to grill: worcestershire, grate, marinade, preheat, pepper,\n",
      "Nearest to sauce: cone, grain, ginger, strip, meat,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, layer, wafer,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 25050 : 3.1065073013305664\n",
      "Loss at step 25100 : 1.862283706665039\n",
      "Loss at step 25150 : 3.2683496475219727\n",
      "Loss at step 25200 : 3.096818447113037\n",
      "Loss at step 25250 : 5.079905033111572\n",
      "Loss at step 25300 : 1.584376573562622\n",
      "Loss at step 25350 : 2.829784393310547\n",
      "Loss at step 25400 : 2.789454460144043\n",
      "Loss at step 25450 : 2.3632473945617676\n",
      "Loss at step 25500 : 3.7408976554870605\n",
      "Loss at step 25550 : 3.6625142097473145\n",
      "Loss at step 25600 : 2.610765218734741\n",
      "Loss at step 25650 : 3.4256234169006348\n",
      "Loss at step 25700 : 2.6396713256835938\n",
      "Loss at step 25750 : 3.2602014541625977\n",
      "Loss at step 25800 : 3.6926827430725098\n",
      "Loss at step 25850 : 1.8756803274154663\n",
      "Loss at step 25900 : 3.6554713249206543\n",
      "Loss at step 25950 : 4.787625789642334\n",
      "Loss at step 26000 : 3.6480016708374023\n",
      "Loss at step 26050 : 4.487462043762207\n",
      "Loss at step 26100 : 4.151505947113037\n",
      "Loss at step 26150 : 2.551311492919922\n",
      "Loss at step 26200 : 4.09446907043457\n",
      "Loss at step 26250 : 3.2604994773864746\n",
      "Loss at step 26300 : 2.621574878692627\n",
      "Loss at step 26350 : 2.4869749546051025\n",
      "Loss at step 26400 : 2.6138601303100586\n",
      "Loss at step 26450 : 3.565365791320801\n",
      "Loss at step 26500 : 3.9696145057678223\n",
      "Loss at step 26550 : 3.07035493850708\n",
      "Loss at step 26600 : 4.5706024169921875\n",
      "Loss at step 26650 : 3.542210817337036\n",
      "Loss at step 26700 : 4.9195966720581055\n",
      "Loss at step 26750 : 2.083263874053955\n",
      "Loss at step 26800 : 2.711074113845825\n",
      "Loss at step 26850 : 2.0953314304351807\n",
      "Loss at step 26900 : 3.3345487117767334\n",
      "Loss at step 26950 : 2.737438678741455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 27000 : 2.9443163871765137\n",
      "Loss at step 27050 : 2.595182418823242\n",
      "Loss at step 27100 : 3.2917532920837402\n",
      "Loss at step 27150 : 2.4603588581085205\n",
      "Loss at step 27200 : 3.7058727741241455\n",
      "Loss at step 27250 : 2.747903823852539\n",
      "Loss at step 27300 : 5.492403984069824\n",
      "Loss at step 27350 : 3.084425687789917\n",
      "Loss at step 27400 : 3.2286510467529297\n",
      "Loss at step 27450 : 3.3954687118530273\n",
      "Loss at step 27500 : 2.8226919174194336\n",
      "Loss at step 27550 : 2.9183273315429688\n",
      "Loss at step 27600 : 2.9342098236083984\n",
      "Loss at step 27650 : 4.356785774230957\n",
      "Loss at step 27700 : 4.089685440063477\n",
      "Loss at step 27750 : 3.3576622009277344\n",
      "Loss at step 27800 : 3.5520176887512207\n",
      "Loss at step 27850 : 3.4748029708862305\n",
      "Loss at step 27900 : 2.6164774894714355\n",
      "Loss at step 27950 : 4.430897235870361\n",
      "Loss at step 28000 : 3.0526623725891113\n",
      "Loss at step 28050 : 2.715397834777832\n",
      "Loss at step 28100 : 2.5369174480438232\n",
      "Loss at step 28150 : 3.08416748046875\n",
      "Loss at step 28200 : 2.2701871395111084\n",
      "Loss at step 28250 : 4.677051544189453\n",
      "Loss at step 28300 : 2.0130186080932617\n",
      "Loss at step 28350 : 2.296818494796753\n",
      "Loss at step 28400 : 3.82780122756958\n",
      "Loss at step 28450 : 2.4236063957214355\n",
      "Loss at step 28500 : 2.9593122005462646\n",
      "Loss at step 28550 : 2.6360716819763184\n",
      "Loss at step 28600 : 2.540597438812256\n",
      "Loss at step 28650 : 1.2635326385498047\n",
      "Loss at step 28700 : 1.914963722229004\n",
      "Loss at step 28750 : 2.6729507446289062\n",
      "Loss at step 28800 : 1.4903826713562012\n",
      "Loss at step 28850 : 2.5985968112945557\n",
      "Loss at step 28900 : 4.2195281982421875\n",
      "Loss at step 28950 : 1.9698641300201416\n",
      "Loss at step 29000 : 4.382310390472412\n",
      "Loss at step 29050 : 2.736633777618408\n",
      "Loss at step 29100 : 3.2386112213134766\n",
      "Loss at step 29150 : 2.4634549617767334\n",
      "Loss at step 29200 : 3.6858935356140137\n",
      "Loss at step 29250 : 1.8057987689971924\n",
      "Loss at step 29300 : 2.664897918701172\n",
      "Loss at step 29350 : 3.641385555267334\n",
      "Loss at step 29400 : 3.4201292991638184\n",
      "Loss at step 29450 : 2.3062374591827393\n",
      "Loss at step 29500 : 3.0562143325805664\n",
      "Loss at step 29550 : 2.2797224521636963\n",
      "Loss at step 29600 : 2.4621405601501465\n",
      "Loss at step 29650 : 2.9349493980407715\n",
      "Loss at step 29700 : 3.170074224472046\n",
      "Loss at step 29750 : 4.941296100616455\n",
      "Loss at step 29800 : 2.568692922592163\n",
      "Loss at step 29850 : 3.513418674468994\n",
      "Loss at step 29900 : 3.666203022003174\n",
      "Loss at step 29950 : 3.584689140319824\n",
      "Loss at step 30000 : 3.175187349319458\n",
      "Nearest to tuna: mushroom, salmon, spring, grain, wafer,\n",
      "Nearest to rice: cling, nori, strip, position, finger,\n",
      "Nearest to sushi: towel, strip, fish, flesh, matchstick,\n",
      "Nearest to roll: sesame, flesh, dipping, bamboo, angle,\n",
      "Nearest to sashimi: piece, leaf, torch, blade, daikon,\n",
      "Nearest to steak: salt, onion, pepper, marinade, grill,\n",
      "Nearest to grill: worcestershire, grate, marinade, preheat, pepper,\n",
      "Nearest to sauce: cone, grain, ginger, chive, press,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, yolk, chocolate,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 30050 : 1.9430460929870605\n",
      "Loss at step 30100 : 2.70867919921875\n",
      "Loss at step 30150 : 2.3072752952575684\n",
      "Loss at step 30200 : 3.2505390644073486\n",
      "Loss at step 30250 : 2.7515437602996826\n",
      "Loss at step 30300 : 2.0536701679229736\n",
      "Loss at step 30350 : 2.1844730377197266\n",
      "Loss at step 30400 : 3.263472080230713\n",
      "Loss at step 30450 : 3.322146415710449\n",
      "Loss at step 30500 : 3.071634292602539\n",
      "Loss at step 30550 : 6.071964263916016\n",
      "Loss at step 30600 : 2.501404285430908\n",
      "Loss at step 30650 : 2.9200799465179443\n",
      "Loss at step 30700 : 3.3378140926361084\n",
      "Loss at step 30750 : 4.9312520027160645\n",
      "Loss at step 30800 : 2.6996984481811523\n",
      "Loss at step 30850 : 2.98093318939209\n",
      "Loss at step 30900 : 4.216719627380371\n",
      "Loss at step 30950 : 4.550516128540039\n",
      "Loss at step 31000 : 4.402140140533447\n",
      "Loss at step 31050 : 4.40810489654541\n",
      "Loss at step 31100 : 2.6723241806030273\n",
      "Loss at step 31150 : 3.895338535308838\n",
      "Loss at step 31200 : 3.6583189964294434\n",
      "Loss at step 31250 : 3.7116551399230957\n",
      "Loss at step 31300 : 3.3845303058624268\n",
      "Loss at step 31350 : 3.997694492340088\n",
      "Loss at step 31400 : 1.8585522174835205\n",
      "Loss at step 31450 : 3.2736191749572754\n",
      "Loss at step 31500 : 2.623298406600952\n",
      "Loss at step 31550 : 4.399329662322998\n",
      "Loss at step 31600 : 2.2355401515960693\n",
      "Loss at step 31650 : 2.476147413253784\n",
      "Loss at step 31700 : 4.5418901443481445\n",
      "Loss at step 31750 : 3.3745534420013428\n",
      "Loss at step 31800 : 3.4867208003997803\n",
      "Loss at step 31850 : 4.338860988616943\n",
      "Loss at step 31900 : 2.9657821655273438\n",
      "Loss at step 31950 : 2.084270477294922\n",
      "Loss at step 32000 : 3.044935703277588\n",
      "Loss at step 32050 : 2.285916328430176\n",
      "Loss at step 32100 : 4.671809673309326\n",
      "Loss at step 32150 : 4.217342376708984\n",
      "Loss at step 32200 : 4.41636848449707\n",
      "Loss at step 32250 : 3.6852951049804688\n",
      "Loss at step 32300 : 2.5548295974731445\n",
      "Loss at step 32350 : 3.514194965362549\n",
      "Loss at step 32400 : 2.564863681793213\n",
      "Loss at step 32450 : 3.2369437217712402\n",
      "Loss at step 32500 : 3.4064576625823975\n",
      "Loss at step 32550 : 2.1380839347839355\n",
      "Loss at step 32600 : 2.8610339164733887\n",
      "Loss at step 32650 : 1.1032568216323853\n",
      "Loss at step 32700 : 5.404908657073975\n",
      "Loss at step 32750 : 2.471867799758911\n",
      "Loss at step 32800 : 4.131019592285156\n",
      "Loss at step 32850 : 3.2527036666870117\n",
      "Loss at step 32900 : 4.299459934234619\n",
      "Loss at step 32950 : 5.6887922286987305\n",
      "Loss at step 33000 : 3.4858593940734863\n",
      "Loss at step 33050 : 5.3243408203125\n",
      "Loss at step 33100 : 3.7832348346710205\n",
      "Loss at step 33150 : 2.8054039478302\n",
      "Loss at step 33200 : 3.8886704444885254\n",
      "Loss at step 33250 : 2.4061520099639893\n",
      "Loss at step 33300 : 3.743478775024414\n",
      "Loss at step 33350 : 2.3221728801727295\n",
      "Loss at step 33400 : 2.132699728012085\n",
      "Loss at step 33450 : 4.7270827293396\n",
      "Loss at step 33500 : 3.7455639839172363\n",
      "Loss at step 33550 : 4.333347320556641\n",
      "Loss at step 33600 : 2.3189303874969482\n",
      "Loss at step 33650 : 2.4451394081115723\n",
      "Loss at step 33700 : 1.2670583724975586\n",
      "Loss at step 33750 : 3.00907564163208\n",
      "Loss at step 33800 : 3.3036556243896484\n",
      "Loss at step 33850 : 2.959381103515625\n",
      "Loss at step 33900 : 2.7113444805145264\n",
      "Loss at step 33950 : 2.762834072113037\n",
      "Loss at step 34000 : 2.279374599456787\n",
      "Loss at step 34050 : 3.794952154159546\n",
      "Loss at step 34100 : 2.1283044815063477\n",
      "Loss at step 34150 : 2.7114548683166504\n",
      "Loss at step 34200 : 2.368309736251831\n",
      "Loss at step 34250 : 2.4922938346862793\n",
      "Loss at step 34300 : 2.534395694732666\n",
      "Loss at step 34350 : 2.77639102935791\n",
      "Loss at step 34400 : 3.255526542663574\n",
      "Loss at step 34450 : 1.041574239730835\n",
      "Loss at step 34500 : 2.747690200805664\n",
      "Loss at step 34550 : 3.080827474594116\n",
      "Loss at step 34600 : 3.948133945465088\n",
      "Loss at step 34650 : 2.4328320026397705\n",
      "Loss at step 34700 : 3.1513009071350098\n",
      "Loss at step 34750 : 3.4506683349609375\n",
      "Loss at step 34800 : 3.4003520011901855\n",
      "Loss at step 34850 : 3.517582654953003\n",
      "Loss at step 34900 : 2.533109426498413\n",
      "Loss at step 34950 : 3.400697708129883\n",
      "Loss at step 35000 : 3.4455766677856445\n",
      "Nearest to tuna: mushroom, spring, salmon, grain, piece,\n",
      "Nearest to rice: cling, cucumber, finger, nori, strip,\n",
      "Nearest to sushi: towel, strip, fish, seed, matchstick,\n",
      "Nearest to roll: sesame, flesh, dipping, piece, bamboo,\n",
      "Nearest to sashimi: leaf, piece, torch, daikon, topping,\n",
      "Nearest to steak: salt, onion, grill, pepper, ground,\n",
      "Nearest to grill: worcestershire, grate, marinade, preheat, pepper,\n",
      "Nearest to sauce: cone, press, salmon, grain, ginger,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, wafer, chocolate,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 35050 : 3.120661735534668\n",
      "Loss at step 35100 : 3.6782593727111816\n",
      "Loss at step 35150 : 1.6198489665985107\n",
      "Loss at step 35200 : 1.903459072113037\n",
      "Loss at step 35250 : 2.4801816940307617\n",
      "Loss at step 35300 : 2.6335337162017822\n",
      "Loss at step 35350 : 3.1175904273986816\n",
      "Loss at step 35400 : 2.7281458377838135\n",
      "Loss at step 35450 : 4.283566951751709\n",
      "Loss at step 35500 : 2.4466474056243896\n",
      "Loss at step 35550 : 3.803170680999756\n",
      "Loss at step 35600 : 2.1507816314697266\n",
      "Loss at step 35650 : 4.047720432281494\n",
      "Loss at step 35700 : 3.9590048789978027\n",
      "Loss at step 35750 : 4.407251358032227\n",
      "Loss at step 35800 : 2.279038667678833\n",
      "Loss at step 35850 : 3.2955899238586426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 35900 : 2.661323308944702\n",
      "Loss at step 35950 : 3.1783242225646973\n",
      "Loss at step 36000 : 3.066302537918091\n",
      "Loss at step 36050 : 5.271042823791504\n",
      "Loss at step 36100 : 3.08550763130188\n",
      "Loss at step 36150 : 2.9678473472595215\n",
      "Loss at step 36200 : 4.599382400512695\n",
      "Loss at step 36250 : 4.209259510040283\n",
      "Loss at step 36300 : 2.052501916885376\n",
      "Loss at step 36350 : 2.4843480587005615\n",
      "Loss at step 36400 : 2.3564658164978027\n",
      "Loss at step 36450 : 2.652085781097412\n",
      "Loss at step 36500 : 2.2219014167785645\n",
      "Loss at step 36550 : 2.2271344661712646\n",
      "Loss at step 36600 : 2.946514129638672\n",
      "Loss at step 36650 : 2.442829132080078\n",
      "Loss at step 36700 : 2.1596226692199707\n",
      "Loss at step 36750 : 2.716336727142334\n",
      "Loss at step 36800 : 4.5653533935546875\n",
      "Loss at step 36850 : 2.561020851135254\n",
      "Loss at step 36900 : 1.828066110610962\n",
      "Loss at step 36950 : 2.5727365016937256\n",
      "Loss at step 37000 : 2.928762435913086\n",
      "Loss at step 37050 : 3.8836286067962646\n",
      "Loss at step 37100 : 3.3459279537200928\n",
      "Loss at step 37150 : 2.9514074325561523\n",
      "Loss at step 37200 : 2.448518991470337\n",
      "Loss at step 37250 : 3.0479540824890137\n",
      "Loss at step 37300 : 2.1867775917053223\n",
      "Loss at step 37350 : 3.531798839569092\n",
      "Loss at step 37400 : 2.8089585304260254\n",
      "Loss at step 37450 : 2.1385178565979004\n",
      "Loss at step 37500 : 2.9446909427642822\n",
      "Loss at step 37550 : 2.968717098236084\n",
      "Loss at step 37600 : 2.5197582244873047\n",
      "Loss at step 37650 : 5.311525344848633\n",
      "Loss at step 37700 : 2.713717460632324\n",
      "Loss at step 37750 : 3.1236331462860107\n",
      "Loss at step 37800 : 1.7844454050064087\n",
      "Loss at step 37850 : 3.227658748626709\n",
      "Loss at step 37900 : 3.3163323402404785\n",
      "Loss at step 37950 : 4.282052993774414\n",
      "Loss at step 38000 : 2.7005767822265625\n",
      "Loss at step 38050 : 1.7954185009002686\n",
      "Loss at step 38100 : 4.961216449737549\n",
      "Loss at step 38150 : 1.5099046230316162\n",
      "Loss at step 38200 : 4.427336692810059\n",
      "Loss at step 38250 : 4.145056247711182\n",
      "Loss at step 38300 : 5.240920066833496\n",
      "Loss at step 38350 : 3.9635586738586426\n",
      "Loss at step 38400 : 2.3918633460998535\n",
      "Loss at step 38450 : 2.7185215950012207\n",
      "Loss at step 38500 : 3.3318464756011963\n",
      "Loss at step 38550 : 2.323060989379883\n",
      "Loss at step 38600 : 3.6795456409454346\n",
      "Loss at step 38650 : 3.6903257369995117\n",
      "Loss at step 38700 : 3.4189577102661133\n",
      "Loss at step 38750 : 3.6101627349853516\n",
      "Loss at step 38800 : 2.4965591430664062\n",
      "Loss at step 38850 : 2.17533016204834\n",
      "Loss at step 38900 : 3.8397603034973145\n",
      "Loss at step 38950 : 2.568600654602051\n",
      "Loss at step 39000 : 1.97981858253479\n",
      "Loss at step 39050 : 2.915861129760742\n",
      "Loss at step 39100 : 2.635939836502075\n",
      "Loss at step 39150 : 5.239809989929199\n",
      "Loss at step 39200 : 2.994478702545166\n",
      "Loss at step 39250 : 3.9259262084960938\n",
      "Loss at step 39300 : 2.2475152015686035\n",
      "Loss at step 39350 : 2.5512208938598633\n",
      "Loss at step 39400 : 1.736816167831421\n",
      "Loss at step 39450 : 3.0773818492889404\n",
      "Loss at step 39500 : 2.125868082046509\n",
      "Loss at step 39550 : 2.546520233154297\n",
      "Loss at step 39600 : 3.0579569339752197\n",
      "Loss at step 39650 : 2.663973331451416\n",
      "Loss at step 39700 : 3.2675747871398926\n",
      "Loss at step 39750 : 2.3761727809906006\n",
      "Loss at step 39800 : 2.911123752593994\n",
      "Loss at step 39850 : 3.279484510421753\n",
      "Loss at step 39900 : 4.336441993713379\n",
      "Loss at step 39950 : 1.7287312746047974\n",
      "Loss at step 40000 : 4.539886474609375\n",
      "Nearest to tuna: piece, mushroom, salmon, spring, grain,\n",
      "Nearest to rice: salmon, finger, cling, cucumber, strip,\n",
      "Nearest to sushi: towel, strip, seed, fish, vinegar,\n",
      "Nearest to roll: sesame, flesh, dipping, bamboo, sushi,\n",
      "Nearest to sashimi: leaf, piece, torch, daikon, coriander,\n",
      "Nearest to steak: salt, onion, grill, marinade, pepper,\n",
      "Nearest to grill: worcestershire, grate, preheat, marinade, oil,\n",
      "Nearest to sauce: salmon, cone, grain, chive, cucumber,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, wafer, yolk,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 40050 : 3.0871224403381348\n",
      "Loss at step 40100 : 3.4065632820129395\n",
      "Loss at step 40150 : 1.7600200176239014\n",
      "Loss at step 40200 : 2.474785566329956\n",
      "Loss at step 40250 : 2.647526979446411\n",
      "Loss at step 40300 : 3.7209599018096924\n",
      "Loss at step 40350 : 2.8383169174194336\n",
      "Loss at step 40400 : 3.2558817863464355\n",
      "Loss at step 40450 : 4.54831600189209\n",
      "Loss at step 40500 : 1.0315728187561035\n",
      "Loss at step 40550 : 4.273160934448242\n",
      "Loss at step 40600 : 2.8106906414031982\n",
      "Loss at step 40650 : 3.9982454776763916\n",
      "Loss at step 40700 : 2.7944226264953613\n",
      "Loss at step 40750 : 3.169114589691162\n",
      "Loss at step 40800 : 2.66025447845459\n",
      "Loss at step 40850 : 3.5410685539245605\n",
      "Loss at step 40900 : 4.814798831939697\n",
      "Loss at step 40950 : 1.9776325225830078\n",
      "Loss at step 41000 : 3.109347105026245\n",
      "Loss at step 41050 : 2.8313488960266113\n",
      "Loss at step 41100 : 4.273404121398926\n",
      "Loss at step 41150 : 2.4925734996795654\n",
      "Loss at step 41200 : 4.153989791870117\n",
      "Loss at step 41250 : 2.5281991958618164\n",
      "Loss at step 41300 : 1.7920689582824707\n",
      "Loss at step 41350 : 2.730090618133545\n",
      "Loss at step 41400 : 2.9400014877319336\n",
      "Loss at step 41450 : 2.943277597427368\n",
      "Loss at step 41500 : 4.32407283782959\n",
      "Loss at step 41550 : 2.626539468765259\n",
      "Loss at step 41600 : 1.195649266242981\n",
      "Loss at step 41650 : 4.3917717933654785\n",
      "Loss at step 41700 : 4.143758773803711\n",
      "Loss at step 41750 : 3.438915967941284\n",
      "Loss at step 41800 : 3.0933854579925537\n",
      "Loss at step 41850 : 3.1318132877349854\n",
      "Loss at step 41900 : 3.642927885055542\n",
      "Loss at step 41950 : 2.96657395362854\n",
      "Loss at step 42000 : 1.7949987649917603\n",
      "Loss at step 42050 : 2.3923206329345703\n",
      "Loss at step 42100 : 3.0267982482910156\n",
      "Loss at step 42150 : 3.7820608615875244\n",
      "Loss at step 42200 : 3.415616512298584\n",
      "Loss at step 42250 : 1.7518963813781738\n",
      "Loss at step 42300 : 4.5166754722595215\n",
      "Loss at step 42350 : 2.7584640979766846\n",
      "Loss at step 42400 : 2.524514675140381\n",
      "Loss at step 42450 : 1.7903800010681152\n",
      "Loss at step 42500 : 3.418294906616211\n",
      "Loss at step 42550 : 2.6850292682647705\n",
      "Loss at step 42600 : 4.340855121612549\n",
      "Loss at step 42650 : 2.9147393703460693\n",
      "Loss at step 42700 : 3.6902759075164795\n",
      "Loss at step 42750 : 2.9110302925109863\n",
      "Loss at step 42800 : 2.380796194076538\n",
      "Loss at step 42850 : 1.7002705335617065\n",
      "Loss at step 42900 : 3.2249526977539062\n",
      "Loss at step 42950 : 2.932023048400879\n",
      "Loss at step 43000 : 2.5408613681793213\n",
      "Loss at step 43050 : 1.8444011211395264\n",
      "Loss at step 43100 : 3.1028597354888916\n",
      "Loss at step 43150 : 2.84800124168396\n",
      "Loss at step 43200 : 1.7106578350067139\n",
      "Loss at step 43250 : 2.701866388320923\n",
      "Loss at step 43300 : 2.0907480716705322\n",
      "Loss at step 43350 : 2.339587688446045\n",
      "Loss at step 43400 : 2.0012831687927246\n",
      "Loss at step 43450 : 2.501783609390259\n",
      "Loss at step 43500 : 2.9859423637390137\n",
      "Loss at step 43550 : 3.1892285346984863\n",
      "Loss at step 43600 : 2.3090572357177734\n",
      "Loss at step 43650 : 4.842260837554932\n",
      "Loss at step 43700 : 2.1744306087493896\n",
      "Loss at step 43750 : 2.709920883178711\n",
      "Loss at step 43800 : 3.156315803527832\n",
      "Loss at step 43850 : 3.1994261741638184\n",
      "Loss at step 43900 : 3.8173458576202393\n",
      "Loss at step 43950 : 3.2868778705596924\n",
      "Loss at step 44000 : 2.324854850769043\n",
      "Loss at step 44050 : 2.9209580421447754\n",
      "Loss at step 44100 : 4.035012722015381\n",
      "Loss at step 44150 : 3.309826374053955\n",
      "Loss at step 44200 : 2.11594820022583\n",
      "Loss at step 44250 : 2.3218190670013428\n",
      "Loss at step 44300 : 2.7443411350250244\n",
      "Loss at step 44350 : 2.3181145191192627\n",
      "Loss at step 44400 : 2.715940237045288\n",
      "Loss at step 44450 : 1.5900137424468994\n",
      "Loss at step 44500 : 3.1551804542541504\n",
      "Loss at step 44550 : 2.8205955028533936\n",
      "Loss at step 44600 : 2.7138280868530273\n",
      "Loss at step 44650 : 2.989708185195923\n",
      "Loss at step 44700 : 3.8447372913360596\n",
      "Loss at step 44750 : 2.962587356567383\n",
      "Loss at step 44800 : 3.6322410106658936\n",
      "Loss at step 44850 : 2.1703710556030273\n",
      "Loss at step 44900 : 4.834834098815918\n",
      "Loss at step 44950 : 2.508845090866089\n",
      "Loss at step 45000 : 2.7988688945770264\n",
      "Nearest to tuna: mushroom, spring, block, crab, salmon,\n",
      "Nearest to rice: finger, salmon, cucumber, strip, cling,\n",
      "Nearest to sushi: towel, vinegar, strip, seed, fish,\n",
      "Nearest to roll: flesh, sesame, sushi, bamboo, piece,\n",
      "Nearest to sashimi: leaf, piece, torch, daikon, angle,\n",
      "Nearest to steak: salt, onion, grill, marinade, pepper,\n",
      "Nearest to grill: worcestershire, preheat, marinade, grate, oil,\n",
      "Nearest to sauce: salmon, chive, cone, press, ginger,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, wafer, layer,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 45050 : 2.9423041343688965\n",
      "Loss at step 45100 : 3.6911678314208984\n",
      "Loss at step 45150 : 2.7664008140563965\n",
      "Loss at step 45200 : 3.050328493118286\n",
      "Loss at step 45250 : 4.157364368438721\n",
      "Loss at step 45300 : 1.6424990892410278\n",
      "Loss at step 45350 : 2.024791717529297\n",
      "Loss at step 45400 : 4.145397186279297\n",
      "Loss at step 45450 : 3.793700695037842\n",
      "Loss at step 45500 : 2.9577395915985107\n",
      "Loss at step 45550 : 4.773894786834717\n",
      "Loss at step 45600 : 2.4538161754608154\n",
      "Loss at step 45650 : 2.523203134536743\n",
      "Loss at step 45700 : 3.7238528728485107\n",
      "Loss at step 45750 : 3.231067419052124\n",
      "Loss at step 45800 : 3.901339530944824\n",
      "Loss at step 45850 : 4.41019344329834\n",
      "Loss at step 45900 : 2.112452268600464\n",
      "Loss at step 45950 : 2.128045082092285\n",
      "Loss at step 46000 : 2.0744540691375732\n",
      "Loss at step 46050 : 3.3924074172973633\n",
      "Loss at step 46100 : 4.242480278015137\n",
      "Loss at step 46150 : 2.4748384952545166\n",
      "Loss at step 46200 : 3.3263840675354004\n",
      "Loss at step 46250 : 1.8863129615783691\n",
      "Loss at step 46300 : 2.998167037963867\n",
      "Loss at step 46350 : 2.436401605606079\n",
      "Loss at step 46400 : 4.643715858459473\n",
      "Loss at step 46450 : 3.8580007553100586\n",
      "Loss at step 46500 : 2.946316719055176\n",
      "Loss at step 46550 : 4.96743106842041\n",
      "Loss at step 46600 : 4.212752819061279\n",
      "Loss at step 46650 : 2.8041768074035645\n",
      "Loss at step 46700 : 2.88547420501709\n",
      "Loss at step 46750 : 2.6578755378723145\n",
      "Loss at step 46800 : 2.9690990447998047\n",
      "Loss at step 46850 : 4.175567150115967\n",
      "Loss at step 46900 : 4.751956939697266\n",
      "Loss at step 46950 : 3.2121987342834473\n",
      "Loss at step 47000 : 3.4233336448669434\n",
      "Loss at step 47050 : 2.666569948196411\n",
      "Loss at step 47100 : 1.952183723449707\n",
      "Loss at step 47150 : 2.4102942943573\n",
      "Loss at step 47200 : 3.8877410888671875\n",
      "Loss at step 47250 : 2.879326343536377\n",
      "Loss at step 47300 : 3.5411088466644287\n",
      "Loss at step 47350 : 3.359142303466797\n",
      "Loss at step 47400 : 3.4000282287597656\n",
      "Loss at step 47450 : 2.9299426078796387\n",
      "Loss at step 47500 : 3.919752836227417\n",
      "Loss at step 47550 : 2.1605165004730225\n",
      "Loss at step 47600 : 2.5529816150665283\n",
      "Loss at step 47650 : 3.6994614601135254\n",
      "Loss at step 47700 : 1.854806661605835\n",
      "Loss at step 47750 : 3.5119383335113525\n",
      "Loss at step 47800 : 2.102648973464966\n",
      "Loss at step 47850 : 2.3898580074310303\n",
      "Loss at step 47900 : 1.9363117218017578\n",
      "Loss at step 47950 : 2.5227982997894287\n",
      "Loss at step 48000 : 2.4566636085510254\n",
      "Loss at step 48050 : 2.799203395843506\n",
      "Loss at step 48100 : 3.054655075073242\n",
      "Loss at step 48150 : 2.745709180831909\n",
      "Loss at step 48200 : 2.7671422958374023\n",
      "Loss at step 48250 : 3.4400010108947754\n",
      "Loss at step 48300 : 4.227446556091309\n",
      "Loss at step 48350 : 2.32827091217041\n",
      "Loss at step 48400 : 2.7382633686065674\n",
      "Loss at step 48450 : 2.3207356929779053\n",
      "Loss at step 48500 : 3.0250847339630127\n",
      "Loss at step 48550 : 1.9747483730316162\n",
      "Loss at step 48600 : 2.6873888969421387\n",
      "Loss at step 48650 : 4.284688949584961\n",
      "Loss at step 48700 : 2.4955379962921143\n",
      "Loss at step 48750 : 4.994176864624023\n",
      "Loss at step 48800 : 2.915849208831787\n",
      "Loss at step 48850 : 3.2100770473480225\n",
      "Loss at step 48900 : 1.4970871210098267\n",
      "Loss at step 48950 : 2.4359490871429443\n",
      "Loss at step 49000 : 3.4173316955566406\n",
      "Loss at step 49050 : 2.901529312133789\n",
      "Loss at step 49100 : 1.6524453163146973\n",
      "Loss at step 49150 : 4.416626930236816\n",
      "Loss at step 49200 : 3.4788880348205566\n",
      "Loss at step 49250 : 2.941967725753784\n",
      "Loss at step 49300 : 4.110846519470215\n",
      "Loss at step 49350 : 2.610283613204956\n",
      "Loss at step 49400 : 2.7621214389801025\n",
      "Loss at step 49450 : 3.828317880630493\n",
      "Loss at step 49500 : 3.050419807434082\n",
      "Loss at step 49550 : 3.178622007369995\n",
      "Loss at step 49600 : 2.514108896255493\n",
      "Loss at step 49650 : 2.2572367191314697\n",
      "Loss at step 49700 : 2.4119386672973633\n",
      "Loss at step 49750 : 2.5377025604248047\n",
      "Loss at step 49800 : 2.597064971923828\n",
      "Loss at step 49850 : 1.4862078428268433\n",
      "Loss at step 49900 : 4.482873916625977\n",
      "Loss at step 49950 : 2.8435311317443848\n",
      "Loss at step 50000 : 2.070261001586914\n",
      "Nearest to tuna: mushroom, block, spring, piece, crab,\n",
      "Nearest to rice: finger, salmon, strip, cucumber, cling,\n",
      "Nearest to sushi: towel, fish, seed, strip, flesh,\n",
      "Nearest to roll: sesame, flesh, bamboo, dipping, piece,\n",
      "Nearest to sashimi: leaf, piece, torch, coriander, daikon,\n",
      "Nearest to steak: onion, salt, marinade, grill, pepper,\n",
      "Nearest to grill: worcestershire, marinade, preheat, grate, steak,\n",
      "Nearest to sauce: chive, salmon, grill, ground, cone,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, wafer, yolk,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 50050 : 1.8861985206604004\n",
      "Loss at step 50100 : 4.335507392883301\n",
      "Loss at step 50150 : 3.160776138305664\n",
      "Loss at step 50200 : 2.2870450019836426\n",
      "Loss at step 50250 : 3.6616697311401367\n",
      "Loss at step 50300 : 4.550032138824463\n",
      "Loss at step 50350 : 1.5175094604492188\n",
      "Loss at step 50400 : 2.843295097351074\n",
      "Loss at step 50450 : 1.898738145828247\n",
      "Loss at step 50500 : 2.554884195327759\n",
      "Loss at step 50550 : 2.438589096069336\n",
      "Loss at step 50600 : 3.722970485687256\n",
      "Loss at step 50650 : 0.9886113405227661\n",
      "Loss at step 50700 : 2.989856481552124\n",
      "Loss at step 50750 : 2.338975667953491\n",
      "Loss at step 50800 : 2.333909511566162\n",
      "Loss at step 50850 : 4.564818382263184\n",
      "Loss at step 50900 : 1.8693149089813232\n",
      "Loss at step 50950 : 2.7869293689727783\n",
      "Loss at step 51000 : 3.1228766441345215\n",
      "Loss at step 51050 : 3.723788261413574\n",
      "Loss at step 51100 : 1.4873201847076416\n",
      "Loss at step 51150 : 3.073066234588623\n",
      "Loss at step 51200 : 3.8686840534210205\n",
      "Loss at step 51250 : 3.091150999069214\n",
      "Loss at step 51300 : 2.9864611625671387\n",
      "Loss at step 51350 : 3.308863639831543\n",
      "Loss at step 51400 : 3.497053623199463\n",
      "Loss at step 51450 : 1.387983798980713\n",
      "Loss at step 51500 : 3.1895029544830322\n",
      "Loss at step 51550 : 2.9211158752441406\n",
      "Loss at step 51600 : 4.040589809417725\n",
      "Loss at step 51650 : 3.2834413051605225\n",
      "Loss at step 51700 : 1.7090871334075928\n",
      "Loss at step 51750 : 1.8163456916809082\n",
      "Loss at step 51800 : 3.086482524871826\n",
      "Loss at step 51850 : 5.0093584060668945\n",
      "Loss at step 51900 : 3.1176130771636963\n",
      "Loss at step 51950 : 2.5638296604156494\n",
      "Loss at step 52000 : 2.7215402126312256\n",
      "Loss at step 52050 : 2.705782413482666\n",
      "Loss at step 52100 : 5.352672100067139\n",
      "Loss at step 52150 : 2.3095035552978516\n",
      "Loss at step 52200 : 3.459770679473877\n",
      "Loss at step 52250 : 2.3551459312438965\n",
      "Loss at step 52300 : 3.085949420928955\n",
      "Loss at step 52350 : 4.080990314483643\n",
      "Loss at step 52400 : 2.6727633476257324\n",
      "Loss at step 52450 : 2.7187442779541016\n",
      "Loss at step 52500 : 1.786775827407837\n",
      "Loss at step 52550 : 2.5938804149627686\n",
      "Loss at step 52600 : 4.201625823974609\n",
      "Loss at step 52650 : 4.306368827819824\n",
      "Loss at step 52700 : 1.9502184391021729\n",
      "Loss at step 52750 : 3.4314346313476562\n",
      "Loss at step 52800 : 5.469832420349121\n",
      "Loss at step 52850 : 2.7184324264526367\n",
      "Loss at step 52900 : 1.9682719707489014\n",
      "Loss at step 52950 : 2.4206178188323975\n",
      "Loss at step 53000 : 2.027695417404175\n",
      "Loss at step 53050 : 4.728608131408691\n",
      "Loss at step 53100 : 3.1622467041015625\n",
      "Loss at step 53150 : 2.093895435333252\n",
      "Loss at step 53200 : 2.6066269874572754\n",
      "Loss at step 53250 : 2.300520658493042\n",
      "Loss at step 53300 : 1.9321634769439697\n",
      "Loss at step 53350 : 2.6486687660217285\n",
      "Loss at step 53400 : 2.430999755859375\n",
      "Loss at step 53450 : 3.068331480026245\n",
      "Loss at step 53500 : 2.880211591720581\n",
      "Loss at step 53550 : 3.6997437477111816\n",
      "Loss at step 53600 : 2.0515847206115723\n",
      "Loss at step 53650 : 4.575703144073486\n",
      "Loss at step 53700 : 4.2869720458984375\n",
      "Loss at step 53750 : 1.5236787796020508\n",
      "Loss at step 53800 : 3.8533072471618652\n",
      "Loss at step 53850 : 2.603095769882202\n",
      "Loss at step 53900 : 2.855071544647217\n",
      "Loss at step 53950 : 2.068019151687622\n",
      "Loss at step 54000 : 2.5703988075256348\n",
      "Loss at step 54050 : 2.560407876968384\n",
      "Loss at step 54100 : 3.118131160736084\n",
      "Loss at step 54150 : 3.051687240600586\n",
      "Loss at step 54200 : 4.501413345336914\n",
      "Loss at step 54250 : 3.0095479488372803\n",
      "Loss at step 54300 : 2.6179111003875732\n",
      "Loss at step 54350 : 3.8294882774353027\n",
      "Loss at step 54400 : 2.2265090942382812\n",
      "Loss at step 54450 : 2.3269662857055664\n",
      "Loss at step 54500 : 2.341364860534668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 54550 : 3.492454767227173\n",
      "Loss at step 54600 : 3.438828945159912\n",
      "Loss at step 54650 : 2.784482479095459\n",
      "Loss at step 54700 : 2.5015997886657715\n",
      "Loss at step 54750 : 1.6735605001449585\n",
      "Loss at step 54800 : 2.465521812438965\n",
      "Loss at step 54850 : 3.055821418762207\n",
      "Loss at step 54900 : 4.563014507293701\n",
      "Loss at step 54950 : 4.09375524520874\n",
      "Loss at step 55000 : 2.792577028274536\n",
      "Nearest to tuna: mushroom, block, spring, piece, crab,\n",
      "Nearest to rice: salmon, finger, strip, cling, cucumber,\n",
      "Nearest to sushi: towel, fish, strip, flesh, roll,\n",
      "Nearest to roll: sesame, flesh, sushi, bamboo, bit,\n",
      "Nearest to sashimi: leaf, piece, coriander, torch, angle,\n",
      "Nearest to steak: onion, salt, grill, marinade, pepper,\n",
      "Nearest to grill: worcestershire, marinade, preheat, grate, steak,\n",
      "Nearest to sauce: chive, salmon, grill, ground, press,\n",
      "Nearest to cream: liqueur, mascarpone, wafer, coffee, chocolate,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 55050 : 2.5167229175567627\n",
      "Loss at step 55100 : 4.005575180053711\n",
      "Loss at step 55150 : 2.816412925720215\n",
      "Loss at step 55200 : 3.28829288482666\n",
      "Loss at step 55250 : 3.736703634262085\n",
      "Loss at step 55300 : 3.1899538040161133\n",
      "Loss at step 55350 : 4.482608795166016\n",
      "Loss at step 55400 : 2.430061101913452\n",
      "Loss at step 55450 : 3.526309013366699\n",
      "Loss at step 55500 : 3.212001085281372\n",
      "Loss at step 55550 : 3.1974527835845947\n",
      "Loss at step 55600 : 3.005784511566162\n",
      "Loss at step 55650 : 1.130044937133789\n",
      "Loss at step 55700 : 4.544242858886719\n",
      "Loss at step 55750 : 4.613142013549805\n",
      "Loss at step 55800 : 1.6786785125732422\n",
      "Loss at step 55850 : 2.2708938121795654\n",
      "Loss at step 55900 : 3.1520252227783203\n",
      "Loss at step 55950 : 3.7062323093414307\n",
      "Loss at step 56000 : 2.2939281463623047\n",
      "Loss at step 56050 : 2.9227278232574463\n",
      "Loss at step 56100 : 3.6879305839538574\n",
      "Loss at step 56150 : 4.9004926681518555\n",
      "Loss at step 56200 : 3.4941940307617188\n",
      "Loss at step 56250 : 2.4770078659057617\n",
      "Loss at step 56300 : 3.1249544620513916\n",
      "Loss at step 56350 : 3.3428144454956055\n",
      "Loss at step 56400 : 4.339363098144531\n",
      "Loss at step 56450 : 1.8222911357879639\n",
      "Loss at step 56500 : 2.932060956954956\n",
      "Loss at step 56550 : 2.248459815979004\n",
      "Loss at step 56600 : 3.212151527404785\n",
      "Loss at step 56650 : 2.1042320728302\n",
      "Loss at step 56700 : 2.6730971336364746\n",
      "Loss at step 56750 : 2.487013816833496\n",
      "Loss at step 56800 : 3.836960792541504\n",
      "Loss at step 56850 : 2.9017603397369385\n",
      "Loss at step 56900 : 4.554203510284424\n",
      "Loss at step 56950 : 1.2327467203140259\n",
      "Loss at step 57000 : 3.7830677032470703\n",
      "Loss at step 57050 : 3.0941383838653564\n",
      "Loss at step 57100 : 2.2012171745300293\n",
      "Loss at step 57150 : 3.4884700775146484\n",
      "Loss at step 57200 : 3.0117106437683105\n",
      "Loss at step 57250 : 2.4911930561065674\n",
      "Loss at step 57300 : 4.1103291511535645\n",
      "Loss at step 57350 : 4.658967971801758\n",
      "Loss at step 57400 : 1.7671456336975098\n",
      "Loss at step 57450 : 3.389744281768799\n",
      "Loss at step 57500 : 2.9992990493774414\n",
      "Loss at step 57550 : 3.2492804527282715\n",
      "Loss at step 57600 : 3.1592483520507812\n",
      "Loss at step 57650 : 3.934915781021118\n",
      "Loss at step 57700 : 3.7403950691223145\n",
      "Loss at step 57750 : 2.7886033058166504\n",
      "Loss at step 57800 : 3.57753324508667\n",
      "Loss at step 57850 : 3.461913824081421\n",
      "Loss at step 57900 : 3.4353997707366943\n",
      "Loss at step 57950 : 2.2095303535461426\n",
      "Loss at step 58000 : 4.575891017913818\n",
      "Loss at step 58050 : 3.7641005516052246\n",
      "Loss at step 58100 : 3.3903398513793945\n",
      "Loss at step 58150 : 4.677454948425293\n",
      "Loss at step 58200 : 2.5410189628601074\n",
      "Loss at step 58250 : 2.565927028656006\n",
      "Loss at step 58300 : 3.237715721130371\n",
      "Loss at step 58350 : 2.8663761615753174\n",
      "Loss at step 58400 : 2.621954917907715\n",
      "Loss at step 58450 : 2.7298007011413574\n",
      "Loss at step 58500 : 3.0693469047546387\n",
      "Loss at step 58550 : 2.586155652999878\n",
      "Loss at step 58600 : 3.2864060401916504\n",
      "Loss at step 58650 : 2.5757248401641846\n",
      "Loss at step 58700 : 3.0067672729492188\n",
      "Loss at step 58750 : 2.380553722381592\n",
      "Loss at step 58800 : 1.993595838546753\n",
      "Loss at step 58850 : 3.2603135108947754\n",
      "Loss at step 58900 : 3.071707010269165\n",
      "Loss at step 58950 : 3.210521697998047\n",
      "Loss at step 59000 : 3.3084633350372314\n",
      "Loss at step 59050 : 3.4136290550231934\n",
      "Loss at step 59100 : 1.932359218597412\n",
      "Loss at step 59150 : 2.814487934112549\n",
      "Loss at step 59200 : 2.799506664276123\n",
      "Loss at step 59250 : 3.171562433242798\n",
      "Loss at step 59300 : 3.0360517501831055\n",
      "Loss at step 59350 : 1.7364500761032104\n",
      "Loss at step 59400 : 3.6779565811157227\n",
      "Loss at step 59450 : 2.7774930000305176\n",
      "Loss at step 59500 : 3.6039369106292725\n",
      "Loss at step 59550 : 3.0956814289093018\n",
      "Loss at step 59600 : 2.5842089653015137\n",
      "Loss at step 59650 : 1.841099500656128\n",
      "Loss at step 59700 : 2.4634575843811035\n",
      "Loss at step 59750 : 2.584099531173706\n",
      "Loss at step 59800 : 2.89197039604187\n",
      "Loss at step 59850 : 2.567821979522705\n",
      "Loss at step 59900 : 2.5145344734191895\n",
      "Loss at step 59950 : 2.522343635559082\n",
      "Loss at step 60000 : 4.61501407623291\n",
      "Nearest to tuna: mushroom, spring, block, crab, piece,\n",
      "Nearest to rice: finger, salmon, cucumber, cling, strip,\n",
      "Nearest to sushi: towel, flesh, fish, strip, blade,\n",
      "Nearest to roll: flesh, sesame, roe, bit, sushi,\n",
      "Nearest to sashimi: leaf, piece, coriander, daikon, angle,\n",
      "Nearest to steak: onion, salt, grill, pepper, marinade,\n",
      "Nearest to grill: worcestershire, preheat, marinade, grate, oil,\n",
      "Nearest to sauce: salmon, chive, grill, filet, grain,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, wafer, yolk,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 60050 : 3.4672625064849854\n",
      "Loss at step 60100 : 3.570005416870117\n",
      "Loss at step 60150 : 1.2962231636047363\n",
      "Loss at step 60200 : 2.00838565826416\n",
      "Loss at step 60250 : 2.1182751655578613\n",
      "Loss at step 60300 : 3.505575656890869\n",
      "Loss at step 60350 : 2.8169052600860596\n",
      "Loss at step 60400 : 3.114192247390747\n",
      "Loss at step 60450 : 3.9671335220336914\n",
      "Loss at step 60500 : 3.2622456550598145\n",
      "Loss at step 60550 : 1.933398962020874\n",
      "Loss at step 60600 : 2.6758029460906982\n",
      "Loss at step 60650 : 4.912147521972656\n",
      "Loss at step 60700 : 3.2210030555725098\n",
      "Loss at step 60750 : 2.31230092048645\n",
      "Loss at step 60800 : 1.9481797218322754\n",
      "Loss at step 60850 : 3.439453601837158\n",
      "Loss at step 60900 : 3.4819962978363037\n",
      "Loss at step 60950 : 2.8500285148620605\n",
      "Loss at step 61000 : 3.323086977005005\n",
      "Loss at step 61050 : 1.9831616878509521\n",
      "Loss at step 61100 : 1.8757340908050537\n",
      "Loss at step 61150 : 3.3603415489196777\n",
      "Loss at step 61200 : 2.7355337142944336\n",
      "Loss at step 61250 : 3.1250646114349365\n",
      "Loss at step 61300 : 2.4009430408477783\n",
      "Loss at step 61350 : 4.53443717956543\n",
      "Loss at step 61400 : 3.7658114433288574\n",
      "Loss at step 61450 : 2.851077079772949\n",
      "Loss at step 61500 : 2.7654786109924316\n",
      "Loss at step 61550 : 2.483724355697632\n",
      "Loss at step 61600 : 2.3030381202697754\n",
      "Loss at step 61650 : 2.7320756912231445\n",
      "Loss at step 61700 : 3.1824700832366943\n",
      "Loss at step 61750 : 3.3248300552368164\n",
      "Loss at step 61800 : 2.9218943119049072\n",
      "Loss at step 61850 : 2.768301486968994\n",
      "Loss at step 61900 : 4.9296159744262695\n",
      "Loss at step 61950 : 3.8978376388549805\n",
      "Loss at step 62000 : 4.833148002624512\n",
      "Loss at step 62050 : 1.6482961177825928\n",
      "Loss at step 62100 : 2.378107786178589\n",
      "Loss at step 62150 : 3.1988894939422607\n",
      "Loss at step 62200 : 1.413646936416626\n",
      "Loss at step 62250 : 3.833357334136963\n",
      "Loss at step 62300 : 2.3622899055480957\n",
      "Loss at step 62350 : 2.2250847816467285\n",
      "Loss at step 62400 : 1.9692513942718506\n",
      "Loss at step 62450 : 1.8694889545440674\n",
      "Loss at step 62500 : 3.004081964492798\n",
      "Loss at step 62550 : 3.7476561069488525\n",
      "Loss at step 62600 : 2.066180467605591\n",
      "Loss at step 62650 : 3.987053394317627\n",
      "Loss at step 62700 : 1.8154010772705078\n",
      "Loss at step 62750 : 3.1548399925231934\n",
      "Loss at step 62800 : 2.5257465839385986\n",
      "Loss at step 62850 : 2.004117727279663\n",
      "Loss at step 62900 : 5.786688327789307\n",
      "Loss at step 62950 : 2.3999571800231934\n",
      "Loss at step 63000 : 4.084669589996338\n",
      "Loss at step 63050 : 4.430418968200684\n",
      "Loss at step 63100 : 3.0092267990112305\n",
      "Loss at step 63150 : 2.7949740886688232\n",
      "Loss at step 63200 : 3.1512198448181152\n",
      "Loss at step 63250 : 2.3618850708007812\n",
      "Loss at step 63300 : 3.6945066452026367\n",
      "Loss at step 63350 : 1.9436728954315186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 63400 : 4.558870792388916\n",
      "Loss at step 63450 : 2.710268974304199\n",
      "Loss at step 63500 : 2.1965832710266113\n",
      "Loss at step 63550 : 2.426025867462158\n",
      "Loss at step 63600 : 5.615057468414307\n",
      "Loss at step 63650 : 3.3772387504577637\n",
      "Loss at step 63700 : 2.8770928382873535\n",
      "Loss at step 63750 : 4.877509117126465\n",
      "Loss at step 63800 : 2.7750353813171387\n",
      "Loss at step 63850 : 2.7906055450439453\n",
      "Loss at step 63900 : 2.877695083618164\n",
      "Loss at step 63950 : 2.4403204917907715\n",
      "Loss at step 64000 : 3.4884424209594727\n",
      "Loss at step 64050 : 3.9508163928985596\n",
      "Loss at step 64100 : 3.767225980758667\n",
      "Loss at step 64150 : 3.4206182956695557\n",
      "Loss at step 64200 : 3.000096082687378\n",
      "Loss at step 64250 : 2.422187328338623\n",
      "Loss at step 64300 : 2.8212547302246094\n",
      "Loss at step 64350 : 3.2788143157958984\n",
      "Loss at step 64400 : 3.5826988220214844\n",
      "Loss at step 64450 : 3.7994251251220703\n",
      "Loss at step 64500 : 2.2215359210968018\n",
      "Loss at step 64550 : 3.3639817237854004\n",
      "Loss at step 64600 : 2.53136944770813\n",
      "Loss at step 64650 : 3.8949837684631348\n",
      "Loss at step 64700 : 3.691114664077759\n",
      "Loss at step 64750 : 3.962588310241699\n",
      "Loss at step 64800 : 3.41992449760437\n",
      "Loss at step 64850 : 2.6828153133392334\n",
      "Loss at step 64900 : 3.337263345718384\n",
      "Loss at step 64950 : 3.325160026550293\n",
      "Loss at step 65000 : 3.770688533782959\n",
      "Nearest to tuna: spring, mushroom, block, crab, piece,\n",
      "Nearest to rice: finger, salmon, cling, leg, cucumber,\n",
      "Nearest to sushi: towel, fish, flesh, strip, asparagus,\n",
      "Nearest to roll: flesh, sesame, roe, soy, bamboo,\n",
      "Nearest to sashimi: leaf, coriander, piece, daikon, angle,\n",
      "Nearest to steak: onion, salt, pepper, grill, marinade,\n",
      "Nearest to grill: worcestershire, grate, marinade, preheat, pepper,\n",
      "Nearest to sauce: salmon, chive, grill, filet, finger,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, yolk, chocolate,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 65050 : 2.697979688644409\n",
      "Loss at step 65100 : 3.4289145469665527\n",
      "Loss at step 65150 : 3.375361442565918\n",
      "Loss at step 65200 : 2.575371026992798\n",
      "Loss at step 65250 : 2.1914472579956055\n",
      "Loss at step 65300 : 1.9448871612548828\n",
      "Loss at step 65350 : 2.222825765609741\n",
      "Loss at step 65400 : 2.416337013244629\n",
      "Loss at step 65450 : 3.0585732460021973\n",
      "Loss at step 65500 : 3.2763314247131348\n",
      "Loss at step 65550 : 3.8611719608306885\n",
      "Loss at step 65600 : 4.226105213165283\n",
      "Loss at step 65650 : 2.995431900024414\n",
      "Loss at step 65700 : 2.3239545822143555\n",
      "Loss at step 65750 : 2.8646984100341797\n",
      "Loss at step 65800 : 2.2006211280822754\n",
      "Loss at step 65850 : 3.489335060119629\n",
      "Loss at step 65900 : 4.021801948547363\n",
      "Loss at step 65950 : 2.2957375049591064\n",
      "Loss at step 66000 : 2.866793155670166\n",
      "Loss at step 66050 : 3.261502981185913\n",
      "Loss at step 66100 : 1.6970815658569336\n",
      "Loss at step 66150 : 2.7567787170410156\n",
      "Loss at step 66200 : 2.4259369373321533\n",
      "Loss at step 66250 : 4.332629203796387\n",
      "Loss at step 66300 : 3.361006736755371\n",
      "Loss at step 66350 : 4.334842205047607\n",
      "Loss at step 66400 : 1.7007323503494263\n",
      "Loss at step 66450 : 0.7840354442596436\n",
      "Loss at step 66500 : 2.1230955123901367\n",
      "Loss at step 66550 : 2.648479700088501\n",
      "Loss at step 66600 : 2.046095609664917\n",
      "Loss at step 66650 : 3.0394821166992188\n",
      "Loss at step 66700 : 2.5277085304260254\n",
      "Loss at step 66750 : 3.0706491470336914\n",
      "Loss at step 66800 : 3.00462007522583\n",
      "Loss at step 66850 : 2.4458136558532715\n",
      "Loss at step 66900 : 3.6367695331573486\n",
      "Loss at step 66950 : 3.7458248138427734\n",
      "Loss at step 67000 : 2.7115118503570557\n",
      "Loss at step 67050 : 3.5772790908813477\n",
      "Loss at step 67100 : 2.8136563301086426\n",
      "Loss at step 67150 : 3.0992984771728516\n",
      "Loss at step 67200 : 1.863433837890625\n",
      "Loss at step 67250 : 3.02144718170166\n",
      "Loss at step 67300 : 3.566789150238037\n",
      "Loss at step 67350 : 2.2833545207977295\n",
      "Loss at step 67400 : 3.193120241165161\n",
      "Loss at step 67450 : 4.915170669555664\n",
      "Loss at step 67500 : 2.4009902477264404\n",
      "Loss at step 67550 : 3.4653093814849854\n",
      "Loss at step 67600 : 2.3483409881591797\n",
      "Loss at step 67650 : 3.344050884246826\n",
      "Loss at step 67700 : 3.08980655670166\n",
      "Loss at step 67750 : 3.113891363143921\n",
      "Loss at step 67800 : 3.7733314037323\n",
      "Loss at step 67850 : 2.4219696521759033\n",
      "Loss at step 67900 : 3.11320161819458\n",
      "Loss at step 67950 : 2.8838233947753906\n",
      "Loss at step 68000 : 3.9659340381622314\n",
      "Loss at step 68050 : 2.008697748184204\n",
      "Loss at step 68100 : 2.022998809814453\n",
      "Loss at step 68150 : 3.18664813041687\n",
      "Loss at step 68200 : 2.802166223526001\n",
      "Loss at step 68250 : 2.7868614196777344\n",
      "Loss at step 68300 : 1.4602878093719482\n",
      "Loss at step 68350 : 1.8588335514068604\n",
      "Loss at step 68400 : 2.116887092590332\n",
      "Loss at step 68450 : 2.940298557281494\n",
      "Loss at step 68500 : 1.702392578125\n",
      "Loss at step 68550 : 2.7893285751342773\n",
      "Loss at step 68600 : 2.2649765014648438\n",
      "Loss at step 68650 : 2.196575164794922\n",
      "Loss at step 68700 : 3.5289530754089355\n",
      "Loss at step 68750 : 3.895430088043213\n",
      "Loss at step 68800 : 2.662236213684082\n",
      "Loss at step 68850 : 2.2536158561706543\n",
      "Loss at step 68900 : 2.53964900970459\n",
      "Loss at step 68950 : 2.441660165786743\n",
      "Loss at step 69000 : 2.5383682250976562\n",
      "Loss at step 69050 : 3.4661121368408203\n",
      "Loss at step 69100 : 4.398340225219727\n",
      "Loss at step 69150 : 2.642407178878784\n",
      "Loss at step 69200 : 2.2881414890289307\n",
      "Loss at step 69250 : 2.8480722904205322\n",
      "Loss at step 69300 : 3.109097957611084\n",
      "Loss at step 69350 : 2.5164055824279785\n",
      "Loss at step 69400 : 2.9587316513061523\n",
      "Loss at step 69450 : 2.500967025756836\n",
      "Loss at step 69500 : 2.413658857345581\n",
      "Loss at step 69550 : 2.0113308429718018\n",
      "Loss at step 69600 : 2.696653127670288\n",
      "Loss at step 69650 : 4.502431869506836\n",
      "Loss at step 69700 : 2.548914909362793\n",
      "Loss at step 69750 : 2.139406204223633\n",
      "Loss at step 69800 : 3.5668959617614746\n",
      "Loss at step 69850 : 2.2127604484558105\n",
      "Loss at step 69900 : 2.7953035831451416\n",
      "Loss at step 69950 : 5.30973482131958\n",
      "Loss at step 70000 : 1.3679604530334473\n",
      "Nearest to tuna: mushroom, spring, block, piece, crab,\n",
      "Nearest to rice: salmon, finger, leg, cling, vinegar,\n",
      "Nearest to sushi: towel, fish, flesh, strip, roe,\n",
      "Nearest to roll: flesh, sesame, bit, sushi, soy,\n",
      "Nearest to sashimi: leaf, coriander, piece, strip, daikon,\n",
      "Nearest to steak: onion, pepper, grill, marinade, salt,\n",
      "Nearest to grill: worcestershire, grate, marinade, preheat, butter,\n",
      "Nearest to sauce: chive, filet, salmon, finger, vinegar,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, yolk, chocolate,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 70050 : 4.03929328918457\n",
      "Loss at step 70100 : 2.4349205493927\n",
      "Loss at step 70150 : 2.727964401245117\n",
      "Loss at step 70200 : 2.700216293334961\n",
      "Loss at step 70250 : 3.587404727935791\n",
      "Loss at step 70300 : 2.2616167068481445\n",
      "Loss at step 70350 : 2.5232560634613037\n",
      "Loss at step 70400 : 2.4872019290924072\n",
      "Loss at step 70450 : 1.3993477821350098\n",
      "Loss at step 70500 : 4.369912147521973\n",
      "Loss at step 70550 : 2.0935070514678955\n",
      "Loss at step 70600 : 4.351540565490723\n",
      "Loss at step 70650 : 1.9273977279663086\n",
      "Loss at step 70700 : 3.3159584999084473\n",
      "Loss at step 70750 : 3.2651455402374268\n",
      "Loss at step 70800 : 5.0340375900268555\n",
      "Loss at step 70850 : 2.560577392578125\n",
      "Loss at step 70900 : 4.467696189880371\n",
      "Loss at step 70950 : 2.6391568183898926\n",
      "Loss at step 71000 : 3.133626699447632\n",
      "Loss at step 71050 : 4.332921981811523\n",
      "Loss at step 71100 : 2.8858349323272705\n",
      "Loss at step 71150 : 2.9890847206115723\n",
      "Loss at step 71200 : 3.8012709617614746\n",
      "Loss at step 71250 : 3.789452075958252\n",
      "Loss at step 71300 : 3.6792023181915283\n",
      "Loss at step 71350 : 2.041567087173462\n",
      "Loss at step 71400 : 3.164260149002075\n",
      "Loss at step 71450 : 2.4352004528045654\n",
      "Loss at step 71500 : 3.1877429485321045\n",
      "Loss at step 71550 : 3.0629239082336426\n",
      "Loss at step 71600 : 3.2534618377685547\n",
      "Loss at step 71650 : 2.4743223190307617\n",
      "Loss at step 71700 : 2.410285234451294\n",
      "Loss at step 71750 : 3.4788124561309814\n",
      "Loss at step 71800 : 2.8464155197143555\n",
      "Loss at step 71850 : 4.163164138793945\n",
      "Loss at step 71900 : 3.1032509803771973\n",
      "Loss at step 71950 : 2.651045799255371\n",
      "Loss at step 72000 : 3.651390790939331\n",
      "Loss at step 72050 : 3.1848223209381104\n",
      "Loss at step 72100 : 2.250664472579956\n",
      "Loss at step 72150 : 2.386943817138672\n",
      "Loss at step 72200 : 3.9267139434814453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 72250 : 2.9077465534210205\n",
      "Loss at step 72300 : 2.947680711746216\n",
      "Loss at step 72350 : 2.6043169498443604\n",
      "Loss at step 72400 : 3.8368682861328125\n",
      "Loss at step 72450 : 1.7162790298461914\n",
      "Loss at step 72500 : 2.349435806274414\n",
      "Loss at step 72550 : 3.710644006729126\n",
      "Loss at step 72600 : 3.0131728649139404\n",
      "Loss at step 72650 : 2.2287354469299316\n",
      "Loss at step 72700 : 2.34416127204895\n",
      "Loss at step 72750 : 3.5570034980773926\n",
      "Loss at step 72800 : 3.4924187660217285\n",
      "Loss at step 72850 : 2.4188828468322754\n",
      "Loss at step 72900 : 2.94832444190979\n",
      "Loss at step 72950 : 2.121757745742798\n",
      "Loss at step 73000 : 2.626128673553467\n",
      "Loss at step 73050 : 2.481660842895508\n",
      "Loss at step 73100 : 3.5667850971221924\n",
      "Loss at step 73150 : 2.682290554046631\n",
      "Loss at step 73200 : 5.021601676940918\n",
      "Loss at step 73250 : 3.5182547569274902\n",
      "Loss at step 73300 : 2.6504740715026855\n",
      "Loss at step 73350 : 3.0604021549224854\n",
      "Loss at step 73400 : 3.68375301361084\n",
      "Loss at step 73450 : 2.2158634662628174\n",
      "Loss at step 73500 : 5.099788188934326\n",
      "Loss at step 73550 : 2.3146679401397705\n",
      "Loss at step 73600 : 4.486095428466797\n",
      "Loss at step 73650 : 4.741045951843262\n",
      "Loss at step 73700 : 2.7574667930603027\n",
      "Loss at step 73750 : 2.847630023956299\n",
      "Loss at step 73800 : 2.947906732559204\n",
      "Loss at step 73850 : 2.263031244277954\n",
      "Loss at step 73900 : 3.3522279262542725\n",
      "Loss at step 73950 : 2.6196556091308594\n",
      "Loss at step 74000 : 1.9334638118743896\n",
      "Loss at step 74050 : 2.4419758319854736\n",
      "Loss at step 74100 : 2.528299570083618\n",
      "Loss at step 74150 : 4.267609596252441\n",
      "Loss at step 74200 : 1.6856679916381836\n",
      "Loss at step 74250 : 2.3265833854675293\n",
      "Loss at step 74300 : 2.4500675201416016\n",
      "Loss at step 74350 : 3.0074734687805176\n",
      "Loss at step 74400 : 3.3240702152252197\n",
      "Loss at step 74450 : 2.8971500396728516\n",
      "Loss at step 74500 : 3.4667739868164062\n",
      "Loss at step 74550 : 3.5772387981414795\n",
      "Loss at step 74600 : 3.1243679523468018\n",
      "Loss at step 74650 : 3.8386785984039307\n",
      "Loss at step 74700 : 3.208336114883423\n",
      "Loss at step 74750 : 2.1707563400268555\n",
      "Loss at step 74800 : 2.351195812225342\n",
      "Loss at step 74850 : 2.1573996543884277\n",
      "Loss at step 74900 : 3.7537174224853516\n",
      "Loss at step 74950 : 1.6609573364257812\n",
      "Loss at step 75000 : 2.824226140975952\n",
      "Nearest to tuna: mushroom, spring, block, onion, piece,\n",
      "Nearest to rice: leg, finger, cling, salmon, vinegar,\n",
      "Nearest to sushi: towel, fish, roe, asparagus, strip,\n",
      "Nearest to roll: flesh, sesame, roe, bit, sushi,\n",
      "Nearest to sashimi: leaf, coriander, piece, strip, angle,\n",
      "Nearest to steak: pepper, grill, marinade, onion, salt,\n",
      "Nearest to grill: grate, preheat, marinade, worcestershire, butter,\n",
      "Nearest to sauce: filet, chive, finger, fryer, salmon,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, chocolate, cocoa,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 75050 : 2.606693983078003\n",
      "Loss at step 75100 : 2.4508869647979736\n",
      "Loss at step 75150 : 2.6643407344818115\n",
      "Loss at step 75200 : 3.0234644412994385\n",
      "Loss at step 75250 : 2.927490472793579\n",
      "Loss at step 75300 : 2.454646110534668\n",
      "Loss at step 75350 : 2.690837860107422\n",
      "Loss at step 75400 : 2.419790267944336\n",
      "Loss at step 75450 : 2.13091778755188\n",
      "Loss at step 75500 : 2.9190938472747803\n",
      "Loss at step 75550 : 2.800081729888916\n",
      "Loss at step 75600 : 1.7754848003387451\n",
      "Loss at step 75650 : 3.6929614543914795\n",
      "Loss at step 75700 : 2.761502742767334\n",
      "Loss at step 75750 : 3.635080099105835\n",
      "Loss at step 75800 : 2.820561408996582\n",
      "Loss at step 75850 : 2.2559547424316406\n",
      "Loss at step 75900 : 1.970360517501831\n",
      "Loss at step 75950 : 2.5875298976898193\n",
      "Loss at step 76000 : 2.6643495559692383\n",
      "Loss at step 76050 : 4.144304275512695\n",
      "Loss at step 76100 : 4.740618705749512\n",
      "Loss at step 76150 : 3.533068895339966\n",
      "Loss at step 76200 : 2.16690731048584\n",
      "Loss at step 76250 : 2.368903875350952\n",
      "Loss at step 76300 : 3.3410050868988037\n",
      "Loss at step 76350 : 3.2704033851623535\n",
      "Loss at step 76400 : 3.4186723232269287\n",
      "Loss at step 76450 : 3.752610445022583\n",
      "Loss at step 76500 : 3.3501901626586914\n",
      "Loss at step 76550 : 4.605685234069824\n",
      "Loss at step 76600 : 3.6923701763153076\n",
      "Loss at step 76650 : 2.7285449504852295\n",
      "Loss at step 76700 : 3.197087526321411\n",
      "Loss at step 76750 : 3.869069814682007\n",
      "Loss at step 76800 : 1.308246374130249\n",
      "Loss at step 76850 : 2.8980629444122314\n",
      "Loss at step 76900 : 3.0112693309783936\n",
      "Loss at step 76950 : 1.847427248954773\n",
      "Loss at step 77000 : 2.5610461235046387\n",
      "Loss at step 77050 : 3.398735523223877\n",
      "Loss at step 77100 : 2.9565670490264893\n",
      "Loss at step 77150 : 2.8056442737579346\n",
      "Loss at step 77200 : 2.795452833175659\n",
      "Loss at step 77250 : 3.0435614585876465\n",
      "Loss at step 77300 : 4.177464008331299\n",
      "Loss at step 77350 : 2.2550463676452637\n",
      "Loss at step 77400 : 4.823936462402344\n",
      "Loss at step 77450 : 3.1505367755889893\n",
      "Loss at step 77500 : 2.856144428253174\n",
      "Loss at step 77550 : 3.327498197555542\n",
      "Loss at step 77600 : 2.558647632598877\n",
      "Loss at step 77650 : 4.359889030456543\n",
      "Loss at step 77700 : 2.9567017555236816\n",
      "Loss at step 77750 : 3.5727925300598145\n",
      "Loss at step 77800 : 3.5082101821899414\n",
      "Loss at step 77850 : 2.5066566467285156\n",
      "Loss at step 77900 : 3.6363654136657715\n",
      "Loss at step 77950 : 2.248763084411621\n",
      "Loss at step 78000 : 2.5908689498901367\n",
      "Loss at step 78050 : 3.0551795959472656\n",
      "Loss at step 78100 : 2.7777421474456787\n",
      "Loss at step 78150 : 2.16910719871521\n",
      "Loss at step 78200 : 2.71030330657959\n",
      "Loss at step 78250 : 4.261593818664551\n",
      "Loss at step 78300 : 2.090977668762207\n",
      "Loss at step 78350 : 2.7505545616149902\n",
      "Loss at step 78400 : 1.9208126068115234\n",
      "Loss at step 78450 : 2.5645554065704346\n",
      "Loss at step 78500 : 5.395414352416992\n",
      "Loss at step 78550 : 2.5451231002807617\n",
      "Loss at step 78600 : 2.2106916904449463\n",
      "Loss at step 78650 : 2.7332592010498047\n",
      "Loss at step 78700 : 2.5684802532196045\n",
      "Loss at step 78750 : 2.8958797454833984\n",
      "Loss at step 78800 : 3.7184832096099854\n",
      "Loss at step 78850 : 1.5937145948410034\n",
      "Loss at step 78900 : 2.8244757652282715\n",
      "Loss at step 78950 : 3.90956449508667\n",
      "Loss at step 79000 : 2.7332024574279785\n",
      "Loss at step 79050 : 4.029283046722412\n",
      "Loss at step 79100 : 2.3532419204711914\n",
      "Loss at step 79150 : 3.334315776824951\n",
      "Loss at step 79200 : 3.4180989265441895\n",
      "Loss at step 79250 : 2.810234785079956\n",
      "Loss at step 79300 : 2.6806230545043945\n",
      "Loss at step 79350 : 3.8080248832702637\n",
      "Loss at step 79400 : 3.244253396987915\n",
      "Loss at step 79450 : 3.274318218231201\n",
      "Loss at step 79500 : 1.3453617095947266\n",
      "Loss at step 79550 : 2.6859381198883057\n",
      "Loss at step 79600 : 2.520786762237549\n",
      "Loss at step 79650 : 3.4624714851379395\n",
      "Loss at step 79700 : 0.8932313919067383\n",
      "Loss at step 79750 : 2.124220132827759\n",
      "Loss at step 79800 : 2.8775928020477295\n",
      "Loss at step 79850 : 4.048277854919434\n",
      "Loss at step 79900 : 2.9792962074279785\n",
      "Loss at step 79950 : 2.5886342525482178\n",
      "Loss at step 80000 : 3.691771984100342\n",
      "Nearest to tuna: mushroom, spring, block, onion, thumb,\n",
      "Nearest to rice: salmon, finger, saucepan, leg, slice,\n",
      "Nearest to sushi: towel, fish, roe, strip, asparagus,\n",
      "Nearest to roll: flesh, sesame, sushi, roe, piece,\n",
      "Nearest to sashimi: leaf, coriander, piece, strip, daikon,\n",
      "Nearest to steak: marinade, grill, pepper, grate, salt,\n",
      "Nearest to grill: grate, marinade, worcestershire, preheat, butter,\n",
      "Nearest to sauce: filet, grill, chive, salmon, finger,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, cocoa, yolk,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 80050 : 2.0889320373535156\n",
      "Loss at step 80100 : 3.365427017211914\n",
      "Loss at step 80150 : 3.0143282413482666\n",
      "Loss at step 80200 : 2.6323623657226562\n",
      "Loss at step 80250 : 3.0119452476501465\n",
      "Loss at step 80300 : 2.0403006076812744\n",
      "Loss at step 80350 : 3.5047881603240967\n",
      "Loss at step 80400 : 2.748967170715332\n",
      "Loss at step 80450 : 2.0608160495758057\n",
      "Loss at step 80500 : 2.9958765506744385\n",
      "Loss at step 80550 : 3.432495594024658\n",
      "Loss at step 80600 : 1.424288034439087\n",
      "Loss at step 80650 : 2.60282039642334\n",
      "Loss at step 80700 : 5.268010139465332\n",
      "Loss at step 80750 : 3.3416829109191895\n",
      "Loss at step 80800 : 2.999850034713745\n",
      "Loss at step 80850 : 3.2496633529663086\n",
      "Loss at step 80900 : 3.5418906211853027\n",
      "Loss at step 80950 : 1.6830586194992065\n",
      "Loss at step 81000 : 3.4208314418792725\n",
      "Loss at step 81050 : 2.4072322845458984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 81100 : 2.795480251312256\n",
      "Loss at step 81150 : 2.957104206085205\n",
      "Loss at step 81200 : 2.9193665981292725\n",
      "Loss at step 81250 : 3.4734623432159424\n",
      "Loss at step 81300 : 2.1194686889648438\n",
      "Loss at step 81350 : 2.806828498840332\n",
      "Loss at step 81400 : 2.2994589805603027\n",
      "Loss at step 81450 : 2.6702280044555664\n",
      "Loss at step 81500 : 4.221332550048828\n",
      "Loss at step 81550 : 2.6607918739318848\n",
      "Loss at step 81600 : 3.2559471130371094\n",
      "Loss at step 81650 : 5.035838603973389\n",
      "Loss at step 81700 : 4.774707794189453\n",
      "Loss at step 81750 : 2.2532453536987305\n",
      "Loss at step 81800 : 1.7159794569015503\n",
      "Loss at step 81850 : 4.329127311706543\n",
      "Loss at step 81900 : 2.3767411708831787\n",
      "Loss at step 81950 : 2.6047966480255127\n",
      "Loss at step 82000 : 3.6123411655426025\n",
      "Loss at step 82050 : 3.1980667114257812\n",
      "Loss at step 82100 : 4.820920944213867\n",
      "Loss at step 82150 : 2.9532103538513184\n",
      "Loss at step 82200 : 2.420614004135132\n",
      "Loss at step 82250 : 2.4464306831359863\n",
      "Loss at step 82300 : 2.5578763484954834\n",
      "Loss at step 82350 : 2.9165492057800293\n",
      "Loss at step 82400 : 3.208404779434204\n",
      "Loss at step 82450 : 2.64217209815979\n",
      "Loss at step 82500 : 2.393378257751465\n",
      "Loss at step 82550 : 2.1943302154541016\n",
      "Loss at step 82600 : 2.4009506702423096\n",
      "Loss at step 82650 : 2.8439602851867676\n",
      "Loss at step 82700 : 3.293118476867676\n",
      "Loss at step 82750 : 2.218299150466919\n",
      "Loss at step 82800 : 4.048601150512695\n",
      "Loss at step 82850 : 2.493224859237671\n",
      "Loss at step 82900 : 3.4690346717834473\n",
      "Loss at step 82950 : 2.8895668983459473\n",
      "Loss at step 83000 : 2.115870475769043\n",
      "Loss at step 83050 : 2.7869763374328613\n",
      "Loss at step 83100 : 2.632721185684204\n",
      "Loss at step 83150 : 2.912036180496216\n",
      "Loss at step 83200 : 2.7570481300354004\n",
      "Loss at step 83250 : 2.414590835571289\n",
      "Loss at step 83300 : 2.686069965362549\n",
      "Loss at step 83350 : 0.7858327627182007\n",
      "Loss at step 83400 : 2.870189666748047\n",
      "Loss at step 83450 : 2.596776247024536\n",
      "Loss at step 83500 : 3.146196126937866\n",
      "Loss at step 83550 : 3.336489677429199\n",
      "Loss at step 83600 : 3.5428130626678467\n",
      "Loss at step 83650 : 2.2308712005615234\n",
      "Loss at step 83700 : 3.0509324073791504\n",
      "Loss at step 83750 : 2.977607011795044\n",
      "Loss at step 83800 : 1.9733607769012451\n",
      "Loss at step 83850 : 2.2571496963500977\n",
      "Loss at step 83900 : 2.200756549835205\n",
      "Loss at step 83950 : 2.8344080448150635\n",
      "Loss at step 84000 : 2.923203468322754\n",
      "Loss at step 84050 : 3.084345817565918\n",
      "Loss at step 84100 : 2.7971649169921875\n",
      "Loss at step 84150 : 3.246828556060791\n",
      "Loss at step 84200 : 3.0929179191589355\n",
      "Loss at step 84250 : 1.5678510665893555\n",
      "Loss at step 84300 : 2.5486459732055664\n",
      "Loss at step 84350 : 2.2677154541015625\n",
      "Loss at step 84400 : 1.7986371517181396\n",
      "Loss at step 84450 : 2.9307620525360107\n",
      "Loss at step 84500 : 3.6346940994262695\n",
      "Loss at step 84550 : 3.1504430770874023\n",
      "Loss at step 84600 : 2.512387752532959\n",
      "Loss at step 84650 : 3.1691231727600098\n",
      "Loss at step 84700 : 1.080675482749939\n",
      "Loss at step 84750 : 2.2538092136383057\n",
      "Loss at step 84800 : 4.141166687011719\n",
      "Loss at step 84850 : 2.0913236141204834\n",
      "Loss at step 84900 : 2.2164881229400635\n",
      "Loss at step 84950 : 2.2037031650543213\n",
      "Loss at step 85000 : 3.6441006660461426\n",
      "Nearest to tuna: mushroom, spring, block, crab, grain,\n",
      "Nearest to rice: spicy, finger, cucumber, salmon, saucepan,\n",
      "Nearest to sushi: towel, fish, roe, strip, flesh,\n",
      "Nearest to roll: flesh, sesame, bamboo, strip, dipping,\n",
      "Nearest to sashimi: leaf, coriander, piece, strip, torch,\n",
      "Nearest to steak: marinade, grill, grate, pepper, salt,\n",
      "Nearest to grill: grate, worcestershire, marinade, preheat, butter,\n",
      "Nearest to sauce: filet, grain, grill, chive, salmon,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, cocoa, yolk,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 85050 : 1.9248104095458984\n",
      "Loss at step 85100 : 2.966136932373047\n",
      "Loss at step 85150 : 3.775595188140869\n",
      "Loss at step 85200 : 3.2193868160247803\n",
      "Loss at step 85250 : 3.4357404708862305\n",
      "Loss at step 85300 : 2.6256766319274902\n",
      "Loss at step 85350 : 1.975372076034546\n",
      "Loss at step 85400 : 2.0558700561523438\n",
      "Loss at step 85450 : 2.607851505279541\n",
      "Loss at step 85500 : 2.597120761871338\n",
      "Loss at step 85550 : 3.6385726928710938\n",
      "Loss at step 85600 : 2.771033763885498\n",
      "Loss at step 85650 : 3.3465070724487305\n",
      "Loss at step 85700 : 2.387827157974243\n",
      "Loss at step 85750 : 3.8455452919006348\n",
      "Loss at step 85800 : 5.110940456390381\n",
      "Loss at step 85850 : 2.937777519226074\n",
      "Loss at step 85900 : 3.7189061641693115\n",
      "Loss at step 85950 : 2.7830679416656494\n",
      "Loss at step 86000 : 3.7943429946899414\n",
      "Loss at step 86050 : 2.7459239959716797\n",
      "Loss at step 86100 : 3.253990650177002\n",
      "Loss at step 86150 : 2.1531310081481934\n",
      "Loss at step 86200 : 2.2484612464904785\n",
      "Loss at step 86250 : 3.4112155437469482\n",
      "Loss at step 86300 : 2.997072696685791\n",
      "Loss at step 86350 : 2.3020710945129395\n",
      "Loss at step 86400 : 2.9326987266540527\n",
      "Loss at step 86450 : 3.024710178375244\n",
      "Loss at step 86500 : 3.210149049758911\n",
      "Loss at step 86550 : 1.4240907430648804\n",
      "Loss at step 86600 : 2.4626975059509277\n",
      "Loss at step 86650 : 2.8417768478393555\n",
      "Loss at step 86700 : 2.935055732727051\n",
      "Loss at step 86750 : 2.1083788871765137\n",
      "Loss at step 86800 : 3.0243704319000244\n",
      "Loss at step 86850 : 3.1688084602355957\n",
      "Loss at step 86900 : 4.387429237365723\n",
      "Loss at step 86950 : 2.2105791568756104\n",
      "Loss at step 87000 : 2.1773664951324463\n",
      "Loss at step 87050 : 2.109551191329956\n",
      "Loss at step 87100 : 4.036496639251709\n",
      "Loss at step 87150 : 2.7191898822784424\n",
      "Loss at step 87200 : 2.3209009170532227\n",
      "Loss at step 87250 : 2.1479837894439697\n",
      "Loss at step 87300 : 2.373884439468384\n",
      "Loss at step 87350 : 2.5458364486694336\n",
      "Loss at step 87400 : 2.864070177078247\n",
      "Loss at step 87450 : 2.22731614112854\n",
      "Loss at step 87500 : 3.5021443367004395\n",
      "Loss at step 87550 : 2.512399435043335\n",
      "Loss at step 87600 : 3.528322219848633\n",
      "Loss at step 87650 : 2.0220277309417725\n",
      "Loss at step 87700 : 3.312704086303711\n",
      "Loss at step 87750 : 2.8924777507781982\n",
      "Loss at step 87800 : 3.2392914295196533\n",
      "Loss at step 87850 : 2.9971280097961426\n",
      "Loss at step 87900 : 2.292919874191284\n",
      "Loss at step 87950 : 2.0892672538757324\n",
      "Loss at step 88000 : 2.838294744491577\n",
      "Loss at step 88050 : 1.4856513738632202\n",
      "Loss at step 88100 : 3.712805986404419\n",
      "Loss at step 88150 : 2.5356152057647705\n",
      "Loss at step 88200 : 2.5729925632476807\n",
      "Loss at step 88250 : 2.7660715579986572\n",
      "Loss at step 88300 : 2.731874465942383\n",
      "Loss at step 88350 : 2.0316967964172363\n",
      "Loss at step 88400 : 3.480722188949585\n",
      "Loss at step 88450 : 3.9259986877441406\n",
      "Loss at step 88500 : 2.277156114578247\n",
      "Loss at step 88550 : 2.531851053237915\n",
      "Loss at step 88600 : 1.5840303897857666\n",
      "Loss at step 88650 : 2.600010871887207\n",
      "Loss at step 88700 : 2.5004119873046875\n",
      "Loss at step 88750 : 2.846188545227051\n",
      "Loss at step 88800 : 2.5776143074035645\n",
      "Loss at step 88850 : 2.7765533924102783\n",
      "Loss at step 88900 : 2.81535005569458\n",
      "Loss at step 88950 : 1.9204845428466797\n",
      "Loss at step 89000 : 4.91223669052124\n",
      "Loss at step 89050 : 2.884800672531128\n",
      "Loss at step 89100 : 2.5799667835235596\n",
      "Loss at step 89150 : 3.3582613468170166\n",
      "Loss at step 89200 : 2.2491750717163086\n",
      "Loss at step 89250 : 3.101823091506958\n",
      "Loss at step 89300 : 2.089616298675537\n",
      "Loss at step 89350 : 2.7394566535949707\n",
      "Loss at step 89400 : 3.438734769821167\n",
      "Loss at step 89450 : 2.967694044113159\n",
      "Loss at step 89500 : 2.9702322483062744\n",
      "Loss at step 89550 : 3.9179630279541016\n",
      "Loss at step 89600 : 2.2650973796844482\n",
      "Loss at step 89650 : 4.317038536071777\n",
      "Loss at step 89700 : 1.9611515998840332\n",
      "Loss at step 89750 : 3.169652223587036\n",
      "Loss at step 89800 : 2.339891195297241\n",
      "Loss at step 89850 : 2.6825757026672363\n",
      "Loss at step 89900 : 2.556856155395508\n",
      "Loss at step 89950 : 2.2044694423675537\n",
      "Loss at step 90000 : 3.0249180793762207\n",
      "Nearest to tuna: mushroom, spring, block, crab, sesame,\n",
      "Nearest to rice: spicy, salmon, finger, cucumber, leg,\n",
      "Nearest to sushi: towel, roe, fish, flesh, zip,\n",
      "Nearest to roll: flesh, sesame, strip, bamboo, sushi,\n",
      "Nearest to sashimi: piece, leaf, coriander, strip, torch,\n",
      "Nearest to steak: grill, pepper, marinade, onion, grate,\n",
      "Nearest to grill: grate, worcestershire, marinade, preheat, butter,\n",
      "Nearest to sauce: filet, soy, chive, grain, finger,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, yolk, cocoa,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 90050 : 3.141036033630371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 90100 : 3.094653606414795\n",
      "Loss at step 90150 : 2.389223575592041\n",
      "Loss at step 90200 : 2.0408384799957275\n",
      "Loss at step 90250 : 3.503093719482422\n",
      "Loss at step 90300 : 2.556767463684082\n",
      "Loss at step 90350 : 2.6688218116760254\n",
      "Loss at step 90400 : 4.00308895111084\n",
      "Loss at step 90450 : 1.9277732372283936\n",
      "Loss at step 90500 : 2.606478691101074\n",
      "Loss at step 90550 : 2.5991709232330322\n",
      "Loss at step 90600 : 2.5254175662994385\n",
      "Loss at step 90650 : 2.7527029514312744\n",
      "Loss at step 90700 : 2.5497875213623047\n",
      "Loss at step 90750 : 2.834559202194214\n",
      "Loss at step 90800 : 3.051604747772217\n",
      "Loss at step 90850 : 3.081176280975342\n",
      "Loss at step 90900 : 3.527125358581543\n",
      "Loss at step 90950 : 1.9408667087554932\n",
      "Loss at step 91000 : 3.209052801132202\n",
      "Loss at step 91050 : 2.760664224624634\n",
      "Loss at step 91100 : 5.228216648101807\n",
      "Loss at step 91150 : 2.953228712081909\n",
      "Loss at step 91200 : 2.3559389114379883\n",
      "Loss at step 91250 : 3.352815628051758\n",
      "Loss at step 91300 : 1.5593271255493164\n",
      "Loss at step 91350 : 4.445687294006348\n",
      "Loss at step 91400 : 3.041869878768921\n",
      "Loss at step 91450 : 2.114074945449829\n",
      "Loss at step 91500 : 2.4685394763946533\n",
      "Loss at step 91550 : 3.14792799949646\n",
      "Loss at step 91600 : 1.7549407482147217\n",
      "Loss at step 91650 : 4.3317084312438965\n",
      "Loss at step 91700 : 3.3855206966400146\n",
      "Loss at step 91750 : 3.042179584503174\n",
      "Loss at step 91800 : 3.8267250061035156\n",
      "Loss at step 91850 : 1.4145175218582153\n",
      "Loss at step 91900 : 2.487725019454956\n",
      "Loss at step 91950 : 4.231793403625488\n",
      "Loss at step 92000 : 1.5393164157867432\n",
      "Loss at step 92050 : 1.1145329475402832\n",
      "Loss at step 92100 : 2.4473469257354736\n",
      "Loss at step 92150 : 2.259711742401123\n",
      "Loss at step 92200 : 2.865713596343994\n",
      "Loss at step 92250 : 2.5366287231445312\n",
      "Loss at step 92300 : 2.856860876083374\n",
      "Loss at step 92350 : 3.035440683364868\n",
      "Loss at step 92400 : 2.7592310905456543\n",
      "Loss at step 92450 : 3.235417127609253\n",
      "Loss at step 92500 : 2.4882750511169434\n",
      "Loss at step 92550 : 3.0247581005096436\n",
      "Loss at step 92600 : 3.1016921997070312\n",
      "Loss at step 92650 : 2.72999906539917\n",
      "Loss at step 92700 : 2.454653739929199\n",
      "Loss at step 92750 : 3.0427441596984863\n",
      "Loss at step 92800 : 1.935875415802002\n",
      "Loss at step 92850 : 2.03102707862854\n",
      "Loss at step 92900 : 3.290121078491211\n",
      "Loss at step 92950 : 3.7648537158966064\n",
      "Loss at step 93000 : 2.1284351348876953\n",
      "Loss at step 93050 : 3.5660340785980225\n",
      "Loss at step 93100 : 2.8297300338745117\n",
      "Loss at step 93150 : 4.028932571411133\n",
      "Loss at step 93200 : 5.347294807434082\n",
      "Loss at step 93250 : 2.1566162109375\n",
      "Loss at step 93300 : 2.8995532989501953\n",
      "Loss at step 93350 : 3.159092426300049\n",
      "Loss at step 93400 : 2.151904821395874\n",
      "Loss at step 93450 : 2.3820629119873047\n",
      "Loss at step 93500 : 2.4518203735351562\n",
      "Loss at step 93550 : 1.5752229690551758\n",
      "Loss at step 93600 : 4.882324695587158\n",
      "Loss at step 93650 : 3.2435739040374756\n",
      "Loss at step 93700 : 2.870086193084717\n",
      "Loss at step 93750 : 2.779942750930786\n",
      "Loss at step 93800 : 2.743990659713745\n",
      "Loss at step 93850 : 3.468306303024292\n",
      "Loss at step 93900 : 3.141448497772217\n",
      "Loss at step 93950 : 2.688292980194092\n",
      "Loss at step 94000 : 2.6511569023132324\n",
      "Loss at step 94050 : 3.1949355602264404\n",
      "Loss at step 94100 : 1.9661939144134521\n",
      "Loss at step 94150 : 3.616746425628662\n",
      "Loss at step 94200 : 3.4934511184692383\n",
      "Loss at step 94250 : 5.17448616027832\n",
      "Loss at step 94300 : 1.805779218673706\n",
      "Loss at step 94350 : 4.04270601272583\n",
      "Loss at step 94400 : 2.657865047454834\n",
      "Loss at step 94450 : 4.175357341766357\n",
      "Loss at step 94500 : 2.4710309505462646\n",
      "Loss at step 94550 : 3.365333318710327\n",
      "Loss at step 94600 : 3.64250111579895\n",
      "Loss at step 94650 : 3.642326831817627\n",
      "Loss at step 94700 : 2.0022592544555664\n",
      "Loss at step 94750 : 4.269906044006348\n",
      "Loss at step 94800 : 2.3768444061279297\n",
      "Loss at step 94850 : 3.603069543838501\n",
      "Loss at step 94900 : 3.356172561645508\n",
      "Loss at step 94950 : 2.595280885696411\n",
      "Loss at step 95000 : 3.3278555870056152\n",
      "Nearest to tuna: mushroom, crab, spring, block, onion,\n",
      "Nearest to rice: cucumber, finger, spicy, bit, bamboo,\n",
      "Nearest to sushi: towel, fish, roe, zip, seed,\n",
      "Nearest to roll: flesh, sesame, sushi, piece, strip,\n",
      "Nearest to sashimi: leaf, piece, coriander, strip, torch,\n",
      "Nearest to steak: pepper, grate, grill, marinade, onion,\n",
      "Nearest to grill: grate, worcestershire, marinade, preheat, butter,\n",
      "Nearest to sauce: soy, grain, filet, oil, finger,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, cocoa, yolk,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n",
      "Loss at step 95050 : 2.9143781661987305\n",
      "Loss at step 95100 : 3.13431978225708\n",
      "Loss at step 95150 : 2.8000569343566895\n",
      "Loss at step 95200 : 3.104137420654297\n",
      "Loss at step 95250 : 3.5473580360412598\n",
      "Loss at step 95300 : 3.160954713821411\n",
      "Loss at step 95350 : 3.8770055770874023\n",
      "Loss at step 95400 : 3.814589262008667\n",
      "Loss at step 95450 : 2.594299793243408\n",
      "Loss at step 95500 : 3.9309983253479004\n",
      "Loss at step 95550 : 3.9222705364227295\n",
      "Loss at step 95600 : 2.7210960388183594\n",
      "Loss at step 95650 : 3.232569694519043\n",
      "Loss at step 95700 : 2.756747245788574\n",
      "Loss at step 95750 : 3.0933146476745605\n",
      "Loss at step 95800 : 3.8773789405822754\n",
      "Loss at step 95850 : 3.3222815990448\n",
      "Loss at step 95900 : 3.0924606323242188\n",
      "Loss at step 95950 : 1.7875399589538574\n",
      "Loss at step 96000 : 3.283198356628418\n",
      "Loss at step 96050 : 2.7478625774383545\n",
      "Loss at step 96100 : 3.6388649940490723\n",
      "Loss at step 96150 : 2.346914768218994\n",
      "Loss at step 96200 : 3.5164194107055664\n",
      "Loss at step 96250 : 2.2566990852355957\n",
      "Loss at step 96300 : 3.4268908500671387\n",
      "Loss at step 96350 : 3.2933261394500732\n",
      "Loss at step 96400 : 2.5967845916748047\n",
      "Loss at step 96450 : 2.4652857780456543\n",
      "Loss at step 96500 : 2.4612579345703125\n",
      "Loss at step 96550 : 2.508746385574341\n",
      "Loss at step 96600 : 3.0367043018341064\n",
      "Loss at step 96650 : 2.7071619033813477\n",
      "Loss at step 96700 : 2.5108675956726074\n",
      "Loss at step 96750 : 4.3590006828308105\n",
      "Loss at step 96800 : 3.6715259552001953\n",
      "Loss at step 96850 : 3.3790841102600098\n",
      "Loss at step 96900 : 2.346994400024414\n",
      "Loss at step 96950 : 2.458346366882324\n",
      "Loss at step 97000 : 3.412623405456543\n",
      "Loss at step 97050 : 2.2689502239227295\n",
      "Loss at step 97100 : 1.8455519676208496\n",
      "Loss at step 97150 : 3.8766613006591797\n",
      "Loss at step 97200 : 2.4432497024536133\n",
      "Loss at step 97250 : 2.214813232421875\n",
      "Loss at step 97300 : 4.226862907409668\n",
      "Loss at step 97350 : 2.8203587532043457\n",
      "Loss at step 97400 : 3.4206645488739014\n",
      "Loss at step 97450 : 3.089427947998047\n",
      "Loss at step 97500 : 2.702362537384033\n",
      "Loss at step 97550 : 1.8634028434753418\n",
      "Loss at step 97600 : 3.5310535430908203\n",
      "Loss at step 97650 : 2.3505606651306152\n",
      "Loss at step 97700 : 5.25849723815918\n",
      "Loss at step 97750 : 3.2259716987609863\n",
      "Loss at step 97800 : 3.2623369693756104\n",
      "Loss at step 97850 : 2.3138532638549805\n",
      "Loss at step 97900 : 2.4800894260406494\n",
      "Loss at step 97950 : 2.898036003112793\n",
      "Loss at step 98000 : 3.649994373321533\n",
      "Loss at step 98050 : 2.442392587661743\n",
      "Loss at step 98100 : 2.53882098197937\n",
      "Loss at step 98150 : 3.3708088397979736\n",
      "Loss at step 98200 : 5.056059837341309\n",
      "Loss at step 98250 : 2.163998603820801\n",
      "Loss at step 98300 : 2.6337430477142334\n",
      "Loss at step 98350 : 3.7853918075561523\n",
      "Loss at step 98400 : 2.6038098335266113\n",
      "Loss at step 98450 : 3.080087661743164\n",
      "Loss at step 98500 : 2.0227153301239014\n",
      "Loss at step 98550 : 2.315288543701172\n",
      "Loss at step 98600 : 2.645112991333008\n",
      "Loss at step 98650 : 4.054275035858154\n",
      "Loss at step 98700 : 4.271628379821777\n",
      "Loss at step 98750 : 3.3024532794952393\n",
      "Loss at step 98800 : 3.018444538116455\n",
      "Loss at step 98850 : 1.162399411201477\n",
      "Loss at step 98900 : 3.2895774841308594\n",
      "Loss at step 98950 : 3.1206302642822266\n",
      "Loss at step 99000 : 1.4833428859710693\n",
      "Loss at step 99050 : 2.066301107406616\n",
      "Loss at step 99100 : 3.3578431606292725\n",
      "Loss at step 99150 : 2.853020668029785\n",
      "Loss at step 99200 : 2.475243091583252\n",
      "Loss at step 99250 : 1.958878517150879\n",
      "Loss at step 99300 : 4.409427165985107\n",
      "Loss at step 99350 : 2.151554822921753\n",
      "Loss at step 99400 : 3.1223535537719727\n",
      "Loss at step 99450 : 3.8171753883361816\n",
      "Loss at step 99500 : 2.9187347888946533\n",
      "Loss at step 99550 : 4.02480411529541\n",
      "Loss at step 99600 : 4.475470542907715\n",
      "Loss at step 99650 : 4.187341690063477\n",
      "Loss at step 99700 : 1.3910928964614868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 99750 : 1.6886131763458252\n",
      "Loss at step 99800 : 2.5632266998291016\n",
      "Loss at step 99850 : 2.465719699859619\n",
      "Loss at step 99900 : 1.6104168891906738\n",
      "Loss at step 99950 : 4.316152572631836\n",
      "Loss at step 100000 : 3.9104461669921875\n",
      "Nearest to tuna: mushroom, spring, block, crab, cucumber,\n",
      "Nearest to rice: cucumber, finger, bit, spicy, salmon,\n",
      "Nearest to sushi: fish, towel, roe, zip, mat,\n",
      "Nearest to roll: flesh, piece, sesame, daikon, strip,\n",
      "Nearest to sashimi: coriander, leaf, piece, strip, sesame,\n",
      "Nearest to steak: grill, grate, pepper, marinade, onion,\n",
      "Nearest to grill: grate, worcestershire, marinade, preheat, pepper,\n",
      "Nearest to sauce: finger, filet, soy, salmon, slice,\n",
      "Nearest to cream: liqueur, mascarpone, coffee, yolk, cocoa,\n",
      "Model saved in file: /notebooks/school/text_feature_extraction/models/doc2vec_recipes_checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "doc_embeddings = tf.Variable(tf.random_uniform([len(preprocessed_texts), doc_embedding_size], -1.0, 1.0))\n",
    "\n",
    "decoder_weights = tf.Variable(tf.truncated_normal([vocabulary_size, concatenated_size],\n",
    "                                               stddev=1.0 / np.sqrt(concatenated_size)))\n",
    "decoder_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[None, 2]) #1 for word index and 1 for doc index\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embed= tf.nn.embedding_lookup(embeddings, x_inputs[:, 0])\n",
    "    \n",
    "doc_indices = tf.slice(x_inputs, [0,1],[batch_size,1])\n",
    "doc_embed = tf.nn.embedding_lookup(doc_embeddings,doc_indices)\n",
    "final_embed = tf.concat([embed, tf.squeeze(doc_embed)],1)\n",
    "\n",
    "logits = tf.matmul(final_embed, tf.transpose(decoder_weights)) + decoder_biases\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_target))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=model_learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "#cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver({\"embeddings\": embeddings, \"doc_embeddings\": doc_embeddings})\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "print('Starting Training')\n",
    "\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = create_batch_data(text_data)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    #run the train step\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    #return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "        \n",
    "    #validation: print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    #save dictionary + embeddings\n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        #save vocabulary dictionary\n",
    "        with open(os.path.join(models_folder_name,'doc2vec_recipes_dict_words_integers.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        #save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),models_folder_name,'doc2vec_recipes_checkpoint.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
